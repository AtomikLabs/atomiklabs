[
    {
        "identifier": "oai:arXiv.org:0802.3300",
        "abstract_url": "http://arxiv.org/abs/0802.3300",
        "authors": [
            {
                "last_name": "La Mura",
                "first_name": "Pierfrancesco"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Motivated by several classic decision-theoretic paradoxes, and by analogies with the paradoxes which in physics motivated the development of quantum mechanics, we introduce a projective generalization of expected utility along the lines of the quantum-mechanical generalization of probability theory. The resulting decision theory accommodates the dominant paradoxes, while retaining significant simplicity and tractability. In particular, every finite game within this larger class of preferences still has an equilibrium. ",
        "title": "Projective Expected Utility",
        "date": "2008-02-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1301.3870",
        "abstract_url": "http://arxiv.org/abs/1301.3870",
        "authors": [
            {
                "last_name": "La Mura",
                "first_name": "Pierfrancesco"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We introduce Game networks (G nets), a novel representation for multi-agent decision problems. Compared to other game-theoretic representations, such as strategic or extensive forms, G nets are more structured and more compact; more fundamentally, G nets constitute a computationally advantageous framework for strategic inference, as both probability and utility independencies are captured in the structure of the network and can be exploited in order to simplify the inference process. An important aspect of multi-agent reasoning is the identification of some or all of the strategic equilibria in a game; we present original convergence methods for strategic equilibrium which can take advantage of strategic separabilities in the G net structure in order to simplify the computations. Specifically, we describe a method which identifies a unique equilibrium as a function of the game payoffs, and one which identifies all equilibria. ",
        "title": "Game Networks",
        "date": "2013-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1301.6714",
        "abstract_url": "http://arxiv.org/abs/1301.6714",
        "authors": [
            {
                "last_name": "La Mura",
                "first_name": "Pierfrancesco"
            },
            {
                "last_name": "Shoham",
                "first_name": "Yoav"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We introduce a new class of graphical representations, expected utility networks (EUNs), and discuss some of its properties and potential applications to artificial intelligence and economic theory. In EUNs not only probabilities, but also utilities enjoy a modular representation. EUNs are undirected graphs with two types of arc, representing probability and utility dependencies respectively. The representation of utilities is based on a novel notion of conditional utility independence, which we introduce and discuss in the context of other existing proposals. Just as probabilistic inference involves the computation of conditional probabilities, strategic inference involves the computation of conditional expected utilities for alternative plans of action. We define a new notion of conditional expected utility (EU) independence, and show that in EUNs node separation with respect to the probability and utility subgraphs implies conditional EU independence. ",
        "title": "Expected Utility Networks",
        "date": "2013-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1604.04792",
        "abstract_url": "http://arxiv.org/abs/1604.04792",
        "authors": [
            {
                "last_name": "Vidal",
                "first_name": "Juan Climent"
            },
            {
                "last_name": "Ll\u00f3pez",
                "first_name": "Enric Cosme"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  A theorem of Eilenberg establishes that there exists a bijection between the set of all varieties of regular languages and the set of all varieties of finite monoids. In this article after defining, for a fixed set of sorts $S$ and a fixed $S$-sorted signature $\\Sigma$, the concepts of formation of congruences with respect to $\\Sigma$ and of formation of $\\Sigma$-algebras, we prove that the algebraic lattices of all $\\Sigma$-congruence formations and of all $\\Sigma$-algebra formations are isomorphic, which is an Eilenberg's type theorem. Moreover, under a suitable condition on the free $\\Sigma$-algebras and after defining the concepts of formation of congruences of finite index with respect to $\\Sigma$, of formation of finite $\\Sigma$-algebras, and of formation of regular languages with respect to $\\Sigma$, we prove that the algebraic lattices of all $\\Sigma$-finite index congruence formations, of all $\\Sigma$-finite algebra formations, and of all $\\Sigma$-regular language formations are isomorphic, which is also an Eilenberg's type theorem. ",
        "title": "Eilenberg theorems for many-sorted formations",
        "date": "2016-04-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1804.09223",
        "abstract_url": "http://arxiv.org/abs/1804.09223",
        "authors": [
            {
                "last_name": "Gonz\u00e1lez-Coma",
                "first_name": "Jos\u00e9 P."
            },
            {
                "last_name": "Utschick",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Castedo",
                "first_name": "Luis"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work jointly addresses user scheduling and precoder/combiner design in the downlink of a wideband millimeter wave (mmWave) communications system. We consider Orthogonal Frequency Division Multiplexing (OFDM) modulation to overcome channel frequency selectivity and obtain a number of equivalent narrowband channels. Hence, the main challenge is that the analog preprocessing network is frequency flat and common to all the users at the transmitter side. Moreover, the effect of the signal bandwidth over the Uniform Linear Array (ULA) steering vectors has to be taken into account to design the hybrid precoders and combiners. The proposed algorithmic solution is based on Linear Successive Allocation (LISA), which greedily allocates streams to different users and computes the corresponding precoders and combiners. By taking into account the rank limitations imposed by the hardware at transmission and reception, the performance loss in terms of achievable sum rate for the hybrid approach is negligible. Numerical experiments show that the proposed method exhibits excellent performance with reasonable computational complexity. ",
        "title": "Hybrid LISA for Wideband Multiuser Millimeter Wave Communication Systems  under Beam Squint",
        "date": "2018-04-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1808.08217",
        "abstract_url": "http://arxiv.org/abs/1808.08217",
        "authors": [
            {
                "last_name": "Vidal",
                "first_name": "Juan Climent"
            },
            {
                "last_name": "Ll\u00f3pez",
                "first_name": "Enric Cosme"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  We generalize several recognizability theorems for free single-sorted algebras to the field of many-sorted algebras and provide, in a uniform way and without using neither regular tree grammars nor tree automata, purely algebraic proofs of them based on the concept of congruence. ",
        "title": "Congruence based proofs of the recognizability theorems for free  many-sorted algebras",
        "date": "2018-08-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1811.04035",
        "abstract_url": "http://arxiv.org/abs/1811.04035",
        "authors": [
            {
                "last_name": "Bhattacharjee",
                "first_name": "Kamalika"
            },
            {
                "last_name": "Maity",
                "first_name": "Krishnendu"
            },
            {
                "last_name": "Das",
                "first_name": "Sukanta"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "MS"
        ],
        "abstract": "  In today's world, several applications demand numbers which appear random but are generated by a background algorithm; that is, pseudo-random numbers. Since late $19^{th}$ century, researchers have been working on pseudo-random number generators (PRNGs). Several PRNGs continue to develop, each one demanding to be better than the previous ones. In this scenario, this paper targets to verify the claim of so-called good generators and rank the existing generators based on strong empirical tests in same platforms. To do this, the genre of PRNGs developed so far has been explored and classified into three groups -- linear congruential generator based, linear feedback shift register based and cellular automata based. From each group, well-known generators have been chosen for empirical testing. Two types of empirical testing has been done on each PRNG -- blind statistical tests with Diehard battery of tests, TestU01 library and NIST statistical test-suite and graphical tests (lattice test and space-time diagram test). Finally, the selected $29$ PRNGs are divided into $24$ groups and are ranked according to their overall performance in all empirical tests. ",
        "title": "A Search for Good Pseudo-random Number Generators : Survey and Empirical  Studies",
        "date": "2018-11-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2005.10310",
        "abstract_url": "http://arxiv.org/abs/2005.10310",
        "authors": [
            {
                "last_name": "Brink",
                "first_name": "Kevin M."
            },
            {
                "last_name": "Zhang",
                "first_name": "Jincheng"
            },
            {
                "last_name": "Willis",
                "first_name": "Andrew R."
            },
            {
                "last_name": "Sherrill",
                "first_name": "Ryan E."
            },
            {
                "last_name": "Godwin",
                "first_name": "Jamie L."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This article introduces an approach to facilitate cooperative exploration and mapping of large-scale, near-ground, underground, or indoor spaces via a novel integration framework for locally-dense agent map data. The effort targets limited Size, Weight, and Power (SWaP) agents with an emphasis on limiting required communications and redundant processing. The approach uses a unique organization of batch optimization engines to enable a highly efficient two-tier optimization structure. Tier I consist of agents that create and potentially share local maplets (local maps, limited in size) which are generated using Simultaneous Localization and Mapping (SLAM) map-building software and then marginalized to a more compact parameterization. Maplets are generated in an overlapping manner and used to estimate the transform and uncertainty between those overlapping maplets, providing accurate and compact odometry or delta-pose representation between maplet's local frames. The delta poses can be shared between agents, and in cases where maplets have salient features (for loop closures), the compact representation of the maplet can also be shared.   The second optimization tier consists of a global optimizer that seeks to optimize those maplet-to-maplet transformations, including any loop closures identified. This can provide an accurate global \"skeleton\"' of the traversed space without operating on the high-density point cloud. This compact version of the map data allows for scalable, cooperative exploration with limited communication requirements where most of the individual maplets, or low fidelity renderings, are only shared if desired. ",
        "title": "Maplets: An Efficient Approach for Cooperative SLAM Map Building Under  Communication and Computation Constraints",
        "date": "2020-05-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2005.10902",
        "abstract_url": "http://arxiv.org/abs/2005.10902",
        "authors": [
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            },
            {
                "last_name": "Bongartz",
                "first_name": "Dominik"
            },
            {
                "last_name": "Grothe",
                "first_name": "Daniel"
            },
            {
                "last_name": "Kerkenhoff",
                "first_name": "Tim"
            },
            {
                "last_name": "Lin",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Najman",
                "first_name": "Jaromil"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Gaussian processes~(Kriging) are interpolating data-driven models that are frequently applied in various disciplines. Often, Gaussian processes are trained on datasets and are subsequently embedded as surrogate models in optimization problems. These optimization problems are nonconvex and global optimization is desired. However, previous literature observed computational burdens limiting deterministic global optimization to Gaussian processes trained on few data points. We propose a reduced-space formulation for deterministic global optimization with trained Gaussian processes embedded. For optimization, the branch-and-bound solver branches only on the degrees of freedom and McCormick relaxations are propagated through explicit Gaussian process models. The approach also leads to significantly smaller and computationally cheaper subproblems for lower and upper bounding. To further accelerate convergence, we derive envelopes of common covariance functions for GPs and tight relaxations of acquisition functions used in Bayesian optimization including expected improvement, probability of improvement, and lower confidence bound. In total, we reduce computational time by orders of magnitude compared to state-of-the-art methods, thus overcoming previous computational burdens. We demonstrate the performance and scaling of the proposed method and apply it to Bayesian optimization with global optimization of the acquisition function and chance-constrained programming. The Gaussian process models, acquisition functions, and training scripts are available open-source within the \"MeLOn - Machine Learning Models for Optimization\" toolbox~(https://git.rwth-aachen.de/avt.svt/public/MeLOn). ",
        "title": "Global Optimization of Gaussian processes",
        "date": "2020-05-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2009.10568",
        "abstract_url": "http://arxiv.org/abs/2009.10568",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Ruizhe"
            },
            {
                "last_name": "Wang",
                "first_name": "Ping"
            },
            {
                "last_name": "Zheng",
                "first_name": "Mengce"
            },
            {
                "last_name": "Hu",
                "first_name": "Honggang"
            },
            {
                "last_name": "Yu",
                "first_name": "Nenghai"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Numerous previous works have studied deep learning algorithms applied in the context of side-channel attacks, which demonstrated the ability to perform successful key recoveries. These studies show that modern cryptographic devices are increasingly threatened by side-channel attacks with the help of deep learning. However, the existing countermeasures are designed to resist classical side-channel attacks, and cannot protect cryptographic devices from deep learning based side-channel attacks. Thus, there arises a strong need for countermeasures against deep learning based side-channel attacks. Although deep learning has the high potential in solving complex problems, it is vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrectly.   In this paper, we propose a kind of novel countermeasures based on adversarial attacks that is specifically designed against deep learning based side-channel attacks. We estimate several models commonly used in deep learning based side-channel attacks to evaluate the proposed countermeasures. It shows that our approach can effectively protect cryptographic devices from deep learning based side-channel attacks in practice. In addition, our experiments show that the new countermeasures can also resist classical side-channel attacks. ",
        "title": "Adversarial Attack Based Countermeasures against Deep Learning  Side-Channel Attacks",
        "date": "2020-09-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2101.10154",
        "abstract_url": "http://arxiv.org/abs/2101.10154",
        "authors": [
            {
                "last_name": "Hibat-Allah",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Inack",
                "first_name": "Estelle M."
            },
            {
                "last_name": "Wiersema",
                "first_name": "Roeland"
            },
            {
                "last_name": "Melko",
                "first_name": "Roger G."
            },
            {
                "last_name": "Carrasquilla",
                "first_name": "Juan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Many important challenges in science and technology can be cast as optimization problems. When viewed in a statistical physics framework, these can be tackled by simulated annealing, where a gradual cooling procedure helps search for groundstate solutions of a target Hamiltonian. While powerful, simulated annealing is known to have prohibitively slow sampling dynamics when the optimization landscape is rough or glassy. Here we show that by generalizing the target distribution with a parameterized model, an analogous annealing framework based on the variational principle can be used to search for groundstate solutions. Modern autoregressive models such as recurrent neural networks provide ideal parameterizations since they can be exactly sampled without slow dynamics even when the model encodes a rough landscape. We implement this procedure in the classical and quantum settings on several prototypical spin glass Hamiltonians, and find that it significantly outperforms traditional simulated annealing in the asymptotic limit, illustrating the potential power of this yet unexplored route to optimization. ",
        "title": "Variational Neural Annealing",
        "date": "2021-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2102.05500",
        "abstract_url": "http://arxiv.org/abs/2102.05500",
        "authors": [
            {
                "last_name": "Angermeir",
                "first_name": "Florian"
            },
            {
                "last_name": "Voggenreiter",
                "first_name": "Markus"
            },
            {
                "last_name": "Moy\u00f3n",
                "first_name": "Fabiola"
            },
            {
                "last_name": "Mendez",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Agile and DevOps are widely adopted by the industry. Hence, integrating security activities with industrial practices, such as continuous integration (CI) pipelines, is necessary to detect security flaws and adhere to regulators' demands early. In this paper, we analyze automated security activities in CI pipelines of enterprise-driven open source software (OSS). This shall allow us, in the long-run, to better understand the extent to which security activities are (or should be) part of automated pipelines. In particular, we mine publicly available OSS repositories and survey a sample of project maintainers to better understand the role that security activities and their related tools play in their CI pipelines. To increase transparency and allow other researchers to replicate our study (and to take different perspectives), we further disclose our research artefacts. Our results indicate that security activities in enterprise-driven OSS projects are scarce and protection coverage is rather low. Only 6.83% of the analyzed 8,243 projects apply security automation in their CI pipelines, even though maintainers consider security to be rather important. This alerts industry to keep the focus on vulnerabilities of 3rd Party software and it opens space for other improvements of practice which we outline in this manuscript. ",
        "title": "Enterprise-Driven Open Source Software: A Case Study on Security  Automation",
        "date": "2021-02-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2105.02813",
        "abstract_url": "http://arxiv.org/abs/2105.02813",
        "authors": [
            {
                "last_name": "De",
                "first_name": "Subhayan"
            },
            {
                "last_name": "Hai",
                "first_name": "Bhuiyan Shameem Mahmood Ebna"
            },
            {
                "last_name": "Doostan",
                "first_name": "Alireza"
            },
            {
                "last_name": "Bause",
                "first_name": "Markus"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Structural health monitoring (SHM) systems use the non-destructive testing principle for damage identification. As part of SHM, the propagation of ultrasonic guided waves (UGWs) is tracked and analyzed for the changes in the associated wave pattern. These changes help identify the location of a structural damage, if any. We advance existing research by accounting for uncertainty in the material and geometric properties of a structure. The physics model used in this study comprises of a monolithically coupled system of acoustic and elastic wave equations, known as the wave propagation in fluid-solid and their interface (WpFSI) problem. As the UGWs propagate in the solid, fluid, and their interface, the wave signal displacement measurements are contrasted against the benchmark pattern. For the numerical solution, we develop an efficient algorithm that successfully addresses the inherent complexity of solving the multiphysics problem under uncertainty. We present a procedure that uses Gaussian process regression and convolutional neural network for predicting the UGW propagation in a solid-fluid and their interface under uncertainty. First, a set of training images for different realizations of the uncertain parameters of the inclusion inside the structure is generated using a monolithically-coupled system of acoustic and elastic wave equations. Next, Gaussian processes trained with these images are used for predicting the propagated wave with convolutional neural networks for further enhancement to produce high-quality images of the wave patterns for new realizations of the uncertainty. The results indicate that the proposed approach provides an accurate prediction for the WpFSI problem in the presence of uncertainty. ",
        "title": "Prediction of Ultrasonic Guided Wave Propagation in Solid-fluid and  their Interface under Uncertainty using Machine Learning",
        "date": "2021-03-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2107.10888",
        "abstract_url": "http://arxiv.org/abs/2107.10888",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Guanrui"
            },
            {
                "last_name": "Tunchez",
                "first_name": "Alex"
            },
            {
                "last_name": "Loianno",
                "first_name": "Giuseppe"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we address the Perception--Constrained Model Predictive Control (PCMPC) and state estimation problems for quadrotors with cable suspended payloads using a single camera and Inertial Measurement Unit (IMU). We design a receding--horizon control strategy for cable suspended payloads directly formulated on the system manifold configuration space SE(3)xS^2. The approach considers the system dynamics, actuator limits and the camera's Field Of View (FOV) constraint to guarantee the payload's visibility during motion. The monocular camera, IMU, and vehicle's motor speeds are combined to provide estimation of the vehicle's states in 3D space, the payload's states, the cable's direction and velocity. The proposed control and state estimation solution runs in real-time at 500 Hz on a small quadrotor equipped with a limited computational unit. The approach is validated through experimental results considering a cable suspended payload trajectory tracking problem at different speeds. ",
        "title": "PCMPC: Perception-Constrained Model Predictive Control for Quadrotors  with Suspended Loads using a Single Camera and IMU",
        "date": "2021-07-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2109.00787",
        "abstract_url": "http://arxiv.org/abs/2109.00787",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Guanqiu"
            },
            {
                "last_name": "Hu",
                "first_name": "Guanghui"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper is concerned with the factorization method with a single far-field pattern to recover an arbitrary convex polygonal scatterer/source in linear elasticity. The approach also applies to the compressional (resp. shear) part of the far-field pattern excited by a single compressional (resp. shear) plane wave. The one-wave factorization is based on the scattering data for a priori given testing scatterers. It can be regarded as a domain-defined sampling method and does not require forward solvers. We derive the spectral system of the far-field operator for rigid disks and show that, using testing disks, the one-wave factorization method can be justified independently of the classical factorization method. ",
        "title": "Factorization method for inverse time-harmonic elastic scattering with a  single plane wave",
        "date": "2021-09-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2110.07542",
        "abstract_url": "http://arxiv.org/abs/2110.07542",
        "authors": [
            {
                "last_name": "Maioli",
                "first_name": "Andrea"
            },
            {
                "last_name": "Mottola",
                "first_name": "Luca"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  We present ALFRED: a virtual memory abstraction that resolves the dichotomy between volatile and non-volatile memory in intermittent computing. Mixed-volatile microcontrollers allow programmers to allocate part of the application state onto non-volatile main memory. Programmers are therefore to explore manually the trade-off between simpler management of persistent state against the energy overhead for non-volatile memory operations and intermittence anomalies due to re-execution of non-idempotent code. This approach is laborious and yields sub-optimal performance. We take a different stand with ALFRED: we provide programmers with a virtual memory abstraction detached from the specific volatile nature of memory and automatically determine an efficient mapping from virtual to volatile or non-volatile memory. Unlike existing works, ALFRED does not require programmers to learn a new programming model or language syntax, while the mapping is entirely resolved at compile-time, reducing the run-time energy overhead. We implement ALFRED through a series of program machine-level code transformations. Compared to existing systems, we demonstrate that ALFRED reduces energy consumption by up to two orders of magnitude given a fixed workload. This enables the workloads to finish sooner, as the use of available energy shifts from ensuring forward progress to useful application processing. ",
        "title": "ALFRED: Virtual Memory for Intermittent Computing",
        "date": "2021-10-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2110.14329",
        "abstract_url": "http://arxiv.org/abs/2110.14329",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Pengyi"
            },
            {
                "last_name": "Huang",
                "first_name": "Hao"
            },
            {
                "last_name": "Liu",
                "first_name": "Chunlei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Feature selection techniques are essential for high-dimensional data analysis. In the last two decades, their popularity has been fuelled by the increasing availability of high-throughput biomolecular data where high-dimensionality is a common data property. Recent advances in biotechnologies enable global profiling of various molecular and cellular features at single-cell resolution, resulting in large-scale datasets with increased complexity. These technological developments have led to a resurgence in feature selection research and application in the single-cell field. Here, we revisit feature selection techniques and summarise recent developments. We review their versatile application to a range of single-cell data types including those generated from traditional cytometry and imaging technologies and the latest array of single-cell omics technologies. We highlight some of the challenges and future directions on which feature selection could have a significant impact. Finally, we consider the scalability and make general recommendations on the utility of each type of feature selection method. We hope this review serves as a reference point to stimulate future research and application of feature selection in the single-cell era. ",
        "title": "Feature selection revisited in the single-cell era",
        "date": "2021-10-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2110.15676",
        "abstract_url": "http://arxiv.org/abs/2110.15676",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Abhinav"
            },
            {
                "last_name": "P\u00e1rtl",
                "first_name": "Ond\u0159ej"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Naveed"
            },
            {
                "last_name": "Kuzmin",
                "first_name": "Dmitri"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider flux-corrected finite element discretizations of 3D convection-dominated transport problems and assess the computational efficiency of algorithms based on such approximations. The methods under investigation include flux-corrected transport schemes and monolithic limiters. We discretize in space using a continuous Galerkin method and $\\mathbb{P}_1$ or $\\mathbb{Q}_1$ finite elements. Time integration is performed using the Crank-Nicolson method or an explicit strong stability preserving Runge-Kutta method. Nonlinear systems are solved using a fixed-point iteration method, which requires solution of large linear systems at each iteration or time step. The great variety of options in the choice of discretization methods and solver components calls for a dedicated comparative study of existing approaches. To perform such a study, we define new 3D test problems for time-dependent and stationary convection-diffusion-reaction equations. The results of our numerical experiments illustrate how the limiting technique, time discretization and solver impact on the overall performance. ",
        "title": "An Assessment of Solvers for Algebraically Stabilized Discretizations of  Convection-Diffusion-Reaction Equations",
        "date": "2021-10-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2111.00084",
        "abstract_url": "http://arxiv.org/abs/2111.00084",
        "authors": [
            {
                "last_name": "Habib",
                "first_name": "Hussam"
            },
            {
                "last_name": "Nithyanand",
                "first_name": "Rishab"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Most platforms, including Reddit, face a dilemma when applying interventions such as subreddit bans to toxic communities -- do they risk angering their user base by proactively enforcing stricter controls on discourse or do they defer interventions at the risk of eventually triggering negative media reactions which might impact their advertising revenue? In this paper, we analyze Reddit's previous administrative interventions to understand one aspect of this dilemma: the relationship between the media and administrative interventions. More specifically, we make two primary contributions. First, using a mediation analysis framework, we find evidence that Reddit's interventions for violating their content policy for toxic content occur because of media pressure. Second, using interrupted time series analysis, we show that media attention on communities with toxic content only increases the problematic behavior associated with that community (both within the community itself and across the platform). However, we find no significant difference in the impact of administrative interventions on subreddits with and without media pressure. Taken all together, this study provides evidence of a media-driven moderation strategy at Reddit and also suggests that such a strategy may not have a significantly different impact than a more proactive strategy. ",
        "title": "Reddit and the Fourth Estate: Exploring the magnitude and effects of  media influence on community level moderation on Reddit",
        "date": "2021-10-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2111.11019",
        "abstract_url": "http://arxiv.org/abs/2111.11019",
        "authors": [
            {
                "last_name": "Habib",
                "first_name": "Hussam"
            },
            {
                "last_name": "Musa",
                "first_name": "Maaz Bin"
            },
            {
                "last_name": "Zaffar",
                "first_name": "Fareed"
            },
            {
                "last_name": "Nithyanand",
                "first_name": "Rishab"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Reddit has found its communities playing a prominent role in originating and propagating problematic socio-political discourse. Reddit administrators have generally struggled to prevent or contain such discourse for several reasons including: (1) the inability for a handful of human administrators to track and react to millions of posts and comments per day and (2) fear of backlash as a consequence of administrative decisions to ban or quarantine hateful communities. Consequently, administrative actions (community bans and quarantines) are often taken only when problematic discourse within a community spills over into the real world with serious consequences. In this paper, we investigate the feasibility of deploying tools to proactively identify problematic communities on Reddit. Proactive identification strategies show promise for three reasons: (1) they have potential to reduce the manual efforts required to track communities for problematic content, (2) they give administrators a scientific rationale to back their decisions and interventions, and (3) they facilitate early and more nuanced interventions (than banning or quarantining) to mitigate problematic discourse. ",
        "title": "Are Proactive Interventions for Reddit Communities Feasible?",
        "date": "2021-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2111.11285",
        "abstract_url": "http://arxiv.org/abs/2111.11285",
        "authors": [
            {
                "last_name": "Craig",
                "first_name": "D. L."
            },
            {
                "last_name": "Moon",
                "first_name": "H."
            },
            {
                "last_name": "Fedele",
                "first_name": "F."
            },
            {
                "last_name": "Lennon",
                "first_name": "D. T."
            },
            {
                "last_name": "Van Straaten",
                "first_name": "B."
            },
            {
                "last_name": "Vigneau",
                "first_name": "F."
            },
            {
                "last_name": "Camenzind",
                "first_name": "L. C."
            },
            {
                "last_name": "Zumb\u00fchl",
                "first_name": "D. M."
            },
            {
                "last_name": "Briggs",
                "first_name": "G. A. D."
            },
            {
                "last_name": "Osborne",
                "first_name": "M. A."
            },
            {
                "last_name": "Sejdinovic",
                "first_name": "D."
            },
            {
                "last_name": "Ares",
                "first_name": "N."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The discrepancies between reality and simulation impede the optimisation and scalability of solid-state quantum devices. Disorder induced by the unpredictable distribution of material defects is one of the major contributions to the reality gap. We bridge this gap using physics-aware machine learning, in particular, using an approach combining a physical model, deep learning, Gaussian random field, and Bayesian inference. This approach has enabled us to infer the disorder potential of a nanoscale electronic device from electron transport data. This inference is validated by verifying the algorithm's predictions about the gate voltage values required for a laterally-defined quantum dot device in AlGaAs/GaAs to produce current features corresponding to a double quantum dot regime. ",
        "title": "Bridging the reality gap in quantum devices with physics-aware machine  learning",
        "date": "2021-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.10671",
        "abstract_url": "http://arxiv.org/abs/2201.10671",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Ruan",
                "first_name": "Changxiao"
            },
            {
                "last_name": "Hadiwijoyo",
                "first_name": "Jessica"
            },
            {
                "last_name": "Chen",
                "first_name": "Brenna"
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Mataric",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  The physical design of a robot suggests expectations of that robot's functionality for human users and collaborators. When those expectations align with the true capabilities of the robot, interaction with the robot is enhanced. However, misalignment of those expectations can result in an unsatisfying interaction. This paper uses Mechanical Turk to evaluate user expectation through the use of design metaphors as applied to a wide range of robot embodiments. The first study (N=382) associates crowd-sourced design metaphors to different robot embodiments. The second study (N=803) assesses initial social expectations of robot embodiments. The final study (N=805) addresses the degree of abstraction of the design metaphors and the functional expectations projected on robot embodiments. Together, these results can guide robot designers toward aligning user expectations with true robot capabilities, facilitating positive human-robot interaction. ",
        "title": "Using Design Metaphors to Understand User Expectations of Socially  Interactive Robot Embodiments",
        "date": "2022-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2202.03004",
        "abstract_url": "http://arxiv.org/abs/2202.03004",
        "authors": [
            {
                "last_name": "Geyer",
                "first_name": "Fabien"
            },
            {
                "last_name": "Scheffler",
                "first_name": "Alexander"
            },
            {
                "last_name": "Bondorf",
                "first_name": "Steffen"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG"
        ],
        "abstract": "  The derivation of upper bounds on data flows' worst-case traversal times is an important task in many application areas. For accurate bounds, model simplifications should be avoided even in large networks. Network Calculus (NC) provides a modeling framework and different analyses for delay bounding. We investigate the analysis of feedforward networks where all queues implement First-In First-Out (FIFO) service. Correctly considering the effect of data flows onto each other under FIFO is already a challenging task. Yet, the fastest available NC FIFO analysis suffers from limitations resulting in unnecessarily loose bounds. A feature called Flow Prolongation (FP) has been shown to improve delay bound accuracy significantly. Unfortunately, FP needs to be executed within the NC FIFO analysis very often and each time it creates an exponentially growing set of alternative networks with prolongations. FP therefore does not scale and has been out of reach for the exhaustive analysis of large networks. We introduce DeepFP, an approach to make FP scale by predicting prolongations using machine learning. In our evaluation, we show that DeepFP can improve results in FIFO networks considerably. Compared to the standard NC FIFO analysis, DeepFP reduces delay bounds by 12.1% on average at negligible additional computational cost. ",
        "title": "Network Calculus with Flow Prolongation -- A Feedforward FIFO Analysis  enabled by ML",
        "date": "2022-02-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2202.03189",
        "abstract_url": "http://arxiv.org/abs/2202.03189",
        "authors": [
            {
                "last_name": "Shimadera",
                "first_name": "Sho"
            },
            {
                "last_name": "Kitagawa",
                "first_name": "Kei"
            },
            {
                "last_name": "Sagehashi",
                "first_name": "Koyo"
            },
            {
                "last_name": "Niiyama",
                "first_name": "Tomoaki"
            },
            {
                "last_name": "Sunada",
                "first_name": "Satoshi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC",
            "LG"
        ],
        "abstract": "  The biological skin enables animals to sense various stimuli. Extensive efforts have been made recently to develop smart skin-like sensors to extend the capabilities of biological skins; however, simultaneous sensing of several types of stimuli in a large area remains challenging because this requires large-scale sensor integration with numerous wire connections. We propose a simple, highly sensitive, and multimodal sensing approach, which does not require integrating multiple sensors. The proposed approach is based on an optical interference technique, which can encode the information of various stimuli as a spatial pattern. In contrast to the existing approach, the proposed approach, combined with a deep neural network, enables us to freely select the sensing mode according to our purpose. As a key example, we demonstrate simultaneous sensing mode of three different physical quantities, contact force, contact location, and temperature, using a single soft material without requiring complex integration. Another unique property of the proposed approach is spatially continuous sensing with ultrahigh resolution of few tens of micrometers, which enables identifying the shape of the object in contact. Furthermore, we present a haptic soft device for a human-machine interface. The proposed approach encourages the development of high-performance optical skins. ",
        "title": "Optical skin: Sensor-integration-free multimodal flexible sensing",
        "date": "2022-02-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2202.08805",
        "abstract_url": "http://arxiv.org/abs/2202.08805",
        "authors": [
            {
                "last_name": "Habib",
                "first_name": "Hussam"
            },
            {
                "last_name": "Srinivasan",
                "first_name": "Padmini"
            },
            {
                "last_name": "Nithyanand",
                "first_name": "Rishab"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  The algorithms and the interactions facilitated by online platforms have been used by radical groups to recruit vulnerable individuals to their cause. This has resulted in the sharp growth of violent events and deteriorating online discourse. The Manosphere, a collection of radical anti-feminist communities, is one such group which has attracted attention due to their rapid growth and increasingly violent real world outbursts. In this paper, we examine the social engagements between Reddit users who have participated in feminist discourse and the Manosphere communities on Reddit to understand the process of development of traits associated with the adoption of extremist ideologies. By using existing research on the psychology of radicalization we track how specific types of social engagement with the Manosphere influence the development of traits associated with radicalization. Our findings show that: (1) participation, even by the simple act of joining the Manosphere, has a significant influence on the language and outlook traits of a user, (2) Manosphere elites are extremely effective propagators of radical traits and cause their increase even outside the Manosphere, and (3) community perception can heavily influence a user's behavior. Finally, we examine how our findings can help draft community and platform moderation policies to help mitigate the problem of online radicalization. ",
        "title": "Making a Radical Misogynist: How online social engagement with the  Manosphere influences traits of radicalization",
        "date": "2022-02-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2205.03262",
        "abstract_url": "http://arxiv.org/abs/2205.03262",
        "authors": [
            {
                "last_name": "Sarkar",
                "first_name": "Abhiroop"
            },
            {
                "last_name": "Svensson",
                "first_name": "Bo Joel"
            },
            {
                "last_name": "Sheeran",
                "first_name": "Mary"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  Programming embedded systems applications involve writing concurrent, event-driven and timing-aware programs. Traditionally, such programs are written in low-level machine-oriented programming languages like C or Assembly. We present an alternative by introducing Synchron, an API that offers high-level abstractions to the programmer while supporting the low-level infrastructure in an associated runtime system and one-time-effort drivers. Embedded systems applications exhibit the general characteristics of being (i) concurrent, (ii) I/O-bound and (iii) timing-aware. To address each of these concerns, the Synchron API consists of three components: (1) a Concurrent ML (CML) inspired message-passing concurrency model, (2) a message-passing--based I/O interface that translates between low-level interrupt based and memory-mapped peripherals, and (3) a timing operator, $syncT$, that marries CML's $sync$ operator with timing windows inspired from the TinyTimber kernel. We implement the Synchron API as the bytecode instructions of a virtual machine called SynchronVM. SynchronVM hosts a Caml-inspired functional language as its frontend language, and the backend of the VM supports the STM32F4 and NRF52 microcontrollers, with RAM in the order of hundreds of kilobytes. We illustrate the expressiveness of the Synchron API by showing examples of expressing state machines commonly found in embedded systems. The timing functionality is demonstrated through a music programming exercise. Finally, we provide benchmarks on the response time, jitter rates, memory, and power usage of the SynchronVM. ",
        "title": "Synchron -- An API and Runtime for Embedded Systems",
        "date": "2022-05-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.04891",
        "abstract_url": "http://arxiv.org/abs/2206.04891",
        "authors": [
            {
                "last_name": "Marton",
                "first_name": "Sascha"
            },
            {
                "last_name": "L\u00fcdtke",
                "first_name": "Stefan"
            },
            {
                "last_name": "Bartelt",
                "first_name": "Christian"
            },
            {
                "last_name": "Tschalzev",
                "first_name": "Andrej"
            },
            {
                "last_name": "Stuckenschmidt",
                "first_name": "Heiner"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We consider generating explanations for neural networks in cases where the network's training data is not accessible, for instance due to privacy or safety issues. Recently, $\\mathcal{I}$-Nets have been proposed as a sample-free approach to post-hoc, global model interpretability that does not require access to training data. They formulate interpretation as a machine learning task that maps network representations (parameters) to a representation of an interpretable function. In this paper, we extend the $\\mathcal{I}$-Net framework to the cases of standard and soft decision trees as surrogate models. We propose a suitable decision tree representation and design of the corresponding $\\mathcal{I}$-Net output layers. Furthermore, we make $\\mathcal{I}$-Nets applicable to real-world tasks by considering more realistic distributions when generating the $\\mathcal{I}$-Net's training data. We empirically evaluate our approach against traditional global, post-hoc interpretability approaches and show that it achieves superior results when the training data is not accessible. ",
        "title": "Explaining Neural Networks without Access to Training Data",
        "date": "2022-06-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.05601",
        "abstract_url": "http://arxiv.org/abs/2206.05601",
        "authors": [
            {
                "last_name": "Sintov",
                "first_name": "Avishai"
            },
            {
                "last_name": "Ben-David",
                "first_name": "Inbar"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Object recognition is an essential capability when performing various tasks. Humans naturally use either or both visual and tactile perception to extract object class and properties. Typical approaches for robots, however, require complex visual systems or multiple high-density tactile sensors which can be highly expensive. In addition, they usually require actual collection of a large dataset from real objects through direct interaction. In this paper, we propose a kinesthetic-based object recognition method that can be performed with any multi-fingered robotic hand in which the kinematics is known. The method does not require tactile sensors and is based on observing grasps of the objects. We utilize a unique and frame invariant parameterization of grasps to learn instances of object shapes. To train a classifier, training data is generated rapidly and solely in a computational process without interaction with real objects. We then propose and compare between two iterative algorithms that can integrate any trained classifier. The classifiers and algorithms are independent of any particular robot hand and, therefore, can be exerted on various ones. We show in experiments, that with few grasps, the algorithms acquire accurate classification. Furthermore, we show that the object recognition approach is scalable to objects of various sizes. Similarly, a global classifier is trained to identify general geometries (e.g., an ellipsoid or a box) rather than particular ones and demonstrated on a large set of objects. Full scale experiments and analysis are provided to show the performance of the method. ",
        "title": "Simple Kinesthetic Haptics for Object Recognition",
        "date": "2022-06-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.08087",
        "abstract_url": "http://arxiv.org/abs/2206.08087",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhonghao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Xu",
                "first_name": "Yixin"
            },
            {
                "last_name": "Yu",
                "first_name": "Tongguang"
            },
            {
                "last_name": "Ye",
                "first_name": "Enze"
            },
            {
                "last_name": "Zheng",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Huazhong"
            },
            {
                "last_name": "George",
                "first_name": "Sumitha"
            },
            {
                "last_name": "Liu",
                "first_name": "Yongpan"
            },
            {
                "last_name": "Narayanan",
                "first_name": "Vijaykrishnan"
            },
            {
                "last_name": "Li",
                "first_name": "Xueqing"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Intellectual property (IP) piracy has become a non-negligible problem as the integrated circuit (IC) production supply chain is becoming increasingly globalized and separated that enables attacks by potentially untrusted attackers. Logic locking is a widely adopted method to lock the circuit module with a key and prevent hackers from cracking it. The key is the critical aspect of logic locking, but the existing works have overlooked three possible challenges of the key: safety of key storage, easy key-attempt from interface and key-related overheads, bringing the further challenges of low error rate and small state space. In this work, the key is dynamically generated by utilizing the huge space of a CPU core, and the unlocking is performed implicitly through the interconnection inside the chip. A novel low-cost logic reconfigurable gate is together proposed with ferroelectric FET (FeFET) to mitigate the reverse engineering and removal attack. Compared to the common logic locking methods, our proposed approach is 19,945 times more time consuming to traverse all the possible combinations in only 9-bit-key condition. Furthermore, our technique let key length increases this complexity exponentially and ensure the logic obfuscation effect. ",
        "title": "ALL-MASK: A Reconfigurable Logic Locking Method for Multicore  Architecture with Sequential-Instruction-Oriented Key",
        "date": "2022-06-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.11776",
        "abstract_url": "http://arxiv.org/abs/2206.11776",
        "authors": [
            {
                "last_name": "Rittig",
                "first_name": "Jan G."
            },
            {
                "last_name": "Hicham",
                "first_name": "Karim Ben"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            },
            {
                "last_name": "Dahmen",
                "first_name": "Manuel"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Ionic liquids (ILs) are important solvents for sustainable processes and predicting activity coefficients (ACs) of solutes in ILs is needed. Recently, matrix completion methods (MCMs), transformers, and graph neural networks (GNNs) have shown high accuracy in predicting ACs of binary mixtures, superior to well-established models, e.g., COSMO-RS and UNIFAC. GNNs are particularly promising here as they learn a molecular graph-to-property relationship without pretraining, typically required for transformers, and are, unlike MCMs, applicable to molecules not included in training. For ILs, however, GNN applications are currently missing. Herein, we present a GNN to predict temperature-dependent infinite dilution ACs of solutes in ILs. We train the GNN on a database including more than 40,000 AC values and compare it to a state-of-the-art MCM. The GNN and MCM achieve similar high prediction performance, with the GNN additionally enabling high-quality predictions for ACs of solutions that contain ILs and solutes not considered during training. ",
        "title": "Graph Neural Networks for Temperature-Dependent Activity Coefficient  Prediction of Solutes in Ionic Liquids",
        "date": "2022-06-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.00063",
        "abstract_url": "http://arxiv.org/abs/2207.00063",
        "authors": [
            {
                "last_name": "Kincl",
                "first_name": "Ond\u0159ej"
            },
            {
                "last_name": "Peshkov",
                "first_name": "Ilya"
            },
            {
                "last_name": "Pavelka",
                "first_name": "Michal"
            },
            {
                "last_name": "Klika",
                "first_name": "V\u00e1clav"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Smoothed Particle Hydrodynamics (SPH) methods are advantageous in simulations of fluids in domains with free boundary. Special SPH methods have also been developed to simulate solids. However, there are situations where the matter behaves partly as a fluid and partly as a solid, for instance, the solidification front in 3D printing, or any system involving both fluid and solid phases. We develop an SPH-like method that is suitable for both fluids and solids at the same time. Instead of the typical discretization of hydrodynamics, we discretize the Symmetric Hyperbolic Thermodynamically Compatible equations (SHTC), which describe both fluids, elastic solids, and visco-elasto-plastic solids within a single framework. The resulting SHTC-SPH method is then tested on various benchmarks from the hydrodynamics and dynamics of solids and shows remarkable agreement with the data. ",
        "title": "Unified description of fluids and solids in Smoothed Particle  Hydrodynamics",
        "date": "2022-06-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.10303",
        "abstract_url": "http://arxiv.org/abs/2207.10303",
        "authors": [
            {
                "last_name": "Dayarathna",
                "first_name": "Shalanika"
            },
            {
                "last_name": "Senanayake",
                "first_name": "Rajitha"
            },
            {
                "last_name": "Smith",
                "first_name": "Peter"
            },
            {
                "last_name": "Evans",
                "first_name": "Jamie"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper focuses on waveform design for joint radar and communication systems and presents a new subset selection process to improve the communication error rate performance and global accuracy of radar sensing of the random stepped frequency permutation waveform. An optimal communication receiver based on integer programming is proposed to handle any subset of permutations followed by a more efficient sub-optimal receiver based on the Hungarian algorithm. Considering optimum maximum likelihood detection, the block error rate is analyzed under both additive white Gaussian noise and correlated Rician fading. We propose two methods to select a permutation subset with an improved block error rate and an efficient encoding scheme to map the information symbols to selected permutations under these subsets. From the radar perspective, the ambiguity function is analyzed with regards to the local and the global accuracy of target detection. Furthermore, a subset selection method to reduce the maximum sidelobe height is proposed by extending the properties of Costas arrays. Finally, the process of remapping the frequency tones to the symbol set used to generate permutations is introduced as a method to improve both the communication and radar performances of the selected permutation subset. ",
        "title": "Frequency Permutation Subsets for Joint Radar and Communication",
        "date": "2022-07-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.12051",
        "abstract_url": "http://arxiv.org/abs/2207.12051",
        "authors": [
            {
                "last_name": "Stops",
                "first_name": "Laura"
            },
            {
                "last_name": "Leenhouts",
                "first_name": "Roel"
            },
            {
                "last_name": "Gao",
                "first_name": "Qinghe"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Process synthesis experiences a disruptive transformation accelerated by digitization and artificial intelligence. We propose a reinforcement learning algorithm for chemical process design based on a state-of-the-art actor-critic logic. Our proposed algorithm represents chemical processes as graphs and uses graph convolutional neural networks to learn from process graphs. In particular, the graph neural networks are implemented within the agent architecture to process the states and make decisions. Moreover, we implement a hierarchical and hybrid decision-making process to generate flowsheets, where unit operations are placed iteratively as discrete decisions and corresponding design variables are selected as continuous decisions. We demonstrate the potential of our method to design economically viable flowsheets in an illustrative case study comprising equilibrium reactions, azeotropic separation, and recycles. The results show quick learning in discrete, continuous, and hybrid action spaces. Due to the flexible architecture of the proposed reinforcement learning agent, the method is predestined to include large action-state spaces and an interface to process simulators in future research. ",
        "title": "Flowsheet synthesis through hierarchical reinforcement learning and  graph neural networks",
        "date": "2022-07-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.13779",
        "abstract_url": "http://arxiv.org/abs/2207.13779",
        "authors": [
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            },
            {
                "last_name": "Rittig",
                "first_name": "Jan G."
            },
            {
                "last_name": "Weber",
                "first_name": "Jana M."
            },
            {
                "last_name": "Grohe",
                "first_name": "Martin"
            },
            {
                "last_name": "Dahmen",
                "first_name": "Manuel"
            },
            {
                "last_name": "Leonhard",
                "first_name": "Kai"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph neural networks (GNNs) are emerging in chemical engineering for the end-to-end learning of physicochemical properties based on molecular graphs. A key element of GNNs is the pooling function which combines atom feature vectors into molecular fingerprints. Most previous works use a standard pooling function to predict a variety of properties. However, unsuitable pooling functions can lead to unphysical GNNs that poorly generalize. We compare and select meaningful GNN pooling methods based on physical knowledge about the learned properties. The impact of physical pooling functions is demonstrated with molecular properties calculated from quantum mechanical computations. We also compare our results to the recent set2set pooling approach. We recommend using sum pooling for the prediction of properties that depend on molecular size and compare pooling functions for properties that are molecular size-independent. Overall, we show that the use of physical pooling functions significantly enhances generalization. ",
        "title": "Physical Pooling Functions in Graph Neural Networks for Molecular  Property Prediction",
        "date": "2022-07-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.00778",
        "abstract_url": "http://arxiv.org/abs/2208.00778",
        "authors": [
            {
                "last_name": "Vogel",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Hirtreiter",
                "first_name": "Edwin"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  SFILES is a text-based notation for chemical process flowsheets. It was originally proposed by d'Anterroches (2006) who was inspired by the text-based SMILES notation for molecules. The text-based format has several advantages compared to flowsheet images regarding the storage format, computational accessibility, and eventually for data analysis and processing. However, the original SFILES version cannot describe essential flowsheet configurations unambiguously, such as the distinction between top and bottom products. Neither is it capable of describing the control structure required for the safe and reliable operation of chemical processes. Also, there is no publicly available software for decoding or encoding chemical process topologies to SFILES. We propose the SFILES 2.0 with a complete description of the extended notation and naming conventions. Additionally, we provide open-source software for the automated conversion between flowsheet graphs and SFILES 2.0 strings. This way, we hope to encourage researchers and engineers to publish their flowsheet topologies as SFILES 2.0 strings. The ultimate goal is to set the standards for creating a FAIR database of chemical process flowsheets, which would be of great value for future data analysis and processing. ",
        "title": "SFILES 2.0: An extended text-based flowsheet representation",
        "date": "2022-07-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.00859",
        "abstract_url": "http://arxiv.org/abs/2208.00859",
        "authors": [
            {
                "last_name": "Vogel",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  We propose a novel method enabling autocompletion of chemical flowsheets. This idea is inspired by the autocompletion of text. We represent flowsheets as strings using the text-based SFILES 2.0 notation and learn the grammatical structure of the SFILES 2.0 language and common patterns in flowsheets using a transformer-based language model. We pre-train our model on synthetically generated flowsheets to learn the flowsheet language grammar. Then, we fine-tune our model in a transfer learning step on real flowsheet topologies. Finally, we use the trained model for causal language modeling to autocomplete flowsheets. Eventually, the proposed method can provide chemical engineers with recommendations during interactive flowsheet synthesis. The results demonstrate a high potential of this approach for future AI-assisted process synthesis. ",
        "title": "Learning from flowsheets: A generative transformer model for  autocompletion of flowsheets",
        "date": "2022-08-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.04852",
        "abstract_url": "http://arxiv.org/abs/2208.04852",
        "authors": [
            {
                "last_name": "Rittig",
                "first_name": "Jan G."
            },
            {
                "last_name": "Gao",
                "first_name": "Qinghe"
            },
            {
                "last_name": "Dahmen",
                "first_name": "Manuel"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Molecular property prediction is of crucial importance in many disciplines such as drug discovery, molecular biology, or material and process design. The frequently employed quantitative structure-property/activity relationships (QSPRs/QSARs) characterize molecules by descriptors which are then mapped to the properties of interest via a linear or nonlinear model. In contrast, graph neural networks, a novel machine learning method, directly work on the molecular graph, i.e., a graph representation where atoms correspond to nodes and bonds correspond to edges. GNNs allow to learn properties in an end-to-end fashion, thereby avoiding the need for informative descriptors as in QSPRs/QSARs. GNNs have been shown to achieve state-of-the-art prediction performance on various property predictions tasks and represent an active field of research. We describe the fundamentals of GNNs and demonstrate the application of GNNs via two examples for molecular property prediction. ",
        "title": "Graph neural networks for the prediction of molecular structure-property  relationships",
        "date": "2022-07-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.12816",
        "abstract_url": "http://arxiv.org/abs/2208.12816",
        "authors": [
            {
                "last_name": "Zawish",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Davy",
                "first_name": "Steven"
            },
            {
                "last_name": "Abraham",
                "first_name": "Lizy"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI"
        ],
        "abstract": "  Recent advances in Artificial Intelligence (AI) on the Internet of Things (IoT)-enabled network edge has realized edge intelligence in several applications such as smart agriculture, smart hospitals, and smart factories by enabling low-latency and computational efficiency. However, deploying state-of-the-art Convolutional Neural Networks (CNNs) such as VGG-16 and ResNets on resource-constrained edge devices is practically infeasible due to their large number of parameters and floating-point operations (FLOPs). Thus, the concept of network pruning as a type of model compression is gaining attention for accelerating CNNs on low-power devices. State-of-the-art pruning approaches, either structured or unstructured do not consider the different underlying nature of complexities being exhibited by convolutional layers and follow a training-pruning-retraining pipeline, which results in additional computational overhead. In this work, we propose a novel and computationally efficient pruning pipeline by exploiting the inherent layer-level complexities of CNNs. Unlike typical methods, our proposed complexity-driven algorithm selects a particular layer for filter-pruning based on its contribution to overall network complexity. We follow a procedure that directly trains the pruned model and avoids the computationally complex ranking and fine-tuning steps. Moreover, we define three modes of pruning, namely parameter-aware (PA), FLOPs-aware (FA), and memory-aware (MA), to introduce versatile compression of CNNs. Our results show the competitive performance of our approach in terms of accuracy and acceleration. Lastly, we present a trade-off between different resources and accuracy which can be helpful for developers in making the right decisions in resource-constrained IoT environments. ",
        "title": "Complexity-Driven CNN Compression for Resource-constrained Edge AI",
        "date": "2022-08-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2209.09964",
        "abstract_url": "http://arxiv.org/abs/2209.09964",
        "authors": [
            {
                "last_name": "Habib",
                "first_name": "Hussam"
            },
            {
                "last_name": "Nithyanand",
                "first_name": "Rishab"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "SI"
        ],
        "abstract": "  Social media platforms have had considerable impact on the real world especially during the Covid-19 pandemic. Misinformation related to Covid-19 might have caused significant impact on the population specifically due to its association with dangerous beliefs such as anti-vaccination and Covid denial. In this work, we study a unique dataset of Facebook posts by users who shared and believed in Covid-19 misinformation before succumbing to Covid-19 often resulting in death. We aim to characterize the dominant themes and sources present in the victim's posts along with identifying the role of the platform in handling deadly narratives. Our analysis reveals the overwhelming politicization of Covid-19 through the prevalence of anti-government themes propagated by right-wing political and media ecosystem. Furthermore, we highlight the failures of Facebook's implementation and completeness of soft moderation actions intended to warn users of misinformation. Results from this study bring insights into the responsibility of political elites in shaping public discourse and the platform's role in dampening the reach of harmful misinformation. ",
        "title": "The Morbid Realities of Social Media: An Investigation into the  Misinformation Shared by the Deceased Victims of COVID-19",
        "date": "2022-09-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2210.02939",
        "abstract_url": "http://arxiv.org/abs/2210.02939",
        "authors": [
            {
                "last_name": "Belzig",
                "first_name": "Paula"
            },
            {
                "last_name": "Christandl",
                "first_name": "Matthias"
            },
            {
                "last_name": "M\u00fcller-Hermes",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Channel capacities quantify the optimal rates of sending information reliably over noisy channels. Usually, the study of capacities assumes that the circuits which sender and receiver use for encoding and decoding consist of perfectly noiseless gates. In the case of communication over quantum channels, however, this assumption is widely believed to be unrealistic, even in the long-term, due to the fragility of quantum information, which is affected by the process of decoherence. Christandl and M\\\"uller-Hermes have therefore initiated the study of fault-tolerant channel coding for quantum channels, i.e. coding schemes where encoder and decoder circuits are affected by noise, and have used techniques from fault-tolerant quantum computing to establish coding theorems for sending classical and quantum information in this scenario. Here, we extend these methods to the case of entanglement-assisted communication, in particular proving that the fault-tolerant capacity approaches the usual capacity when the gate error approaches zero. A main tool, which might be of independent interest, is the introduction of fault-tolerant entanglement distillation. We furthermore focus on the modularity of the techniques used, so that they can be easily adopted in other fault-tolerant communication scenarios. ",
        "title": "Fault-tolerant Coding for Entanglement-Assisted Communication",
        "date": "2022-10-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2211.05583",
        "abstract_url": "http://arxiv.org/abs/2211.05583",
        "authors": [
            {
                "last_name": "Hirtreiter",
                "first_name": "Edwin"
            },
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step during the development of chemical processes. Currently, this is a tedious, manual, and time-consuming task. We propose a novel, completely data-driven method for the prediction of control structures. Our methodology is inspired by end-to-end transformer-based human language translation models. We cast the control structure prediction as a translation task where Process Flow Diagrams (PFDs) are translated to P&IDs. To use established transformer-based language translation models, we represent the P&IDs and PFDs as strings using our recently proposed SFILES 2.0 notation. Model training is performed in a transfer learning approach. Firstly, we pre-train our model using generated P&IDs to learn the grammatical structure of the process diagrams. Thereafter, the model is fine-tuned leveraging transfer learning on real P&IDs. The model achieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on 100,000 generated P&IDs. These promising results show great potential for AI-assisted process engineering. The tests on a dataset of 312 real P&IDs indicate the need of a larger P&IDs dataset for industry applications. ",
        "title": "Towards automatic generation of Piping and Instrumentation Diagrams  (P&IDs) with Artificial Intelligence",
        "date": "2022-10-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.00756",
        "abstract_url": "http://arxiv.org/abs/2212.00756",
        "authors": [
            {
                "last_name": "Chremos",
                "first_name": "Ioannis Vasileios"
            },
            {
                "last_name": "Malikopoulos",
                "first_name": "Andreas A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This article provides an introduction to the theory of mechanism design and its application to engineering problems. Our aim is to provide the fundamental principles of the theory of mechanism design for control engineers and theorists along with the state-of-the-art methods in engineering applications. We start our exposition with a brief overview of game theory highlighting the key notions that are necessary to introduce mechanism design, and then we offer a comprehensive discussion of the principles in mechanism design. Finally, we explore four key applications of mechanism design in engineering, i.e., communication networks, power grids, transportation, and security systems. ",
        "title": "Mechanism Design Theory in Control Engineering: A Tutorial and Overview  of Applications in Communication, Power Grid, Transportation, and Security  Systems",
        "date": "2022-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.00851",
        "abstract_url": "http://arxiv.org/abs/2212.00851",
        "authors": [
            {
                "last_name": "Ranasinghe",
                "first_name": "Tharindu"
            },
            {
                "last_name": "Anuradha",
                "first_name": "Isuri"
            },
            {
                "last_name": "Premasiri",
                "first_name": "Damith"
            },
            {
                "last_name": "Silva",
                "first_name": "Kanishka"
            },
            {
                "last_name": "Hettiarachchi",
                "first_name": "Hansi"
            },
            {
                "last_name": "Uyangodage",
                "first_name": "Lasitha"
            },
            {
                "last_name": "Zampieri",
                "first_name": "Marcos"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  The widespread of offensive content online, such as hate speech and cyber-bullying, is a global phenomenon. This has sparked interest in the artificial intelligence (AI) and natural language processing (NLP) communities, motivating the development of various systems trained to detect potentially harmful content automatically. These systems require annotated datasets to train the machine learning (ML) models. However, with a few notable exceptions, most datasets on this topic have dealt with English and a few other high-resource languages. As a result, the research in offensive language identification has been limited to these languages. This paper addresses this gap by tackling offensive language identification in Sinhala, a low-resource Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments on this dataset. SOLD is a manually annotated dataset containing 10,000 posts from Twitter annotated as offensive and not offensive at both sentence-level and token-level, improving the explainability of the ML models. SOLD is the first large publicly available offensive language dataset compiled for Sinhala. We also introduce SemiSOLD, a larger dataset containing more than 145,000 Sinhala tweets, annotated following a semi-supervised approach. ",
        "title": "SOLD: Sinhala Offensive Language Dataset",
        "date": "2022-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.09879",
        "abstract_url": "http://arxiv.org/abs/2212.09879",
        "authors": [
            {
                "last_name": "Jungmannov\u00e1",
                "first_name": "Lenka"
            },
            {
                "last_name": "Plech\u00e1\u010d",
                "first_name": "Petr"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In addition to being a widely recognised novelist, Milan Kundera has also authored three pieces for theatre: The Owners of the Keys (Majitel\\'e kl\\'i\\v{c}\\r{u}, 1961), The Blunder (Pt\\'akovina, 1967), and Jacques and his Master (Jakub a jeho p\\'an, 1971). In recent years, however, the hypothesis has been raised that Kundera is the true author of a fourth play: Juro J\\'ano\\v{s}\\'ik, first performed in a 1974 production under the name of Karel Steigerwald, who was Kundera's student at the time. In this study, we make use of supervised machine learning to settle the question of authorship attribution in the case of Juro J\\'ano\\v{s}\\'ik, with results strongly supporting the hypothesis of Kundera's authorship. ",
        "title": "Unsigned Play by Milan Kundera? An Authorship Attribution Study",
        "date": "2022-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.14236",
        "abstract_url": "http://arxiv.org/abs/2212.14236",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Hu",
                "first_name": "Guanghui"
            },
            {
                "last_name": "Ma",
                "first_name": "Guanqiu"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a multi-frequency algorithm for imaging the trajectory of a moving point source from one and sparse far-field observation directions in the frequency domain. The starting and terminal time points of the moving source are both supposed to be known. We introduce the concept of observable directions (angles) in the far-field region and derive all observable directions (angles) for straight and circular motions. At an observable direction, it is verified that the smallest trip containing the trajectory and perpendicular to the direction can be imaged, provided the orbit function possesses a certain monotonical property. Without the monotonicity one can only expect to recover a thinner strip. The far-field data measured at sparse observable directions can be used to recover the $\\Theta$-convex domain of the trajectory. Both two- and three-dimensional numerical examples are implemented to show effectiveness and feasibility of the approach. ",
        "title": "Imaging a moving point source from multi-frequency data measured at one  and sparse observation directions (part I): far-field case",
        "date": "2022-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2301.10931",
        "abstract_url": "http://arxiv.org/abs/2301.10931",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Linfeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingbo"
            },
            {
                "last_name": "Pan",
                "first_name": "Lili"
            },
            {
                "last_name": "Meng",
                "first_name": "Fanman"
            },
            {
                "last_name": "Li",
                "first_name": "Hongliang"
            },
            {
                "last_name": "He",
                "first_name": "Chiyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Hanxin"
            },
            {
                "last_name": "Cheng",
                "first_name": "Shaoxu"
            },
            {
                "last_name": "Dai",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications. ",
        "title": "Towards Continual Egocentric Activity Recognition: A Multi-modal  Egocentric Activity Dataset for Continual Learning",
        "date": "2023-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.01364",
        "abstract_url": "http://arxiv.org/abs/2302.01364",
        "authors": [
            {
                "last_name": "Proskurnikov",
                "first_name": "Anton V."
            },
            {
                "last_name": "Runvik",
                "first_name": "H\u00e5kan"
            },
            {
                "last_name": "Medvedev",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Existence of periodical solutions, i.e. cycles, in the Impulsive Goodwin's Oscillator (IGO) with the continuous part of an arbitrary order m is considered. The original IGO with a third-order continuous part is a hybrid model that portrays a chemical or biochemical system composed of three substances represented by their concentrations and arranged in a cascade. The first substance in the chain is introduced via an impulsive feedback where both the impulse frequency and weights are modulated by the measured output of the continuous part. It is shown that, under the standard assumptions on the IGO, a positive periodic solution with one firing of the pulse-modulated feedback in the least period also exists in models with any m >= 1. Furthermore, the uniqueness of this 1-cycle is proved for the IGO with m <= 10 whereas, for m > 10, the uniqueness can still be guaranteed under mild assumptions on the frequency modulation function. ",
        "title": "Cycles in Impulsive Goodwin's Oscillators of Arbitrary Order",
        "date": "2023-02-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.01500",
        "abstract_url": "http://arxiv.org/abs/2302.01500",
        "authors": [
            {
                "last_name": "Suetake",
                "first_name": "Kazuma"
            },
            {
                "last_name": "Ushimaru",
                "first_name": "Takuya"
            },
            {
                "last_name": "Saiin",
                "first_name": "Ryuji"
            },
            {
                "last_name": "Sawada",
                "first_name": "Yoshihide"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Spiking neural networks (SNNs) are energy-efficient neural networks because of their spiking nature. However, as the spike firing rate of SNNs increases, the energy consumption does as well, and thus, the advantage of SNNs diminishes. Here, we tackle this problem by introducing a novel penalty term for the spiking activity into the objective function in the training phase. Our method is designed so as to optimize the energy consumption metric directly without modifying the network architecture. Therefore, the proposed method can reduce the energy consumption more than other methods while maintaining the accuracy. We conducted experiments for image classification tasks, and the results indicate the effectiveness of the proposed method, which mitigates the dilemma of the energy--accuracy trade-off. ",
        "title": "Spiking Synaptic Penalty: Appropriate Penalty Term for Energy-Efficient  Spiking Neural Networks",
        "date": "2023-02-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.01578",
        "abstract_url": "http://arxiv.org/abs/2302.01578",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Taoan"
            },
            {
                "last_name": "Ferber",
                "first_name": "Aaron"
            },
            {
                "last_name": "Tian",
                "first_name": "Yuandong"
            },
            {
                "last_name": "Dilkina",
                "first_name": "Bistra"
            },
            {
                "last_name": "Steiner",
                "first_name": "Benoit"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Integer Linear Programs (ILPs) are powerful tools for modeling and solving a large number of combinatorial optimization problems. Recently, it has been shown that Large Neighborhood Search (LNS), as a heuristic algorithm, can find high quality solutions to ILPs faster than Branch and Bound. However, how to find the right heuristics to maximize the performance of LNS remains an open problem. In this paper, we propose a novel approach, CL-LNS, that delivers state-of-the-art anytime performance on several ILP benchmarks measured by metrics including the primal gap, the primal integral, survival rates and the best performing rate. Specifically, CL-LNS collects positive and negative solution samples from an expert heuristic that is slow to compute and learns a new one with a contrastive loss. We use graph attention networks and a richer set of features to further improve its performance. ",
        "title": "Searching Large Neighborhoods for Integer Linear Programs with  Contrastive Learning",
        "date": "2023-02-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.03379",
        "abstract_url": "http://arxiv.org/abs/2302.03379",
        "authors": [
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Hirtreiter",
                "first_name": "Edwin"
            },
            {
                "last_name": "Luderer",
                "first_name": "Lynn"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Artificial intelligence has great potential for accelerating the design and engineering of chemical processes. Recently, we have shown that transformer-based language models can learn to auto-complete chemical process flowsheets using the SFILES 2.0 string notation. Also, we showed that language translation models can be used to translate Process Flow Diagrams (PFDs) into Process and Instrumentation Diagrams (P&IDs). However, artificial intelligence methods require big data and flowsheet data is currently limited. To mitigate this challenge of limited data, we propose a new data augmentation methodology for flowsheet data that is represented in the SFILES 2.0 notation. We show that the proposed data augmentation improves the performance of artificial intelligence-based process design models. In our case study flowsheet data augmentation improved the prediction uncertainty of the flowsheet autocompletion model by 14.7%. In the future, our flowsheet data augmentation can be used for other machine learning algorithms on chemical process flowsheets that are based on SFILES notation. ",
        "title": "Data augmentation for machine learning of chemical process flowsheets",
        "date": "2023-02-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.07875",
        "abstract_url": "http://arxiv.org/abs/2302.07875",
        "authors": [
            {
                "last_name": "Yamaguchi",
                "first_name": "Tomoya"
            },
            {
                "last_name": "Arai",
                "first_name": "Kohei"
            },
            {
                "last_name": "Niiyama",
                "first_name": "Tomoaki"
            },
            {
                "last_name": "Uchida",
                "first_name": "Atsushi"
            },
            {
                "last_name": "Sunada",
                "first_name": "Satoshi"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "NE"
        ],
        "abstract": "  High-speed machine vision is increasing its importance in both scientific and technological applications. Neuro-inspired photonic computing is a promising approach to speed-up machine vision processing with ultralow latency. However, the processing rate is fundamentally limited by the low frame rate of image sensors, typically operating at tens of hertz. Here, we propose an image-sensor-free machine vision framework, which optically processes real-world visual information with only a single input channel, based on a random temporal encoding technique. This approach allows for compressive acquisitions of visual information with a single channel at gigahertz rates, outperforming conventional approaches, and enables its direct photonic processing using a photonic reservoir computer in a time domain. We experimentally demonstrate that the proposed approach is capable of high-speed image recognition and anomaly detection, and furthermore, it can be used for high-speed imaging. The proposed approach is multipurpose and can be extended for a wide range of applications, including tracking, controlling, and capturing sub-nanosecond phenomena. ",
        "title": "Ultrafast single-channel machine vision based on neuro-inspired photonic  computing",
        "date": "2023-02-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.13397",
        "abstract_url": "http://arxiv.org/abs/2302.13397",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Xinquan"
            },
            {
                "last_name": "Alkhalifah",
                "first_name": "Tariq"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Physics-informed neural networks (PINNs) have attracted a lot of attention in scientific computing as their functional representation of partial differential equation (PDE) solutions offers flexibility and accuracy features. However, their training cost has limited their practical use as a real alternative to classic numerical methods. Thus, we propose to incorporate multi-resolution hash encoding into PINNs to improve the training efficiency, as such encoding offers a locally-aware (at multi resolution) coordinate inputs to the neural network. Borrowed from the neural representation field community (NeRF), we investigate the robustness of calculating the derivatives of such hash encoded neural networks with respect to the input coordinates, which is often needed by the PINN loss terms. We propose to replace the automatic differentiation with finite-difference calculations of the derivatives to address the discontinuous nature of such derivatives. We also share the appropriate ranges for the hash encoding hyperparameters to obtain robust derivatives. We test the proposed method on three problems, including Burgers equation, Helmholtz equation, and Navier-Stokes equation. The proposed method admits about a 10-fold improvement in efficiency over the vanilla PINN implementation. ",
        "title": "Efficient physics-informed neural networks using hash encoding",
        "date": "2023-02-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.02669",
        "abstract_url": "http://arxiv.org/abs/2303.02669",
        "authors": [
            {
                "last_name": "Ali",
                "first_name": "Hassan"
            },
            {
                "last_name": "Butt",
                "first_name": "Muhammad Atif"
            },
            {
                "last_name": "Filali",
                "first_name": "Fethi"
            },
            {
                "last_name": "Al-Fuqaha",
                "first_name": "Ala"
            },
            {
                "last_name": "Qadir",
                "first_name": "Junaid"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Recent works have shown that deep learning (DL) models can effectively learn city-wide crowd-flow patterns, which can be used for more effective urban planning and smart city management. However, DL models have been known to perform poorly on inconspicuous adversarial perturbations. Although many works have studied these adversarial perturbations in general, the adversarial vulnerabilities of deep crowd-flow prediction models in particular have remained largely unexplored. In this paper, we perform a rigorous analysis of the adversarial vulnerabilities of DL-based crowd-flow prediction models under multiple threat settings, making three-fold contributions. (1) We propose CaV-detect by formally identifying two novel properties - Consistency and Validity - of the crowd-flow prediction inputs that enable the detection of standard adversarial inputs with 0% false acceptance rate (FAR). (2) We leverage universal adversarial perturbations and an adaptive adversarial loss to present adaptive adversarial attacks to evade CaV-detect defense. (3) We propose CVPR, a Consistent, Valid and Physically-Realizable adversarial attack, that explicitly inducts the consistency and validity priors in the perturbation generation mechanism. We find out that although the crowd-flow models are vulnerable to adversarial perturbations, it is extremely challenging to simulate these perturbations in physical settings, notably when CaV-detect is in place. We also show that CVPR attack considerably outperforms the adaptively modified standard attacks in FAR and adversarial loss metrics. We conclude with useful insights emerging from our work and highlight promising future research directions. ",
        "title": "Consistent Valid Physically-Realizable Adversarial Attack against  Crowd-flow Prediction Models",
        "date": "2023-03-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.03825",
        "abstract_url": "http://arxiv.org/abs/2303.03825",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Kanghyun"
            },
            {
                "last_name": "Park",
                "first_name": "Daehyung"
            },
            {
                "last_name": "Kim",
                "first_name": "Min Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper presents a novel algorithm for robot task and motion planning (TAMP) problems by utilizing a reachability tree. While tree-based algorithms are known for their speed and simplicity in motion planning (MP), they are not well-suited for TAMP problems that involve both abstracted and geometrical state variables. To address this challenge, we propose a hierarchical sampling strategy, which first generates an abstracted task plan using Monte Carlo tree search (MCTS) and then fills in the details with a geometrically feasible motion trajectory. Moreover, we show that the performance of the proposed method can be significantly enhanced by selecting an appropriate reward for MCTS and by using a pre-generated goal state that is guaranteed to be geometrically feasible. A comparative study using TAMP benchmark problems demonstrates the effectiveness of the proposed approach. ",
        "title": "A Reachability Tree-Based Algorithm for Robot Task and Motion Planning",
        "date": "2023-03-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.06386",
        "abstract_url": "http://arxiv.org/abs/2303.06386",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Yixuan"
            },
            {
                "last_name": "Canham",
                "first_name": "Luke J. W."
            },
            {
                "last_name": "Western",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A key task in clinical EEG interpretation is to classify a recording or session as normal or abnormal. In machine learning approaches to this task, recordings are typically divided into shorter windows for practical reasons, and these windows inherit the label of their parent recording. We hypothesised that window labels derived in this manner can be misleading for example, windows without evident abnormalities can be labelled `abnormal' disrupting the learning process and degrading performance. We explored two separable approaches to mitigate this problem: increasing the window length and introducing a second-stage model to arbitrate between the window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, we significantly improved state-of-the-art average accuracy from 89.8 percent to 93.3 percent. This result defies previous estimates of the upper limit for performance on this dataset and represents a major step towards clinical translation of machine learning approaches to this problem. ",
        "title": "Scope and Arbitration in Machine Learning Clinical EEG Classification",
        "date": "2023-03-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10191",
        "abstract_url": "http://arxiv.org/abs/2303.10191",
        "authors": [
            {
                "last_name": "Dreher",
                "first_name": "Kris K."
            },
            {
                "last_name": "Ayala",
                "first_name": "Leonardo"
            },
            {
                "last_name": "Schellenberg",
                "first_name": "Melanie"
            },
            {
                "last_name": "H\u00fcbner",
                "first_name": "Marco"
            },
            {
                "last_name": "N\u00f6lke",
                "first_name": "Jan-Hinrich"
            },
            {
                "last_name": "Adler",
                "first_name": "Tim J."
            },
            {
                "last_name": "Seidlitz",
                "first_name": "Silvia"
            },
            {
                "last_name": "Sellner",
                "first_name": "Jan"
            },
            {
                "last_name": "Studier-Fischer",
                "first_name": "Alexander"
            },
            {
                "last_name": "Gr\u00f6hl",
                "first_name": "Janek"
            },
            {
                "last_name": "Nickel",
                "first_name": "Felix"
            },
            {
                "last_name": "K\u00f6the",
                "first_name": "Ullrich"
            },
            {
                "last_name": "Seitel",
                "first_name": "Alexander"
            },
            {
                "last_name": "Maier-Hein",
                "first_name": "Lena"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Synthetic medical image generation has evolved as a key technique for neural network training and validation. A core challenge, however, remains in the domain gap between simulations and real data. While deep learning-based domain transfer using Cycle Generative Adversarial Networks and similar architectures has led to substantial progress in the field, there are use cases in which state-of-the-art approaches still fail to generate training images that produce convincing results on relevant downstream tasks. Here, we address this issue with a domain transfer approach based on conditional invertible neural networks (cINNs). As a particular advantage, our method inherently guarantees cycle consistency through its invertible architecture, and network training can efficiently be conducted with maximum likelihood training. To showcase our method's generic applicability, we apply it to two spectral imaging modalities at different scales, namely hyperspectral imaging (pixel-level) and photoacoustic tomography (image-level). According to comprehensive experiments, our method enables the generation of realistic spectral data and outperforms the state of the art on two downstream classification tasks (binary and multi-class). cINN-based domain transfer could thus evolve as an important method for realistic synthetic data generation in the field of spectral imaging and beyond. ",
        "title": "Unsupervised Domain Transfer with Conditional Invertible Neural Networks",
        "date": "2023-03-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10806",
        "abstract_url": "http://arxiv.org/abs/2303.10806",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xin-Yu"
            },
            {
                "last_name": "Hsieh",
                "first_name": "Chung-Han"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we extend the existing double linear policy by incorporating time-varying weights instead of constant weights and study a certain robustness property, called robust positive expectation (RPE), in a discrete-time setting. We prove that the RPE property holds by employing a novel elementary symmetric polynomials characterization approach and derive an explicit expression for both the expected cumulative gain-loss function and its variance. To validate our theory, we perform extensive Monte Carlo simulations using various weighting functions. Furthermore, we demonstrate how this policy can be effectively incorporated with standard technical analysis techniques, using the moving average as a trading signal. ",
        "title": "On Robustness of Double Linear Policy with Time-Varying Weights",
        "date": "2023-03-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.12806",
        "abstract_url": "http://arxiv.org/abs/2303.12806",
        "authors": [
            {
                "last_name": "Chanda",
                "first_name": "Tirtha"
            },
            {
                "last_name": "Hauser",
                "first_name": "Katja"
            },
            {
                "last_name": "Hobelsberger",
                "first_name": "Sarah"
            },
            {
                "last_name": "Bucher",
                "first_name": "Tabea-Clara"
            },
            {
                "last_name": "Garcia",
                "first_name": "Carina Nogueira"
            },
            {
                "last_name": "Wies",
                "first_name": "Christoph"
            },
            {
                "last_name": "Kittler",
                "first_name": "Harald"
            },
            {
                "last_name": "Tschandl",
                "first_name": "Philipp"
            },
            {
                "last_name": "Navarrete-Dechent",
                "first_name": "Cristian"
            },
            {
                "last_name": "Podlipnik",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Chousakos",
                "first_name": "Emmanouil"
            },
            {
                "last_name": "Crnaric",
                "first_name": "Iva"
            },
            {
                "last_name": "Majstorovic",
                "first_name": "Jovana"
            },
            {
                "last_name": "Alhajwan",
                "first_name": "Linda"
            },
            {
                "last_name": "Foreman",
                "first_name": "Tanya"
            },
            {
                "last_name": "Peternel",
                "first_name": "Sandra"
            },
            {
                "last_name": "Sarap",
                "first_name": "Sergei"
            },
            {
                "last_name": "\u00d6zdemir",
                "first_name": "\u0130rem"
            },
            {
                "last_name": "Barnhill",
                "first_name": "Raymond L."
            },
            {
                "last_name": "Velasco",
                "first_name": "Mar Llamas"
            },
            {
                "last_name": "Poch",
                "first_name": "Gabriela"
            },
            {
                "last_name": "Korsing",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Sondermann",
                "first_name": "Wiebke"
            },
            {
                "last_name": "Gellrich",
                "first_name": "Frank Friedrich"
            },
            {
                "last_name": "Heppt",
                "first_name": "Markus V."
            },
            {
                "last_name": "Erdmann",
                "first_name": "Michael"
            },
            {
                "last_name": "Haferkamp",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Drexler",
                "first_name": "Konstantin"
            },
            {
                "last_name": "Goebeler",
                "first_name": "Matthias"
            },
            {
                "last_name": "Schilling",
                "first_name": "Bastian"
            },
            {
                "last_name": "Utikal",
                "first_name": "Jochen S."
            },
            {
                "last_name": "Ghoreschi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Fr\u00f6hling",
                "first_name": "Stefan"
            },
            {
                "last_name": "Krieghoff-Henning",
                "first_name": "Eva"
            },
            {
                "last_name": "Brinker",
                "first_name": "Titus J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Although artificial intelligence (AI) systems have been shown to improve the accuracy of initial melanoma diagnosis, the lack of transparency in how these systems identify melanoma poses severe obstacles to user acceptance. Explainable artificial intelligence (XAI) methods can help to increase transparency, but most XAI methods are unable to produce precisely located domain-specific explanations, making the explanations difficult to interpret. Moreover, the impact of XAI methods on dermatologists has not yet been evaluated. Extending on two existing classifiers, we developed an XAI system that produces text and region based explanations that are easily interpretable by dermatologists alongside its differential diagnoses of melanomas and nevi. To evaluate this system, we conducted a three-part reader study to assess its impact on clinicians' diagnostic accuracy, confidence, and trust in the XAI-support. We showed that our XAI's explanations were highly aligned with clinicians' explanations and that both the clinicians' trust in the support system and their confidence in their diagnoses were significantly increased when using our XAI compared to using a conventional AI system. The clinicians' diagnostic accuracy was numerically, albeit not significantly, increased. This work demonstrates that clinicians are willing to adopt such an XAI system, motivating their future use in the clinic. ",
        "title": "Dermatologist-like explainable AI enhances trust and confidence in  diagnosing melanoma",
        "date": "2023-03-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.15228",
        "abstract_url": "http://arxiv.org/abs/2303.15228",
        "authors": [
            {
                "last_name": "Braghetto",
                "first_name": "Anna"
            },
            {
                "last_name": "Orlandini",
                "first_name": "Enzo"
            },
            {
                "last_name": "Baiesi",
                "first_name": "Marco"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Explainable and interpretable unsupervised machine learning helps understand the underlying structure of data. We introduce an ensemble analysis of machine learning models to consolidate their interpretation. Its application shows that restricted Boltzmann machines compress consistently into a few bits the information stored in a sequence of five amino acids at the start or end of $\\alpha$-helices or $\\beta$-sheets. The weights learned by the machines reveal unexpected properties of the amino acids and the secondary structure of proteins: (i) His and Thr have a negligible contribution to the amphiphilic pattern of $\\alpha$-helices; (ii) there is a class of $\\alpha$-helices particularly rich in Ala at their end; (iii) Pro occupies most often slots otherwise occupied by polar or charged amino acids, and its presence at the start of helices is relevant; (iv) Glu and especially Asp on one side, and Val, Leu, Iso, and Phe on the other, display the strongest tendency to mark amphiphilic patterns, i.e., extreme values of an \"effective hydrophobicity\", though they are not the most powerful (non) hydrophobic amino acids. ",
        "title": "Interpretable machine learning of amino acid patterns in proteins: a  statistical ensemble approach",
        "date": "2023-03-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.17551",
        "abstract_url": "http://arxiv.org/abs/2303.17551",
        "authors": [
            {
                "last_name": "Lechowicz",
                "first_name": "Adam"
            },
            {
                "last_name": "Christianson",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Zuo",
                "first_name": "Jinhang"
            },
            {
                "last_name": "Bashir",
                "first_name": "Noman"
            },
            {
                "last_name": "Hajiesmaili",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Wierman",
                "first_name": "Adam"
            },
            {
                "last_name": "Shenoy",
                "first_name": "Prashant"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DC"
        ],
        "abstract": "  We introduce and study the online pause and resume problem. In this problem, a player attempts to find the $k$ lowest (alternatively, highest) prices in a sequence of fixed length $T$, which is revealed sequentially. At each time step, the player is presented with a price and decides whether to accept or reject it. The player incurs a switching cost whenever their decision changes in consecutive time steps, i.e., whenever they pause or resume purchasing. This online problem is motivated by the goal of carbon-aware load shifting, where a workload may be paused during periods of high carbon intensity and resumed during periods of low carbon intensity and incurs a cost when saving or restoring its state. It has strong connections to existing problems studied in the literature on online optimization, though it introduces unique technical challenges that prevent the direct application of existing algorithms. Extending prior work on threshold-based algorithms, we introduce double-threshold algorithms for both the minimization and maximization variants of this problem. We further show that the competitive ratios achieved by these algorithms are the best achievable by any deterministic online algorithm. Finally, we empirically validate our proposed algorithm through case studies on the application of carbon-aware load shifting using real carbon trace data and existing baseline algorithms. ",
        "title": "The Online Pause and Resume Problem: Optimal Algorithms and An  Application to Carbon-Aware Load Shifting",
        "date": "2023-03-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.10182",
        "abstract_url": "http://arxiv.org/abs/2304.10182",
        "authors": [
            {
                "last_name": "Paananen",
                "first_name": "Ville"
            },
            {
                "last_name": "Oppenlaender",
                "first_name": "Jonas"
            },
            {
                "last_name": "Visuri",
                "first_name": "Aku"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV"
        ],
        "abstract": "  The recent progress of text-to-image generation has been recognized in architectural design. Our study is the first to investigate the potential of text-to-image generators in supporting creativity during the early stages of the architectural design process. We conducted a laboratory study with 17 architecture students, who developed a concept for a culture center using three popular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E. Through standardized questionnaires and group interviews, we found that image generation could be a meaningful part of the design process when design constraints are carefully considered. Generative tools support serendipitous discovery of ideas and an imaginative mindset, enriching the design process. We identified several challenges of image generators and provided considerations for software development and educators to support creativity and emphasize designers' imaginative mindset. By understanding the limitations and potential of text-to-image generators, architects and designers can leverage this technology in their design process and education, facilitating innovation and effective communication of concepts. ",
        "title": "Using Text-to-Image Generation for Architectural Design Ideation",
        "date": "2023-04-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.11214",
        "abstract_url": "http://arxiv.org/abs/2304.11214",
        "authors": [
            {
                "last_name": "Qureshi",
                "first_name": "Basit"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The application of Artificial intelligence for teaching and learning in the academic sphere is a trending subject of interest in the computing education. ChatGPT, as an AI-based tool, provides various advantages, such as heightened student involvement, cooperation, accessibility and availability. This paper addresses the prospects and obstacles associated with utilizing ChatGPT as a tool for learning and assessment in undergraduate Computer Science curriculum in particular to teaching and learning fundamental programming courses. Students having completed the course work for a Data Structures and Algorithms (a sophomore level course) participated in this study. Two groups of students were given programming challenges to solve within a short period of time. The control group (group A) had access to text books and notes of programming courses, however no Internet access was provided. Group B students were given access to ChatGPT and were encouraged to use it to help solve the programming challenges. The challenge was conducted in a computer lab environment using PC2 environment. Each team of students address the problem by writing executable code that satisfies certain number of test cases. Student teams were scored based on their performance in terms of number of successful passed testcases. Results show that students using ChatGPT had an advantage in terms of earned scores, however there were inconsistencies and inaccuracies in the submitted code consequently affecting the overall performance. After a thorough analysis, the paper's findings indicate that incorporating AI in higher education brings about various opportunities and challenges. ",
        "title": "Exploring the Use of ChatGPT as a Tool for Learning and Assessment in  Undergraduate Computer Science Curriculum: Opportunities and Challenges",
        "date": "2023-04-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.06624",
        "abstract_url": "http://arxiv.org/abs/2305.06624",
        "authors": [
            {
                "last_name": "Omanovi\u0107",
                "first_name": "Amra"
            },
            {
                "last_name": "Oblak",
                "first_name": "Polona"
            },
            {
                "last_name": "Curk",
                "first_name": "Toma\u017e"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Tropical semiring has proven successful in several research areas, including optimal control, bioinformatics, discrete event systems, or solving a decision problem. In previous studies, a matrix two-factorization algorithm based on the tropical semiring has been applied to investigate bipartite and tripartite networks. Tri-factorization algorithms based on standard linear algebra are used for solving tasks such as data fusion, co-clustering, matrix completion, community detection, and more. However, there is currently no tropical matrix tri-factorization approach, which would allow for the analysis of multipartite networks with a high number of parts. To address this, we propose the triFastSTMF algorithm, which performs tri-factorization over the tropical semiring. We apply it to analyze a four-partition network structure and recover the edge lengths of the network. We show that triFastSTMF performs similarly to Fast-NMTF in terms of approximation and prediction performance when fitted on the whole network. When trained on a specific subnetwork and used to predict the whole network, triFastSTMF outperforms Fast-NMTF by several orders of magnitude smaller error. The robustness of triFastSTMF is due to tropical operations, which are less prone to predict large values compared to standard operations. ",
        "title": "Matrix tri-factorization over the tropical semiring",
        "date": "2023-05-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.09082",
        "abstract_url": "http://arxiv.org/abs/2305.09082",
        "authors": [
            {
                "last_name": "Shin",
                "first_name": "Jiho"
            },
            {
                "last_name": "Wei",
                "first_name": "Moshi"
            },
            {
                "last_name": "Wang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Shi",
                "first_name": "Lin"
            },
            {
                "last_name": "Wang",
                "first_name": "Song"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Machine learning (ML) has been increasingly used in a variety of domains, while solving ML programming tasks poses unique challenges because of the fundamentally different nature and construction from general programming tasks, especially for developers who do not have ML backgrounds. Automatic code generation that produces a code snippet from a natural language description can be a promising technique to accelerate ML programming tasks. In recent years, although many deep learning-based neural code generation models have been proposed with high accuracy, the fact that most of them are mainly evaluated on general programming tasks calls into question their effectiveness and usefulness in ML programming tasks. In this paper, we set out to investigate the effectiveness of existing neural code generation models on ML programming tasks. For our analysis, we select six state-of-the-art neural code generation models, and evaluate their performance on four widely used ML libraries, with newly-created 83K pairs of natural-language described ML programming tasks. Our empirical study reveals some good, bad, and missing aspects of neural code generation models on ML tasks, with a few major ones listed below. (Good) Neural code generation models perform significantly better on ML tasks than on non-ML tasks. (Bad) Most of the generated code is semantically incorrect. (Bad) Code generation models cannot significantly improve developers' completion time. (Good) The generated code can help developers write more correct code by providing developers with clues for using correct APIs. (Missing) The observation from our user study reveals the missing aspects of code generation for ML tasks, e.g., decomposing code generation for divide-and-conquer into two tasks: API sequence identification and API usage generation. ",
        "title": "The Good, the Bad, and the Missing: Neural Code Generation for Machine  Learning Tasks",
        "date": "2023-05-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.19069",
        "abstract_url": "http://arxiv.org/abs/2305.19069",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yifu"
            },
            {
                "last_name": "Li",
                "first_name": "Hongru"
            },
            {
                "last_name": "Yang",
                "first_name": "Tao"
            },
            {
                "last_name": "Tao",
                "first_name": "Rui"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengyuan"
            },
            {
                "last_name": "Shi",
                "first_name": "Shimeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiansong"
            },
            {
                "last_name": "Ma",
                "first_name": "Ning"
            },
            {
                "last_name": "Feng",
                "first_name": "Wujin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhanhu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xinyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Lesion segmentation of ultrasound medical images based on deep learning techniques is a widely used method for diagnosing diseases. Although there is a large amount of ultrasound image data in medical centers and other places, labeled ultrasound datasets are a scarce resource, and it is likely that no datasets are available for new tissues/organs. Transfer learning provides the possibility to solve this problem, but there are too many features in natural images that are not related to the target domain. As a source domain, redundant features that are not conducive to the task will be extracted. Migration between ultrasound images can avoid this problem, but there are few types of public datasets, and it is difficult to find sufficiently similar source domains. Compared with natural images, ultrasound images have less information, and there are fewer transferable features between different ultrasound images, which may cause negative transfer. To this end, a multi-source adversarial transfer learning network for ultrasound image segmentation is proposed. Specifically, to address the lack of annotations, the idea of adversarial transfer learning is used to adaptively extract common features between a certain pair of source and target domains, which provides the possibility to utilize unlabeled ultrasound data. To alleviate the lack of knowledge in a single source domain, multi-source transfer learning is adopted to fuse knowledge from multiple source domains. In order to ensure the effectiveness of the fusion and maximize the use of precious data, a multi-source domain independent strategy is also proposed to improve the estimation of the target domain data distribution, which further increases the learning ability of the multi-source adversarial migration learning network in multiple domains. ",
        "title": "Multi-source adversarial transfer learning for ultrasound image  segmentation with limited similarity",
        "date": "2023-05-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.00563",
        "abstract_url": "http://arxiv.org/abs/2306.00563",
        "authors": [
            {
                "last_name": "Erdogan",
                "first_name": "Mete"
            },
            {
                "last_name": "Baytekin",
                "first_name": "Nuri Berke"
            },
            {
                "last_name": "Coban",
                "first_name": "Serhat Emre"
            },
            {
                "last_name": "Demir",
                "first_name": "Alper"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Nanomechanical resonant sensors are used in mass spectrometry via detection of resonance frequency jumps. There is a fundamental trade-off between detection speed and accuracy. Temporal and size resolution are limited by the resonator characteristics and noise. A Kalman filtering technique, augmented with maximum-likelihood estimation, was recently proposed as a Pareto optimal solution. We present enhancements and robust realizations for this technique, including a confidence boosted thresholding approach as well as machine learning for event detection. We describe learning techniques that are based on neural networks and boosted decision trees for temporal location and event size estimation. In the pure learning based approach that discards the Kalman filter, the raw data from the sensor are used in training a model for both location and size prediction. In the alternative approach that augments a Kalman filter, the event likelihood history is used in a binary classifier for event occurrence. Locations and sizes are predicted using maximum-likelihood, followed by a Kalman filter that continually improves the size estimate. We present detailed comparisons of the learning based schemes and the confidence boosted thresholding approach, and demonstrate robust performance for a practical realization. ",
        "title": "Machine Learning and Kalman Filtering for Nanomechanical Mass  Spectrometry",
        "date": "2023-06-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.01820",
        "abstract_url": "http://arxiv.org/abs/2306.01820",
        "authors": [
            {
                "last_name": "Reviriego",
                "first_name": "Pedro"
            },
            {
                "last_name": "Wang",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Alonso",
                "first_name": "Alvaro"
            },
            {
                "last_name": "Gao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Niknia",
                "first_name": "Farzad"
            },
            {
                "last_name": "Liu",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Lombardi",
                "first_name": "Fabrizio"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The complexity of Machine Learning (ML) systems increases each year, with current implementations of large language models or text-to-image generators having billions of parameters and requiring billions of arithmetic operations. As these systems are widely utilized, ensuring their reliable operation is becoming a design requirement. Traditional error detection mechanisms introduce circuit or time redundancy that significantly impacts system performance. An alternative is the use of Concurrent Error Detection (CED) schemes that operate in parallel with the system and exploit their properties to detect errors. CED is attractive for large ML systems because it can potentially reduce the cost of error detection. In this paper, we introduce Concurrent Classifier Error Detection (CCED), a scheme to implement CED in ML systems using a concurrent ML classifier to detect errors. CCED identifies a set of check signals in the main ML system and feeds them to the concurrent ML classifier that is trained to detect errors. The proposed CCED scheme has been implemented and evaluated on two widely used large-scale ML models: Contrastive Language Image Pretraining (CLIP) used for image classification and Bidirectional Encoder Representations from Transformers (BERT) used for natural language applications. The results show that more than 95 percent of the errors are detected when using a simple Random Forest classifier that is order of magnitude simpler than CLIP or BERT. These results illustrate the potential of CCED to implement error detection in large-scale ML models. ",
        "title": "Concurrent Classifier Error Detection (CCED) in Large Scale Machine  Learning Systems",
        "date": "2023-06-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.06366",
        "abstract_url": "http://arxiv.org/abs/2306.06366",
        "authors": [
            {
                "last_name": "Nkongolo",
                "first_name": "Mike"
            },
            {
                "last_name": "Tokmak",
                "first_name": "Mahmut"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  Technological advancements in various industries, such as network intelligence, vehicle networks, e-commerce, the Internet of Things (IoT), ubiquitous computing, and cloud-based applications, have led to an exponential increase in the volume of information flowing through critical systems. As a result, protecting critical infrastructures from intrusions and security threats have become a paramount concern in the field of intrusion detection systems (IDS). To address this concern, this research paper focuses on the importance of defending critical infrastructures against intrusions and security threats. It proposes a computational framework that incorporates feature selection through fuzzification. The effectiveness and performance of the proposed framework is evaluated using the NSL-KDD and UGRansome datasets in combination with selected machine learning (ML) models. The findings of the study highlight the effectiveness of fuzzy logic and the use of ensemble learning to enhance the performance of ML models. The research identifies Random Forest (RF) and Extreme Gradient Boosting (XGB) as the top performing algorithms to detect zero-day attacks. The results obtained from the implemented computational framework outperform previous methods documented in the IDS literature, reaffirming the significance of safeguarding critical infrastructures from intrusions and security threats. ",
        "title": "Zero-Day Threats Detection for Critical Infrastructures",
        "date": "2023-06-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.10950",
        "abstract_url": "http://arxiv.org/abs/2306.10950",
        "authors": [
            {
                "last_name": "Velay",
                "first_name": "Marc"
            },
            {
                "last_name": "Doan",
                "first_name": "Bich-Li\u00ean"
            },
            {
                "last_name": "Rimmel",
                "first_name": "Arpad"
            },
            {
                "last_name": "Popineau",
                "first_name": "Fabrice"
            },
            {
                "last_name": "Daniel",
                "first_name": "Fabrice"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Deep Reinforcement Learning approaches to Online Portfolio Selection have grown in popularity in recent years. The sensitive nature of training Reinforcement Learning agents implies a need for extensive efforts in market representation, behavior objectives, and training processes, which have often been lacking in previous works. We propose a training and evaluation process to assess the performance of classical DRL algorithms for portfolio management. We found that most Deep Reinforcement Learning algorithms were not robust, with strategies generalizing poorly and degrading quickly during backtesting. ",
        "title": "Benchmarking Robustness of Deep Reinforcement Learning approaches to  Online Portfolio Management",
        "date": "2023-06-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.11447",
        "abstract_url": "http://arxiv.org/abs/2306.11447",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Feiyang"
            },
            {
                "last_name": "\u00d8stvold",
                "first_name": "Bjarte M."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  The rise of mobile apps has brought greater convenience and many options for users. However, many apps use analytics services to collect a wide range of user interaction data, with privacy policies often failing to reveal the types of interaction data collected or the extent of the data collection practices. This lack of transparency potentially breaches data protection laws and also undermines user trust. We conducted an analysis of the top 20 analytic libraries for Android apps to identify common practices of interaction data collection and used this information to develop a standardized collection claim template for summarizing an app's data collection practices wrt. user interaction data. We selected the top 100 apps from popular categories on Google Play and used automatic static analysis to extract collection evidence from their data collection implementations. Our analysis found that a significant majority of these apps actively collected interaction data from UI types such as View (89%), Button (76%), and Textfield (63%), highlighting the pervasiveness of user interaction data collection. By comparing the collection evidence to the claims derived from privacy policy analysis, we manually fact-checked the completeness and accuracy of these claims for the top 10 apps. We found that, except for one app, they all failed to declare all types of interaction data they collect and did not specify some of the collection techniques used. ",
        "title": "Transparency in App Analytics: Analyzing the Collection of User  Interaction Data",
        "date": "2023-06-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.15327",
        "abstract_url": "http://arxiv.org/abs/2306.15327",
        "authors": [
            {
                "last_name": "Landi",
                "first_name": "Leonardo"
            },
            {
                "last_name": "Timpanella",
                "first_name": "Marco"
            },
            {
                "last_name": "Vicino",
                "first_name": "Lara"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we investigate two-point Algebraic Geometry codes associated to the Skabelund maximal curve constructed as a cyclic cover of the Suzuki curve. In order to estimate the minimum distance of such codes, we make use of the generalized order bound introduced by P. Beelen and determine certain two-point Weierstrass semigroups of the curve. ",
        "title": "Two-point AG codes from one of the Skabelund maximal curves",
        "date": "2023-06-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.04154",
        "abstract_url": "http://arxiv.org/abs/2307.04154",
        "authors": [
            {
                "last_name": "Carpio",
                "first_name": "Ana"
            },
            {
                "last_name": "Duro",
                "first_name": "Gema"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Two phase solid-fluid mixture models are ubiquitous in biological applications. For instance, models for growth of tissues and biofilms combine time dependent and quasi-stationary boundary value problems set in domains whose boundary moves in response to variations in the mechano-chemical variables. For a model of biofilm spread, we show how to obtain better posed models by characterizing the time derivatives of relevant quasi-stationary magnitudes in terms of additional boundary value problems. We also give conditions for well posedness of time dependent submodels set in moving domains depending on the motion of the boundary. After constructing solutions for transport, diffusion and elliptic submodels for volume fractions, displacements, velocities, pressures and concentrations with the required regularity, we are able to handle the full model of biofilm spread in moving domains assuming we know the dynamics of the boundary. These techniques are general and can be applied in models with a similar structure arising in biological and chemical engineering applications. ",
        "title": "Well posedness of fluid/solid mixture models for biofilm spread",
        "date": "2023-07-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.08318",
        "abstract_url": "http://arxiv.org/abs/2307.08318",
        "authors": [
            {
                "last_name": "Keuth",
                "first_name": "Ron"
            },
            {
                "last_name": "Heinrich",
                "first_name": "Mattias"
            },
            {
                "last_name": "Eichenlaub",
                "first_name": "Martin"
            },
            {
                "last_name": "Himstedt",
                "first_name": "Marian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Purpose: Navigation guidance is a key requirement for a multitude of lung interventions using video bronchoscopy. State-of-the-art solutions focus on lung biopsies using electromagnetic tracking and intraoperative image registration w.r.t. preoperative CT scans for guidance. The requirement of patient-specific CT scans hampers the utilisation of navigation guidance for other applications such as intensive care units.   Methods: This paper addresses navigation guidance solely incorporating bronchosopy video data. In contrast to state-of-the-art approaches we entirely omit the use of electromagnetic tracking and patient-specific CT scans. Guidance is enabled by means of topological bronchoscope localization w.r.t. an interpatient airway model. Particularly, we take maximally advantage of anatomical constraints of airway trees being sequentially traversed. This is realized by incorporating sequences of CNN-based airway likelihoods into a Hidden Markov Model.   Results: Our approach is evaluated based on multiple experiments inside a lung phantom model. With the consideration of temporal context and use of anatomical knowledge for regularization, we are able to improve the accuracy up to to 0.98 compared to 0.81 (weighted F1: 0.98 compared to 0.81) for a classification based on individual frames.   Conclusion: We combine CNN-based single image classification of airway segments with anatomical constraints and temporal HMM-based inference for the first time. Our approach renders vision-only guidance for bronchoscopy interventions in the absence of electromagnetic tracking and patient-specific CT scans possible. ",
        "title": "Airway Label Prediction in Video Bronchoscopy: Capturing Temporal  Dependencies Utilizing Anatomical Knowledge",
        "date": "2023-07-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.09529",
        "abstract_url": "http://arxiv.org/abs/2307.09529",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Cheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Fan"
            },
            {
                "last_name": "Richerme",
                "first_name": "Philip"
            },
            {
                "last_name": "Jiang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Quantum neural networks (QNNs) succeed in object recognition, natural language processing, and financial analysis. To maximize the accuracy of a QNN on a Noisy Intermediate Scale Quantum (NISQ) computer, approximate synthesis modifies the QNN circuit by reducing error-prone 2-qubit quantum gates. The success of QNNs motivates adversaries to attack QNNs via backdoors. However, na\\\"ively transplanting backdoors designed for classical neural networks to QNNs yields only low attack success rate, due to the noises and approximate synthesis on NISQ computers. Prior quantum circuit-based backdoors cannot selectively attack some inputs or work with all types of encoding layers of a QNN circuit. Moreover, it is easy to detect both transplanted and circuit-based backdoors in a QNN.   In this paper, we propose a novel and stealthy backdoor attack, QDoor, to achieve high attack success rate in approximately-synthesized QNN circuits by weaponizing unitary differences between uncompiled QNNs and their synthesized counterparts. QDoor trains a QNN behaving normally for all inputs with and without a trigger. However, after approximate synthesis, the QNN circuit always predicts any inputs with a trigger to a predefined class while still acts normally for benign inputs. Compared to prior backdoor attacks, QDoor improves the attack success rate by $13\\times$ and the clean data accuracy by $65\\%$ on average. Furthermore, prior backdoor detection techniques cannot find QDoor attacks in uncompiled QNN circuits. ",
        "title": "QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum  Neural Networks",
        "date": "2023-07-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.01675",
        "abstract_url": "http://arxiv.org/abs/2308.01675",
        "authors": [
            {
                "last_name": "Sohlbach",
                "first_name": "Lukas"
            },
            {
                "last_name": "Hobbani",
                "first_name": "Hamza"
            },
            {
                "last_name": "Blase",
                "first_name": "Chistopher"
            },
            {
                "last_name": "Perez-Pe\u00f1a",
                "first_name": "Fernando"
            },
            {
                "last_name": "Schmidt",
                "first_name": "Karsten"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In order to fully harness the potential of dielectric elastomer actu-ators (DEAs) in soft robots, advanced control methods are need-ed. An important groundwork for this is the development of a control-oriented model that can adequately describe the underly-ing dynamics of a DEA. A common feature of existing models is that always custom-made DEAs were investigated. This makes the modelling process easier, as all specifications and the struc-ture of the actuator are well known. In the case of a commercial actuator, however, only the information from the manufacturer is available and must be checked or completed during the modelling process. The aim of this paper is to explore how a commercial stacked silicone-based DEA can be modelled and how complex the model should be to properly replicate the features of the actu-ator. The static description has demonstrated the suitability of Hooke's law. In the case of dynamic description, it is shown that no viscoelastic model is needed for control-oriented modelling. However, if all features of the DEA are considered, the general-ized Kelvin-Maxwell model with three Maxwell elements shows good results, stability and computational efficiency. ",
        "title": "Modelling and simulation of a commercially available dielectric  elastomer actuator",
        "date": "2023-08-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.16562",
        "abstract_url": "http://arxiv.org/abs/2308.16562",
        "authors": [
            {
                "last_name": "Rigaki",
                "first_name": "Maria"
            },
            {
                "last_name": "Garcia",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future. ",
        "title": "The Power of MEME: Adversarial Malware Creation with Model-Based  Reinforcement Learning",
        "date": "2023-08-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.04631",
        "abstract_url": "http://arxiv.org/abs/2309.04631",
        "authors": [
            {
                "last_name": "Kaczmarzyk",
                "first_name": "Jakub R."
            },
            {
                "last_name": "O'Callaghan",
                "first_name": "Alan"
            },
            {
                "last_name": "Inglis",
                "first_name": "Fiona"
            },
            {
                "last_name": "Kurc",
                "first_name": "Tahsin"
            },
            {
                "last_name": "Gupta",
                "first_name": "Rajarsi"
            },
            {
                "last_name": "Bremer",
                "first_name": "Erich"
            },
            {
                "last_name": "Bankhead",
                "first_name": "Peter"
            },
            {
                "last_name": "Saltz",
                "first_name": "Joel H."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The field of digital pathology has seen a proliferation of deep learning models in recent years. Despite substantial progress, it remains rare for other researchers and pathologists to be able to access models published in the literature and apply them to their own images. This is due to difficulties in both sharing and running models. To address these concerns, we introduce WSInfer: a new, open-source software ecosystem designed to make deep learning for pathology more streamlined and accessible. WSInfer comprises three main elements: 1) a Python package and command line tool to efficiently apply patch-based deep learning inference to whole slide images; 2) a QuPath extension that provides an alternative inference engine through user-friendly and interactive software, and 3) a model zoo, which enables pathology models and metadata to be easily shared in a standardized form. Together, these contributions aim to encourage wider reuse, exploration, and interrogation of deep learning models for research purposes, by putting them into the hands of pathologists and eliminating a need for coding experience when accessed through QuPath. The WSInfer source code is hosted on GitHub and documentation is available at https://wsinfer.readthedocs.io. ",
        "title": "Open and reusable deep learning for pathology with WSInfer and QuPath",
        "date": "2023-09-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.11456",
        "abstract_url": "http://arxiv.org/abs/2309.11456",
        "authors": [
            {
                "last_name": "Ghaffarzadegan",
                "first_name": "Navid"
            },
            {
                "last_name": "Majumdar",
                "first_name": "Aritra"
            },
            {
                "last_name": "Williams",
                "first_name": "Ross"
            },
            {
                "last_name": "Hosseinichimeh",
                "first_name": "Niyousha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "MA"
        ],
        "abstract": "  We discuss the emerging new opportunity for building feedback-rich computational models of social systems using generative artificial intelligence. Referred to as Generative Agent-Based Models (GABMs), such individual-level models utilize large language models such as ChatGPT to represent human decision-making in social settings. We provide a GABM case in which human behavior can be incorporated in simulation models by coupling a mechanistic model of human interactions with a pre-trained large language model. This is achieved by introducing a simple GABM of social norm diffusion in an organization. For educational purposes, the model is intentionally kept simple. We examine a wide range of scenarios and the sensitivity of the results to several changes in the prompt. We hope the article and the model serve as a guide for building useful diffusion models that include realistic human reasoning and decision-making. ",
        "title": "Generative Agent-Based Modeling: Unveiling Social System Dynamics  through Coupling Mechanistic Models with Generative Artificial Intelligence",
        "date": "2023-09-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.17054",
        "abstract_url": "http://arxiv.org/abs/2309.17054",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Ling"
            },
            {
                "last_name": "Su",
                "first_name": "Hang"
            },
            {
                "last_name": "Gehrig",
                "first_name": "Daniel"
            },
            {
                "last_name": "Cannici",
                "first_name": "Marco"
            },
            {
                "last_name": "Scaramuzza",
                "first_name": "Davide"
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Event-based cameras are ideal for line-based motion estimation, since they predominantly respond to edges in the scene. However, accurately determining the camera displacement based on events continues to be an open problem. This is because line feature extraction and dynamics estimation are tightly coupled when using event cameras, and no precise model is currently available for describing the complex structures generated by lines in the space-time volume of events. We solve this problem by deriving the correct non-linear parametrization of such manifolds, which we term eventails, and demonstrate its application to event-based linear motion estimation, with known rotation from an Inertial Measurement Unit. Using this parametrization, we introduce a novel minimal 5-point solver that jointly estimates line parameters and linear camera velocity projections, which can be fused into a single, averaged linear velocity when considering multiple lines. We demonstrate on both synthetic and real data that our solver generates more stable relative motion estimates than other methods while capturing more inliers than clustering based on spatio-temporal planes. In particular, our method consistently achieves a 100% success rate in estimating linear velocity where existing closed-form solvers only achieve between 23% and 70%. The proposed eventails contribute to a better understanding of spatio-temporal event-generated geometries and we thus believe it will become a core building block of future event-based motion estimation algorithms. ",
        "title": "A 5-Point Minimal Solver for Event Camera Relative Motion Estimation",
        "date": "2023-09-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.04128",
        "abstract_url": "http://arxiv.org/abs/2311.04128",
        "authors": [
            {
                "last_name": "Gilpin",
                "first_name": "William"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Modern generative machine learning models demonstrate surprising ability to create realistic outputs far beyond their training data, such as photorealistic artwork, accurate protein structures, or conversational text. These successes suggest that generative models learn to effectively parametrize and sample arbitrarily complex distributions. Beginning half a century ago, foundational works in nonlinear dynamics used tools from information theory to infer properties of chaotic attractors from time series, motivating the development of algorithms for parametrizing chaos in real datasets. In this perspective, we aim to connect these classical works to emerging themes in large-scale generative statistical learning. We first consider classical attractor reconstruction, which mirrors constraints on latent representations learned by state space models of time series. We next revisit early efforts to use symbolic approximations to compare minimal discrete generators underlying complex processes, a problem relevant to modern efforts to distill and interpret black-box statistical models. Emerging interdisciplinary works bridge nonlinear dynamics and learning theory, such as operator-theoretic methods for complex fluid flows, or detection of broken detailed balance in biological datasets. We anticipate that future machine learning techniques may revisit other classical concepts from nonlinear dynamics, such as transinformation decay and complexity-entropy tradeoffs. ",
        "title": "Generative learning for nonlinear dynamics",
        "date": "2023-11-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.05669",
        "abstract_url": "http://arxiv.org/abs/2311.05669",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhongqun"
            },
            {
                "last_name": "Horanyi",
                "first_name": "Nora"
            },
            {
                "last_name": "Moon",
                "first_name": "Jaewon"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yihua"
            },
            {
                "last_name": "Chang",
                "first_name": "Hyung Jin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Gaze following estimates gaze targets of in-scene person by understanding human behavior and scene information. Existing methods usually analyze scene images for gaze following. However, compared with visual images, audio also provides crucial cues for determining human behavior.This suggests that we can further improve gaze following considering audio cues. In this paper, we explore gaze following tasks in conversational scenarios. We propose a novel multi-modal gaze following framework based on our observation ``audiences tend to focus on the speaker''. We first leverage the correlation between audio and lips, and classify speakers and listeners in a scene. We then use the identity information to enhance scene images and propose a gaze candidate estimation network. The network estimates gaze candidates from enhanced scene images and we use MLP to match subjects with candidates as classification tasks. Existing gaze following datasets focus on visual images while ignore audios.To evaluate our method, we collect a conversational dataset, VideoGazeSpeech (VGS), which is the first gaze following dataset including images and audio. Our method significantly outperforms existing methods in VGS datasets. The visualization result also prove the advantage of audio cues in gaze following tasks. Our work will inspire more researches in multi-modal gaze following estimation. ",
        "title": "Multi-Modal Gaze Following in Conversational Scenarios",
        "date": "2023-11-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.11234",
        "abstract_url": "http://arxiv.org/abs/2311.11234",
        "authors": [
            {
                "last_name": "K.",
                "first_name": "Keshav Kumar"
            },
            {
                "last_name": "Narasimham",
                "first_name": "Dr N V S L"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The transformative power of Convolutional Neural Networks (CNNs) in radiology diagnostics is examined in this study, with a focus on interpretability, effectiveness, and ethical issues. With an altered DenseNet architecture, the CNN performs admirably in terms of particularity, sensitivity, as well as accuracy. Its superiority over conventional methods is validated by comparative analyses, which highlight efficiency gains. Nonetheless, interpretability issues highlight the necessity of sophisticated methods in addition to continuous model improvement. Integration issues like interoperability and radiologists' training lead to suggestions for teamwork. Systematic consideration of the ethical implications is carried out, necessitating extensive frameworks. Refinement of architectures, interpretability, alongside ethical considerations need to be prioritized in future work for responsible CNN deployment in radiology diagnostics. ",
        "title": "Enhancing Radiology Diagnosis through Convolutional Neural Networks for  Computer Vision in Healthcare",
        "date": "2023-11-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.13123",
        "abstract_url": "http://arxiv.org/abs/2311.13123",
        "authors": [
            {
                "last_name": "Cervenjak",
                "first_name": "Philip"
            },
            {
                "last_name": "Gan",
                "first_name": "Junhao"
            },
            {
                "last_name": "Wirth",
                "first_name": "Anthony"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Maximizing a non-negative, monontone, submodular function $f$ over $n$ elements under a cardinality constraint $k$ (SMCC) is a well-studied NP-hard problem. It has important applications in, e.g., machine learning and influence maximization. Though the theoretical problem admits polynomial-time approximation algorithms, solving it in practice often involves frequently querying submodular functions that are expensive to compute. This has motivated significant research into designing parallel approximation algorithms in the adaptive complexity model; adaptive complexity (adaptivity) measures the number of sequential rounds of $\\text{poly}(n)$ function queries an algorithm requires. The state-of-the-art algorithms can achieve $(1-\\frac{1}{e}-\\varepsilon)$-approximate solutions with $O(\\frac{1}{\\varepsilon^2}\\log n)$ adaptivity, which approaches the known adaptivity lower-bounds. However, the $O(\\frac{1}{\\varepsilon^2} \\log n)$ adaptivity only applies to maximizing worst-case functions that are unlikely to appear in practice. Thus, in this paper, we consider the special class of $p$-superseparable submodular functions, which places a reasonable constraint on $f$, based on the parameter $p$, and is more amenable to maximization, while also having real-world applicability. Our main contribution is the algorithm LS+GS, a finer-grained version of the existing LS+PGB algorithm, designed for instances of SMCC when $f$ is $p$-superseparable; it achieves an expected $(1-\\frac{1}{e}-\\varepsilon)$-approximate solution with $O(\\frac{1}{\\varepsilon^2}\\log(p k))$ adaptivity independent of $n$. Additionally, unrelated to $p$-superseparability, our LS+GS algorithm uses only $O(\\frac{n}{\\varepsilon} + \\frac{\\log n}{\\varepsilon^2})$ oracle queries, which has an improved dependence on $\\varepsilon^{-1}$ over the state-of-the-art LS+PGB; this is achieved through the design of a novel thresholding subroutine. ",
        "title": "Fast Parallel Algorithms for Submodular $p$-Superseparable Maximization",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.17287",
        "abstract_url": "http://arxiv.org/abs/2311.17287",
        "authors": [
            {
                "last_name": "Sultan",
                "first_name": "Youssef"
            },
            {
                "last_name": "Rafter",
                "first_name": "Jackson C."
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huyen T."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Understanding whether a property is priced fairly hinders buyers and sellers since they usually do not have an objective viewpoint of the price distribution for the overall market of their interest. Drawing from data collected of all possible available properties for rent in Manhattan as of September 2023, this paper aims to strengthen our understanding of model residuals; specifically on machine learning models which generalize for a majority of the distribution of a well-proportioned dataset. Most models generally perceive deviations from predicted values as mere inaccuracies, however this paper proposes a different vantage point: when generalizing to at least 75\\% of the data-set, the remaining deviations reveal significant insights. To harness these insights, we introduce the Price Anomaly Score (PAS), a metric capable of capturing boundaries between irregularly predicted prices. By combining relative pricing discrepancies with statistical significance, the Price Anomaly Score (PAS) offers a multifaceted view of rental valuations. This metric allows experts to identify overpriced or underpriced properties within a dataset by aggregating PAS values, then fine-tuning upper and lower boundaries to any threshold to set indicators of choice. ",
        "title": "Utilizing Model Residuals to Identify Rental Properties of Interest: The  Price Anomaly Score (PAS) and Its Application to Real-time Data in Manhattan",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.17816",
        "abstract_url": "http://arxiv.org/abs/2311.17816",
        "authors": [
            {
                "last_name": "Holland",
                "first_name": "Kieran"
            },
            {
                "last_name": "Ipp",
                "first_name": "Andreas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "David I."
            },
            {
                "last_name": "Wenger",
                "first_name": "Urs"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Lattice gauge-equivariant convolutional neural networks (L-CNNs) can be used to form arbitrarily shaped Wilson loops and can approximate any gauge-covariant or gauge-invariant function on the lattice. Here we use L-CNNs to describe fixed point (FP) actions which are based on renormalization group transformations. FP actions are classically perfect, i.e., they have no lattice artifacts on classical gauge-field configurations satisfying the equations of motion, and therefore possess scale invariant instanton solutions. FP actions are tree-level Symanzik-improved to all orders in the lattice spacing and can produce physical predictions with very small lattice artifacts even on coarse lattices. We find that L-CNNs are much more accurate at parametrizing the FP action compared to older approaches. They may therefore provide a way to circumvent critical slowing down and topological freezing towards the continuum limit. ",
        "title": "Fixed point actions from convolutional neural networks",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.18042",
        "abstract_url": "http://arxiv.org/abs/2311.18042",
        "authors": [
            {
                "last_name": "Molavi",
                "first_name": "Abtin"
            },
            {
                "last_name": "Xu",
                "first_name": "Amanda"
            },
            {
                "last_name": "Tannu",
                "first_name": "Swamit"
            },
            {
                "last_name": "Albarghouthi",
                "first_name": "Aws"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  Practical applications of quantum computing depend on fault-tolerant devices with error correction. Today, the most promising approach is a class of error-correcting codes called surface codes. In this paper, we study the problem of compiling quantum circuits for quantum computers implementing surface codes. The problem involves (1) mapping circuit qubits to the device qubits and (2) routing execution paths between pairs of interacting qubits. We call this the surface code mapping and routing problem (SCMR).   Solving SCMR near-optimally is critical for both efficiency and correctness. An optimal solution limits the cost of a computation in terms of precious quantum resources and also minimizes the probability of incurring an undetected logical error, which increases with each additional time step.   We study SCMR from a theoretical and practical perspective. First, we prove that SCMR, as well as a constrained version of the problem, is NP-complete. Second, we present a optimal algorithm for solving SCMR that is based on a SAT encoding. Third, we present a spectrum of efficient relaxations of SCMR, for example, by exploiting greedy algorithms for solving the problem of node-disjoint paths. Finally, we implement and evaluate our algorithms on a large suite of real and synthetic circuits. Our results suggest that our relaxations are a powerful tool for compiling realistic workloads. The relaxation-based algorithms are orders of magnitude faster than the optimal algorithm (solving instances with tens of thousands of gates in minutes), while still finding high-quality solutions, achieving the theoretical lower bound on up to 55 out of 168 circuits from a diverse benchmark suite. ",
        "title": "Compilation for Surface Code Quantum Computers",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.11954",
        "abstract_url": "http://arxiv.org/abs/2312.11954",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Huafeng"
            },
            {
                "last_name": "Jin",
                "first_name": "Xin"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yun"
            },
            {
                "last_name": "El-Yacoubi",
                "first_name": "Mounim A."
            },
            {
                "last_name": "Gao",
                "first_name": "Xinbo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Data mixing augmentation has been widely applied to improve the generalization ability of deep neural networks. Recently, offline data mixing augmentation, e.g. handcrafted and saliency information-based mixup, has been gradually replaced by automatic mixing approaches. Through minimizing two sub-tasks, namely, mixed sample generation and mixup classification in an end-to-end way, AutoMix significantly improves accuracy on image classification tasks. However, as the optimization objective is consistent for the two sub-tasks, this approach is prone to generating consistent instead of diverse mixed samples, which results in overfitting for target task training. In this paper, we propose AdAutomixup, an adversarial automatic mixup augmentation approach that generates challenging samples to train a robust classifier for image classification, by alternatively optimizing the classifier and the mixup sample generator. AdAutomixup comprises two modules, a mixed example generator, and a target classifier. The mixed sample generator aims to produce hard mixed examples to challenge the target classifier while the target classifier`s aim is to learn robust features from hard mixed examples to improve generalization. To prevent the collapse of the inherent meanings of images, we further introduce an exponential moving average (EMA) teacher and cosine similarity to train AdAutomixup in an end-to-end way. Extensive experiments on seven image benchmarks consistently prove that our approach outperforms the state of the art in various classification scenarios. ",
        "title": "Adversarial AutoMixup",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.14129",
        "abstract_url": "http://arxiv.org/abs/2312.14129",
        "authors": [
            {
                "last_name": "Choi",
                "first_name": "Dongjin"
            },
            {
                "last_name": "Xiang",
                "first_name": "Andy"
            },
            {
                "last_name": "Ozturk",
                "first_name": "Ozgur"
            },
            {
                "last_name": "Shrestha",
                "first_name": "Deep"
            },
            {
                "last_name": "Drake",
                "first_name": "Barry"
            },
            {
                "last_name": "Haidarian",
                "first_name": "Hamid"
            },
            {
                "last_name": "Javed",
                "first_name": "Faizan"
            },
            {
                "last_name": "Park",
                "first_name": "Haesun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's effectiveness. It produces better results compared to other existing methods in classification performance, yields meaningful clustering of patients, and delivers consistent results in patient similarity searches and predictions. ",
        "title": "WellFactor: Patient Profiling using Integrative Embedding of Healthcare  Data",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.16253",
        "abstract_url": "http://arxiv.org/abs/2312.16253",
        "authors": [
            {
                "last_name": "Albouy",
                "first_name": "Timoth\u00e9"
            },
            {
                "last_name": "Frey",
                "first_name": "Davide"
            },
            {
                "last_name": "Gelles",
                "first_name": "Ran"
            },
            {
                "last_name": "Hazay",
                "first_name": "Carmit"
            },
            {
                "last_name": "Raynal",
                "first_name": "Michel"
            },
            {
                "last_name": "Schiller",
                "first_name": "Elad Michael"
            },
            {
                "last_name": "Taiani",
                "first_name": "Francois"
            },
            {
                "last_name": "Zikas",
                "first_name": "Vassilis"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We address the problem of Reliable Broadcast in asynchronous message-passing systems with $n$ nodes, of which up to $t$ are malicious (faulty), in addition to a message adversary that can drop some of the messages sent by correct (non-faulty) nodes.   We present a Message-Adversary-Tolerant Byzantine Reliable Broadcast (MBRB) algorithm that communicates an almost optimal amount of $O(|m|+n^2\\kappa)$ bits per node, where $|m|$ represents the length of the application message and $\\kappa=\\Omega(\\log n)$ is a security parameter. This improves upon the state-of-the-art MBRB solution (Albouy, Frey, Raynal, and Ta\\\"iani, SSS 2021), which incurs communication of $O(n|m|+n^2\\kappa )$ bits per node.   Our solution sends at most $4n^2$ messages overall, which is asymptotically optimal. Reduced communication is achieved by employing coding techniques that replace the need for all nodes to (re-)broadcast the entire message~$m$. Instead, nodes forward authenticated fragments of the encoding of $m$ using an erasure-correcting code. Under the cryptographic assumptions of PKI and collision-resistant hash, and assuming $n > 3t + 2d$, where the adversary drops at most~$d$ messages per broadcast, our algorithm allows most of the correct nodes to reconstruct~$m$, despite missing fragments caused by the malicious nodes and the message adversary. ",
        "title": "Towards Optimal Communication Byzantine Reliable Broadcast under a  Message Adversary",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.00682",
        "abstract_url": "http://arxiv.org/abs/2401.00682",
        "authors": [
            {
                "last_name": "Van Nguyen",
                "first_name": "Hoa"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Tran Thien Dat"
            },
            {
                "last_name": "Shim",
                "first_name": "Changbeom"
            },
            {
                "last_name": "Anuar",
                "first_name": "Marzhar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a smooth-trajectory estimator for the labelled multi-Bernoulli (LMB) filter by exploiting the special structure of the generalised labelled multi-Bernoulli (GLMB) filter. We devise a simple and intuitive approach to store the best association map when approximating the GLMB random finite set (RFS) to the LMB RFS. In particular, we construct a smooth-trajectory estimator (i.e., an estimator over the entire trajectories of labelled estimates) for the LMB filter based on the history of the best association map and all of the measurements up to the current time. Experimental results under two challenging scenarios demonstrate significant tracking accuracy improvements with negligible additional computational time compared to the conventional LMB filter. The source code is publicly available at https://tinyurl.com/ste-lmb, aimed at promoting advancements in MOT algorithms. ",
        "title": "The Smooth Trajectory Estimator for LMB Filters",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01193",
        "abstract_url": "http://arxiv.org/abs/2401.01193",
        "authors": [
            {
                "last_name": "Dong",
                "first_name": "Qingxiu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guangyan"
            },
            {
                "last_name": "Xu",
                "first_name": "Ke"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "DS"
        ],
        "abstract": "  Recently, Xu and Zhou [2023] introduced a constructive approach for exploring computational hardness, proving that SAT requires exhaustive search. In light of certain misinterpretations concerning the contributions and proofs in that paper, we focus on providing detailed explanations in this work. We begin by delineating the core innovation of the constructive approach, shedding light on the pivotal concept of algorithm designability. We address the overlooked white-box diagonalization method and highlight the concept of an almost independent solution space. In response to specific misunderstandings, such as the concerns surrounding the assumptions of Lemma 3.1, we offer comprehensive clarifications aimed at improving the comprehension of the proof. We are grateful for the feedback received on our prior paper and hope this work can foster a more well-informed discussion. ",
        "title": "Further Explanations on \"SAT Requires Exhaustive Search\"",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01738",
        "abstract_url": "http://arxiv.org/abs/2401.01738",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Ruoyu"
            },
            {
                "last_name": "Cheng",
                "first_name": "Lei"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Lou",
                "first_name": "Yi"
            },
            {
                "last_name": "Gao",
                "first_name": "Yulong"
            },
            {
                "last_name": "Wu",
                "first_name": "Wen"
            },
            {
                "last_name": "Ng",
                "first_name": "Derrick Wing Kwan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Benefitting from the vast spatial degrees of freedom, the amalgamation of integrated sensing and communication (ISAC) and massive multiple-input multiple-output (MIMO) is expected to simultaneously improve spectral and energy efficiencies as well as the sensing capability. However, a large number of antennas deployed in massive MIMO-ISAC raises critical challenges in acquiring both accurate channel state information and target parameter information. To overcome these two challenges with a unified framework, we first analyze their underlying system models and then propose a novel tensor-based approach that addresses both the channel estimation and target sensing problems. Specifically, by parameterizing the high-dimensional communication channel exploiting a small number of physical parameters, we associate the channel state information with the sensing parameters of targets in terms of angular, delay, and Doppler dimensions. Then, we propose a shared training pattern adopting the same time-frequency resources such that both the channel estimation and target parameter estimation can be formulated as a canonical polyadic decomposition problem with a similar mathematical expression. On this basis, we first investigate the uniqueness condition of the tensor factorization and the maximum number of resolvable targets by utilizing the specific Vandermonde ",
        "title": "Integrated Sensing and Communication with Massive MIMO: A Unified Tensor  Approach for Channel and Target Parameter Estimation",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01739",
        "abstract_url": "http://arxiv.org/abs/2401.01739",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinran"
            },
            {
                "last_name": "Lu",
                "first_name": "Qiujie"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongmyoung"
            },
            {
                "last_name": "Gan",
                "first_name": "Zhongxue"
            },
            {
                "last_name": "Rojas",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper introduces a new type of soft continuum robot, called SCoReS, which is capable of self-controlling continuously its curvature at the segment level; in contrast to previous designs which either require external forces or machine elements, or whose variable curvature capabilities are discrete -- depending on the number of locking mechanisms and segments. The ability to have a variable curvature, whose control is continuous and independent from external factors, makes a soft continuum robot more adaptive in constrained environments, similar to what is observed in nature in the elephant's trunk or ostrich's neck for instance which exhibit multiple curvatures. To this end, our soft continuum robot enables reconfigurable variable curvatures utilizing a variable stiffness growing spine based on micro-particle granular jamming for the first time. We detail the design of the proposed robot, presenting its modeling through beam theory and FEA simulation -- which is validated through experiments. The robot's versatile bending profiles are then explored in experiments and an application to grasp fruits at different configurations is demonstrated. ",
        "title": "A Soft Continuum Robot with Self-Controllable Variable Curvature",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03173",
        "abstract_url": "http://arxiv.org/abs/2401.03173",
        "authors": [
            {
                "last_name": "Minh",
                "first_name": "Tran Cao"
            },
            {
                "last_name": "Quoc",
                "first_name": "Nguyen Kim"
            },
            {
                "last_name": "Vinh",
                "first_name": "Phan Cong"
            },
            {
                "last_name": "Phu",
                "first_name": "Dang Nhu"
            },
            {
                "last_name": "Chi",
                "first_name": "Vuong Xuan"
            },
            {
                "last_name": "Tan",
                "first_name": "Ha Minh"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  In the field of medical imaging, breast ultrasound has emerged as a crucial diagnostic tool for early detection of breast cancer. However, the accuracy of diagnosing the location of the affected area and the extent of the disease depends on the experience of the physician. In this paper, we propose a novel model called UGGNet, combining the power of the U-Net and VGG architectures to enhance the performance of breast ultrasound image analysis. The U-Net component of the model helps accurately segment the lesions, while the VGG component utilizes deep convolutional layers to extract features. The fusion of these two architectures in UGGNet aims to optimize both segmentation and feature representation, providing a comprehensive solution for accurate diagnosis in breast ultrasound images. Experimental results have demonstrated that the UGGNet model achieves a notable accuracy of 78.2% on the \"Breast Ultrasound Images Dataset.\" ",
        "title": "UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04448",
        "abstract_url": "http://arxiv.org/abs/2401.04448",
        "authors": [
            {
                "last_name": "Breci",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Guarnera",
                "first_name": "Luca"
            },
            {
                "last_name": "Battiato",
                "first_name": "Sebastiano"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author. These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features. If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual. The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations. This is made possible by algorithmic solutions based on purely mathematical concepts. Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand. In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper\" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline. Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data. The datasets are available at https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/. ",
        "title": "A Novel Dataset for Non-Destructive Inspection of Handwritten Documents",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05731",
        "abstract_url": "http://arxiv.org/abs/2401.05731",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Xiaohui"
            },
            {
                "last_name": "Li",
                "first_name": "Wenxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhongzhi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In order to investigate the relationship between Shannon information measure of random variables, scholars such as Yeung utilized information diagrams to explore the structured representation of information measures, establishing correspondences with sets. However, this method has limitations when studying information measures of five or more random variables. In this paper, we consider employing algebraic methods to study the relationship of information measures of random variables. By introducing a semiring generated by random variables, we establish correspondences between sets and elements of the semiring. Utilizing the Grobner-Shirshov basis, we present the structure of the semiring and its standard form. Furthermore, we delve into the structure of the semiring generated under Markov chain conditions (referred to as Markov semiring), obtaining its standard form. ",
        "title": "On Grobner-Shirshov bases for Markov semirings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06132",
        "abstract_url": "http://arxiv.org/abs/2401.06132",
        "authors": [
            {
                "last_name": "Akkurt",
                "first_name": "Semih"
            },
            {
                "last_name": "Witherden",
                "first_name": "Freddie"
            },
            {
                "last_name": "Vincent",
                "first_name": "Peter"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF"
        ],
        "abstract": "  In this article, cache blocking is implemented for the Navier Stokes equations with anti-aliasing support on mixed grids in PyFR for CPUs. In particular, cache blocking is used as an alternative to kernel fusion to eliminate unnecessary data movements between kernels at the main memory level. Specifically, kernels that exchange data are grouped together, and these groups are then executed on small sub-regions of the domain that fit in per-core private data cache. Additionally, cache blocking is also used to efficiently implement a tensor product factorisation of the interpolation operators associated with anti-aliasing. By using cache blocking, the intermediate results between application of the sparse factors are stored in per-core private data cache, and a significant amount of data movement from main memory is avoided. In order to assess the performance gains a theoretical model is developed, and the implementation is benchmarked using a compressible 3D Taylor-Green vortex test case on both hexahedral and prismatic grids, with third- and forth-order solution polynomials. The expected performance gains based on the theoretical model range from 1.99 to 2.62, and the speedups obtained in practice range from 1.67 to 3.67 compared to PyFR v1.11.0. ",
        "title": "Cache Blocking for Flux Reconstruction: Extension to Navier-Stokes  Equations and Anti-aliasing",
        "date": "2023-11-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06133",
        "abstract_url": "http://arxiv.org/abs/2401.06133",
        "authors": [
            {
                "last_name": "Kong",
                "first_name": "Chung To"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Every country must dispose of old banknotes. At the Hong Kong Monetary Authority visitor center, visitors can buy a paperweight souvenir full of shredded banknotes. Even though the shredded banknotes are small, by using computer vision, it is possible to reconstruct the whole banknote like a jigsaw puzzle. Each paperweight souvenir costs $\\$100$ HKD, and it is claimed to contain shredded banknotes equivalent to 138 complete $\\$1000$ HKD banknotes. In theory, $\\$138,000$ HKD can be recovered by using computer vision. This paper discusses the technique of collecting shredded banknote pieces and applying a computer vision program. ",
        "title": "The possibility of making $\\$138,000$ from shredded banknote pieces  using computer vision",
        "date": "2023-11-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06135",
        "abstract_url": "http://arxiv.org/abs/2401.06135",
        "authors": [
            {
                "last_name": "Pase",
                "first_name": "Francesco"
            },
            {
                "last_name": "Giordani",
                "first_name": "Marco"
            },
            {
                "last_name": "Cavallero",
                "first_name": "Sara"
            },
            {
                "last_name": "Schellmann",
                "first_name": "Malte"
            },
            {
                "last_name": "Eichinger",
                "first_name": "Josef"
            },
            {
                "last_name": "Verdone",
                "first_name": "Roberto"
            },
            {
                "last_name": "Zorzi",
                "first_name": "Michele"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG",
            "MA"
        ],
        "abstract": "  Industrial Internet of Things (IIoT) networks will provide Ultra-Reliable Low-Latency Communication (URLLC) to support critical processes underlying the production chains. However, standard protocols for allocating wireless resources may not optimize the latency-reliability trade-off, especially for uplink communication. For example, centralized grant-based scheduling can ensure almost zero collisions, but introduces delays in the way resources are requested by the User Equipments (UEs) and granted by the gNB. In turn, distributed scheduling (e.g., based on random access), in which UEs autonomously choose the resources for transmission, may lead to potentially many collisions especially when the traffic increases. In this work we propose DIStributed combinatorial NEural linear Thompson Sampling (DISNETS), a novel scheduling framework that combines the best of the two worlds. By leveraging a feedback signal from the gNB and reinforcement learning, the UEs are trained to autonomously optimize their uplink transmissions by selecting the available resources to minimize the number of collisions, without additional message exchange to/from the gNB. DISNETS is a distributed, multi-agent adaptation of the Neural Linear Thompson Sampling (NLTS) algorithm, which has been further extended to admit multiple parallel actions. We demonstrate the superior performance of DISNETS in addressing URLLC in IIoT scenarios compared to other baselines. ",
        "title": "A Distributed Neural Linear Thompson Sampling Framework to Achieve URLLC  in Industrial IoT",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06137",
        "abstract_url": "http://arxiv.org/abs/2401.06137",
        "authors": [
            {
                "last_name": "Malinovsk\u00e1",
                "first_name": "Krist\u00edna"
            },
            {
                "last_name": "Holenda",
                "first_name": "Slavom\u00edr"
            },
            {
                "last_name": "Malinovsk\u00fd",
                "first_name": "\u013dudov\u00edt"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications. ",
        "title": "QuasiNet: a neural network with trainable product layers",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06139",
        "abstract_url": "http://arxiv.org/abs/2401.06139",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Bohan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Lu",
                "first_name": "Yuchao"
            },
            {
                "last_name": "Hu",
                "first_name": "Tianzixuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Jinling"
            },
            {
                "last_name": "Houlihan",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce \"Stockformer,\" a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized return of 30.80%, significantly surpassing current state-of-the-art models. Stockformer has emerged as a beacon of innovation in these volatile times, offering investors a potent tool for market forecasting. To advance the field and foster community collaboration, we have open-sourced Stockformer, available at https://github.com/Eric991005/Stockformer. ",
        "title": "StockFormer: A Swing Trading Strategy Based on STL Decomposition and  Self-Attention Networks",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06143",
        "abstract_url": "http://arxiv.org/abs/2401.06143",
        "authors": [
            {
                "last_name": "Surmann",
                "first_name": "Hartmut"
            },
            {
                "last_name": "Digakis",
                "first_name": "Niklas"
            },
            {
                "last_name": "Kremer",
                "first_name": "Jan-Nicklas"
            },
            {
                "last_name": "Meine",
                "first_name": "Julien"
            },
            {
                "last_name": "Schulte",
                "first_name": "Max"
            },
            {
                "last_name": "Voigt",
                "first_name": "Niklas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the realm of digital situational awareness during disaster situations, accurate digital representations, like 3D models, play an indispensable role. To ensure the safety of rescue teams, robotic platforms are often deployed to generate these models. In this paper, we introduce an innovative approach that synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller than 30 cm, equipped with 360 degree cameras and the advances of Neural Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D representation of any scene using 2D images and then synthesize it from various angles upon request. This method is especially tailored for urban environments which have experienced significant destruction, where the structural integrity of buildings is compromised to the point of barring entry-commonly observed post-earthquakes and after severe fires. We have tested our approach through recent post-fire scenario, underlining the efficacy of NeRFs even in challenging outdoor environments characterized by water, snow, varying light conditions, and reflective surfaces. ",
        "title": "Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and  Neural Radiance Fields",
        "date": "2023-11-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06144",
        "abstract_url": "http://arxiv.org/abs/2401.06144",
        "authors": [
            {
                "last_name": "Havrilla",
                "first_name": "Alex"
            },
            {
                "last_name": "Rojas",
                "first_name": "Kevin"
            },
            {
                "last_name": "Liao",
                "first_name": "Wenjing"
            },
            {
                "last_name": "Tao",
                "first_name": "Molei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion generative models have achieved remarkable success in generating images with a fixed resolution. However, existing models have limited ability to generalize to different resolutions when training data at those resolutions are not available. Leveraging techniques from operator learning, we present a novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the score operator by combining both spatial and spectral information at multiple resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1) simultaneously training on multiple resolutions improves FID over training at any single fixed resolution; 2) DFU generalizes beyond its training resolutions, allowing for coherent, high-fidelity generation at higher-resolutions with the same model, i.e. zero-shot super-resolution image-generation; 3) we propose a fine-tuning strategy to further enhance the zero-shot super-resolution image-generation capability of our model, leading to a FID of 11.3 at 1.66 times the maximum training resolution on FFHQ, which no other method can come close to achieving. ",
        "title": "DFU: scale-robust diffusion model for zero-shot super-resolution image  generation",
        "date": "2023-11-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06145",
        "abstract_url": "http://arxiv.org/abs/2401.06145",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Giannoula",
                "first_name": "Christina"
            },
            {
                "last_name": "Wu",
                "first_name": "Jun"
            },
            {
                "last_name": "Elhoushi",
                "first_name": "Mostafa"
            },
            {
                "last_name": "Gleeson",
                "first_name": "James"
            },
            {
                "last_name": "Pekhimenko",
                "first_name": "Gennady"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "CV",
            "LG",
            "PF"
        ],
        "abstract": "  Sparse Convolution (SC) is widely used for processing 3D point clouds that are inherently sparse. Different from dense convolution, SC preserves the sparsity of the input point cloud by only allowing outputs to specific locations. To efficiently compute SC, prior SC engines first use hash tables to build a kernel map that stores the necessary General Matrix Multiplication (GEMM) operations to be executed (Map step), and then use a Gather-GEMM-Scatter process to execute these GEMM operations (GMaS step). In this work, we analyze the shortcomings of prior state-of-the-art SC engines, and propose Minuet, a novel memory-efficient SC engine tailored for modern GPUs. Minuet proposes to (i) replace the hash tables used in the Map step with a novel segmented sorting double-traversed binary search algorithm that highly utilizes the on-chip memory hierarchy of GPUs, (ii) use a lightweight scheme to autotune the tile size in the Gather and Scatter operations of the GMaS step, such that to adapt the execution to the particular characteristics of each SC layer, dataset, and GPU architecture, and (iii) employ a padding-efficient GEMM grouping approach that reduces both memory padding and kernel launching overheads. Our evaluations show that Minuet significantly outperforms prior SC engines by on average $1.74\\times$ (up to $2.22\\times$) for end-to-end point cloud network executions. Our novel segmented sorting double-traversed binary search algorithm achieves superior speedups by $15.8\\times$ on average (up to $26.8\\times$) over prior SC engines in the Map step. The source code of Minuet is publicly available at https://github.com/UofT-EcoSystem/Minuet. ",
        "title": "Minuet: Accelerating 3D Sparse Convolutions on GPUs",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06146",
        "abstract_url": "http://arxiv.org/abs/2401.06146",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Qiao",
                "first_name": "Calvin"
            },
            {
                "last_name": "Ren",
                "first_name": "Guanqiao"
            },
            {
                "last_name": "Yin",
                "first_name": "KangKang"
            },
            {
                "last_name": "Ha",
                "first_name": "Sehoon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Interactive motion synthesis is essential in creating immersive experiences in entertainment applications, such as video games and virtual reality. However, generating animations that are both high-quality and contextually responsive remains a challenge. Traditional techniques in the game industry can produce high-fidelity animations but suffer from high computational costs and poor scalability. Trained neural network models alleviate the memory and speed issues, yet fall short on generating diverse motions. Diffusion models offer diverse motion synthesis with low memory usage, but require expensive reverse diffusion processes. This paper introduces the Accelerated Auto-regressive Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to achieve quality, diversity, and efficiency all together. AAMDM integrates Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a lower-dimensional embedded space rather than the full-dimensional pose space, which reduces the training complexity as well as further improves the performance. We show that AAMDM outperforms existing methods in motion quality, diversity, and runtime efficiency, through comprehensive quantitative analyses and visual comparisons. We also demonstrate the effectiveness of each algorithmic component through ablation studies. ",
        "title": "AAMDM: Accelerated Auto-regressive Motion Diffusion Model",
        "date": "2023-12-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06148",
        "abstract_url": "http://arxiv.org/abs/2401.06148",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Andrew H."
            },
            {
                "last_name": "Jaume",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Williamson",
                "first_name": "Drew F. K."
            },
            {
                "last_name": "Lu",
                "first_name": "Ming Y."
            },
            {
                "last_name": "Vaidya",
                "first_name": "Anurag"
            },
            {
                "last_name": "Miller",
                "first_name": "Tiffany R."
            },
            {
                "last_name": "Mahmood",
                "first_name": "Faisal"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Advances in digitizing tissue slides and the fast-paced progress in artificial intelligence, including deep learning, have boosted the field of computational pathology. This field holds tremendous potential to automate clinical diagnosis, predict patient prognosis and response to therapy, and discover new morphological biomarkers from tissue images. Some of these artificial intelligence-based systems are now getting approved to assist clinical diagnosis; however, technical barriers remain for their widespread clinical adoption and integration as a research tool. This Review consolidates recent methodological advances in computational pathology for predicting clinical end points in whole-slide images and highlights how these developments enable the automation of clinical practice and the discovery of new biomarkers. We then provide future perspectives as the field expands into a broader range of clinical and research tasks with increasingly diverse modalities of clinical data. ",
        "title": "Artificial Intelligence for Digital and Computational Pathology",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06149",
        "abstract_url": "http://arxiv.org/abs/2401.06149",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Yang"
            },
            {
                "last_name": "Dou",
                "first_name": "Weiping"
            },
            {
                "last_name": "Cohen",
                "first_name": "Andrew"
            },
            {
                "last_name": "Bisharat",
                "first_name": "Dia'a"
            },
            {
                "last_name": "Tian",
                "first_name": "Yuandong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Qing Huo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  To extend the antenna design on printed circuit boards (PCBs) for more engineers of interest, we propose a simple method that models PCB antennas with a few basic components. By taking two separate steps to decide their geometric dimensions and positions, antenna prototypes can be facilitated with no experience required. Random sampling statistics relate to the quality of dimensions are used in selecting among dimension candidates. A novel image-based classifier using a convolutional neural network (CNN) is introduced to further determine the positions of these fixed-dimension components. Two examples from wearable products have been chosen to examine the entire workflow. Their final designs are realistic and their performance metrics are not inferior to the ones designed by experienced engineers. ",
        "title": "Image Classifier Based Generative Method for Planar Antenna Design",
        "date": "2023-12-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06150",
        "abstract_url": "http://arxiv.org/abs/2401.06150",
        "authors": [
            {
                "last_name": "Mourchid",
                "first_name": "Youssef"
            },
            {
                "last_name": "Slama",
                "first_name": "Rim"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our proposed approach on the KIMORE and UI-PRMD datasets highlighted its potential, surpassing state-of-the-art methods in terms of accuracy and computational time. This resulted in faster and more accurate learning and assessment of rehabilitation exercises. Additionally, our model provides valuable feedback through qualitative illustrations, effectively highlighting the significance of joints in specific exercises. ",
        "title": "D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on  transformer for assessment of patient physical rehabilitation",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06151",
        "abstract_url": "http://arxiv.org/abs/2401.06151",
        "authors": [
            {
                "last_name": "Morehead",
                "first_name": "Alex"
            },
            {
                "last_name": "Ruffolo",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Bhatnagar",
                "first_name": "Aadyot"
            },
            {
                "last_name": "Madani",
                "first_name": "Ali"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Generative models of macromolecules carry abundant and impactful implications for industrial and biomedical efforts in protein engineering. However, existing methods are currently limited to modeling protein structures or sequences, independently or jointly, without regard to the interactions that commonly occur between proteins and other macromolecules. In this work, we introduce MMDiff, a generative model that jointly designs sequences and structures of nucleic acid and protein complexes, independently or in complex, using joint SE(3)-discrete diffusion noise. Such a model has important implications for emerging areas of macromolecular design including structure-based transcription factor design and design of noncoding RNA sequences. We demonstrate the utility of MMDiff through a rigorous new design benchmark for macromolecular complex generation that we introduce in this work. Our results demonstrate that MMDiff is able to successfully generate micro-RNA and single-stranded DNA molecules while being modestly capable of joint modeling DNA and RNA molecules in interaction with multi-chain protein complexes. Source code: https://github.com/Profluent-Internships/MMDiff. ",
        "title": "Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein  Complexes with SE(3)-Discrete Diffusion",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06152",
        "abstract_url": "http://arxiv.org/abs/2401.06152",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Wonseok"
            },
            {
                "last_name": "Chong",
                "first_name": "Sanggyu"
            },
            {
                "last_name": "Kim",
                "first_name": "Jihan"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In this study, a versatile methodology for initiating polymerization from monomers in highly cross-linked materials is investigated. As polymerization progresses, force-field parameters undergo continuous modification due to the formation of new chemical bonds. This dynamic process not only impacts the atoms directly involved in bonding, but also influences the neighboring atomic environment. Monitoring these complex changes in highly cross-linked structures poses a challenge. To address this issue, we introduce a graph-network-based algorithm that offers both rapid and accurate predictions. The algorithm merges polymer construction protocols with LAMMPS, a large-scale molecular dynamics simulation software. The adaptability of this code has been demonstrated by its successful application to various amorphous polymers, including porous polymer networks (PPNs), and epoxy-resins, while the algorithm has been employed for additional tasks, such as implementing pore-piercing deformations and calculating material properties. ",
        "title": "Graph-Network-Based Predictive Modeling for Highly Cross-Linked Polymer  Systems",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06153",
        "abstract_url": "http://arxiv.org/abs/2401.06153",
        "authors": [
            {
                "last_name": "Yenin",
                "first_name": "Kemal Erdem"
            },
            {
                "last_name": "Sayin",
                "first_name": "Reha Oguz"
            },
            {
                "last_name": "Arar",
                "first_name": "Kuzey"
            },
            {
                "last_name": "Atalay",
                "first_name": "Kadir Kaan"
            },
            {
                "last_name": "Stroppa",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Multi-modal optimization is often encountered in engineering problems, especially when different and alternative solutions are sought. Evolutionary algorithms can efficiently tackle multi-modal optimization thanks to their features such as the concept of population, exploration/exploitation, and being suitable for parallel computation.   This paper introduces a multi-modal optimization version of the Big Bang-Big Crunch algorithm based on clustering, namely, k-BBBC. This algorithm guarantees a complete convergence of the entire population, retrieving on average the 99\\% of local optima for a specific problem. Additionally, we introduce two post-processing methods to (i) identify the local optima in a set of retrieved solutions (i.e., a population), and (ii) quantify the number of correctly retrieved optima against the expected ones (i.e., success rate).   Our results show that k-BBBC performs well even with problems having a large number of optima (tested on 379 optima) and high dimensionality (tested on 32 decision variables). When compared to other multi-modal optimization methods, it outperforms them in terms of accuracy (in both search and objective space) and success rate (number of correctly retrieved optima) -- especially when elitism is applied. Lastly, we validated our proposed post-processing methods by comparing their success rate to the actual one. Results suggest that these methods can be used to evaluate the performance of a multi-modal optimization algorithm by correctly identifying optima and providing an indication of success -- without the need to know where the optima are located in the search space. ",
        "title": "Multi-Modal Optimization with k-Cluster Big Bang-Big Crunch Algorithm",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06154",
        "abstract_url": "http://arxiv.org/abs/2401.06154",
        "authors": [
            {
                "last_name": "Verma",
                "first_name": "Rajat"
            },
            {
                "last_name": "Mittal",
                "first_name": "Shagun"
            },
            {
                "last_name": "Lei",
                "first_name": "Zengxiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaowei"
            },
            {
                "last_name": "Ukkusuri",
                "first_name": "Satish V."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "SI"
        ],
        "abstract": "  Estimation of people's home locations using location-based services data from smartphones is a common task in human mobility assessment. However, commonly used home detection algorithms (HDAs) are often arbitrary and unexamined. In this study, we review existing HDAs and examine five HDAs using eight high-quality mobile phone geolocation datasets. These include four commonly used HDAs as well as an HDA proposed in this work. To make quantitative comparisons, we propose three novel metrics to assess the quality of detected home locations and test them on eight datasets across four U.S. cities. We find that all three metrics show a consistent rank of HDAs' performances, with the proposed HDA outperforming the others. We infer that the temporal and spatial continuity of the geolocation data points matters more than the overall size of the data for accurate home detection. We also find that HDAs with high (and similar) performance metrics tend to create results with better consistency and closer to common expectations. Further, the performance deteriorates with decreasing data quality of the devices, though the patterns of relative performance persist. Finally, we show how the differences in home detection can lead to substantial differences in subsequent inferences using two case studies - (i) hurricane evacuation estimation, and (ii) correlation of mobility patterns with socioeconomic status. Our work contributes to improving the transparency of large-scale human mobility assessment applications. ",
        "title": "Comparison of home detection algorithms using smartphone GPS data",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06155",
        "abstract_url": "http://arxiv.org/abs/2401.06155",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiuyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Guoqing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "LG"
        ],
        "abstract": "  De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT. ",
        "title": "De novo Drug Design using Reinforcement Learning with Multiple GPT  Agents",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06156",
        "abstract_url": "http://arxiv.org/abs/2401.06156",
        "authors": [
            {
                "last_name": "Peleska",
                "first_name": "Jan"
            },
            {
                "last_name": "Br\u00fcning",
                "first_name": "Felix"
            },
            {
                "last_name": "Gleirscher",
                "first_name": "Mario"
            },
            {
                "last_name": "Huang",
                "first_name": "Wen-ling"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This technical report presents research results achieved in the field of verification of trained Convolutional Neural Network (CNN) used for image classification in safety-critical applications. As running example, we use the obstacle detection function needed in future autonomous freight trains with Grade of Automation (GoA) 4. It is shown that systems like GoA 4 freight trains are indeed certifiable today with new standards like ANSI/UL 4600 and ISO 21448 used in addition to the long-existing standards EN 50128 and EN 50129. Moreover, we present a quantitative analysis of the system-level hazard rate to be expected from an obstacle detection function. It is shown that using sensor/perceptor fusion, the fused detection system can meet the tolerable hazard rate deemed to be acceptable for the safety integrity level to be applied (SIL-3). A mathematical analysis of CNN models is performed which results in the identification of classification clusters and equivalence classes partitioning the image input space of the CNN. These clusters and classes are used to introduce a novel statistical testing method for determining the residual error probability of a trained CNN and an associated upper confidence limit. We argue that this greybox approach to CNN verification, taking into account the CNN model's internal structure, is essential for justifying that the statistical tests have covered the trained CNN with its neurons and inter-layer mappings in a comprehensive way. ",
        "title": "A Stochastic Approach to Classification Error Estimates in Convolutional  Neural Networks",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06157",
        "abstract_url": "http://arxiv.org/abs/2401.06157",
        "authors": [
            {
                "last_name": "Monari",
                "first_name": "Dennis"
            },
            {
                "last_name": "Larkin",
                "first_name": "Jack"
            },
            {
                "last_name": "Machado",
                "first_name": "Pedro"
            },
            {
                "last_name": "Bird",
                "first_name": "Jordan J."
            },
            {
                "last_name": "Ihianle",
                "first_name": "Isibor Kennedy"
            },
            {
                "last_name": "Yahaya",
                "first_name": "Salisu Wada"
            },
            {
                "last_name": "Tash",
                "first_name": "Farhad Fassihi"
            },
            {
                "last_name": "Hasan",
                "first_name": "Md Mahmudul"
            },
            {
                "last_name": "Lotfi",
                "first_name": "Ahmad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources and leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90% in certain English counties, making them highly susceptible to extinction. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and discarded plastics in the United Kingdom's river ecosystem's. The UDEEP platform can play a crucial role in environmental monitoring by performing on-the-fly classification of Signal crayfish and plastic debris while leveraging the efficacy of AI, IoT devices and the power of edge computing (i.e., NJN). By providing accurate data on the presence, spread and abundance of these species, the UDEEP platform can contribute to monitoring efforts and aid in mitigating the spread of invasive species. ",
        "title": "UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and  Plastic Detection",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06159",
        "abstract_url": "http://arxiv.org/abs/2401.06159",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Chanho"
            },
            {
                "last_name": "Son",
                "first_name": "Jinsu"
            },
            {
                "last_name": "Shon",
                "first_name": "Hyounguk"
            },
            {
                "last_name": "Jeon",
                "first_name": "Yunho"
            },
            {
                "last_name": "Kim",
                "first_name": "Junmo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets in the deformable convolution, thereby enhancing the existing advantages of spatial adaptation. Leveraging full rotation-equivariance, our FRED demonstrates higher robustness to image-level rotation compared to existing methods. Furthermore, we show that FRED is one step closer to non-axis aligned learning through our experiments. Compared to state-of-the-art methods, our proposed method delivers comparable performance on DOTA-v1.0 and outperforms by 1.5 mAP on DOTA-v1.5, all while significantly reducing the model parameters to 16%. ",
        "title": "FRED: Towards a Full Rotation-Equivariance in Aerial Image Object  Detection",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06160",
        "abstract_url": "http://arxiv.org/abs/2401.06160",
        "authors": [
            {
                "last_name": "Nitze",
                "first_name": "Andr\u00e9"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This study explores the impact of Large Language Models (LLMs) in higher education, focusing on an automated oral examination simulation using a prototype. The design considerations of the prototype are described, and the system is evaluated with a select group of educators and students. Technical and pedagogical observations are discussed. The prototype proved to be effective in simulating oral exams, providing personalized feedback, and streamlining educators' workloads. The promising results of the prototype show the potential for LLMs in democratizing education, inclusion of diverse student populations, and improvement of teaching quality and efficiency. ",
        "title": "Future-proofing Education: A Prototype for Simulating Oral Examinations  Using Large Language Models",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06161",
        "abstract_url": "http://arxiv.org/abs/2401.06161",
        "authors": [
            {
                "last_name": "Cabrera",
                "first_name": "Marcelino"
            },
            {
                "last_name": "Cruz",
                "first_name": "Carlos"
            },
            {
                "last_name": "Novoa-Hern\u00e1ndez",
                "first_name": "Pavel"
            },
            {
                "last_name": "Pelta",
                "first_name": "David A."
            },
            {
                "last_name": "Verdegay",
                "first_name": "Jos\u00e9 Luis"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Automated Decision-Making Systems (ADS) have become pervasive across various fields, activities, and occupations, to enhance performance. However, this widespread adoption introduces potential risks, including the misuse of ADS. Such misuse may manifest when ADS is employed in situations where it is unnecessary or when essential requirements, conditions, and terms are overlooked, leading to unintended consequences. This research paper presents a thorough examination of the implications, distinctions, and ethical considerations associated with digitalization, digital transformation, and the utilization of ADS in contemporary society and future contexts. Emphasis is placed on the imperative need for regulation, transparency, and ethical conduct in the deployment of ADS. ",
        "title": "Trustworthy human-centric based Automated Decision-Making Systems",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06162",
        "abstract_url": "http://arxiv.org/abs/2401.06162",
        "authors": [
            {
                "last_name": "Einarsson",
                "first_name": "Alexander"
            },
            {
                "last_name": "Oestmo",
                "first_name": "Simen"
            },
            {
                "last_name": "Wollman",
                "first_name": "Lester"
            },
            {
                "last_name": "Purves",
                "first_name": "Duncan"
            },
            {
                "last_name": "Jenkins",
                "first_name": "Ryan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  In recent years, there has been a revolution in data-driven policing. With that has come scrutiny on how bias in historical data affects algorithmic decision making. In this exploratory work, we introduce a debiasing technique for place-based algorithmic patrol management systems. We show that the technique efficiently eliminates racially biased features while retaining high accuracy in the models. Finally, we provide a lengthy list of potential future research in the realm of fairness and data-driven policing which this work uncovered. ",
        "title": "A debiasing technique for place-based algorithmic patrol management",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06164",
        "abstract_url": "http://arxiv.org/abs/2401.06164",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Lezhi"
            },
            {
                "last_name": "Chang",
                "first_name": "Ting-Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hai"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: https://github.com/Firenze11/finance_lm. ",
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "date": "2023-12-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06165",
        "abstract_url": "http://arxiv.org/abs/2401.06165",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Yifan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bingjie"
            },
            {
                "last_name": "Wu",
                "first_name": "Ke"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In this work, simulation-based equations to calculate propagation constant in uniform or periodic structures (SES) are deduced and verified through simulations in various types of structures. The modeling of those structures are essentially based on field distributions from a driven-mode solver, and the field distributions are used as the input parameters of the FPPS. It allows the separation of forward and backward waves from a total wave inside such a uniform or periodic structure, and thus it can be used to calculate the propagation constants inside both uniform and periodic structures even with a strong reflection. In order to test the performance and function of the FPPS, it has been applied to a variety of typical structures, including uniform waveguides, lossfree closed structures, lossy closed structures, and open radiation structures, and compared with the results of eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The comparison shows the easy-to-use and adaptable nature of the FPPS. the FPPS. This FPPS could be also applied to open radiating structures, and even multi-dimensional periodic/uniform structures. ",
        "title": "Simulation-Based Equations for Propagation Constant in Uniform or  Periodic Transmission",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06166",
        "abstract_url": "http://arxiv.org/abs/2401.06166",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Hao"
            },
            {
                "last_name": "Ye",
                "first_name": "Zeliang"
            },
            {
                "last_name": "Feng",
                "first_name": "Ruyi"
            },
            {
                "last_name": "Gu",
                "first_name": "Zhongze"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combined with generative molecular canonicalization pre-training tasks, enhances the validity, novelty, and uniqueness in generative tasks. These features of AdaMR demonstrate its strong performance in numerous downstream tasks. We use different molecular properties prediction tasks on six different datasets on MoleculeNet and two generative tasks on ZINC250K dataset to evaluate our proposed molecular encoding and pre-training methods, and obtain state-of-the-art (SOTA) results on five of these tasks. ",
        "title": "Adjustable Molecular Representation for Unified Pre-training Strategy",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06167",
        "abstract_url": "http://arxiv.org/abs/2401.06167",
        "authors": [
            {
                "last_name": "Che",
                "first_name": "Chang"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Yu",
                "first_name": "Liqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The process of transforming input images into corresponding textual explanations stands as a crucial and complex endeavor within the domains of computer vision and natural language processing. In this paper, we propose an innovative ensemble approach that harnesses the capabilities of Contrastive Language-Image Pretraining models. ",
        "title": "Enhancing Multimodal Understanding with CLIP-Based Image-to-Text  Transformation",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06168",
        "abstract_url": "http://arxiv.org/abs/2401.06168",
        "authors": [
            {
                "last_name": "Sonawane",
                "first_name": "Prathamesh"
            },
            {
                "last_name": "Chheda",
                "first_name": "Arav"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Poker is in the family of imperfect information games unlike other games such as chess, connect four, etc which are perfect information game instead. While many perfect information games have been solved, no non-trivial imperfect information game has been solved to date. This makes poker a great test bed for Artificial Intelligence research. In this paper we firstly compare Game theory optimal poker to Exploitative poker. Secondly, we discuss the intricacies of abstraction techniques, betting models, and specific strategies employed by successful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also explore 2-player vs multi-player games and the limitations that come when playing with more players. Finally, this paper discusses the role of machine learning and theoretical approaches in developing winning strategies and suggests future directions for this rapidly evolving field. ",
        "title": "A Survey on Game Theory Optimal Poker",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06169",
        "abstract_url": "http://arxiv.org/abs/2401.06169",
        "authors": [
            {
                "last_name": "Puget",
                "first_name": "Chlo\u00e9"
            },
            {
                "last_name": "Ganz",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Ostermaier",
                "first_name": "Julian"
            },
            {
                "last_name": "Konrad",
                "first_name": "Thomas"
            },
            {
                "last_name": "Parlak",
                "first_name": "Eda"
            },
            {
                "last_name": "Bertram",
                "first_name": "Christof Albert"
            },
            {
                "last_name": "Kiupel",
                "first_name": "Matti"
            },
            {
                "last_name": "Breininger",
                "first_name": "Katharina"
            },
            {
                "last_name": "Aubreville",
                "first_name": "Marc"
            },
            {
                "last_name": "Klopfleisch",
                "first_name": "Robert"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Numerous prognostic factors are currently assessed histopathologically in biopsies of canine mast cell tumors to evaluate clinical behavior. In addition, PCR analysis of the c-Kit exon 11 mutational status is often performed to evaluate the potential success of a tyrosine kinase inhibitor therapy. This project aimed at training deep learning models (DLMs) to identify the c-Kit-11 mutational status of MCTs solely based on morphology without additional molecular analysis. HE slides of 195 mutated and 173 non-mutated tumors were stained consecutively in two different laboratories and scanned with three different slide scanners. This resulted in six different datasets (stain-scanner variations) of whole slide images. DLMs were trained with single and mixed datasets and their performances was assessed under scanner and staining domain shifts. The DLMs correctly classified HE slides according to their c-Kit 11 mutation status in, on average, 87% of cases for the best-suited stain-scanner variant. A relevant performance drop could be observed when the stain-scanner combination of the training and test dataset differed. Multi-variant datasets improved the average accuracy but did not reach the maximum accuracy of algorithms trained and tested on the same stain-scanner variant. In summary, DLM-assisted morphological examination of MCTs can predict c-Kit-exon 11 mutational status of MCTs with high accuracy. However, the recognition performance is impeded by a change of scanner or staining protocol. Larger data sets with higher numbers of scans originating from different laboratories and scanners may lead to more robust DLMs to identify c-Kit mutations in HE slides. ",
        "title": "Deep Learning model predicts the c-Kit-11 mutational status of canine  cutaneous mast cell tumors by HE stained histological slides",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06171",
        "abstract_url": "http://arxiv.org/abs/2401.06171",
        "authors": [
            {
                "last_name": "Gikunda",
                "first_name": "Kinyua"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper explores the transformative potential of artificial intelligence (AI) in the context of sustainable agricultural development across diverse regions in Africa. Delving into opportunities, challenges, and impact, the study navigates through the dynamic landscape of AI applications in agriculture. Opportunities such as precision farming, crop monitoring, and climate-resilient practices are examined, alongside challenges related to technological infrastructure, data accessibility, and skill gaps. The article analyzes the impact of AI on smallholder farmers, supply chains, and inclusive growth. Ethical considerations and policy implications are also discussed, offering insights into responsible AI integration. By providing a nuanced understanding, this paper contributes to the ongoing discourse on leveraging AI for fostering sustainability in African agriculture. ",
        "title": "Harnessing Artificial Intelligence for Sustainable Agricultural  Development in Africa: Opportunities, Challenges, and Impact",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06172",
        "abstract_url": "http://arxiv.org/abs/2401.06172",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yue"
            },
            {
                "last_name": "Andrew",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Supasanya",
                "first_name": "Salintip"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events. ",
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine  Learning Methods",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06173",
        "abstract_url": "http://arxiv.org/abs/2401.06173",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Hui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jinghong"
            },
            {
                "last_name": "Chen",
                "first_name": "Wentao"
            },
            {
                "last_name": "Wang",
                "first_name": "Huazheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengdi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  While modern biotechnologies allow synthesizing new proteins and function measurements at scale, efficiently exploring a protein sequence space and engineering it remains a daunting task due to the vast sequence space of any given protein. Protein engineering is typically conducted through an iterative process of adding mutations to the wild-type or lead sequences, recombination of mutations, and running new rounds of screening. To enhance the efficiency of such a process, we propose a tree search-based bandit learning method, which expands a tree starting from the initial sequence with the guidance of a bandit machine learning model. Under simplified assumptions and a Gaussian Process prior, we provide theoretical analysis and a Bayesian regret bound, demonstrating that the combination of local search and bandit learning method can efficiently discover a near-optimal design. The full algorithm is compatible with a suite of randomized tree search heuristics, machine learning models, pre-trained embeddings, and bandit techniques. We test various instances of the algorithm across benchmark protein datasets using simulated screens. Experiment results demonstrate that the algorithm is both sample-efficient and able to find top designs using reasonably small mutation counts. ",
        "title": "Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06174",
        "abstract_url": "http://arxiv.org/abs/2401.06174",
        "authors": [
            {
                "last_name": "Ghezelbash",
                "first_name": "Farshid"
            },
            {
                "last_name": "Eskandari",
                "first_name": "Amir Hossein"
            },
            {
                "last_name": "Robert-Lachaine",
                "first_name": "Xavier"
            },
            {
                "last_name": "Cao",
                "first_name": "Frank"
            },
            {
                "last_name": "Pesteie",
                "first_name": "Mehran"
            },
            {
                "last_name": "Qiao",
                "first_name": "Zhuohua"
            },
            {
                "last_name": "Shirazi-Adl",
                "first_name": "Aboulfazl"
            },
            {
                "last_name": "Larivi\u00e8re",
                "first_name": "Christian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Spine biomechanics is at a transformation with the advent and integration of machine learning and computer vision technologies. These novel techniques facilitate the estimation of 3D body shapes, anthropometrics, and kinematics from as simple as a single-camera image, making them more accessible and practical for a diverse range of applications. This study introduces a framework that merges these methodologies with traditional musculoskeletal modeling, enabling comprehensive analysis of spinal biomechanics during complex activities from a single camera. Additionally, we aim to evaluate their performance and limitations in spine biomechanics applications. The real-world applications explored in this study include assessment in workplace lifting, evaluation of whiplash injuries in car accidents, and biomechanical analysis in professional sports. Our results demonstrate potential and limitations of various algorithms in estimating body shape, kinematics, and conducting in-field biomechanical analyses. In industrial settings, the potential to utilize these new technologies for biomechanical risk assessments offers a pathway for preventive measures against back injuries. In sports activities, the proposed framework provides new opportunities for performance optimization, injury prevention, and rehabilitation. The application in forensic domain further underscores the wide-reaching implications of this technology. While certain limitations were identified, particularly in accuracy of predictions, complex interactions, and external load estimation, this study demonstrates their potential for advancement in spine biomechanics, heralding an optimistic future in both research and practical applications. ",
        "title": "Machine Learning Applications in Spine Biomechanics",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06175",
        "abstract_url": "http://arxiv.org/abs/2401.06175",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jinyang"
            },
            {
                "last_name": "Gu",
                "first_name": "Wenwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuangbin"
            },
            {
                "last_name": "Li",
                "first_name": "Yichen"
            },
            {
                "last_name": "Su",
                "first_name": "Yuxin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Michael R."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Key Performance Indicators (KPIs) are essential time-series metrics for ensuring the reliability and stability of many software systems. They faithfully record runtime states to facilitate the understanding of anomalous system behaviors and provide informative clues for engineers to pinpoint the root causes. The unprecedented scale and complexity of modern software systems, however, make the volume of KPIs explode. Consequently, many traditional methods of KPI anomaly detection become impractical, which serves as a catalyst for the fast development of machine learning-based solutions in both academia and industry. However, there is currently a lack of rigorous comparison among these KPI anomaly detection methods, and re-implementation demands a non-trivial effort. Moreover, we observe that different works adopt independent evaluation processes with different metrics. Some of them may not fully reveal the capability of a model and some are creating an illusion of progress. To better understand the characteristics of different KPI anomaly detectors and address the evaluation issue, in this paper, we provide a comprehensive review and evaluation of twelve state-of-the-art methods, and propose a novel metric called salience. Particularly, the selected methods include five traditional machine learning-based methods and seven deep learning-based methods. These methods are evaluated with five multivariate KPI datasets that are publicly available. A unified toolkit with easy-to-use interfaces is also released. We report the benchmark results in terms of accuracy, salience, efficiency, and delay, which are of practical importance for industrial deployment. We believe our work can contribute as a basis for future academic research and industrial application. ",
        "title": "MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06176",
        "abstract_url": "http://arxiv.org/abs/2401.06176",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Luzhi"
            },
            {
                "last_name": "He",
                "first_name": "Dongxiao"
            },
            {
                "last_name": "Zhang",
                "first_name": "He"
            },
            {
                "last_name": "Liu",
                "first_name": "Yixin"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Pan",
                "first_name": "Shirui"
            },
            {
                "last_name": "Jin",
                "first_name": "Di"
            },
            {
                "last_name": "Chua",
                "first_name": "Tat-Seng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets. The code is available at Github: https://github.com/Ee1s/GOODAT ",
        "title": "GOODAT: Towards Test-time Graph Out-of-Distribution Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06178",
        "abstract_url": "http://arxiv.org/abs/2401.06178",
        "authors": [
            {
                "last_name": "Goetze",
                "first_name": "Trystan S."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Since the launch of applications such as DALL-E, Midjourney, and Stable Diffusion, generative artificial intelligence has been controversial as a tool for creating artwork. While some have presented longtermist worries about these technologies as harbingers of fully automated futures to come, more pressing is the impact of generative AI on creative labour in the present. Already, business leaders have begun replacing human artistic labour with AI-generated images. In response, the artistic community has launched a protest movement, which argues that AI image generation is a kind of theft. This paper analyzes, substantiates, and critiques these arguments, concluding that AI image generators involve an unethical kind of labour theft. If correct, many other AI applications also rely upon theft. ",
        "title": "AI Art is Theft: Labour, Extraction, and Exploitation, Or, On the  Dangers of Stochastic Pollocks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06179",
        "abstract_url": "http://arxiv.org/abs/2401.06179",
        "authors": [
            {
                "last_name": "Montazeri",
                "first_name": "Sina"
            },
            {
                "last_name": "Mirzaeinia",
                "first_name": "Akram"
            },
            {
                "last_name": "Jumakhan",
                "first_name": "Haseebullah"
            },
            {
                "last_name": "Mirzaeinia",
                "first_name": "Amir"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards. ",
        "title": "CNN-DRL for Scalable Actions in Finance",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06180",
        "abstract_url": "http://arxiv.org/abs/2401.06180",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jingyun"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yading"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  Federated learning (FL) has emerged as a promising strategy for collaboratively training complicated machine learning models from different medical centers without the need of data sharing. However, the traditional FL relies on a central server to orchestrate the global model training among clients. This makes it vulnerable to the failure of the model server. Meanwhile, the model trained based on the global data property may not yield the best performance on the local data of a particular site due to the variations of data characteristics among them. To address these limitations, we proposed Gossip Mutual Learning(GML), a decentralized collaborative learning framework that employs Gossip Protocol for direct peer-to-peer communication and encourages each site to optimize its local model by leveraging useful information from peers through mutual learning. On the task of tumor segmentation on PET/CT images using HECKTOR21 dataset with 223 cases from five clinical sites, we demonstrated GML could improve tumor segmentation performance in terms of Dice Similarity Coefficient (DSC) by 3.2%, 4.6% and 10.4% on site-specific testing cases as compared to three baseline methods: pooled training, FedAvg and individual training, respectively. We also showed GML has comparable generalization performance as pooled training and FedAvg when applying them on 78 cases from two out-of-sample sites where no case was used for model training. In our experimental setup, GML showcased a sixfold decrease in communication overhead compared to FedAvg, requiring only 16.67% of the total communication overhead. ",
        "title": "Decentralized Gossip Mutual Learning (GML) for automatic head and neck  tumor segmentation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06182",
        "abstract_url": "http://arxiv.org/abs/2401.06182",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Baiyang"
            },
            {
                "last_name": "Yang",
                "first_name": "Jiamin"
            },
            {
                "last_name": "Shroff",
                "first_name": "Hari"
            },
            {
                "last_name": "La Riviere",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Determining cell identities in imaging sequences is an important yet challenging task. The conventional method for cell identification is via cell tracking, which is complex and can be time-consuming. In this study, we propose an innovative approach to cell identification during early C. elegans embryogenesis using machine learning. We employed random forest, MLP, and LSTM models, and tested cell classification accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of embryogenesis. By leveraging a small number of spatial-temporal features of individual cells, including cell trajectory and cell fate information, our models achieve an accuracy of over 90%, even with limited data. We also determine the most important feature contributions and can interpret these features in the context of biological knowledge. Our research demonstrates the success of predicting cell identities in 4D imaging sequences directly from simple spatio-temporal features. ",
        "title": "Prediction of Cellular Identities from Trajectory and Cell Fate  Information",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06183",
        "abstract_url": "http://arxiv.org/abs/2401.06183",
        "authors": [
            {
                "last_name": "Tathe",
                "first_name": "Aniket"
            },
            {
                "last_name": "Kamble",
                "first_name": "Anand"
            },
            {
                "last_name": "Kumbharkar",
                "first_name": "Suyash"
            },
            {
                "last_name": "Bhandare",
                "first_name": "Atharva"
            },
            {
                "last_name": "Mitra",
                "first_name": "Anirban C."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio. ",
        "title": "End to end Hindi to English speech conversion using Bark, mBART and a  finetuned XLSR Wav2Vec2",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06184",
        "abstract_url": "http://arxiv.org/abs/2401.06184",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Wu",
                "first_name": "Yanan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we construct a new family of distance-optimal binary cyclic codes with the minimum distance $6$ and a new family of distance-optimal quaternary cyclic codes with the minimum distance $4$. We also construct several families of cyclic and negacyclic codes over ${\\bf F}_2$, ${\\bf F}_3$, ${\\bf F}_4$, ${\\bf F}_5$, ${\\bf F}_7$ and ${\\bf F}_9$ with good parameters $n,\\,k,\\,d$, such that the maximal possible minimum distance $d_{max}$ of a linear $[n, k]_q$ code is at most $d_{max} \\leq d+8$. The first codes in these families have optimal or best known minimum distances. $145$ optimal or best known codes are constructed as cyclic codes, negacyclic codes, their shortening codes and punctured codes. All optimal or best known codes constructed in this paper are not equivalent to the presently best known codes. Several infinite families of negacyclic $[n,\\frac{n+1}{2}, d]_q$ codes or $[n, \\frac{n}{2}, d]_q$ codes, such that their minimum distances satisfy $d\\approx O(\\frac{n}{\\log_q n})$, are also constructed. These are first several families of such negacyclic codes. ",
        "title": "Cyclic and Negacyclic Codes with Optimal or Best Known Minimum Distances",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06187",
        "abstract_url": "http://arxiv.org/abs/2401.06187",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jing"
            },
            {
                "last_name": "Harandi",
                "first_name": "Mehrtash"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve information on the remaining data while discarding information related to the forgetting data. Our experimental results, conducted across five distinct datasets and utilizing both CNN and ViT, demonstrate that Scissorhands, despite utilizing only a limited portion of the training data, showcases competitive performance when compared to existing methods. ",
        "title": "Scissorhands: Scrub Data Influence via Connection Sensitivity in  Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06188",
        "abstract_url": "http://arxiv.org/abs/2401.06188",
        "authors": [
            {
                "last_name": "Vallero",
                "first_name": "Marzio"
            },
            {
                "last_name": "Vella",
                "first_name": "Flavio"
            },
            {
                "last_name": "Rech",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The frontier of quantum computing (QC) simulation on classical hardware is quickly reaching the hard scalability limits for computational feasibility. Nonetheless, there is still a need to simulate large quantum systems classically, as the Noisy Intermediate Scale Quantum (NISQ) devices are yet to be considered fault tolerant and performant enough in terms of operations per second. Each of the two main exact simulation techniques, state vector and tensor network simulators, boasts specific limitations. The exponential memory requirement of state vector simulation, when compared to the qubit register sizes of currently available quantum computers, quickly saturates the capacity of the top HPC machines currently available. Tensor network contraction approaches, which encode quantum circuits into tensor networks and then contract them over an output bit string to obtain its probability amplitude, still fall short of the inherent complexity of finding an optimal contraction path, which maps to a max-cut problem on a dense mesh, a notably NP-hard problem.   This article aims at investigating the limits of current state-of-the-art simulation techniques on a test bench made of eight widely used quantum subroutines, each in 31 different configurations, with special emphasis on performance. We then correlate the performance measures of the simulators with the metrics that characterise the benchmark circuits, identifying the main reasons behind the observed performance trend. From our observations, given the structure of a quantum circuit and the number of qubits, we highlight how to select the best simulation strategy, obtaining a speedup of up to an order of magnitude. ",
        "title": "State of practice: evaluating GPU performance of state vector and tensor  network methods",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06191",
        "abstract_url": "http://arxiv.org/abs/2401.06191",
        "authors": [
            {
                "last_name": "Khatib",
                "first_name": "Rajaei"
            },
            {
                "last_name": "Giryes",
                "first_name": "Raja"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, the neural radiance field (NeRF) model has gained popularity due to its ability to recover complex 3D scenes. Following its success, many approaches proposed different NeRF representations in order to further improve both runtime and performance. One such example is Triplane, in which NeRF is represented using three 2D feature planes. This enables easily using existing 2D neural networks in this framework, e.g., to generate the three planes. Despite its advantage, the triplane representation lagged behind in its 3D recovery quality compared to NeRF solutions. In this work, we propose TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF, which closes the 3D recovery performance gap and is competitive with current state-of-the-art methods. Building upon the triplane framework, we also propose a novel super-resolution (SR) technique that combines a diffusion model with TriNeRFLet for improving NeRF resolution. ",
        "title": "TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06192",
        "abstract_url": "http://arxiv.org/abs/2401.06192",
        "authors": [
            {
                "last_name": "Christensen",
                "first_name": "Kristoffer"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng Grace"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Electric vehicles are expected to significantly contribute to CO2-eq. emissions reduction, but the increasing number of EVs also introduces chal-lenges to the energy system, and to what extent it contributes to achieving cli-mate goals remains unknown. Static modeling and assumption-based simula-tions have been used for such investigation, but they cannot capture the realistic ecosystem dynamics. To fill the gap, this paper investigates the impacts of two adoption curves of private EVs on the electricity distribution grids and national climate goals. This paper develops a multi-agent based simulation with two adoption curves, the Traditional EV charging strategy, various EV models, driv-ing patterns, and CO2-eq. emission data to capture the full ecosystem dynamics during a long-term period from 2020 to 2032. The Danish 2030 climate goal and a Danish distribution network with 126 residential consumers are chosen as the case study. The results show that both EV adoption curves of 1 million and 775k EVs by 2030 will not satisfy the Danish climate goal of reducing transport sector emissions by 30% by 2030. The results also show that the current resi-dential electricity distribution grids cannot handle the load from increasing EVs. The first grid overload will occur in 2031 (around 16 and 24 months later for the 1 million and 775k EVs adopted by 2030) with a 67% share of EVs in the grid. ",
        "title": "Multi-Agent Based Simulation for Investigating Electric Vehicle Adoption  and Its Impacts on Electricity Distribution Grids and CO2 Emissions",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06194",
        "abstract_url": "http://arxiv.org/abs/2401.06194",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Shubham"
            },
            {
                "last_name": "Saini",
                "first_name": "Nandini"
            },
            {
                "last_name": "Kundu",
                "first_name": "Suman"
            },
            {
                "last_name": "Das",
                "first_name": "Debasis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction algorithm. Along with this, a guided cross-attention module is implemented to fill the semantic gap in integrating visual and textual data. In order to ensure reliability, we employ a model-specific approach called Gradient-weighted Class Activation Mapping (Grad-CAM) that provides a robust explanation of the predictions of the proposed model. The comprehensive experiments conducted on the CrisisMMD dataset yield in-depth analysis across various crisis-specific tasks and settings. As a result, CrisisKAN outperforms existing SOTA methodologies and provides a novel view in the domain of explainable multimodal event classification. ",
        "title": "CrisisKAN: Knowledge-infused and Explainable Multimodal Attention  Network for Crisis Event Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06195",
        "abstract_url": "http://arxiv.org/abs/2401.06195",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            },
            {
                "last_name": "Danouchi",
                "first_name": "Kamal"
            },
            {
                "last_name": "Prenat",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Anghel",
                "first_name": "Lorena"
            },
            {
                "last_name": "Tahoori",
                "first_name": "Mehdi B."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG",
            "NE"
        ],
        "abstract": "  Internet of Things (IoT) and smart wearable devices for personalized healthcare will require storing and computing ever-increasing amounts of data. The key requirements for these devices are ultra-low-power, high-processing capabilities, autonomy at low cost, as well as reliability and accuracy to enable Green AI at the edge. Artificial Intelligence (AI) models, especially Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges with traditional computing architectures due to the memory wall problem. Computing-in-Memory (CIM) with emerging resistive memories offers a solution by combining memory blocks and computing units for higher efficiency and lower power consumption. However, implementing BayNNs on CIM hardware, particularly with spintronic technologies, presents technical challenges due to variability and manufacturing defects. The NeuSPIN project aims to address these challenges through full-stack hardware and software co-design, developing novel algorithmic and circuit design approaches to enhance the performance, energy-efficiency and robustness of BayNNs on sprintronic-based CIM platforms. ",
        "title": "NeuSpin: Design of a Reliable Edge Neuromorphic System Based on  Spintronics for Green AI",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06196",
        "abstract_url": "http://arxiv.org/abs/2401.06196",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Cao",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Liao",
                "first_name": "Fei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weiwei"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Physics-informed neural networks (PINNs) have shown remarkable prospects in the solving the forward and inverse problems involving partial differential equations (PDEs). The method embeds PDEs into the neural network by calculating PDE loss at a series of collocation points, providing advantages such as meshfree and more convenient adaptive sampling. However, when solving PDEs using nonuniform collocation points, PINNs still face challenge regarding inefficient convergence of PDE residuals or even failure. In this work, we first analyze the ill-conditioning of the PDE loss in PINNs under nonuniform collocation points. To address the issue, we define volume-weighted residual and propose volume-weighted physics-informed neural networks (VW-PINNs). Through weighting the PDE residuals by the volume that the collocation points occupy within the computational domain, we embed explicitly the spatial distribution characteristics of collocation points in the residual evaluation. The fast and sufficient convergence of the PDE residuals for the problems involving nonuniform collocation points is guaranteed. Considering the meshfree characteristics of VW-PINNs, we also develop a volume approximation algorithm based on kernel density estimation to calculate the volume of the collocation points. We verify the universality of VW-PINNs by solving the forward problems involving flow over a circular cylinder and flow over the NACA0012 airfoil under different inflow conditions, where conventional PINNs fail; By solving the Burgers' equation, we verify that VW-PINNs can enhance the efficiency of existing the adaptive sampling method in solving the forward problem by 3 times, and can reduce the relative error of conventional PINNs in solving the inverse problem by more than one order of magnitude. ",
        "title": "VW-PINNs: A volume weighting method for PDE residuals in  physics-informed neural networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06197",
        "abstract_url": "http://arxiv.org/abs/2401.06197",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Yuwen"
            },
            {
                "last_name": "Li",
                "first_name": "Zhiqi"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuntao"
            },
            {
                "last_name": "Wang",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xizhou"
            },
            {
                "last_name": "Luo",
                "first_name": "Jiapeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenhai"
            },
            {
                "last_name": "Lu",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Hongsheng"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            },
            {
                "last_name": "Lu",
                "first_name": "Lewei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jie"
            },
            {
                "last_name": "Dai",
                "first_name": "Jifeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models. ",
        "title": "Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator  for Vision Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06199",
        "abstract_url": "http://arxiv.org/abs/2401.06199",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Bo"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Li",
                "first_name": "Pan"
            },
            {
                "last_name": "Geng",
                "first_name": "Yangli-ao"
            },
            {
                "last_name": "Gong",
                "first_name": "Jing"
            },
            {
                "last_name": "Li",
                "first_name": "Shen"
            },
            {
                "last_name": "Bei",
                "first_name": "Zhilei"
            },
            {
                "last_name": "Tan",
                "first_name": "Xu"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Chiming"
            },
            {
                "last_name": "Zeng",
                "first_name": "Aohan"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuxiao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jie"
            },
            {
                "last_name": "Song",
                "first_name": "Le"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can perform programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contributing to the evolving landscape of foundation models in protein science. ",
        "title": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering  the Language of Protein",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06201",
        "abstract_url": "http://arxiv.org/abs/2401.06201",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Siyu"
            },
            {
                "last_name": "Song",
                "first_name": "Kaitao"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiangjie"
            },
            {
                "last_name": "Tan",
                "first_name": "Xu"
            },
            {
                "last_name": "Shen",
                "first_name": "Yongliang"
            },
            {
                "last_name": "Kan",
                "first_name": "Ren"
            },
            {
                "last_name": "Li",
                "first_name": "Dongsheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Deqing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at \\url{https://github.com/microsoft/JARVIS/} in the future. ",
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06203",
        "abstract_url": "http://arxiv.org/abs/2401.06203",
        "authors": [
            {
                "last_name": "Daly",
                "first_name": "Matthew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD"
        ],
        "abstract": "  This paper introduces our system submission for the Cadenza ICASSP 2024 Grand Challenge, which presents the problem of remixing and enhancing music for hearing aid users. Our system placed first in the challenge, achieving the best average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data set. We describe the system, which uses an ensemble of deep learning music source separators that are fine tuned on the challenge data. We demonstrate the effectiveness of our system through the challenge results and analyze the importance of different system aspects through ablation studies. ",
        "title": "Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source  Separators",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06204",
        "abstract_url": "http://arxiv.org/abs/2401.06204",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Qilei"
            },
            {
                "last_name": "Mott",
                "first_name": "John H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Large Language Models (LLMs) hold transformative potential in aviation, particularly in reconstructing flight trajectories. This paper investigates this potential, grounded in the notion that LLMs excel at processing sequential data and deciphering complex data structures. Utilizing the LLaMA 2 model, a pre-trained open-source LLM, the study focuses on reconstructing flight trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with irregularities inherent in real-world scenarios. The findings demonstrate the model's proficiency in filtering noise and estimating both linear and curved flight trajectories. However, the analysis also reveals challenges in managing longer data sequences, which may be attributed to the token length limitations of LLM models. The study's insights underscore the promise of LLMs in flight trajectory reconstruction and open new avenues for their broader application across the aviation and transportation sectors. ",
        "title": "An Exploratory Assessment of LLM's Potential Toward Flight Trajectory  Reconstruction Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06205",
        "abstract_url": "http://arxiv.org/abs/2401.06205",
        "authors": [
            {
                "last_name": "Smith",
                "first_name": "D. Hudson"
            },
            {
                "last_name": "Ehrett",
                "first_name": "Carl"
            },
            {
                "last_name": "Warren",
                "first_name": "Patrick L."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  This paper introduces and tests an unsupervised method for detecting novel coordinated inauthentic information operations (CIOs) in realistic settings. This method uses Bayesian inference to identify groups of accounts that share similar account-level characteristics and target similar narratives. We solve the inferential problem using amortized variational inference, allowing us to efficiently infer group identities for millions of accounts. We validate this method using a set of five CIOs from three countries discussing four topics on Twitter. Our unsupervised approach increases detection power (area under the precision-recall curve) relative to a naive baseline (by a factor of 76 to 580), relative to the use of simple flags or narratives on their own (by a factor of 1.3 to 4.8), and comes quite close to a supervised benchmark. Our method is robust to observing only a small share of messaging on the topic, having only weak markers of inauthenticity, and to the CIO accounts making up a tiny share of messages and accounts on the topic. Although we evaluate the results on Twitter, the method is general enough to be applied in many social-media settings. ",
        "title": "Unsupervised detection of coordinated information operations in the wild",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06207",
        "abstract_url": "http://arxiv.org/abs/2401.06207",
        "authors": [
            {
                "last_name": "Campos",
                "first_name": "Beatriz"
            },
            {
                "last_name": "Canela",
                "first_name": "Jordi"
            },
            {
                "last_name": "Rodr\u00edguez-Arenas",
                "first_name": "Alberto"
            },
            {
                "last_name": "Vindel",
                "first_name": "Pura"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we present an algorithm to obtain the parameter planes of families of root-finding methods with several free critical points. The parameter planes show the joint behaviour of all critical points. This algorithm avoids the inconsistencies arising from the relationship between the different critical points as well as the indeterminacy caused by the square roots involved in their computation.   We analyse the suitability of this algorithm by drawing the parameter planes of different Newton-like methods with two and three critical points. We also present some results of the expressions of the Newton-like operators and their derivatives in terms of palindromic polynomials, and we show how to obtain the expression of the critical points of a Newton-like method with real coefficients. ",
        "title": "Computing parameter planes of iterative root-finding methods with  several free critical points",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06209",
        "abstract_url": "http://arxiv.org/abs/2401.06209",
        "authors": [
            {
                "last_name": "Tong",
                "first_name": "Shengbang"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Zhai",
                "first_name": "Yuexiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Yi"
            },
            {
                "last_name": "LeCun",
                "first_name": "Yann"
            },
            {
                "last_name": "Xie",
                "first_name": "Saining"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems. ",
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06210",
        "abstract_url": "http://arxiv.org/abs/2401.06210",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Hao-Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Pu-Jen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "IR"
        ],
        "abstract": "  Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin. ",
        "title": "Learning Unsupervised Semantic Document Representation for Fine-grained  Aspect-based Sentiment Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06224",
        "abstract_url": "http://arxiv.org/abs/2401.06224",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinyuan"
            },
            {
                "last_name": "Pan",
                "first_name": "Chengwei"
            },
            {
                "last_name": "Dai",
                "first_name": "Hongming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Gangming"
            },
            {
                "last_name": "Li",
                "first_name": "Jinpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Yu",
                "first_name": "Yizhou"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Coronary microvascular disease constitutes a substantial risk to human health. Employing computer-aided analysis and diagnostic systems, medical professionals can intervene early in disease progression, with 3D vessel segmentation serving as a crucial component. Nevertheless, conventional U-Net architectures tend to yield incoherent and imprecise segmentation outcomes, particularly for small vessel structures. While models with attention mechanisms, such as Transformers and large convolutional kernels, demonstrate superior performance, their extensive computational demands during training and inference lead to increased time complexity. In this study, we leverage Fourier domain learning as a substitute for multi-scale convolutional kernels in 3D hierarchical segmentation models, which can reduce computational expenses while preserving global receptive fields within the network. Furthermore, a zero-parameter frequency domain fusion method is designed to improve the skip connections in U-Net architecture. Experimental results on a public dataset and an in-house dataset indicate that our novel Fourier transformation-based network achieves remarkable dice performance (84.37\\% on ASACA500 and 80.32\\% on ImageCAS) in tubular vessel segmentation tasks and substantially reduces computational requirements without compromising global receptive fields. ",
        "title": "Leveraging Frequency Domain Learning in 3D Vessel Segmentation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06226",
        "abstract_url": "http://arxiv.org/abs/2401.06226",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Yanying"
            },
            {
                "last_name": "Garcke",
                "first_name": "Jochen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios. ",
        "title": "Learning Crowd Behaviors in Navigation with Attention-based  Spatial-Temporal Graphs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06230",
        "abstract_url": "http://arxiv.org/abs/2401.06230",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Ziyi"
            },
            {
                "last_name": "Orozco",
                "first_name": "Rafael"
            },
            {
                "last_name": "Louboutin",
                "first_name": "Mathias"
            },
            {
                "last_name": "Herrmann",
                "first_name": "Felix J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a probabilistic technique for full-waveform inversion, employing variational inference and conditional normalizing flows to quantify uncertainty in migration-velocity models and its impact on imaging. Our approach integrates generative artificial intelligence with physics-informed common-image gathers, reducing reliance on accurate initial velocity models. Considered case studies demonstrate its efficacy producing realizations of migration-velocity models conditioned by the data. These models are used to quantify amplitude and positioning effects during subsequent imaging. ",
        "title": "WISE: full-Waveform variational Inference via Subsurface Extensions",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06233",
        "abstract_url": "http://arxiv.org/abs/2401.06233",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Shruti"
            },
            {
                "last_name": "Alam",
                "first_name": "Shoaib"
            },
            {
                "last_name": "Singh",
                "first_name": "Mayank"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c . ",
        "title": "LEGOBench: Leaderboard Generation Benchmark for Scientific Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06234",
        "abstract_url": "http://arxiv.org/abs/2401.06234",
        "authors": [
            {
                "last_name": "Bertossi",
                "first_name": "Leopoldo"
            },
            {
                "last_name": "Kimelfeld",
                "first_name": "Benny"
            },
            {
                "last_name": "Livshits",
                "first_name": "Ester"
            },
            {
                "last_name": "Monet",
                "first_name": "Mika\u00ebl"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  Attribution scores can be applied in data management to quantify the contribution of individual items to conclusions from the data, as part of the explanation of what led to these conclusions. In Artificial Intelligence, Machine Learning, and Data Management, some of the common scores are deployments of the Shapley value, a formula for profit sharing in cooperative game theory. Since its invention in the 1950s, the Shapley value has been used for contribution measurement in many fields, from economics to law, with its latest researched applications in modern machine learning. Recent studies investigated the application of the Shapley value to database management. This article gives an overview of recent results on the computational complexity of the Shapley value for measuring the contribution of tuples to query answers and to the extent of inconsistency with respect to integrity constraints. More specifically, the article highlights lower and upper bounds on the complexity of calculating the Shapley value, either exactly or approximately, as well as solutions for realizing the calculation in practice. ",
        "title": "The Shapley Value in Database Management",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06238",
        "abstract_url": "http://arxiv.org/abs/2401.06238",
        "authors": [
            {
                "last_name": "Conni",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Piccardo",
                "first_name": "Stefano"
            },
            {
                "last_name": "Perotto",
                "first_name": "Simona"
            },
            {
                "last_name": "Porta",
                "first_name": "Giovanni Michele"
            },
            {
                "last_name": "Icardi",
                "first_name": "Matteo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a new model reduction technique for multiscale scalar transport problems that exhibit dominant axial dynamics. To this aim, we rely on the separation of variables to combine a Hierarchical Model (HiMod) reduction with a two-scale asymptotic expansion. We extend the two-scale asymptotic expansion to an arbitrary order and exploit the high-order correctors to define the HiMod modal basis, which approximates the transverse dynamics of the flow, while we adopt a finite element discretisation to model the leading stream. The resulting method, which is named HiPhom$\\varepsilon$ (HIgh-order Projection-based HOMogEnisation), is successfully assessed both in steady and unsteady advection-diffusion-reaction settings. The numerical results confirm the very good performance of HiPhom$\\varepsilon$, which improves the accuracy and the convergence rate of HiMod and extends the reliability of the standard homogenised solution to transient and pre-asymptotic regimes. ",
        "title": "HiPhom$\\varepsilon$ -: HIgh order Projection-based HOMogenisation for  advection diffusion reaction problems",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06240",
        "abstract_url": "http://arxiv.org/abs/2401.06240",
        "authors": [
            {
                "last_name": "Low",
                "first_name": "Guang Hao"
            },
            {
                "last_name": "Su",
                "first_name": "Yuan"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Many problems in linear algebra -- such as those arising from non-Hermitian physics and differential equations -- can be solved on a quantum computer by processing eigenvalues of the non-normal input matrices. However, the existing Quantum Singular Value Transformation (QSVT) framework is ill-suited to this task, as eigenvalues and singular values are different in general. We present a Quantum EigenValue Transformation (QEVT) framework for applying arbitrary polynomial transformations on eigenvalues of block-encoded non-normal operators, and a related Quantum EigenValue Estimation (QEVE) algorithm for operators with real spectra. QEVT has query complexity to the block encoding nearly recovering that of the QSVT for a Hermitian input, and QEVE achieves the Heisenberg-limited scaling for diagonalizable input matrices. As applications, we develop a linear differential equation solver with strictly linear time query complexity for average-case diagonalizable operators, as well as a ground state preparation algorithm that upgrades previous nearly optimal results for Hermitian Hamiltonians to diagonalizable matrices with real spectra. Underpinning our algorithms is an efficient method to prepare a quantum superposition of Faber polynomials, which generalize the nearly-best uniform approximation properties of Chebyshev polynomials to the complex plane. Of independent interest, we also develop techniques to generate $n$ Fourier coefficients with $\\mathbf{O}(\\mathrm{polylog}(n))$ gates compared to prior approaches with linear cost. ",
        "title": "Quantum eigenvalue processing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06243",
        "abstract_url": "http://arxiv.org/abs/2401.06243",
        "authors": [
            {
                "last_name": "Herrin",
                "first_name": "Baker"
            },
            {
                "last_name": "Close",
                "first_name": "Victoria"
            },
            {
                "last_name": "Berner",
                "first_name": "Nathan"
            },
            {
                "last_name": "Herbert",
                "first_name": "Joshua"
            },
            {
                "last_name": "Reussow",
                "first_name": "Ethan"
            },
            {
                "last_name": "James",
                "first_name": "Ryan"
            },
            {
                "last_name": "Woodward",
                "first_name": "Cale"
            },
            {
                "last_name": "Mindlin",
                "first_name": "Jared"
            },
            {
                "last_name": "Paez",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Bretas",
                "first_name": "Nilson"
            },
            {
                "last_name": "Shin",
                "first_name": "Jane"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Autonomous underwater robots typically require higher cost and time for demonstrations compared to other domains due to the complexity of the environment. Due to the limited capacity and payload flexibility, it is challenging to find off-the-shelf underwater robots that are affordable, customizable, and subject to environmental variability. Custom-built underwater robots may be necessary for specialized applications or missions, but the process can be more costly and time-consuming than purchasing an off-the-shelf autonomous underwater vehicle (AUV). To address these challenges, we propose a modular underwater robot, Modularis, that can serve as an open-source testbed system. Our proposed system expedites the testing of perception, planning, and control algorithms. ",
        "title": "Modularis: Modular Underwater Robot for Rapid Development and Validation  of Autonomous Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06244",
        "abstract_url": "http://arxiv.org/abs/2401.06244",
        "authors": [
            {
                "last_name": "Khoramdel",
                "first_name": "Javad"
            },
            {
                "last_name": "Moori",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Borhani",
                "first_name": "Yasamin"
            },
            {
                "last_name": "Ghanbarzadeh",
                "first_name": "Armin"
            },
            {
                "last_name": "Najafi",
                "first_name": "Esmaeil"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The proposed YOLO-Former method seamlessly integrates the ideas of transformer and YOLOv4 to create a highly accurate and efficient object detection system. The method leverages the fast inference speed of YOLOv4 and incorporates the advantages of the transformer architecture through the integration of convolutional attention and transformer modules. The results demonstrate the effectiveness of the proposed approach, with a mean average precision (mAP) of 85.76\\% on the Pascal VOC dataset, while maintaining high prediction speed with a frame rate of 10.85 frames per second. The contribution of this work lies in the demonstration of how the innovative combination of these two state-of-the-art techniques can lead to further improvements in the field of object detection. ",
        "title": "YOLO-Former: YOLO Shakes Hand With ViT",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06247",
        "abstract_url": "http://arxiv.org/abs/2401.06247",
        "authors": [
            {
                "last_name": "Kronhardt",
                "first_name": "Kirill"
            },
            {
                "last_name": "Rolfes",
                "first_name": "Kevin"
            },
            {
                "last_name": "Gerken",
                "first_name": "Jens"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Dark patterns are often used in interface design to manipulate users into performing actions they would otherwise not take, such as consenting to excessive data collection. We present a narrative serious game concept, along with seven game-adapted dark patterns designed to create awareness of and bolster resistance against dark patterns through direct consequences of player actions. We performed a qualitative, exploratory study investigating player behavior when confronted with game-adapted dark patterns. A thematic analysis provides insights into influencing factors for adapting dark patterns into gameplay, as well as player motivations and driving forces influencing player behavior. ",
        "title": "Player Beware: Driving Forces and Influencing Factors for Game-Adapted  Dark Patterns",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06251",
        "abstract_url": "http://arxiv.org/abs/2401.06251",
        "authors": [
            {
                "last_name": "Khorshidi",
                "first_name": "Mohammad Sadegh"
            },
            {
                "last_name": "Yazdanjue",
                "first_name": "Navid"
            },
            {
                "last_name": "Gharoun",
                "first_name": "Hassan"
            },
            {
                "last_name": "Yazdani",
                "first_name": "Danial"
            },
            {
                "last_name": "Nikoo",
                "first_name": "Mohammad Reza"
            },
            {
                "last_name": "Chen",
                "first_name": "Fang"
            },
            {
                "last_name": "Gandomi",
                "first_name": "Amir H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT"
        ],
        "abstract": "  In machine learning, the exponential growth of data and the associated ``curse of dimensionality'' pose significant challenges, particularly with expansive yet sparse datasets. Addressing these challenges, multi-view ensemble learning (MEL) has emerged as a transformative approach, with feature partitioning (FP) playing a pivotal role in constructing artificial views for MEL. Our study introduces the Semantic-Preserving Feature Partitioning (SPFP) algorithm, a novel method grounded in information theory. The SPFP algorithm effectively partitions datasets into multiple semantically consistent views, enhancing the MEL process. Through extensive experiments on eight real-world datasets, ranging from high-dimensional with limited instances to low-dimensional with high instances, our method demonstrates notable efficacy. It maintains model accuracy while significantly improving uncertainty measures in scenarios where high generalization performance is achievable. Conversely, it retains uncertainty metrics while enhancing accuracy where high generalization accuracy is less attainable. An effect size analysis further reveals that the SPFP algorithm outperforms benchmark models by large effect size and reduces computational demands through effective dimensionality reduction. The substantial effect sizes observed in most experiments underscore the algorithm's significant improvements in model performance. ",
        "title": "Semantic-Preserving Feature Partitioning for Multi-View Ensemble  Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06252",
        "abstract_url": "http://arxiv.org/abs/2401.06252",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shaochun"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanjun"
            },
            {
                "last_name": "Cai",
                "first_name": "Hengfan"
            },
            {
                "last_name": "Deng",
                "first_name": "Lina"
            },
            {
                "last_name": "Lin",
                "first_name": "Yunhao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Real-time and accurate information on fine-grained changes in crop cultivation is of great significance for crop growth monitoring, yield prediction and agricultural structure adjustment. Aiming at the problems of serious spectral confusion in visible high-resolution unmanned aerial vehicle (UAV) images of different phases, interference of large complex background and salt-and-pepper noise by existing semantic change detection (SCD) algorithms, in order to effectively extract deep image features of crops and meet the demand of agricultural practical engineering applications, this paper designs and proposes an agricultural geographic scene and parcel-scale constrained SCD framework for crops (AGSPNet). AGSPNet framework contains three parts: agricultural geographic scene (AGS) division module, parcel edge extraction module and crop SCD module. Meanwhile, we produce and introduce an UAV image SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple semantic variation types of crops in complex geographical scene. We conduct comparative experiments and accuracy evaluations in two test areas of this dataset, and the results show that the crop SCD results of AGSPNet consistently outperform other deep learning SCD models in terms of quantity and quality, with the evaluation metrics F1-score, kappa, OA, and mIoU obtaining improvements of 0.038, 0.021, 0.011 and 0.062, respectively, on average over the sub-optimal method. The method proposed in this paper can clearly detect the fine-grained change information of crop types in complex scenes, which can provide scientific and technical support for smart agriculture monitoring and management, food policy formulation and food security assurance. ",
        "title": "AGSPNet: A framework for parcel-scale crop fine-grained semantic change  detection from UAV high-resolution imagery with agricultural geographic scene  constraints",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06256",
        "abstract_url": "http://arxiv.org/abs/2401.06256",
        "authors": [
            {
                "last_name": "Sukhobokov",
                "first_name": "Artem"
            },
            {
                "last_name": "Belousov",
                "first_name": "Evgeny"
            },
            {
                "last_name": "Gromozdov",
                "first_name": "Danila"
            },
            {
                "last_name": "Zenger",
                "first_name": "Anna"
            },
            {
                "last_name": "Popov",
                "first_name": "Ilya"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The article identified 42 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a new cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph models are used, constructed as a development of annotated metagraphs. As components, the cognitive architecture being developed includes machine consciousness, machine subconsciousness, blocks of interaction with the external environment, a goal management block, an emotional control system, a block of social interaction, a block of reflection, an ethics block and a worldview block, a learning block, a monitoring block, blocks of statement and solving problems, self-organization and meta learning block. ",
        "title": "A Universal Knowledge Model and Cognitive Architecture for Prototyping  AGI",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06262",
        "abstract_url": "http://arxiv.org/abs/2401.06262",
        "authors": [
            {
                "last_name": "Koike",
                "first_name": "Amy"
            },
            {
                "last_name": "Wehner",
                "first_name": "Michael"
            },
            {
                "last_name": "Mutlu",
                "first_name": "Bilge"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we explore how techniques from soft robotics can help create a new form of robot expression. We present Sprout, a soft expressive robot that conveys its internal states by changing its body shape. Sprout can extend, bend, twist, and expand using fiber-embedded actuators integrated into its construction. These deformations enable Sprout to express its internal states, for example, by expanding to express anger and bending its body sideways to express curiosity. Through two user studies, we investigated how users interpreted Sprout's expressions, their perceptions of Sprout, and their expectations from future iterations of Sprout's design. We argue that the use of soft actuators opens a novel design space for robot expressions to convey internal states, emotions, and intent. ",
        "title": "Sprout: Designing Expressivity for Robots Using Fiber-Embedded Actuator",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06263",
        "abstract_url": "http://arxiv.org/abs/2401.06263",
        "authors": [
            {
                "last_name": "Sattarov",
                "first_name": "Timur"
            },
            {
                "last_name": "Schreyer",
                "first_name": "Marco"
            },
            {
                "last_name": "Borth",
                "first_name": "Damian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Realistic synthetic tabular data generation encounters significant challenges in preserving privacy, especially when dealing with sensitive information in domains like finance and healthcare. In this paper, we introduce \\textit{Federated Tabular Diffusion} (FedTabDiff) for generating high-fidelity mixed-type tabular data without centralized access to the original tabular datasets. Leveraging the strengths of \\textit{Denoising Diffusion Probabilistic Models} (DDPMs), our approach addresses the inherent complexities in tabular data, such as mixed attribute types and implicit relationships. More critically, FedTabDiff realizes a decentralized learning scheme that permits multiple entities to collaboratively train a generative model while respecting data privacy and locality. We extend DDPMs into the federated setting for tabular data generation, which includes a synchronous update scheme and weighted averaging for effective model aggregation. Experimental evaluations on real-world financial and medical datasets attest to the framework's capability to produce synthetic data that maintains high fidelity, utility, privacy, and coverage. ",
        "title": "FedTabDiff: Federated Learning of Diffusion Probabilistic Models for  Synthetic Mixed-Type Tabular Data Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06268",
        "abstract_url": "http://arxiv.org/abs/2401.06268",
        "authors": [
            {
                "last_name": "Amiriara",
                "first_name": "Hamid"
            },
            {
                "last_name": "Mirmohseni",
                "first_name": "Mahtab"
            },
            {
                "last_name": "Ashtiani",
                "first_name": "Farid"
            },
            {
                "last_name": "Nasiri-Kenari",
                "first_name": "Masoumeh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper presents exact formulas for the probability distribution function (PDF) and moment generating function (MGF) of the sum-product of statistically independent but not necessarily identically distributed (i.n.i.d.) Nakagami-$m$ random variables (RVs) in terms of Meijer's G-function. Additionally, exact series representations are also derived for the sum of double-Nakagami RVs, providing useful insights on the trade-off between accuracy and computational cost. Simple asymptotic analytical expressions are provided to gain further insight into the derived formula, and the achievable diversity order is obtained. The suggested statistical properties are proved to be a highly useful tool for modeling parallel cascaded Nakagami-$m$ fading channels. The application of these new results is illustrated by deriving exact expressions and simple tight upper bounds for the outage probability (OP) and average symbol error rate (ASER) of several binary and multilevel modulation signals in intelligent reflecting surfaces (IRSs)-assisted communication systems operating over Nakagami-$m$ fading channels. It is demonstrated that the new asymptotic expression is highly accurate and can be extended to encompass a wider range of scenarios. To validate the theoretical frameworks and formulations, Monte-Carlo simulation results are presented. Additionally, supplementary simulations are provided to compare the derived results with two common types of approximations available in the literature, namely the central limit theorem (CLT) and gamma distribution. ",
        "title": "A Novel Stochastic Model for IRS-Assisted Communication Systems Based on  the Sum-Product of Nakagami-$m$ Random Variables",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06270",
        "abstract_url": "http://arxiv.org/abs/2401.06270",
        "authors": [
            {
                "last_name": "Ji",
                "first_name": "Shixin"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhuoping"
            },
            {
                "last_name": "Cahoon",
                "first_name": "Stephen"
            },
            {
                "last_name": "Jones",
                "first_name": "Alex K."
            },
            {
                "last_name": "Zhou",
                "first_name": "Peipei"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Embodied carbon has been widely reported as a significant component in the full system lifecycle of various computing systems green house gas emissions. Many efforts have been undertaken to quantify the elements that comprise this embodied carbon, from tools that evaluate semiconductor manufacturing to those that can quantify different elements of the computing system from commercial and academic sources. However, these tools cannot easily reproduce results reported by server vendors' product carbon reports and the accuracy can vary substantially due to various assumptions. Furthermore, attempts to determine green house gas contributions using bottom-up methodologies often do not agree with system-level studies and are hard to rectify. Nonetheless, given there is a need to consider all contributions to green house gas emissions in datacenters, we propose the Server Carbon including Accelerator Reporter with Intelligence-based Formulation (SCARIF) tool. SCARIF has three main contributions: (1) We first collect reported carbon cost data from server vendors and design learning models to predict the embodied carbon cost so that users can get the embodied carbon cost for their server configurations. (2) We provide embodied carbon cost if users configure servers with accelerators including GPUs, and FPGAs. (3) We provide an interface of SCARIF to the ACT and GreenChip tools and demonstrate the end-to-end system flow through indifference analysis considering the embodied and operational energy and green house gas emissions on different years servers with or without accelerators. Thus, SCARIF provides an opportunity for large-scale datacenter and hyperscaler design. ",
        "title": "Towards Carbon Modeling of Cloud Servers with Accelerators",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06272",
        "abstract_url": "http://arxiv.org/abs/2401.06272",
        "authors": [
            {
                "last_name": "Mathai",
                "first_name": "Tejas Sudharshan"
            },
            {
                "last_name": "Liu",
                "first_name": "Bohan"
            },
            {
                "last_name": "Summers",
                "first_name": "Ronald M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Purpose: Lymph nodes (LNs) in the chest have a tendency to enlarge due to various pathologies, such as lung cancer or pneumonia. Clinicians routinely measure nodal size to monitor disease progression, confirm metastatic cancer, and assess treatment response. However, variations in their shapes and appearances make it cumbersome to identify LNs, which reside outside of most organs. Methods: We propose to segment LNs in the mediastinum by leveraging the anatomical priors of 28 different structures (e.g., lung, trachea etc.) generated by the public TotalSegmentator tool. The CT volumes from 89 patients available in the public NIH CT Lymph Node dataset were used to train three 3D nnUNet models to segment LNs. The public St. Olavs dataset containing 15 patients (out-of-training-distribution) was used to evaluate the segmentation performance. Results: For the 15 test patients, the 3D cascade nnUNet model obtained the highest Dice score of 72.2 +- 22.3 for mediastinal LNs with short axis diameter $\\geq$ 8mm and 54.8 +- 23.8 for all LNs respectively. These results represent an improvement of 10 points over a current approach that was evaluated on the same test dataset. Conclusion: To our knowledge, we are the first to harness 28 distinct anatomical priors to segment mediastinal LNs, and our work can be extended to other nodal zones in the body. The proposed method has immense potential for improved patient outcomes through the identification of enlarged nodes in initial staging CT scans. ",
        "title": "Segmentation of Mediastinal Lymph Nodes in CT with Anatomical Priors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06273",
        "abstract_url": "http://arxiv.org/abs/2401.06273",
        "authors": [
            {
                "last_name": "Grislain",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Roussel",
                "first_name": "Paul"
            },
            {
                "last_name": "Agathe",
                "first_name": "Victoria de Sainte"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  This paper introduces Qrlew, an open source library that can parse SQL queries into Relations -- an intermediate representation -- that keeps track of rich data types, value ranges, and row ownership; so that they can easily be rewritten into differentially-private equivalent and turned back into SQL queries for execution in a variety of standard data stores.   With Qrlew, a data practitioner can express their data queries in standard SQL; the data owner can run the rewritten query without any technical integration and with strong privacy guarantees on the output; and the query rewriting can be operated by a privacy-expert who must be trusted by the owner, but may belong to a separate organization. ",
        "title": "Qrlew: Rewriting SQL into Differentially Private SQL",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06274",
        "abstract_url": "http://arxiv.org/abs/2401.06274",
        "authors": [
            {
                "last_name": "Zayat",
                "first_name": "Abdullah"
            },
            {
                "last_name": "Hasabelnaby",
                "first_name": "Mahmoud A."
            },
            {
                "last_name": "Obeed",
                "first_name": "Mohanad"
            },
            {
                "last_name": "Chaaban",
                "first_name": "Anas"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Next-generation communication networks are expected to exploit recent advances in data science and cutting-edge communications technologies to improve the utilization of the available communications resources. In this article, we introduce an emerging deep learning (DL) architecture, the transformer-masked autoencoder (TMAE), and discuss its potential in next-generation wireless networks. We discuss the limitations of current DL techniques in meeting the requirements of 5G and beyond 5G networks, and how the TMAE differs from the classical DL techniques can potentially address several wireless communication problems. We highlight various areas in next-generation mobile networks which can be addressed using a TMAE, including source and channel coding, estimation, and security. Furthermore, we demonstrate a case study showing how a TMAE can improve data compression performance and complexity compared to existing schemes. Finally, we discuss key challenges and open future research directions for deploying the TMAE in intelligent next-generation mobile networks. ",
        "title": "Transformer Masked Autoencoders for Next-Generation Wireless  Communications: Architecture and Opportunities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06275",
        "abstract_url": "http://arxiv.org/abs/2401.06275",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Siyi"
            },
            {
                "last_name": "He",
                "first_name": "Zihao"
            },
            {
                "last_name": "Rao",
                "first_name": "Ashwin"
            },
            {
                "last_name": "Morstatter",
                "first_name": "Fred"
            },
            {
                "last_name": "Brantingham",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Lerman",
                "first_name": "Kristina"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  The rich and dynamic information environment of social media provides researchers, policy makers, and entrepreneurs with opportunities to learn about social phenomena in a timely manner. However, using these data to understand social behavior is difficult due to heterogeneity of topics and events discussed in the highly dynamic online information environment. To address these challenges, we present a method for systematically detecting and measuring emotional reactions to offline events using change point detection on the time series of collective affect, and further explaining these reactions using a transformer-based topic model. We demonstrate the utility of the method by successfully detecting major and smaller events on three different datasets, including (1) a Los Angeles Tweet dataset between Jan. and Aug. 2020, in which we revealed the complex psychological impact of the BlackLivesMatter movement and the COVID-19 pandemic, (2) a dataset related to abortion rights discussions in USA, in which we uncovered the strong emotional reactions to the overturn of Roe v. Wade and state abortion bans, and (3) a dataset about the 2022 French presidential election, in which we discovered the emotional and moral shift from positive before voting to fear and criticism after voting. The capability of our method allows for better sensing and monitoring of population's reactions during crises using online data. ",
        "title": "The Pulse of Mood Online: Unveiling Emotional Reactions in a Dynamic  Social Media Landscape",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06277",
        "abstract_url": "http://arxiv.org/abs/2401.06277",
        "authors": [
            {
                "last_name": "Spies",
                "first_name": "Lukas"
            },
            {
                "last_name": "Olson",
                "first_name": "Luke"
            },
            {
                "last_name": "MacLachlan",
                "first_name": "Scott"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years, solvers for finite-element discretizations of linear or linearized saddle-point problems, like the Stokes and Oseen equations, have become well established. There are two main classes of preconditioners for such systems: those based on block-factorization approach and those based on monolithic multigrid. Both classes of preconditioners have several critical choices to be made in their composition, such as the selection of a suitable relaxation scheme for monolithic multigrid. From existing studies, some insight can be gained as to what options are preferable in low-performance computing settings, but there are very few fair comparisons of these approaches in the literature, particularly for modern architectures, such as GPUs. In this paper, we perform a comparison between a block-triangular preconditioner and a monolithic multigrid method with the three most common choices of relaxation scheme - Braess-Sarazin, Vanka, and Schur-Uzawa. We develop a performant Vanka relaxation algorithm for structured-grid discretizations, which takes advantage of memory efficiencies in this setting. We detail the behavior of the various CUDA kernels for the multigrid relaxation schemes and evaluate their individual arithmetic intensity, performance, and runtime. Running a preconditioned FGMRES solver for the Stokes equations with these preconditioners allows us to compare their efficiency in a practical setting. We show monolithic multigrid can outperform block-triangular preconditioning, and that using Vanka or Braess-Sarazin relaxation is most efficient. Even though multigrid with Vanka relaxation exhibits reduced performance on the CPU (up to $100\\%$ slower than Braess-Sarazin), it is able to outperform Braess-Sarazin by more than $20\\%$ on the GPU, making it a competitive algorithm, especially given the high amount of algorithmic tuning needed for effective Braess-Sarazin relaxation. ",
        "title": "Exploiting mesh structure to improve multigrid performance for saddle  point problems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06278",
        "abstract_url": "http://arxiv.org/abs/2401.06278",
        "authors": [
            {
                "last_name": "Sanderson",
                "first_name": "Edward"
            },
            {
                "last_name": "Matuszewski",
                "first_name": "Bogdan J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exception of monocular depth estimation in colonoscopy; and that ViT-Bs are more suitable in polyp segmentation and monocular depth estimation in colonoscopy, ResNet50s are more suitable in polyp detection, and both architectures perform similarly in anatomical landmark recognition and pathological finding characterisation. We hope this work draws attention to the complexity of pretraining for GIE vision tasks, informs this development of more suitable approaches than the convention, and inspires further research on this topic to help advance this development. Code available: \\underline{github.com/ESandML/SSL4GIE} ",
        "title": "A Study on Self-Supervised Pretraining for Vision Problems in  Gastrointestinal Endoscopy",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06279",
        "abstract_url": "http://arxiv.org/abs/2401.06279",
        "authors": [
            {
                "last_name": "Parada-Mayorga",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Ribeiro",
                "first_name": "Alejandro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we study the properties of sampling sets on families of large graphs by leveraging the theory of graphons and graph limits. To this end, we extend to graphon signals the notion of removable and uniqueness sets, which was developed originally for the analysis of signals on graphs. We state the formal definition of a $\\Lambda-$removable set and conditions under which a bandlimited graphon signal can be represented in a unique way when its samples are obtained from the complement of a given $\\Lambda-$removable set in the graphon. By leveraging such results we show that graphon representations of graphs and graph signals can be used as a common framework to compare sampling sets between graphs with different numbers of nodes and edges, and different node labelings. Additionally, given a sequence of graphs that converges to a graphon, we show that the sequences of sampling sets whose graphon representation is identical in $[0,1]$ are convergent as well. We exploit the convergence results to provide an algorithm that obtains approximately close to optimal sampling sets. Performing a set of numerical experiments, we evaluate the quality of these sampling sets. Our results open the door for the efficient computation of optimal sampling sets in graphs of large size. ",
        "title": "Sampling and Uniqueness Sets in Graphon Signal Processing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06280",
        "abstract_url": "http://arxiv.org/abs/2401.06280",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Alvin"
            },
            {
                "last_name": "Bishop",
                "first_name": "Joseph E."
            },
            {
                "last_name": "Sukumar",
                "first_name": "N."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we present a first-order Stress-Hybrid Virtual Element Method (SH-VEM) on six-noded triangular meshes for linear plane elasticity. We adopt the Hellinger--Reissner variational principle to construct a weak equilibrium condition and a stress based projection operator. On applying the divergence theorem to the weak strain-displacement relations, the stress projection operator is expressed in terms of the nodal displacements, which leads to a displacement-based formulation. This stress-hybrid approach assumes a globally continuous displacement field while the stress field is discontinuous across each element. The stress field is initially represented by divergence-free tensor polynomials based on Airy stress functions. However, for flexibility in choosing basis functions, we also present a formulation that uses a penalty term to enforce the element equilibrium conditions. This method is referred to as the Penalty Stress-Hybrid Virtual Element Method (PSH-VEM). Numerical results are presented for PSH-VEM and SH-VEM, and we compare their convergence to the composite triangle FEM and B-bar VEM on benchmark problems in linear elasticity. The SH-VEM converges optimally in the $L^2$ norm of the displacement, energy seminorm, and the $L^2$ norm of hydrostatic stress. Furthermore, the results reveal that PSH-VEM converges in most cases at a faster rate than the expected optimal rate, but it requires the selection of a suitably chosen penalty parameter. ",
        "title": "Stress-hybrid virtual element method on six-noded triangular meshes for  compressible and nearly-incompressible linear elasticity",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06281",
        "abstract_url": "http://arxiv.org/abs/2401.06281",
        "authors": [
            {
                "last_name": "Ribeiro",
                "first_name": "Fabio De Sousa"
            },
            {
                "last_name": "Glocker",
                "first_name": "Ben"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Despite the growing popularity of diffusion models, gaining a deep understanding of the model class remains somewhat elusive for the uninitiated in non-equilibrium statistical physics. With that in mind, we present what we believe is a more straightforward introduction to diffusion models using directed graphical modelling and variational Bayesian principles, which imposes relatively fewer prerequisites on the average reader. Our exposition constitutes a comprehensive technical review spanning from foundational concepts like deep latent variable models to recent advances in continuous-time diffusion-based modelling, highlighting theoretical connections between model classes along the way. We provide additional mathematical insights that were omitted in the seminal works whenever possible to aid in understanding, while avoiding the introduction of new notation. We envision this article serving as a useful educational supplement for both researchers and practitioners in the area, and we welcome feedback and contributions from the community at https://github.com/biomedia-mira/demystifying-diffusion. ",
        "title": "Demystifying Variational Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06287",
        "abstract_url": "http://arxiv.org/abs/2401.06287",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Yukun"
            },
            {
                "last_name": "Yao",
                "first_name": "Hantao"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Liansheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Changsheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Audio-visual video recognition (AVVR) aims to integrate audio and visual clues to categorize videos accurately. While existing methods train AVVR models using provided datasets and achieve satisfactory results, they struggle to retain historical class knowledge when confronted with new classes in real-world situations. Currently, there are no dedicated methods for addressing this problem, so this paper concentrates on exploring Class Incremental Audio-Visual Video Recognition (CIAVVR). For CIAVVR, since both stored data and learned model of past classes contain historical knowledge, the core challenge is how to capture past data knowledge and past model knowledge to prevent catastrophic forgetting. We introduce Hierarchical Augmentation and Distillation (HAD), which comprises the Hierarchical Augmentation Module (HAM) and Hierarchical Distillation Module (HDM) to efficiently utilize the hierarchical structure of data and models, respectively. Specifically, HAM implements a novel augmentation strategy, segmental feature augmentation, to preserve hierarchical model knowledge. Meanwhile, HDM introduces newly designed hierarchical (video-distribution) logical distillation and hierarchical (snippet-video) correlative distillation to capture and maintain the hierarchical intra-sample knowledge of each data and the hierarchical inter-sample knowledge between data, respectively. Evaluations on four benchmarks (AVE, AVK-100, AVK-200, and AVK-400) demonstrate that the proposed HAD effectively captures hierarchical information in both data and models, resulting in better preservation of historical class knowledge and improved performance. Furthermore, we provide a theoretical analysis to support the necessity of the segmental feature augmentation strategy. ",
        "title": "Hierarchical Augmentation and Distillation for Class Incremental  Audio-Visual Video Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06289",
        "abstract_url": "http://arxiv.org/abs/2401.06289",
        "authors": [
            {
                "last_name": "O'Connell",
                "first_name": "Amy"
            },
            {
                "last_name": "Banga",
                "first_name": "Ashveen"
            },
            {
                "last_name": "Ayissi",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Yaminrafie",
                "first_name": "Nikki"
            },
            {
                "last_name": "Ko",
                "first_name": "Ellen"
            },
            {
                "last_name": "Le",
                "first_name": "Andrew"
            },
            {
                "last_name": "Cislowski",
                "first_name": "Bailey"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  College students with ADHD respond positively to simple socially assistive robots (SARs) that monitor attention and provide non-verbal feedback, but studies have been done only in brief in-lab sessions. We present an initial design and evaluation of an in-dorm SAR study companion for college students with ADHD. This work represents the introductory stages of an ongoing user-centered, participatory design process. In a three-week within-subjects user study, university students (N=11) with self-reported symptoms of adult ADHD had a SAR study companion in their dorm room for two weeks and a computer-based system for one week. Toward developing SARs for long-term, in-dorm use, we focus on 1) evaluating the usability and desire for SAR study companions by college students with ADHD and 2) collecting participant feedback about the SAR design and functionality. Participants responded positively to the robot; after one week of regular use, 91% (10 of 11) chose to continue using the robot voluntarily in the second week. ",
        "title": "Design and Evaluation of a Socially Assistive Robot Schoolwork Companion  for College Students with ADHD",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06291",
        "abstract_url": "http://arxiv.org/abs/2401.06291",
        "authors": [
            {
                "last_name": "Kalkhof",
                "first_name": "John"
            },
            {
                "last_name": "K\u00fchn",
                "first_name": "Arlene"
            },
            {
                "last_name": "Frisch",
                "first_name": "Yannik"
            },
            {
                "last_name": "Mukhopadhyay",
                "first_name": "Anirban"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Denoising Diffusion Models (DDMs) have become the leading generative technique for synthesizing high-quality images but are often constrained by their UNet-based architectures that impose certain limitations. In particular, the considerable size of often hundreds of millions of parameters makes them impractical when hardware resources are limited. However, even with powerful hardware, processing images in the gigapixel range is difficult. This is especially true in fields such as microscopy or satellite imaging, where such challenges arise from the limitation to a predefined generative size and the inefficient scaling to larger images. We present two variations of Neural Cellular Automata (NCA)-based DDM methods to address these challenges and jumpstart NCA-based DDMs: Diff-NCA and FourierDiff-NCA. Diff-NCA performs diffusion by using only local features of the underlying distribution, making it suitable for applications where local features are critical. To communicate global knowledge in image space, naive NCA setups require timesteps that increase with the image scale. We solve this bottleneck of current NCA architectures by introducing FourierDiff-NCA, which advances Diff-NCA by adding a Fourier-based diffusion process and combines the frequency-organized Fourier space with the image space. By initiating diffusion in the Fourier domain and finalizing it in the image space, FourierDiff-NCA accelerates global communication. We validate our techniques by using Diff-NCA (208k parameters) to generate high-resolution digital pathology scans at 576x576 resolution and FourierDiff-NCA (887k parameters) to synthesize CelebA images at 64x64, outperforming VNCA and five times bigger UNet-based DDMs. In addition, we demonstrate FourierDiff-NCA's capabilities in super-resolution, OOD image synthesis, and inpainting without additional training. ",
        "title": "Frequency-Time Diffusion with Neural Cellular Automata",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06293",
        "abstract_url": "http://arxiv.org/abs/2401.06293",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Qiang Charles"
            },
            {
                "last_name": "Muralidharan",
                "first_name": "Ajith"
            },
            {
                "last_name": "Tiwana",
                "first_name": "Birjodh"
            },
            {
                "last_name": "Jia",
                "first_name": "Johnson"
            },
            {
                "last_name": "Borisyuk",
                "first_name": "Fedor"
            },
            {
                "last_name": "Gupta",
                "first_name": "Aman"
            },
            {
                "last_name": "Woodard",
                "first_name": "Dawn"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In this paper, we propose a generic model-based re-ranking framework, MultiSlot ReRanker, which simultaneously optimizes relevance, diversity, and freshness. Specifically, our Sequential Greedy Algorithm (SGA) is efficient enough (linear time complexity) for large-scale production recommendation engines. It achieved a lift of $+6\\%$ to $ +10\\%$ offline Area Under the receiver operating characteristic Curve (AUC) which is mainly due to explicitly modeling mutual influences among items of a list, and leveraging the second pass ranking scores of multiple objectives. In addition, we have generalized the offline replay theory to multi-slot re-ranking scenarios, with trade-offs among multiple objectives. The offline replay results can be further improved by Pareto Optimality. Moreover, we've built a multi-slot re-ranking simulator based on OpenAI Gym integrated with the Ray framework. It can be easily configured for different assumptions to quickly benchmark both reinforcement learning and supervised learning algorithms. ",
        "title": "MultiSlot ReRanker: A Generic Model-based Re-Ranking Framework in  Recommendation Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06299",
        "abstract_url": "http://arxiv.org/abs/2401.06299",
        "authors": [
            {
                "last_name": "Estiri",
                "first_name": "Elham"
            },
            {
                "last_name": "Mirinejad",
                "first_name": "Hossein"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Fluid administration, also called fluid resuscitation, is a medical treatment to restore the lost blood volume and optimize cardiac functions in critical care scenarios such as burn, hemorrhage, and septic shock. Automated fluid administration systems (AFAS), a potential means to improve the treatment, employ computational control algorithms to automatically adjust optimal fluid infusion dosages by targeting physiological variables (e.g., blood volume or blood pressure). Most of the existing AFAS control algorithms are model-based approaches, and their performance is highly dependent on the model accuracy, making them less desirable in real-world care of critically ill patients due to complexity and variability of modeling patients physiological states. This work presents a novel model-free reinforcement learning (RL) approach for the control of fluid infusion dosages in AFAS systems. The proposed RL agent learns to adjust the blood volume to a desired value by choosing the optimal infusion dosages using a Q-learning algorithm. The RL agent learns the optimal actions by interacting with the environment (without having the knowledge of system dynamics). The proposed methodology (i) overcomes the need for a precise mathematical model in AFAS systems and (ii) provides a robust performance in rejecting clinical noises and reaching desired hemodynamic states, as will be shown by simulation results. ",
        "title": "Model-Free Reinforcement Learning for Automated Fluid Administration in  Critical Care",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06300",
        "abstract_url": "http://arxiv.org/abs/2401.06300",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Weishun"
            },
            {
                "last_name": "Shtanko",
                "first_name": "Oles"
            },
            {
                "last_name": "Movassagh",
                "first_name": "Ramis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A promising strategy to protect quantum information from noise-induced errors is to encode it into the low-energy states of a topological quantum memory device. However, readout errors from such memory under realistic settings is less understood. We study the problem of decoding quantum information encoded in the groundspaces of topological stabilizer Hamiltonians in the presence of generic perturbations, such as quenched disorder. We first prove that the standard stabilizer-based error correction and decoding schemes work adequately well in such perturbed quantum codes by showing that the decoding error diminishes exponentially in the distance of the underlying unperturbed code. We then prove that Quantum Neural Network (QNN) decoders provide an almost quadratic improvement on the readout error. Thus, we demonstrate provable advantage of using QNNs for decoding realistic quantum error-correcting codes, and our result enables the exploration of a wider range of non-stabilizer codes in the near-term laboratory settings. ",
        "title": "Advantage of Quantum Neural Networks as Quantum Information Decoders",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06301",
        "abstract_url": "http://arxiv.org/abs/2401.06301",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Shangqing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In-context learning with large language models (LLMs) excels at adapting to various tasks rapidly. However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice. Current approaches to this problem either rely on hard-to-acquire external supervision or require frequent interactions with LLMs, resulting in high costs. We propose a new method called In-Context Reflection (ICR) to overcome these challenges. ICR strategically selects demonstrations to reduce the discrepancy between the LLM's outputs and the actual input-output mappings. Specifically, ICR starts with a random set of initial demonstrations, then iteratively refines it. In each step, it analyzes a pool of candidate examples and identifies the ones most likely to challenge the LLM's current understanding, measured by a new metric called misconfidence. These most confusing examples are then selected to replace the less informative demonstrations in the current set. Our comprehensive evaluation across five diverse datasets encompassing 13 subtasks shows the efficacy of ICR. Compared to existing methods, ICR achieves an average performance boost of 4%, while demonstrating remarkable cross-task generalization capabilities. ",
        "title": "Misconfidence-based Demonstration Selection for LLM In-Context Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06305",
        "abstract_url": "http://arxiv.org/abs/2401.06305",
        "authors": [
            {
                "last_name": "Anon",
                "first_name": "Alexandre Miranda"
            },
            {
                "last_name": "Bae",
                "first_name": "Sangjae"
            },
            {
                "last_name": "Saroya",
                "first_name": "Manish"
            },
            {
                "last_name": "Isele",
                "first_name": "David"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Smooth and safe speed planning is imperative for the successful deployment of autonomous vehicles. This paper presents a mathematical formulation for the optimal speed planning of autonomous driving, which has been validated in high-fidelity simulations and real-road demonstrations with practical constraints. The algorithm explores the inter-traffic gaps in the time and space domain using a breadth-first search. For each gap, quadratic programming finds an optimal speed profile, synchronizing the time and space pair along with dynamic obstacles. Qualitative and quantitative analysis in Carla is reported to discuss the smoothness and robustness of the proposed algorithm. Finally, we present a road demonstration result for urban city driving. ",
        "title": "Multi-Profile Quadratic Programming (MPQP) for Optimal Gap Selection and  Speed Planning of Autonomous Driving",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06306",
        "abstract_url": "http://arxiv.org/abs/2401.06306",
        "authors": [
            {
                "last_name": "Sasan",
                "first_name": "Zeinab"
            },
            {
                "last_name": "Shokrnezhad",
                "first_name": "Masoud"
            },
            {
                "last_name": "Khorsandi",
                "first_name": "Siavash"
            },
            {
                "last_name": "Taleb",
                "first_name": "Tarik"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "DC",
            "PF"
        ],
        "abstract": "  To address the evolving landscape of next-generation mobile networks, characterized by an increasing number of connected users, surging traffic demands, and the continuous emergence of new services, a novel communication paradigm is essential. One promising candidate is the integration of network slicing and in-network computing, offering resource isolation, deterministic networking, enhanced resource efficiency, network expansion, and energy conservation. Although prior research has explored resource allocation within network slicing, routing, and in-network computing independently, a comprehensive investigation into their joint approach has been lacking. This paper tackles the joint problem of network slicing, path selection, and the allocation of in-network and cloud computing resources, aiming to maximize the number of accepted users while minimizing energy consumption. First, we introduce a Mixed-Integer Linear Programming (MILP) formulation of the problem and analyze its complexity, proving that the problem is NP-hard. Next, a Water Filling-based Joint Slicing, Routing, and In-Network Computing (WF-JSRIN) heuristic algorithm is proposed to solve it. Finally, a comparative analysis was conducted among WF-JSRIN, a random allocation technique, and two optimal approaches, namely Opt-IN (utilizing in-network computation) and Opt-C (solely relying on cloud node resources). The results emphasize WF-JSRIN's efficiency in delivering highly efficient near-optimal solutions with significantly reduced execution times, solidifying its suitability for practical real-world applications. ",
        "title": "Joint Network Slicing, Routing, and In-Network Computing for  Energy-Efficient 6G",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06308",
        "abstract_url": "http://arxiv.org/abs/2401.06308",
        "authors": [
            {
                "last_name": "Mazandarani",
                "first_name": "Hamidreza"
            },
            {
                "last_name": "Shokrnezhad",
                "first_name": "Masoud"
            },
            {
                "last_name": "Taleb",
                "first_name": "Tarik"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG",
            "MA"
        ],
        "abstract": "  The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL), enabling the user equipment to autonomously make decisions regarding wireless spectrum access based solely on their local individual observations. The efficiency of the proposed technique is evaluated through two scenarios: single-channel and multi-channel. The findings illustrate that, across a spectrum of $\\alpha$ values, association matrices, and channels, SAMA-D3QL consistently outperforms alternative approaches. This establishes it as a promising candidate for facilitating the realization of future federated, dynamically evolving applications. ",
        "title": "A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic  6G-Based Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06309",
        "abstract_url": "http://arxiv.org/abs/2401.06309",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shian"
            },
            {
                "last_name": "Shang",
                "first_name": "Mingfeng"
            },
            {
                "last_name": "Stern",
                "first_name": "Raphael"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  While automated vehicles (AVs) are expected to revolutionize future transportation systems, emerging AV technologies open a door for malicious actors to compromise intelligent vehicles. As the first generation of AVs, adaptive cruise control (ACC) vehicles are vulnerable to cyberattacks. While recent effort has been made to understanding the impact of attacks on transportation systems, little work has been done to systematically model and characterize the malicious nature of candidate attacks. In this study, we develop a general framework for modeling and synthesizing two types of candidate attacks on ACC vehicles, namely direct attacks on vehicle control commands and false data injection attacks on sensor measurement, with explicit characterization of their adverse effects. Based on linear stability analysis of car-following dynamics, we derive a series of analytical conditions characterizing the malicious nature of potential attacks. This ensures a higher degree of realism in modeling attacks with adverse effects, as opposed to simply considering attacks as constants or random variables. Notably, the conditions derived provide an effective method for strategically synthesizing an array of candidate attacks on ACC vehicles. We conduct extensive simulation to examine the impacts of intelligently designed attacks on microscopic car-following dynamics and macroscopic traffic flow. Numerical results illustrate the mechanism of candidate attacks, offering useful insights into understanding the vulnerability of future transportation systems. The methodology developed allows for further study of the widespread impact of strategically designed attacks on traffic cybersecurity, and is expected to inspire the development of efficient attack detection techniques and advanced vehicle controls. ",
        "title": "Cyberattacks on Adaptive Cruise Control Vehicles: An Analytical  Characterization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06310",
        "abstract_url": "http://arxiv.org/abs/2401.06310",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Akshita"
            },
            {
                "last_name": "Prabhakaran",
                "first_name": "Vinodkumar"
            },
            {
                "last_name": "Denton",
                "first_name": "Remi"
            },
            {
                "last_name": "Laszlo",
                "first_name": "Sarah"
            },
            {
                "last_name": "Dave",
                "first_name": "Shachi"
            },
            {
                "last_name": "Qadri",
                "first_name": "Rida"
            },
            {
                "last_name": "Reddy",
                "first_name": "Chandan K."
            },
            {
                "last_name": "Dev",
                "first_name": "Sunipa"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "CY"
        ],
        "abstract": "  Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images of these identities as compared to other attributes. We further investigate how disparately offensive the depictions of generated images are for different nationalities. Finally, through a detailed case study, we reveal how the 'default' representations of all identity groups have a stereotypical appearance. Moreover, for the Global South, images across different attributes are visually similar, even when explicitly prompted otherwise. CONTENT WARNING: Some examples may contain offensive stereotypes. ",
        "title": "Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in  Text-to-Image Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06311",
        "abstract_url": "http://arxiv.org/abs/2401.06311",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Le"
            },
            {
                "last_name": "Wu",
                "first_name": "Yihong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large Language Models (LLMs) have emerged as a pivotal force in language technology. Their robust reasoning capabilities and expansive knowledge repositories have enabled exceptional zero-shot generalization abilities across various facets of the natural language processing field, including information retrieval (IR). In this paper, we conduct an in-depth investigation into the utility of documents generated by LLMs for IR. We introduce a simple yet effective framework, Multi-Text Generation Integration (MuGI), to augment existing IR methodologies. Specifically, we prompt LLMs to generate multiple pseudo references and integrate with query for retrieval. The training-free MuGI model eclipses existing query expansion strategies, setting a new standard in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR, achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a 7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity re-ranking pipeline. This allows a relatively small 110M parameter retriever to surpass the performance of larger 3B models in in-domain evaluations, while also bridging the gap in out-of-distribution situations. We release our code and all generated references at https://github.com/lezhang7/Retrieval_MuGI. ",
        "title": "MuGI: Enhancing Information Retrieval through Multi-Text Generation  Intergration with Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06318",
        "abstract_url": "http://arxiv.org/abs/2401.06318",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Yaowei"
            },
            {
                "last_name": "Lear",
                "first_name": "Jacob"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  While significant advancements have been made in the field of fair machine learning, the majority of studies focus on scenarios where the decision model operates on a static population. In this paper, we study fairness in dynamic systems where sequential decisions are made. Each decision may shift the underlying distribution of features or user behavior. We model the dynamic system through a Markov Decision Process (MDP). By acknowledging that traditional fairness notions and long-term fairness are distinct requirements that may not necessarily align with one another, we propose an algorithmic framework to integrate various fairness considerations with reinforcement learning using both pre-processing and in-processing approaches. Three case studies show that our method can strike a balance between traditional fairness notions, long-term fairness, and utility. ",
        "title": "Striking a Balance in Fairness for Dynamic Systems Through Reinforcement  Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06320",
        "abstract_url": "http://arxiv.org/abs/2401.06320",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Scells",
                "first_name": "Harrisen"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Shengyao"
            },
            {
                "last_name": "Potthast",
                "first_name": "Martin"
            },
            {
                "last_name": "Koopman",
                "first_name": "Bevan"
            },
            {
                "last_name": "Zuccon",
                "first_name": "Guido"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches. ",
        "title": "Zero-shot Generative Large Language Models for Systematic Review  Screening Automation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06321",
        "abstract_url": "http://arxiv.org/abs/2401.06321",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Wonjune"
            },
            {
                "last_name": "Wang",
                "first_name": "Yun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shun"
            },
            {
                "last_name": "Hinsvark",
                "first_name": "Arthur"
            },
            {
                "last_name": "He",
                "first_name": "Qing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that incorporating this dataset into training significantly improves HD performance over only using a commonly used, but imbalanced, pre-existing dataset. ",
        "title": "Multi-Task Learning for Front-End Text Processing in TTS",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06323",
        "abstract_url": "http://arxiv.org/abs/2401.06323",
        "authors": [
            {
                "last_name": "Abate",
                "first_name": "Marcus"
            },
            {
                "last_name": "Chang",
                "first_name": "Yun"
            },
            {
                "last_name": "Hughes",
                "first_name": "Nathan"
            },
            {
                "last_name": "Carlone",
                "first_name": "Luca"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We present improvements to Kimera, an open-source metric-semantic visual-inertial SLAM library. In particular, we enhance Kimera-VIO, the visual-inertial odometry pipeline powering Kimera, to support better feature tracking, more efficient keyframe selection, and various input modalities (eg monocular, stereo, and RGB-D images, as well as wheel odometry). Additionally, Kimera-RPGO and Kimera-PGMO, Kimera's pose-graph optimization backends, are updated to support modern outlier rejection methods - specifically, Graduated-Non-Convexity - for improved robustness to spurious loop closures. These new features are evaluated extensively on a variety of simulated and real robotic platforms, including drones, quadrupeds, wheeled robots, and simulated self-driving cars. We present comparisons against several state-of-the-art visual-inertial SLAM pipelines and discuss strengths and weaknesses of the new release of Kimera. The newly added features have been released open-source at https://github.com/MIT-SPARK/Kimera. ",
        "title": "Kimera2: Robust and Accurate Metric-Semantic SLAM in the Real World",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06325",
        "abstract_url": "http://arxiv.org/abs/2401.06325",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Xunpeng"
            },
            {
                "last_name": "Zou",
                "first_name": "Difan"
            },
            {
                "last_name": "Dong",
                "first_name": "Hanze"
            },
            {
                "last_name": "Ma",
                "first_name": "Yian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  To sample from a general target distribution $p_*\\propto e^{-f_*}$ beyond the isoperimetric condition, Huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of DMC originates from its redundant design of score estimation, and proposed a more efficient algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation and sampling subproblems accordingly, which are correlated in a recursive manner. Importantly, we show that with a proper design of the segment decomposition, all sampling subproblems will only need to tackle a strongly log-concave distribution, which can be very efficient to solve using the Langevin-based samplers with a provably rapid convergence rate. As a result, we prove that the gradient complexity of RS-DMC only has a quasi-polynomial dependency on $\\epsilon$, which significantly improves exponential gradient complexity in Huang et al. (2023). Furthermore, under commonly used dissipative conditions, our algorithm is provably much faster than the popular Langevin-based algorithms. Our algorithm design and theoretical framework illuminate a novel direction for addressing sampling problems, which could be of broader applicability in the community. ",
        "title": "Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06327",
        "abstract_url": "http://arxiv.org/abs/2401.06327",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lingling"
            },
            {
                "last_name": "Liu",
                "first_name": "Jun"
            },
            {
                "last_name": "Guo",
                "first_name": "Tianlin"
            },
            {
                "last_name": "Wu",
                "first_name": "Wenjun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We introduce a novel task, called Generalized Relation Discovery (GRD), for open-world relation extraction. GRD aims to identify unlabeled instances in existing pre-defined relations or discover novel relations by assigning instances to clusters as well as providing specific meanings for these clusters. The key challenges of GRD are how to mitigate the serious model biases caused by labeled pre-defined relations to learn effective relational representations and how to determine the specific semantics of novel relations during classifying or clustering unlabeled instances. We then propose a novel framework, SFGRD, for this task to solve the above issues by learning from semi-factuals in two stages. The first stage is semi-factual generation implemented by a tri-view debiased relation representation module, in which we take each original sentence as the main view and design two debiased views to generate semi-factual examples for this sentence. The second stage is semi-factual thinking executed by a dual-space tri-view collaborative relation learning module, where we design a cluster-semantic space and a class-index space to learn relational semantics and relation label indices, respectively. In addition, we devise alignment and selection strategies to integrate two spaces and establish a self-supervised learning loop for unlabeled data by doing semi-factual thinking across three views. Extensive experimental results show that SFGRD surpasses state-of-the-art models in terms of accuracy by 2.36\\% $\\sim$5.78\\% and cosine similarity by 32.19\\%$\\sim$ 84.45\\% for relation label index and relation semantic quality, respectively. To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals in relation extraction. ",
        "title": "Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for  Generalized Relation Discovery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06328",
        "abstract_url": "http://arxiv.org/abs/2401.06328",
        "authors": [
            {
                "last_name": "Eppstein",
                "first_name": "David"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  The Erd\\H{o}s--Anning theorem states that every point set in the Euclidean plane with integer distances must be either collinear or finite. More strongly, for any (non-degenerate) triangle of diameter $\\delta$, at most $O(\\delta^2)$ points can have integer distances from all three triangle vertices. We prove the same results for any strictly convex distance function on the plane, and analogous results for every two-dimensional complete Riemannian manifold of bounded genus and for geodesic distance on the boundary of every three-dimensional Euclidean convex set. Our proofs are based on the properties of additively weighted Voronoi diagrams of these distances. ",
        "title": "Non-Euclidean Erd\\H{o}s--Anning Theorems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06331",
        "abstract_url": "http://arxiv.org/abs/2401.06331",
        "authors": [
            {
                "last_name": "Felfeliyan",
                "first_name": "Banafshe"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuyue"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Shrimanti"
            },
            {
                "last_name": "Kupper",
                "first_name": "Jessica"
            },
            {
                "last_name": "Liu",
                "first_name": "Shaobo"
            },
            {
                "last_name": "Hareendranathan",
                "first_name": "Abhilash"
            },
            {
                "last_name": "Jaremko",
                "first_name": "Jacob L."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Osteoarthritis (OA) poses a global health challenge, demanding precise diagnostic methods. Current radiographic assessments are time consuming and prone to variability, prompting the need for automated solutions. The existing deep learning models for OA assessment are unimodal single task systems and they don't incorporate relevant text information such as patient demographics, disease history, or physician reports. This study investigates employing Vision Language Processing (VLP) models to predict OA severity using Xray images and corresponding reports. Our method leverages Xray images of the knee and diverse report templates generated from tabular OA scoring values to train a CLIP (Contrastive Language Image PreTraining) style VLP model. Furthermore, we incorporate additional contrasting captions to enforce the model to discriminate between positive and negative reports. Results demonstrate the efficacy of these models in learning text image representations and their contextual relationships, showcase potential advancement in OA assessment, and establish a foundation for specialized vision language models in medical contexts. ",
        "title": "Application Of Vision-Language Models For Assessing Osteoarthritis  Disease Severity",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06332",
        "abstract_url": "http://arxiv.org/abs/2401.06332",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lei"
            },
            {
                "last_name": "Ren",
                "first_name": "Zihao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Deming"
            },
            {
                "last_name": "Shi",
                "first_name": "Guodong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we study distributed solvers for network linear equations over a network with node-to-node communication messages compressed as scalar values. Our key idea lies in a dimension compression scheme including a dimension compressing vector that applies to individual node states to generate a real-valued message for node communication as an inner product, and a data unfolding step in the local computations where the scalar message is plotted along the subspace generated by the compression vector. We first present a compressed average consensus flow that relies only on such scalar communication, and show that exponential convergence can be achieved with well excited signals for the compression vector. We then employ such a compressed consensus flow as a fundamental consensus subroutine to develop distributed continuous-time and discrete-time solvers for network linear equations, and prove their exponential convergence properties under scalar node communications. With scalar communications, a direct benefit would be the reduced node-to-node communication channel capacity requirement for distributed computing. Numerical examples are presented to illustrate the effectiveness of the established theoretical results. ",
        "title": "Distributed Solvers for Network Linear Equations with Scalarized  Compression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06336",
        "abstract_url": "http://arxiv.org/abs/2401.06336",
        "authors": [
            {
                "last_name": "Sivakumar",
                "first_name": "Suharsh"
            },
            {
                "last_name": "Shen",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Monga",
                "first_name": "Rajat"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "DB"
        ],
        "abstract": "  A large class of data questions can be modeled as identifying important slices of data driven by user defined metrics. This paper presents TRACE, a Time-Relational Approximate Cubing Engine that enables interactive analysis on such slices with a low upfront cost - both in space and computation. It does this by materializing the most important parts of the cube over time enabling interactive querying for a large class of analytical queries e.g. what part of my business has the highest revenue growth ([SubCategory=Sports Equipment, Gender=Female]), what slices are lagging in revenue per user ([State=CA, Age=20-30]). Many user defined metrics are supported including common aggregations such as SUM, COUNT, DISTINCT COUNT and more complex ones such as AVERAGE. We implemented and deployed TRACE for a variety of business use cases. ",
        "title": "TRACE: A Time-Relational Approximate Cubing Engine for Fast Data  Insights",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06337",
        "abstract_url": "http://arxiv.org/abs/2401.06337",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Zhaoming"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  User interaction is one of the most effective ways to improve the ontology alignment quality. However, this approach faces the challenge of how users can participate effectively in the matching process. To solve this challenge. In this paper, an interactive ontology alignment approach using compact differential evolution algorithm with adaptive parameter control (IOACDE) is proposed. In this method, the ontology alignment process is modeled as an interactive optimization problem and users are allowed to intervene in matching in two ways. One is that the mapping suggestions generated by IOACDE as a complete candidate alignment is evaluated by user during optimization process. The other is that the user ameliorates the alignment results by evaluating single mapping after the automatic matching process. To demonstrate the effectiveness of the proposed algorithm, the neural embedding model and K nearest neighbor (KNN) is employed to simulate user for the ontologies of the real world. The experimental results show that the proposed interactive approach can improve the alignment quality compared to the non-interactive. Compared with the state-of-the-art methods from OAEI, the results show that the proposed algorithm has a better performance under the same error rate. ",
        "title": "An ontology alignment method with user intervention using compact  differential evolution with adaptive parameter control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06340",
        "abstract_url": "http://arxiv.org/abs/2401.06340",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xujin"
            },
            {
                "last_name": "Wei",
                "first_name": "Wei"
            },
            {
                "last_name": "Qiu",
                "first_name": "Shuang"
            },
            {
                "last_name": "He",
                "first_name": "Huiguang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an efficient technology for target retrieval using electroencephalography (EEG) signals. The performance improvement of traditional decoding methods relies on a substantial amount of training data from new test subjects, which increases preparation time for BCI systems. Several studies introduce data from existing subjects to reduce the dependence of performance improvement on data from new subjects, but their optimization strategy based on adversarial learning with extensive data increases training time during the preparation procedure. Moreover, most previous methods only focus on the single-view information of EEG signals, but ignore the information from other views which may further improve performance. To enhance decoding performance while reducing preparation time, we propose a Temporal-Spectral fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a cross-view interaction module is proposed to facilitate information transfer and extract common representations across two-view features extracted from EEG temporal signals and spectrogram images. Then, an attention-based fusion module fuses the features of two views to obtain comprehensive discriminative features for classification. Furthermore, a multi-view consistency loss is proposed to maximize the feature similarity between two views of the same EEG signal. Finally, we propose a subject-specific adapter to rapidly transfer the knowledge of the model trained on data from existing subjects to decode data from new subjects. Experimental results show that TSformer-SA significantly outperforms comparison methods and achieves outstanding performance with limited training data from new subjects. This facilitates efficient decoding and rapid deployment of BCI systems in practical use. ",
        "title": "A Temporal-Spectral Fusion Transformer with Subject-specific Adapter for  Enhancing RSVP-BCI Decoding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06341",
        "abstract_url": "http://arxiv.org/abs/2401.06341",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Shengyi"
            },
            {
                "last_name": "Chen",
                "first_name": "Weifeng"
            },
            {
                "last_name": "Bai",
                "first_name": "Min"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiong"
            },
            {
                "last_name": "Tu",
                "first_name": "Zhuowen"
            },
            {
                "last_name": "Li",
                "first_name": "Li Erran"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Affordance grounding refers to the task of finding the area of an object with which one can interact. It is a fundamental but challenging task, as a successful solution requires the comprehensive understanding of a scene in multiple aspects including detection, localization, and recognition of objects with their parts, of geo-spatial configuration/layout of the scene, of 3D shapes and physics, as well as of the functionality and potential interaction of the objects and humans. Much of the knowledge is hidden and beyond the image content with the supervised labels from a limited training set. In this paper, we make an attempt to improve the generalization capability of the current affordance grounding by taking the advantage of the rich world, abstract, and human-object-interaction knowledge from pretrained large-scale vision language models. Under the AGD20K benchmark, our proposed model demonstrates a significant performance gain over the competing methods for in-the-wild object affordance grounding. We further demonstrate it can ground affordance for objects from random Internet images, even if both objects and actions are unseen during training. Project site: https://jasonqsy.github.io/AffordanceLLM/ ",
        "title": "AffordanceLLM: Grounding Affordance from Vision Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06344",
        "abstract_url": "http://arxiv.org/abs/2401.06344",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weizheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Le"
            },
            {
                "last_name": "Yang",
                "first_name": "Baijian"
            },
            {
                "last_name": "Chen",
                "first_name": "Guohua"
            },
            {
                "last_name": "Min",
                "first_name": "Byung-Cheol"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer network. Hyper-STTN outperformes other state-of-the-art baselines and ablation models on 5 real-world pedestrian motion datasets. ",
        "title": "Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for  Human Trajectory Prediction with Hypergraph Reasoning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06345",
        "abstract_url": "http://arxiv.org/abs/2401.06345",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Chang"
            },
            {
                "last_name": "Peng",
                "first_name": "Junran"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoxiang"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            },
            {
                "last_name": "Lei",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The text-to-image synthesis by diffusion models has recently shown remarkable performance in generating high-quality images. Although performs well for simple texts, the models may get confused when faced with complex texts that contain multiple objects or spatial relationships. To get the desired images, a feasible way is to manually adjust the textual descriptions, i.e., narrating the texts or adding some words, which is labor-consuming. In this paper, we propose a framework to learn the proper textual descriptions for diffusion models through prompt learning. By utilizing the quality guidance and the semantic guidance derived from the pre-trained diffusion model, our method can effectively learn the prompts to improve the matches between the input text and the generated images. Extensive experiments and analyses have validated the effectiveness of the proposed method. ",
        "title": "Seek for Incantations: Towards Accurate Text-to-Image Diffusion  Synthesis through Prompt Engineering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06349",
        "abstract_url": "http://arxiv.org/abs/2401.06349",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yifeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Ke"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yihan"
            },
            {
                "last_name": "Wang",
                "first_name": "Haohan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Automated diagnosis of AD in brain images is becoming a clinically important technique to support precision and efficient diagnosis and treatment planning. A few efforts have been made to automatically diagnose AD in magnetic resonance imaging (MRI) using three-dimensional CNNs. However, due to the complexity of 3D models, the performance is still unsatisfactory, both in terms of accuracy and efficiency. To overcome the complexities of 3D images and 3D models, in this study, we aim to attack this problem with 2D vision Transformers. We propose a 2D transformer-based medical image model with various transformer attention encoders to diagnose AD in 3D MRI images, by cutting the 3D images into multiple 2D slices.The model consists of four main components: shared encoders across three dimensions, dimension-specific encoders, attention across images from the same dimension, and attention across three dimensions. It is used to obtain attention relationships among multiple sequences from different dimensions (axial, coronal, and sagittal) and multiple slices. We also propose morphology augmentation, an erosion and dilation based method to increase the structural difference between AD and normal images. In this experiment, we use multiple datasets from ADNI, AIBL, MIRAID, OASIS to show the performance of our model. Our proposed MedTransformer demonstrates a strong ability in diagnosing AD. These results demonstrate the effectiveness of MedTransformer in learning from 3D data using a much smaller model and its capability to generalize among different medical tasks, which provides a possibility to help doctors diagnose AD in a simpler way. ",
        "title": "MedTransformer: Accurate AD Diagnosis for 3D MRI Images through 2D  Vision Transformers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06354",
        "abstract_url": "http://arxiv.org/abs/2401.06354",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Jungpyo"
            },
            {
                "last_name": "Lee",
                "first_name": "Sebastian D."
            },
            {
                "last_name": "Huh",
                "first_name": "Tae Myung"
            },
            {
                "last_name": "Stuart",
                "first_name": "Hannah S."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Suction cups offer a useful gripping solution, particularly in industrial robotics and warehouse applications. Vision-based grasp algorithms, like Dex-Net, show promise but struggle to accurately perceive dark or reflective objects, sub-resolution features, and occlusions, resulting in suction cup grip failures. In our prior work, we designed the Smart Suction Cup, which estimates the flow state within the cup and provides a mechanically resilient end-effector that can inform arm feedback control through a sense of touch. We then demonstrated how this cup's signals enable haptically-driven search behaviors for better grasping points on adversarial objects. This prior work uses a model-based approach to predict the desired motion direction, which opens up the question: does a data-driven approach perform better? This technical report provides an initial analysis harnessing the data previously collected. Specifically, we compare the model-based method with a preliminary data-driven approach to accurately estimate lateral pose adjustment direction for improved grasp success. ",
        "title": "Initial Analysis of Data-Driven Haptic Search for the Smart Suction Cup",
        "date": "2023-10-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06356",
        "abstract_url": "http://arxiv.org/abs/2401.06356",
        "authors": [
            {
                "last_name": "Sultan",
                "first_name": "Md Arafat"
            },
            {
                "last_name": "Trivedi",
                "first_name": "Aashka"
            },
            {
                "last_name": "Awasthy",
                "first_name": "Parul"
            },
            {
                "last_name": "Sil",
                "first_name": "Avirup"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board. ",
        "title": "An Empirical Investigation into the Effect of Parameter Choices in  Knowledge Distillation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06361",
        "abstract_url": "http://arxiv.org/abs/2401.06361",
        "authors": [
            {
                "last_name": "Sola",
                "first_name": "Mar Canet"
            },
            {
                "last_name": "Guljajeva",
                "first_name": "Varvara"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper discusses the artwork \"Visions of Destruction\", with a primary conceptual focus on the Anthropocene, which is communicated through audience interaction and generative AI as artistic research methods. Gaze-based interaction transitions the audience from mere observers to agents of landscape transformation, fostering a profound, on-the-edge engagement with pressing issues such as climate change and planetary destruction. The paper looks into early references of interactive art history that deploy eye-tracking as a method for audience interaction, and presents recent AI-aided artworks that demonstrate interactive latent space navigation. ",
        "title": "Visions Of Destruction: Exploring Human Impact on Nature by Navigating  the Latent Space of a Diffusion Model via Gaze",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06366",
        "abstract_url": "http://arxiv.org/abs/2401.06366",
        "authors": [
            {
                "last_name": "Lyu",
                "first_name": "Minzhao"
            },
            {
                "last_name": "Madanapalli",
                "first_name": "Sharat Chandra"
            },
            {
                "last_name": "Vishwanath",
                "first_name": "Arun"
            },
            {
                "last_name": "Sivaraman",
                "first_name": "Vijay"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "PF"
        ],
        "abstract": "  Cloud gaming, wherein game graphics is rendered in the cloud and streamed back to the user as real-time video, expands the gaming market to billions of users who do not have gaming consoles or high-power graphics PCs. Companies like Nvidia, Amazon, Sony and Microsoft are investing in building cloud gaming platforms to tap this large unserved market. However, cloud gaming requires the user to have high bandwidth and stable network connectivity - whereas a typical console game needs about 100-200 kbps, a cloud game demands minimum 10-20 Mbps. This makes the Internet Service Provider (ISP) a key player in ensuring the end-user's good gaming experience. In this paper we develop a method to detect user experience to detect Nvidia's GeForce NOW cloud gaming sessions over their network infrastructure, and measure associated user experience. In particular, we envision ISPs taking advantage of our method to provision network capacity at the right time and in the right place to support growth in cloud gaming at the right experience level; as well as identify the role of contextual factors such as user setup (browser vs app) and connectivity type (wired vs wireless) in performance degradation. We first present a detailed anatomy of flow establishment and volumetric profiles of cloud gaming sessions over multiple platforms, followed by a method to detect gameplay and measure key experience aspects such as latency, frame rate and resolution via real-time analysis of network traffic. The insights and methods are also validated in the lab for XBox Cloud Gaming platform. We then implement and deploy our method in a campus network to capture gameplay behaviors and experience measures across various user setups and connectivity types which we believe are valuable for network operators. ",
        "title": "Network Anatomy and Real-Time Measurement of Nvidia GeForce NOW Cloud  Gaming",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06370",
        "abstract_url": "http://arxiv.org/abs/2401.06370",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yueyi"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Huang",
                "first_name": "Wei"
            },
            {
                "last_name": "Hu",
                "first_name": "Bo"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Wu",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Instance-aware embeddings predicted by deep neural networks have revolutionized biomedical instance segmentation, but its resource requirements are substantial. Knowledge distillation offers a solution by transferring distilled knowledge from heavy teacher networks to lightweight yet high-performance student networks. However, existing knowledge distillation methods struggle to extract knowledge for distinguishing instances and overlook global relation information. To address these challenges, we propose a graph relation distillation approach for efficient biomedical instance segmentation, which considers three essential types of knowledge: instance-level features, instance relations, and pixel-level boundaries. We introduce two graph distillation schemes deployed at both the intra-image level and the inter-image level: instance graph distillation (IGD) and affinity graph distillation (AGD). IGD constructs a graph representing instance features and relations, transferring these two types of knowledge by enforcing instance graph consistency. AGD constructs an affinity graph representing pixel relations to capture structured knowledge of instance boundaries, transferring boundary-related knowledge by ensuring pixel affinity consistency. Experimental results on a number of biomedical datasets validate the effectiveness of our approach, enabling student models with less than $ 1\\%$ parameters and less than $10\\%$ inference time while achieving promising performance compared to teacher models. ",
        "title": "Graph Relation Distillation for Efficient Biomedical Instance  Segmentation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06373",
        "abstract_url": "http://arxiv.org/abs/2401.06373",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Yi"
            },
            {
                "last_name": "Lin",
                "first_name": "Hongpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Yang",
                "first_name": "Diyi"
            },
            {
                "last_name": "Jia",
                "first_name": "Ruoxi"
            },
            {
                "last_name": "Shi",
                "first_name": "Weiyan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs ",
        "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to  Challenge AI Safety by Humanizing LLMs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06374",
        "abstract_url": "http://arxiv.org/abs/2401.06374",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Haoxuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Junyu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the emergence of foundation model, this novel paradigm of deep learning has encouraged many powerful achievements in natural language processing and computer vision. There are many advantages of foundation model, such as excellent feature extraction power, mighty generalization ability, great few-shot and zero-shot learning capacity, etc. which are beneficial to vision tasks. As the unique identity of vehicle, different countries and regions have diverse license plate (LP) styles and appearances, and even different types of vehicles have different LPs. However, recent deep learning based license plate detectors are mainly trained on specific datasets, and these limited datasets constrain the effectiveness and robustness of LP detectors. To alleviate the negative impact of limited data, an attempt to exploit the advantages of foundation model is implement in this paper. We customize a vision foundation model, i.e. Segment Anything Model (SAM), for LP detection task and propose the first LP detector based on vision foundation model, named SamLP. Specifically, we design a Low-Rank Adaptation (LoRA) fine-tuning strategy to inject extra parameters into SAM and transfer SAM into LP detection task. And then, we further propose a promptable fine-tuning step to provide SamLP with prompatable segmentation capacity. The experiments show that our proposed SamLP achieves promising detection performance compared to other LP detectors. Meanwhile, the proposed SamLP has great few-shot and zero-shot learning ability, which shows the potential of transferring vision foundation model. The code is available at https://github.com/Dinghaoxuan/SamLP ",
        "title": "SamLP: A Customized Segment Anything Model for License Plate Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06375",
        "abstract_url": "http://arxiv.org/abs/2401.06375",
        "authors": [
            {
                "last_name": "Banks",
                "first_name": "Gordon"
            },
            {
                "last_name": "Bierhuizen",
                "first_name": "Gates"
            },
            {
                "last_name": "McCrum",
                "first_name": "Katherine"
            },
            {
                "last_name": "Wengert",
                "first_name": "Ellen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We examine ProcessGPT, an AI model designed to automate, augment, and improve business processes, to study the challenges of managing business processes within the cognitive limitations of the human workforce, particularly individuals with cognitive disabilities. ProcessGPT provides a blueprint for designing efficient business processes that take into account human cognitive limitations. By viewing this through the lens of cognitive disabilities, we show that ProcessGPT improves process usability for individuals with and without cognitive disabilities. We also demonstrate that organizations implementing ProcessGPT-like capabilities will realize increased productivity, morale, and inclusion. ",
        "title": "Cognitive BPM as an Equalizer: Improving Access and Efficiency for  Employees with (and without) Cognitive Disabilities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06377",
        "abstract_url": "http://arxiv.org/abs/2401.06377",
        "authors": [
            {
                "last_name": "Qi",
                "first_name": "Xinda"
            },
            {
                "last_name": "Mei",
                "first_name": "Yu"
            },
            {
                "last_name": "Chen",
                "first_name": "Dong"
            },
            {
                "last_name": "Li",
                "first_name": "Zhaojian"
            },
            {
                "last_name": "Tan",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We propose a novel multi-section cable-driven soft robotic arm inspired by octopus tentacles along with a new modeling approach. Each section of the modular manipulator is made of a soft tubing backbone, a soft silicon arm body, and two rigid endcaps, which connect adjacent sections and decouple the actuation cables of different sections. The soft robotic arm is made with casting after the rigid endcaps are 3D-printed, achieving low-cost and convenient fabrication. To capture the nonlinear effect of cables pushing into the soft silicon arm body, which results from the absence of intermediate rigid cable guides for higher compliance, an analytical static model is developed to capture the relationship between the bending curvature and the cable lengths. The proposed model shows superior prediction performance in experiments over that of a baseline model, especially under large bending conditions. Based on the nonlinear static model, a kinematic model of a multi-section arm is further developed and used to derive a motion planning algorithm. Experiments show that the proposed soft arm has high flexibility and a large workspace, and the tracking errors under the algorithm based on the proposed modeling approach are up to 52$\\%$ smaller than those with the algorithm derived from the baseline model. The presented modeling approach is expected to be applicable to a broad range of soft cable-driven actuators and manipulators. ",
        "title": "Design and Nonlinear Modeling of a Modular Cable Driven Soft Robotic Arm",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06378",
        "abstract_url": "http://arxiv.org/abs/2401.06378",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Prantar"
            },
            {
                "last_name": "Shah",
                "first_name": "Vihan"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We show new lower bounds in the \\emph{Merlin-Arthur} (MA) communication model and the related \\emph{annotated streaming} or stream verification model. The MA communication model is an enhancement of the classical communication model, where in addition to the usual players Alice and Bob, there is an all-powerful but untrusted player Merlin who knows their inputs and tries to convince them about the output. Most functions have MA protocols with total communication significantly smaller than what would be needed without Merlin. We focus on the online MA (OMA) model, which is the MA analogue of one-way communication, and introduce the notion of \\emph{non-trivial-OMA} complexity of a function. This is the minimum total communication needed by any non-trivial OMA protocol computing that function, where a trivial OMA protocol is one where Alice sends Bob roughly as many bits as she would have sent without Merlin. We prove a lower bound on the non-trivial-OMA complexity of a natural function \\emph{Equals-Index} (basically the well-known Index problem on large domains) and identify it as a canonical problem for proving strong lower bounds on this complexity: reductions from it (i) reproduce and/or improve upon the lower bounds for all functions that were previously known to have large non-trivial-OMA complexity, (ii) exhibit the first explicit functions whose non-trivial-OMA complexity is superlinear, and even exponential, in their classical one-way complexity, and (iii) show functions on input size $n$ for which this complexity is as large as $n/\\log n$. While exhibiting a function with $\\omega(\\sqrt{n})$ (standard) OMA complexity is a longstanding open problem, we did not even know of any function with $\\omega(\\sqrt{n})$ non-trivial-OMA complexity. We further extend the lower bounds to a related streaming model called annotated streaming. ",
        "title": "New Lower Bounds in Merlin-Arthur Communication and Graph Streaming  Verification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06379",
        "abstract_url": "http://arxiv.org/abs/2401.06379",
        "authors": [
            {
                "last_name": "Daggitt",
                "first_name": "Matthew L."
            },
            {
                "last_name": "Kokke",
                "first_name": "Wen"
            },
            {
                "last_name": "Atkey",
                "first_name": "Robert"
            },
            {
                "last_name": "Slusarz",
                "first_name": "Natalia"
            },
            {
                "last_name": "Arnaboldi",
                "first_name": "Luca"
            },
            {
                "last_name": "Komendantskaya",
                "first_name": "Ekaterina"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Neuro-symbolic programs -- programs containing both machine learning components and traditional symbolic code -- are becoming increasingly widespread. However, we believe that there is still a lack of a general methodology for verifying these programs whose correctness depends on the behaviour of the machine learning components. In this paper, we identify the ``embedding gap'' -- the lack of techniques for linking semantically-meaningful ``problem-space'' properties to equivalent ``embedding-space'' properties -- as one of the key issues, and describe Vehicle, a tool designed to facilitate the end-to-end verification of neural-symbolic programs in a modular fashion. Vehicle provides a convenient language for specifying ``problem-space'' properties of neural networks and declaring their relationship to the ``embedding-space\", and a powerful compiler that automates interpretation of these properties in the language of a chosen machine-learning training environment, neural network verifier, and interactive theorem prover. We demonstrate Vehicle's utility by using it to formally verify the safety of a simple autonomous car equipped with a neural network controller. ",
        "title": "Vehicle: Bridging the Embedding Gap in the Verification of  Neuro-Symbolic Programs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06382",
        "abstract_url": "http://arxiv.org/abs/2401.06382",
        "authors": [
            {
                "last_name": "Adkins",
                "first_name": "Mark"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CL"
        ],
        "abstract": "  As Artificial Intelligence (AI) technology becomes more and more prevalent, it becomes increasingly important to explore how we as humans interact with AI. The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer Interaction (HCI) field and aims to examine this very notion. Many interaction patterns have been implemented without fully understanding the changes in required cognition as well as the cognitive science implications of using these alternative interfaces that aim to be more human-like in nature. Prior research suggests that theory of mind representations are crucial to successful and effortless communication, however very little is understood when it comes to how theory of mind representations are established when interacting with AI. ",
        "title": "What should I say? -- Interacting with AI and Natural Language  Interfaces",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06384",
        "abstract_url": "http://arxiv.org/abs/2401.06384",
        "authors": [
            {
                "last_name": "Mollah",
                "first_name": "Muhammad Baqer"
            },
            {
                "last_name": "Azad",
                "first_name": "Md Abul Kalam"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yinghui"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  Smart devices are considered as an integral part of Internet of Things (IoT), have an aim to make a dynamic network to exchange information, collect data, analysis, and make optimal decisions in an autonomous way to achieve more efficient, automatic, and economical services. Message dissemination among these smart devices allows adding new features, sending updated instructions, alerts or safety messages, informing the pricing information or billing amount, incentives, and installing security patches. On one hand, such message disseminations are directly beneficial to the all parties involved in the IoT system. On the other hand, due to remote procedure, smart devices, vendors, and other involved authorities might have to meet a number of security, privacy, and performance related concerns while disseminating messages among targeted devices. To this end, in this paper, we design STarEdgeChain, a security and privacy aware targeted message dissemination in IoT to show how blockchain along with advanced cryptographic techniques are devoted to address such concerns. In fact, the STarEdgeChain employs a permissioned blockchain assisted edge computing in order to expedite a single signcrypted message dissemination among targeted groups of devices, at the same time avoiding the dependency of utilizing multiple unicasting approaches. Finally, we develop a software prototype of STarEdgeChain and show it's practicability for smart devices. The codes are publicly available at https://github.com/mbaqer/Blockchain-IoT ",
        "title": "Secure Targeted Message Dissemination in IoT Using Blockchain Enabled  Edge Computing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06385",
        "abstract_url": "http://arxiv.org/abs/2401.06385",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Zhenlong"
            },
            {
                "last_name": "Cao",
                "first_name": "Jiakai"
            },
            {
                "last_name": "Li",
                "first_name": "Zhaoxin"
            },
            {
                "last_name": "Jiang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaoqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we introduce Segmentation-Driven Deformation Multi-View Stereo (SD-MVS), a method that can effectively tackle challenges in 3D reconstruction of textureless areas. We are the first to adopt the Segment Anything Model (SAM) to distinguish semantic instances in scenes and further leverage these constraints for pixelwise patch deformation on both matching cost and propagation. Concurrently, we propose a unique refinement strategy that combines spherical coordinates and gradient descent on normals and pixelwise search interval on depths, significantly improving the completeness of reconstructed 3D model. Furthermore, we adopt the Expectation-Maximization (EM) algorithm to alternately optimize the aggregate matching cost and hyperparameters, effectively mitigating the problem of parameters being excessively dependent on empirical tuning. Evaluations on the ETH3D high-resolution multi-view stereo benchmark and the Tanks and Temples dataset demonstrate that our method can achieve state-of-the-art results with less time consumption. ",
        "title": "SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical  Refinement and EM optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06386",
        "abstract_url": "http://arxiv.org/abs/2401.06386",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Minrui"
            },
            {
                "last_name": "Niyato",
                "first_name": "Dusit"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zehui"
            },
            {
                "last_name": "Guo",
                "first_name": "Song"
            },
            {
                "last_name": "Fang",
                "first_name": "Yuguang"
            },
            {
                "last_name": "Kim",
                "first_name": "Dong In"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Mobile multimedia networks (MMNs) demonstrate great potential in delivering low-latency and high-quality entertainment and tactical applications, such as short-video sharing, online conferencing, and battlefield surveillance. For instance, in tactical surveillance of battlefields, scalability and sustainability are indispensable for maintaining large-scale military multimedia applications in MMNs. Therefore, many data-driven networking solutions are leveraged to optimize streaming strategies based on real-time traffic analysis and resource monitoring. In addition, generative AI (GAI) can not only increase the efficiency of existing data-driven solutions through data augmentation but also develop potential capabilities for MMNs, including AI-generated content (AIGC) and AI-aided perception. In this article, we propose the framework of GAI-enabled MMNs that leverage the capabilities of GAI in data and content synthesis to distribute high-quality and immersive interactive content in wireless networks. Specifically, we outline the framework of GAI-enabled MMNs and then introduce its three main features, including distribution, generation, and perception. Furthermore, we propose a second-score auction mechanism for allocating network resources by considering GAI model values and other metrics jointly. The experimental results show that the proposed auction mechanism can effectively increase social welfare by allocating resources and models with the highest user satisfaction. ",
        "title": "Generative AI-enabled Mobile Tactical Multimedia Networks: Distribution,  Generation, and Perception",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06387",
        "abstract_url": "http://arxiv.org/abs/2401.06387",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Ye-Xin"
            },
            {
                "last_name": "Ai",
                "first_name": "Yang"
            },
            {
                "last_name": "Du",
                "first_name": "Hui-Peng"
            },
            {
                "last_name": "Ling",
                "first_name": "Zhen-Hua"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Speech bandwidth extension (BWE) refers to widening the frequency bandwidth range of speech signals, enhancing the speech quality towards brighter and fuller. This paper proposes a generative adversarial network (GAN) based BWE model with parallel prediction of Amplitude and Phase spectra, named AP-BWE, which achieves both high-quality and efficient wideband speech waveform generation. The proposed AP-BWE generator is entirely based on convolutional neural networks (CNNs). It features a dual-stream architecture with mutual interaction, where the amplitude stream and the phase stream communicate with each other and respectively extend the high-frequency components from the input narrowband amplitude and phase spectra. To improve the naturalness of the extended speech signals, we employ a multi-period discriminator at the waveform level and design a pair of multi-resolution amplitude and phase discriminators at the spectral level, respectively. Experimental results demonstrate that our proposed AP-BWE achieves state-of-the-art performance in terms of speech quality for BWE tasks targeting sampling rates of both 16 kHz and 48 kHz. In terms of generation efficiency, due to the all-convolutional architecture and all-frame-level operations, the proposed AP-BWE can generate 48 kHz waveform samples 292.3 times faster than real-time on a single RTX 4090 GPU and 18.1 times faster than real-time on a single CPU. Notably, to our knowledge, AP-BWE is the first to achieve the direct extension of the high-frequency phase spectrum, which is beneficial for improving the effectiveness of existing BWE methods. ",
        "title": "Towards High-Quality and Efficient Speech Bandwidth Extension with  Parallel Amplitude and Phase Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06390",
        "abstract_url": "http://arxiv.org/abs/2401.06390",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Fan"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoxu"
            },
            {
                "last_name": "Shi",
                "first_name": "Xian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shiliang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "MM"
        ],
        "abstract": "  The growing prevalence of online conferences and courses presents a new challenge in improving automatic speech recognition (ASR) with enriched textual information from video slides. In contrast to rare phrase lists, the slides within videos are synchronized in real-time with the speech, enabling the extraction of long contextual bias. Therefore, we propose a novel long-context biasing network (LCB-net) for audio-visual speech recognition (AVSR) to leverage the long-context information available in videos effectively. Specifically, we adopt a bi-encoder architecture to simultaneously model audio and long-context biasing. Besides, we also propose a biasing prediction module that utilizes binary cross entropy (BCE) loss to explicitly determine biased phrases in the long-context biasing. Furthermore, we introduce a dynamic contextual phrases simulation to enhance the generalization and robustness of our LCB-net. Experiments on the SlideSpeech, a large-scale audio-visual corpus enriched with slides, reveal that our proposed LCB-net outperforms general ASR model by 9.4%/9.1%/10.9% relative WER/U-WER/B-WER reduction on test set, which enjoys high unbiased and biased performance. Moreover, we also evaluate our model on LibriSpeech corpus, leading to 23.8%/19.2%/35.4% relative WER/U-WER/B-WER reduction over the ASR model. ",
        "title": "LCB-net: Long-Context Biasing for Audio-Visual Speech Recognition",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06391",
        "abstract_url": "http://arxiv.org/abs/2401.06391",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jian"
            },
            {
                "last_name": "Feng",
                "first_name": "Yebo"
            },
            {
                "last_name": "Li",
                "first_name": "Tianlin"
            },
            {
                "last_name": "Sun",
                "first_name": "Weisong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Peng",
                "first_name": "Xin"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Recent code large language models (LLMs) have shown promising performance in generating standalone functions but face limitations in repository-level code generation due to their lack of awareness of repository-level dependencies (e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen, an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Data Augmentation and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding docstrings, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one.   We conduct comprehensive experiments to evaluate ToolGen's effectiveness in repository-level code generation. To facilitate this evaluation, we create a benchmark comprising 680 real-world code repositories and introduce two new repository-level metrics: Dependency Coverage and Success Rate. The results demonstrate that ToolGen significantly improves dependency coverage by 15.2% to 45.8% and success rates by 10.9% to 42.2% across three distinct code LLMs, while maintaining competitive performance in widely-recognized similarity metrics. Furthermore, our generalizability evaluation confirms ToolGen's consistent performance when applied to diverse code LLMs, including various model architectures and scales. ",
        "title": "Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code  Generation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06394",
        "abstract_url": "http://arxiv.org/abs/2401.06394",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Wenyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xinghua"
            },
            {
                "last_name": "Cui",
                "first_name": "Shiyao"
            },
            {
                "last_name": "Huang",
                "first_name": "Kun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuebin"
            },
            {
                "last_name": "Liu",
                "first_name": "Tingwen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment elements for a given sentence, which is a critical task in the field of aspect-based sentiment analysis. However, the data imbalance issue has not received sufficient attention in ASQP task. In this paper, we divide the issue into two-folds, quad-pattern imbalance and aspect-category imbalance, and propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance issue. Specifically, a data augmentation process with a condition function adaptively enhances the tail quad patterns and aspect categories, alleviating the data imbalance in ASQP. Following previous studies, we also further explore the generative framework for extracting complete quads by introducing the category prior knowledge and syntax-guided decoding target. Experimental results demonstrate that data augmentation for imbalance in ASQP task can improve the performance, and the proposed ADA method is superior to naive data oversampling. ",
        "title": "Adaptive Data Augmentation for Aspect Sentiment Quad Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06395",
        "abstract_url": "http://arxiv.org/abs/2401.06395",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Bohan"
            },
            {
                "last_name": "Wu",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Humans possess the capability to comprehend diverse modalities and seamlessly transfer information between them. In this work, we introduce ModaVerse, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio. Predominant MLLM frameworks have largely relied on the alignment of latent spaces of textual and non-textual features. This alignment process, which synchronizes a language model trained on textual data with encoders and decoders trained on multi-modal data, often necessitates extensive training of several projection layers in multiple stages. Inspired by LLM-as-agent methodologies, we propose a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language. It aligns the LLM's output with the input of generative models, avoiding the complexities associated with latent feature alignments, and simplifying the multiple training stages of existing MLLMs into a single, efficient process. This conceptual advancement leads to significant reductions in both data and computational costs. By conducting experiments on several benchmarks, we demonstrate that our approach attains comparable performance with the state of the art while achieving considerable efficiencies in data usage and training duration. ",
        "title": "ModaVerse: Efficiently Transforming Modalities with LLMs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06397",
        "abstract_url": "http://arxiv.org/abs/2401.06397",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Bowen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Peisen"
            },
            {
                "last_name": "Wang",
                "first_name": "Zichen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaoming"
            },
            {
                "last_name": "Li",
                "first_name": "Jin"
            },
            {
                "last_name": "Dai",
                "first_name": "Wenrui"
            },
            {
                "last_name": "Zou",
                "first_name": "Junni"
            },
            {
                "last_name": "Xiong",
                "first_name": "Hongkai"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaopeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vision-language foundation models, represented by Contrastive language-image pre-training (CLIP), have gained increasing attention for jointly understanding both vision and textual tasks. However, existing approaches primarily focus on training models to match global image representations with textual descriptions, thereby overlooking the critical alignment between local regions and corresponding text tokens. This paper extends CLIP with multi-granularity alignment. Notably, we deliberately construct a new dataset comprising pseudo annotations at various levels of granularities, encompassing image-level, region-level, and pixel-level captions/tags. Accordingly, we develop a unified multi-granularity learning framework, named UMG-CLIP, that simultaneously empowers the model with versatile perception abilities across different levels of detail. Equipped with parameter efficient tuning, UMG-CLIP surpasses current widely used CLIP models and achieves state-of-the-art performance on diverse image understanding benchmarks, including open-world recognition, retrieval, semantic segmentation, and panoptic segmentation tasks. We hope UMG-CLIP can serve as a valuable option for advancing vision-language foundation models. ",
        "title": "UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World  Understanding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06398",
        "abstract_url": "http://arxiv.org/abs/2401.06398",
        "authors": [
            {
                "last_name": "Das",
                "first_name": "Sudhansu Bala"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Leo Raphael"
            },
            {
                "last_name": "Mishra",
                "first_name": "Tapas Kumar"
            },
            {
                "last_name": "Patra",
                "first_name": "Bidyut Kr."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The conversion of content from one language to another utilizing a computer system is known as Machine Translation (MT). Various techniques have come up to ensure effective translations that retain the contextual and lexical interpretation of the source language. End-to-end Neural Machine Translation (NMT) is a popular technique and it is now widely used in real-world MT systems. Massive amounts of parallel datasets (sentences in one language alongside translations in another) are required for MT systems. These datasets are crucial for an MT system to learn linguistic structures and patterns of both languages during the training phase. One such dataset is Samanantar, the largest publicly accessible parallel dataset for Indian languages (ILs). Since the corpus has been gathered from various sources, it contains many incorrect translations. Hence, the MT systems built using this dataset cannot perform to their usual potential. In this paper, we propose an algorithm to remove mistranslations from the training corpus and evaluate its performance and efficiency. Two Indic languages (ILs), namely, Hindi (HIN) and Odia (ODI) are chosen for the experiment. A baseline NMT system is built for these two ILs, and the effect of different dataset sizes is also investigated. The quality of the translations in the experiment is evaluated using standard metrics such as BLEU, METEOR, and RIBES. From the results, it is observed that removing the incorrect translation from the dataset makes the translation quality better. It is also noticed that, despite the fact that the ILs-English and English-ILs systems are trained using the same corpus, ILs-English works more effectively across all the evaluation metrics. ",
        "title": "An approach for mistranslation removal from popular dataset for Indic MT  Task",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06401",
        "abstract_url": "http://arxiv.org/abs/2401.06401",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Li",
                "first_name": "Ge"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Li",
                "first_name": "Yongmin"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hao"
            },
            {
                "last_name": "Liu",
                "first_name": "Huanyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Kaibo"
            },
            {
                "last_name": "Wang",
                "first_name": "Lecheng"
            },
            {
                "last_name": "Fang",
                "first_name": "Zheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lanshen"
            },
            {
                "last_name": "Ding",
                "first_name": "Jiazheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xuanming"
            },
            {
                "last_name": "Dong",
                "first_name": "Yihong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            },
            {
                "last_name": "Yang",
                "first_name": "Mengfei"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL"
        ],
        "abstract": "  How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experiments. We also discuss the challenges and future directions of code generation in practical projects. We open-source DevEval and hope it can facilitate the development of code generation in practical projects. ",
        "title": "DevEval: Evaluating Code Generation in Practical Software Projects",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06405",
        "abstract_url": "http://arxiv.org/abs/2401.06405",
        "authors": [
            {
                "last_name": "Kimura",
                "first_name": "Kei"
            },
            {
                "last_name": "Makino",
                "first_name": "Kazuhisa"
            },
            {
                "last_name": "Yamada",
                "first_name": "Shota"
            },
            {
                "last_name": "Yoshizumi",
                "first_name": "Ryo"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Characterizing the solution sets in a problem by closedness under operations is recognized as one of the key aspects of algorithm development, especially in constraint satisfaction. An example from the Boolean satisfiability problem is that the solution set of a Horn conjunctive normal form (CNF) is closed under the minimum operation, and this property implies that minimizing a nonnegative linear function over a Horn CNF can be done in polynomial time. In this paper, we focus on the set of integer points (vectors) in a polyhedron, and study the relation between these sets and closedness under operations from the viewpoint of 2-decomposability. By adding further conditions to the 2-decomposable polyhedra, we show that important classes of sets of integer vectors in polyhedra are characterized by 2-decomposability and closedness under certain operations, and in some classes, by closedness under operations alone. The most prominent result we show is that the set of integer vectors in a unit-two-variable-per-inequality polyhedron can be characterized by closedness under the median and directed discrete midpoint operations, each of these operations was independently considered in constraint satisfaction and discrete convex analysis. ",
        "title": "Characterizing the integer points in 2-decomposable polyhedra by  closedness under operations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06406",
        "abstract_url": "http://arxiv.org/abs/2401.06406",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Lingchao"
            },
            {
                "last_name": "Wang",
                "first_name": "Hairong"
            },
            {
                "last_name": "Hu",
                "first_name": "Leland S."
            },
            {
                "last_name": "Tran",
                "first_name": "Nhan L"
            },
            {
                "last_name": "Canoll",
                "first_name": "Peter D"
            },
            {
                "last_name": "Swanson",
                "first_name": "Kristin R"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning has enabled in-depth analysis of rich multi-omics profiles and medical imaging for cancer diagnosis and prognosis. Despite these advancements, machine learning models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to surmount these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art machine learning studies that adopted the fusion of biomedical knowledge and data, termed knowledge-informed machine learning, for cancer diagnosis and prognosis. Emphasizing the properties inherent in four primary data types including clinical, imaging, molecular, and treatment data, we highlight modeling considerations relevant to these contexts. We provide an overview of diverse forms of knowledge representation and current strategies of knowledge integration into machine learning pipelines with concrete examples. We conclude the review article by discussing future directions to advance cancer research through knowledge-informed machine learning. ",
        "title": "Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis:  A review",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06407",
        "abstract_url": "http://arxiv.org/abs/2401.06407",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jincheng"
            },
            {
                "last_name": "Wolek",
                "first_name": "Artur"
            },
            {
                "last_name": "Willis",
                "first_name": "Andrew R."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  This article presents a comprehensive review of and analysis of state-of-the-art mapping algorithms for UAV (Unmanned Aerial Vehicle) applications, focusing on canopy-level and high-speed scenarios. This article presents a comprehensive exploration of sensor technologies suitable for UAV mapping, assessing their capabilities to provide measurements that meet the requirements of fast UAV mapping. Furthermore, the study conducts extensive experiments in a simulated environment to evaluate the performance of three distinct mapping algorithms: Direct Sparse Odometry (DSO), Stereo DSO (SDSO), and DSO Lite (DSOL). The experiments delve into mapping accuracy and mapping speed, providing valuable insights into the strengths and limitations of each algorithm. The results highlight the versatility and shortcomings of these algorithms in meeting the demands of modern UAV applications. The findings contribute to a nuanced understanding of UAV mapping dynamics, emphasizing their applicability in complex environments and high-speed scenarios. This research not only serves as a benchmark for mapping algorithm comparisons but also offers practical guidance for selecting sensors tailored to specific UAV mapping applications. ",
        "title": "UAV-borne Mapping Algorithms for Canopy-Level and High-Speed Drone  Applications",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06411",
        "abstract_url": "http://arxiv.org/abs/2401.06411",
        "authors": [
            {
                "last_name": "Aviles",
                "first_name": "Robert S."
            },
            {
                "last_name": "Li",
                "first_name": "Xi"
            },
            {
                "last_name": "Lu",
                "first_name": "Lei"
            },
            {
                "last_name": "Ni",
                "first_name": "Zhaorui"
            },
            {
                "last_name": "Beerel",
                "first_name": "Peter A."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  A key distinguishing feature of single flux quantum (SFQ) circuits is that each logic gate is clocked. This feature forces the introduction of path-balancing flip-flops to ensure proper synchronization of inputs at each gate. This paper proposes a polynomial time complexity approximation algorithm for clocking assignments that minimizes the insertion of path balancing buffers for multi-threaded multi-phase clocking of SFQ circuits. Existing SFQ multi-phase clocking solutions have been shown to effectively reduce the number of required buffers inserted while maintaining high throughput, however, the associated clock assignment algorithms have exponential complexity and can have prohibitively long runtimes for large circuits, limiting the scalability of this approach. Our proposed algorithm is based on a linear program (LP) that leads to solutions that are experimentally on average within 5% of the optimum and helps accelerate convergence towards the optimal integer linear program (ILP) based solution. The improved LP and ILP runtimes permit multi-phase clocking schemes to scale to larger SFQ circuits than previous state of the art clocking assignment methods. We further extend the existing algorithm to support fanout sharing of the added buffers, saving, on average, an additional 10% of the inserted DFFs. Compared to traditional full path balancing (FPB) methods across 10 benchmarks, our enhanced LP saves 79.9%, 87.8%, and 91.2% of the inserted buffers for 2, 3, and 4 clock phases respectively. Finally, we extend this approach to the generation of circuits that completely mitigate potential hold-time violations at the cost of either adding on average less than 10% more buffers (for designs with 3 or more clock phases) or, more generally, adding a clock phase and thereby reducing throughput. ",
        "title": "An Efficient and Scalable Clocking Assignment Algorithm for  Multi-Threaded Multi-Phase Single Flux Quantum Circuits",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06412",
        "abstract_url": "http://arxiv.org/abs/2401.06412",
        "authors": [
            {
                "last_name": "Takamido",
                "first_name": "Ryota"
            },
            {
                "last_name": "Suzuki",
                "first_name": "Chiharu"
            },
            {
                "last_name": "Ota",
                "first_name": "Jun"
            },
            {
                "last_name": "Nakamoto",
                "first_name": "Hiroki"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Background: Simultaneously focusing on intra- and inter-individual body dynamics and elucidating how these affect each other will help understand human inter-personal coordination behavior. However, this association has not been investigated previously owing to difficulties in analyzing complex causal relations among several body components.To address this issue, this study proposes a new analytical framework that attempts to understand the underlying causal structures behind each joint movement of individual baseball players using neural Granger causality (NGC) as the explainable AI. Methods: In the NGC analysis, causal relationships were defined as the size of the weight parameters of the first layer of a machine-learning model trained to predict the future state of a specific time-series variable. To verify the approach in a practical context, we conducted an experiment with 16 pairs of expert baseball pitchers and batters; input datasets with 27 joint resultant velocity data (joints of 13 pitchers and 14 batters) were generated and used for model training.Results: NGC analysis revealed significant causal relations among intra- and inter-individual body components such as the batter's hands having a causal effect from the pitcher's throwing arm. Remarkably, although the causality from the batter's body to pitcher's body is much lower than the reverse, it is significantly correlated with batter performance outcomes. Conclusions: The above results suggest the effectiveness of NGC analysis for understanding whole-body inter-personal coordination dynamics and that of the AI technique as a new approach for analyzing complex human behavior from a different perspective than conventional techniques. ",
        "title": "Understanding whole-body inter-personal dynamics between two players  using neural Granger causality as the explainable AI (XAI)",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06413",
        "abstract_url": "http://arxiv.org/abs/2401.06413",
        "authors": [
            {
                "last_name": "Ahuja",
                "first_name": "Sanju"
            },
            {
                "last_name": "Jain",
                "first_name": "Ridhi"
            },
            {
                "last_name": "Kumar",
                "first_name": "Jyoti"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  'Automating the user away' has been designated as a dark pattern in literature for performing tasks without user consent or confirmation. However, limited studies have been reported on how users experience the sense of autonomy when digital systems fully or partially bypass consent. More research is required to understand what makes automaticity a threat to autonomy. To address this gap, a qualitative interview study with 10 users was conducted to investigate the user experience of Microsoft Windows updates. It was found that ten design features of Windows updates impact the autonomy experience. For each design feature, the contextual factors which influence its impact on autonomy were also noted. The findings of this paper can help designers understand the ethical concerns posed by automaticity in design and identify measures to mitigate these concerns. ",
        "title": "Why Doesn't Microsoft Let Me Sleep? How Automaticity of Windows Updates  Impacts User Autonomy",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06415",
        "abstract_url": "http://arxiv.org/abs/2401.06415",
        "authors": [
            {
                "last_name": "Cha",
                "first_name": "Junuk"
            },
            {
                "last_name": "Lee",
                "first_name": "Hansol"
            },
            {
                "last_name": "Kim",
                "first_name": "Jaewon"
            },
            {
                "last_name": "Truong",
                "first_name": "Nhat Nguyen Bao"
            },
            {
                "last_name": "Yoon",
                "first_name": "Jae Shin"
            },
            {
                "last_name": "Baek",
                "first_name": "Seungryul"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces a novel pipeline to reconstruct the geometry of interacting multi-person in clothing on a globally coherent scene space from a single image. The main challenge arises from the occlusion: a part of a human body is not visible from a single view due to the occlusion by others or the self, which introduces missing geometry and physical implausibility (e.g., penetration). We overcome this challenge by utilizing two human priors for complete 3D geometry and surface contacts. For the geometry prior, an encoder learns to regress the image of a person with missing body parts to the latent vectors; a decoder decodes these vectors to produce 3D features of the associated geometry; and an implicit network combines these features with a surface normal map to reconstruct a complete and detailed 3D humans. For the contact prior, we develop an image-space contact detector that outputs a probability distribution of surface contacts between people in 3D. We use these priors to globally refine the body poses, enabling the penetration-free and accurate reconstruction of interacting multi-person in clothing on the scene space. The results demonstrate that our method is complete, globally coherent, and physically plausible compared to existing methods. ",
        "title": "3D Reconstruction of Interacting Multi-Person in Clothing from a Single  Image",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06416",
        "abstract_url": "http://arxiv.org/abs/2401.06416",
        "authors": [
            {
                "last_name": "Kallini",
                "first_name": "Julie"
            },
            {
                "last_name": "Papadimitriou",
                "first_name": "Isabel"
            },
            {
                "last_name": "Futrell",
                "first_name": "Richard"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            },
            {
                "last_name": "Potts",
                "first_name": "Christopher"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations. ",
        "title": "Mission: Impossible Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06421",
        "abstract_url": "http://arxiv.org/abs/2401.06421",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Geethen"
            },
            {
                "last_name": "Moncrieff",
                "first_name": "Glenn"
            },
            {
                "last_name": "Venter",
                "first_name": "Zander"
            },
            {
                "last_name": "Cawse-Nicholson",
                "first_name": "Kerry"
            },
            {
                "last_name": "Slingsby",
                "first_name": "Jasper"
            },
            {
                "last_name": "Robinson",
                "first_name": "Tamara B"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Unreliable predictions can occur when using artificial intelligence (AI) systems with negative consequences for downstream applications, particularly when employed for decision-making. Conformal prediction provides a model-agnostic framework for uncertainty quantification that can be applied to any dataset, irrespective of its distribution, post hoc. In contrast to other pixel-level uncertainty quantification methods, conformal prediction operates without requiring access to the underlying model and training dataset, concurrently offering statistically valid and informative prediction regions, all while maintaining computational efficiency. In response to the increased need to report uncertainty alongside point predictions, we bring attention to the promise of conformal prediction within the domain of Earth Observation (EO) applications. To accomplish this, we assess the current state of uncertainty quantification in the EO domain and found that only 20% of the reviewed Google Earth Engine (GEE) datasets incorporated a degree of uncertainty information, with unreliable methods prevalent. Next, we introduce modules that seamlessly integrate into existing GEE predictive modelling workflows and demonstrate the application of these tools for datasets spanning local to global scales, including the Dynamic World and Global Ecosystem Dynamics Investigation (GEDI) datasets. These case studies encompass regression and classification tasks, featuring both traditional and deep learning-based workflows. Subsequently, we discuss the opportunities arising from the use of conformal prediction in EO. We anticipate that the increased availability of easy-to-use implementations of conformal predictors, such as those provided here, will drive wider adoption of rigorous uncertainty quantification in EO, thereby enhancing the reliability of uses such as operational monitoring and decision making. ",
        "title": "Uncertainty quantification for probabilistic machine learning in earth  observation using conformal prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06422",
        "abstract_url": "http://arxiv.org/abs/2401.06422",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Doyoung"
            },
            {
                "last_name": "Jeong",
                "first_name": "Seongah"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this letter, we propose a joint mechanical and electrical adjustment of intelligent reflecting surface (IRS) for the performance improvements of low-earth orbit (LEO) satellite multiple-input multiple-output (MIMO) communications. In particular, we construct a three-dimensional (3D) MIMO channel model for the mechanically-tilted IRS, and consider two types of scenarios with and without the direct path of LEO-ground user link due to the orbital flight. With the aim of maximizing the end-to-end performance, we jointly optimize tilting angle and phase shift of IRS along with the transceiver beamforming, whose performance superiority is verified via simulations. ",
        "title": "Joint Mechanical and Electrical Adjustment of IRS-aided LEO Satellite  MIMO Communications",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06426",
        "abstract_url": "http://arxiv.org/abs/2401.06426",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Ji"
            },
            {
                "last_name": "Tang",
                "first_name": "Dehua"
            },
            {
                "last_name": "Huang",
                "first_name": "Yuanxian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Li"
            },
            {
                "last_name": "Zeng",
                "first_name": "Xiaocheng"
            },
            {
                "last_name": "Li",
                "first_name": "Dong"
            },
            {
                "last_name": "Lu",
                "first_name": "Mingjie"
            },
            {
                "last_name": "Peng",
                "first_name": "Jinzhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Fan"
            },
            {
                "last_name": "Tian",
                "first_name": "Lu"
            },
            {
                "last_name": "Sirasao",
                "first_name": "Ashish"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet by directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 models with our method applying on ConvNeXtV1, which surpass most SOTA efficient models with comparable inference performance. Our method also achieves state-of-the-art pruning performance on the vision transformer model. ",
        "title": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06430",
        "abstract_url": "http://arxiv.org/abs/2401.06430",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Huiyuan"
            },
            {
                "last_name": "Cui",
                "first_name": "Kuilong"
            },
            {
                "last_name": "Wang",
                "first_name": "Chuanming"
            },
            {
                "last_name": "Qi",
                "first_name": "Mengshi"
            },
            {
                "last_name": "Ma",
                "first_name": "Huadong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the rapid advancements in deep learning technologies, person re-identification (ReID) has witnessed remarkable performance improvements. However, the majority of prior works have traditionally focused on solving the problem via extracting features solely from a single perspective, such as uniform partitioning, hard attention mechanisms, or semantic masks. While these approaches have demonstrated efficacy within specific contexts, they fall short in diverse situations. In this paper, we propose a novel approach, Mutual Distillation Learning For Person Re-identification (termed as MDPR), which addresses the challenging problem from multiple perspectives within a single unified model, leveraging the power of mutual distillation to enhance the feature representations collectively. Specifically, our approach encompasses two branches: a hard content branch to extract local features via a uniform horizontal partitioning strategy and a Soft Content Branch to dynamically distinguish between foreground and background and facilitate the extraction of multi-granularity features via a carefully designed attention mechanism. To facilitate knowledge exchange between these two branches, a mutual distillation and fusion process is employed, promoting the capability of the outputs of each branch. Extensive experiments are conducted on widely used person ReID datasets to validate the effectiveness and superiority of our approach. Notably, our method achieves an impressive $88.7\\%/94.4\\%$ in mAP/Rank-1 on the DukeMTMC-reID dataset, surpassing the current state-of-the-art results. Our source code is available at https://github.com/KuilongCui/MDPR. ",
        "title": "Mutual Distillation Learning For Person Re-Identification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06431",
        "abstract_url": "http://arxiv.org/abs/2401.06431",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Changrong"
            },
            {
                "last_name": "Ma",
                "first_name": "Wenxing"
            },
            {
                "last_name": "Xu",
                "first_name": "Sean Xin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Yufang"
            },
            {
                "last_name": "Fu",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more efficient and maintain greater consistency in their assessments. These results underscore the potential of LLMs in educational technology, paving the way for effective collaboration between humans and AI, ultimately leading to transformative learning experiences through AI-generated feedback. ",
        "title": "From Automation to Augmentation: Large Language Models Elevating Essay  Scoring Landscape",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06432",
        "abstract_url": "http://arxiv.org/abs/2401.06432",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Yae Jee"
            },
            {
                "last_name": "Liu",
                "first_name": "Luyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Fahrezi",
                "first_name": "Aldi"
            },
            {
                "last_name": "Joshi",
                "first_name": "Gauri"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we combine the advantages of high and low-rank LoRAs, which achieves improved convergence speed and final performance compared to homogeneous LoRA. Furthermore, it offers enhanced computation efficiency compared to full fine-tuning, making it suitable for heterogeneous devices while preserving data privacy. ",
        "title": "Heterogeneous Low-Rank Approximation for Federated Fine-tuning of  On-Device Foundation Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06435",
        "abstract_url": "http://arxiv.org/abs/2401.06435",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Jiaming"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Jialong"
            },
            {
                "last_name": "Guo",
                "first_name": "Yiran"
            },
            {
                "last_name": "Li",
                "first_name": "Lun"
            },
            {
                "last_name": "Ai",
                "first_name": "Bo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  For massive multiple-input multiple-output systems in the frequency division duplex (FDD) mode, accurate downlink channel state information (CSI) is required at the base station (BS). However, the increasing number of transmit antennas aggravates the feedback overhead of CSI. Recently, deep learning (DL) has shown considerable potential to reduce CSI feedback overhead. In this paper, we propose a Swin Transformer-based autoencoder network called SwinCFNet for the CSI feedback task. In particular, the proposed method can effectively capture the long-range dependence information of CSI. Moreover, we explore the impact of the number of Swin Transformer blocks and the dimension of feature channels on the performance of SwinCFNet. Experimental results show that SwinCFNet significantly outperforms other DL-based methods with comparable model sizes, especially for the outdoor scenario. ",
        "title": "Swin Transformer-Based CSI Feedback for Massive MIMO",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06436",
        "abstract_url": "http://arxiv.org/abs/2401.06436",
        "authors": [
            {
                "last_name": "Hoang",
                "first_name": "Thi Linh"
            },
            {
                "last_name": "Pham",
                "first_name": "Tuan Dung"
            },
            {
                "last_name": "Ta",
                "first_name": "Viet Cuong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  In this work, we have proposed an approach for improving the GCN for predicting ratings in social networks. Our model is expanded from the standard model with several layers of transformer architecture. The main focus of the paper is on the encoder architecture for node embedding in the network. Using the embedding layer from the graph-based convolution layer, the attention mechanism could rearrange the feature space to get a more efficient embedding for the downstream task. The experiments showed that our proposed architecture achieves better performance than GCN on the traditional link prediction task. ",
        "title": "Improving Graph Convolutional Networks with Transformer Layer in  social-based items recommendation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06437",
        "abstract_url": "http://arxiv.org/abs/2401.06437",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Zeqing"
            },
            {
                "last_name": "Lan",
                "first_name": "Haoxuan"
            },
            {
                "last_name": "Zou",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Junbo"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CL"
        ],
        "abstract": "  Recent advancements in implicit 3D representations and generative models have markedly propelled the field of 3D object generation forward. However, it remains a significant challenge to accurately model geometries with defined sharp features under parametric controls, which is crucial in fields like industrial design and manufacturing. To bridge this gap, we introduce a framework that employs Large Language Models (LLMs) to generate text-driven 3D shapes, manipulating 3D software via program synthesis. We present 3D-PreMise, a dataset specifically tailored for 3D parametric modeling of industrial shapes, designed to explore state-of-the-art LLMs within our proposed pipeline. Our work reveals effective generation strategies and delves into the self-correction capabilities of LLMs using a visual interface. Our work highlights both the potential and limitations of LLMs in 3D parametric modeling for industrial applications. ",
        "title": "3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp  Features and Parametric Control?",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06438",
        "abstract_url": "http://arxiv.org/abs/2401.06438",
        "authors": [
            {
                "last_name": "Ono",
                "first_name": "Seitaro"
            },
            {
                "last_name": "Ogino",
                "first_name": "Yuka"
            },
            {
                "last_name": "Toizumi",
                "first_name": "Takahiro"
            },
            {
                "last_name": "Ito",
                "first_name": "Atsushi"
            },
            {
                "last_name": "Tsukada",
                "first_name": "Masato"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, significant progress has been made in image recognition technology based on deep neural networks. However, improving recognition performance under low-light conditions remains a significant challenge. This study addresses the enhancement of recognition model performance in low-light conditions. We propose an image-adaptive learnable module which apply appropriate image processing on input images and a hyperparameter predictor to forecast optimal parameters used in the module. Our proposed approach allows for the enhancement of recognition performance under low-light conditions by easily integrating as a front-end filter without the need to retrain existing recognition models designed for low-light conditions. Through experiments, our proposed method demonstrates its contribution to enhancing image recognition performance under low-light conditions. ",
        "title": "Improving Low-Light Image Recognition Performance Based on  Image-adaptive Learnable Module",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06439",
        "abstract_url": "http://arxiv.org/abs/2401.06439",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Bin-Bin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yanxin"
            },
            {
                "last_name": "Wei",
                "first_name": "Henglai"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Lv",
                "first_name": "Chen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we propose a cooperative long-term task execution (LTTE) algorithm for protecting a moving target into the interior of an ordering-flexible convex hull by a team of robots resiliently in the changing environments. Particularly, by designing target-approaching and sensing-neighbor collision-free subtasks, and incorporating these subtasks into the constraints rather than the traditional cost function in an online constraint-based optimization framework, the proposed LTTE can systematically guarantee long-term target convoying under changing environments in the n-dimensional Euclidean space. Then, the introduction of slack variables allow for the constraint violation of different subtasks; i.e., the attraction from target-approaching constraints and the repulsion from time-varying collision-avoidance constraints, which results in the desired formation with arbitrary spatial ordering sequences. Rigorous analysis is provided to guarantee asymptotical convergence with challenging nonlinear couplings induced by time-varying collision-free constraints. Finally, 2D experiments using three autonomous mobile robots (AMRs) are conducted to validate the effectiveness of the proposed algorithm, and 3D simulations tackling changing environmental elements, such as different initial positions, some robots suddenly breakdown and static obstacles are presented to demonstrate the multi-dimensional adaptability, robustness and the ability of obstacle avoidance of the proposed method. ",
        "title": "Ordering-Flexible Multi-Robot Coordination for MovingTarget Convoying  Using Long-TermTask Execution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06441",
        "abstract_url": "http://arxiv.org/abs/2401.06441",
        "authors": [
            {
                "last_name": "Pirayeshshirazinezhad",
                "first_name": "Reza"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This study introduces lateral pendulum as an innovative balancer design for bicycle stabilization. This pendulum, operating in the bicycle's vertical plane, enables the bicycle to remain stationary. The paper develops a dynamic model for a bicycle equipped with this lateral pendulum, using Lagrange's method, where the equations are validated with ADAMS software. The stabilization is demonstrated with traditional vertical and novel lateral pendulums, managed through a genetic-pole placement control algorithm. This approach showcases the superiority of the lateral pendulum over traditional methods, including vertical pendulums and steering the handlebar. Additionally, a Digital Linear Quadratic Regulator controller is implemented for practical application, further enhancing system stability. ",
        "title": "Bicycle Stabilization using mechanism optimization and Digital LQR",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06442",
        "abstract_url": "http://arxiv.org/abs/2401.06442",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Minxing"
            },
            {
                "last_name": "Cheng",
                "first_name": "Wentao"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A precise and user-friendly manipulation of image content while preserving image fidelity has always been crucial to the field of image editing. Thanks to the power of generative models, recent point-based image editing methods allow users to interactively change the image content with high generalizability by clicking several control points. But the above mentioned editing process is usually based on the assumption that features stay constant in the motion supervision step from initial to target points. In this work, we conduct a comprehensive investigation in the feature space of diffusion models, and find that features change acutely under in-plane rotation. Based on this, we propose a novel approach named RotationDrag, which significantly improves point-based image editing performance when users intend to in-plane rotate the image content. Our method tracks handle points more precisely by utilizing the feature map of the rotated images, thus ensuring precise optimization and high image fidelity. Furthermore, we build a in-plane rotation focused benchmark called RotateBench, the first benchmark to evaluate the performance of point-based image editing method under in-plane rotation scenario on both real images and generated images. A thorough user study demonstrates the superior capability in accomplishing in-plane rotation that users intend to achieve, comparing the DragDiffusion baseline and other existing diffusion-based methods. See the project page https://github.com/Tony-Lowe/RotationDrag for code and experiment results. ",
        "title": "RotationDrag: Point-based Image Editing with Rotated Diffusion Features",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06443",
        "abstract_url": "http://arxiv.org/abs/2401.06443",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Minjun"
            },
            {
                "last_name": "Song",
                "first_name": "Seungwoo"
            },
            {
                "last_name": "Lee",
                "first_name": "Youhan"
            },
            {
                "last_name": "Jang",
                "first_name": "Haneol"
            },
            {
                "last_name": "Lim",
                "first_name": "Kyungtae"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data on VQA. ",
        "title": "BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via  Graph Representation Pretraining",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06445",
        "abstract_url": "http://arxiv.org/abs/2401.06445",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Chenwei"
            },
            {
                "last_name": "Ke",
                "first_name": "Qiao"
            },
            {
                "last_name": "Chen",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Chuang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Xiu-Xiu"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Analyzing and characterizing the differences between networks is a fundamental and challenging problem in network science. Previously, most network comparison methods that rely on topological properties have been restricted to measuring differences between two undirected networks. However, many networks, such as biological networks, social networks, and transportation networks, exhibit inherent directionality and higher-order attributes that should not be ignored when comparing networks. Therefore, we propose a motif-based directed network comparison method that captures local, global, and higher-order differences between two directed networks. Specifically, we first construct a motif distribution vector for each node, which captures the information of a node's involvement in different directed motifs. Then, the dissimilarity between two directed networks is defined on the basis of a matrix which is composed of the motif distribution vector of every node and Jensen-Shannon divergence. The performance of our method is evaluated via the comparison of six real directed networks with their null models as well as their perturbed networks based on edge perturbation. Our method is superior to the state-of-the-art baselines and is robust with different parameter settings. ",
        "title": "Directed network comparison using motifs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06451",
        "abstract_url": "http://arxiv.org/abs/2401.06451",
        "authors": [
            {
                "last_name": "van Ditmarsch",
                "first_name": "Hans"
            },
            {
                "last_name": "Fruzsa",
                "first_name": "Krisztina"
            },
            {
                "last_name": "Kuznets",
                "first_name": "Roman"
            },
            {
                "last_name": "Schmid",
                "first_name": "Ulrich"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We provide an epistemic logical language and semantics for the modeling and analysis of byzantine fault-tolerant multi-agent systems. This not only facilitates reasoning about the agents' fault status but also supports model updates for implementing repair and state recovery. For each agent, besides the standard knowledge modality our logic provides an additional modality called hope, which is capable of expressing that the agent is correct (not faulty), and also dynamic modalities enabling change of the agents' correctness status. These dynamic modalities are interpreted as model updates that come in three flavours: fully public, more private, or involving factual change. We provide complete axiomatizations for all these variants in the form of reduction systems: formulas with dynamic modalities are equivalent to formulas without. Therefore, they have the same expressivity as the logic of knowledge and hope. Multiple examples are provided to demonstrate the utility and flexibility of our logic for modeling a wide range of repair and state recovery techniques that have been implemented in the context of fault-detection, isolation, and recovery (FDIR) approaches in fault-tolerant distributed computing with byzantine agents. ",
        "title": "A Logic for Repair and State Recovery in Byzantine Fault-tolerant  Multi-agent Systems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06452",
        "abstract_url": "http://arxiv.org/abs/2401.06452",
        "authors": [
            {
                "last_name": "Saunders",
                "first_name": "Jack D."
            },
            {
                "last_name": "Freitas",
                "first_name": "Alex A."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Positive-Unlabelled (PU) learning is a growing field of machine learning that aims to learn classifiers from data consisting of labelled positive and unlabelled instances, which can be in reality positive or negative, but whose label is unknown. An extensive number of methods have been proposed to address PU learning over the last two decades, so many so that selecting an optimal method for a given PU learning task presents a challenge. Our previous work has addressed this by proposing GA-Auto-PU, the first Automated Machine Learning (Auto-ML) system for PU learning. In this work, we propose two new Auto-ML systems for PU learning: BO-Auto-PU, based on a Bayesian Optimisation approach, and EBO-Auto-PU, based on a novel evolutionary/Bayesian optimisation approach. We also present an extensive evaluation of the three Auto-ML systems, comparing them to each other and to well-established PU learning methods across 60 datasets (20 real-world datasets, each with 3 versions in terms of PU learning characteristics). ",
        "title": "Automated Machine Learning for Positive-Unlabelled Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06453",
        "abstract_url": "http://arxiv.org/abs/2401.06453",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuyao"
            },
            {
                "last_name": "Guo",
                "first_name": "Ke"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiao"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Artificial light plays an integral role in modern cities, significantly enhancing human productivity and the efficiency of civilization. However, excessive illumination can lead to light pollution, posing non-negligible threats to economic burdens, ecosystems, and human health. Despite its critical importance, the exploration of its causes remains relatively limited within the field of artificial intelligence, leaving an incomplete understanding of the factors contributing to light pollution and sustainable illumination planning distant. To address this gap, we introduce a novel framework named Causally Aware Generative Adversarial Networks (CAGAN). This innovative approach aims to uncover the fundamental drivers of light pollution within cities and offer intelligent solutions for optimal illumination resource allocation in the context of sustainable urban development. We commence by examining light pollution across 33,593 residential areas in seven global metropolises. Our findings reveal substantial influences on light pollution levels from various building types, notably grasslands, commercial centers and residential buildings as significant contributors. These discovered causal relationships are seamlessly integrated into the generative modeling framework, guiding the process of generating light pollution maps for diverse residential areas. Extensive experiments showcase CAGAN's potential to inform and guide the implementation of effective strategies to mitigate light pollution. Our code and data are publicly available at https://github.com/zhangyuuao/Light_Pollution_CAGAN. ",
        "title": "Causally Aware Generative Adversarial Networks for Light Pollution  Control",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06461",
        "abstract_url": "http://arxiv.org/abs/2401.06461",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Yuling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Wan",
                "first_name": "Chengcheng"
            },
            {
                "last_name": "Gu",
                "first_name": "Xiaodong"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL"
        ],
        "abstract": "  Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by capturing the distinct structural patterns of code. Diverging from conventional techniques that depend on external LLMs for perturbations, DetectCodeGPT perturbs the code corpus by strategically inserting spaces and newlines, ensuring both efficacy and efficiency. Experiment results show that our approach significantly outperforms state-of-the-art techniques in detecting machine-generated code. ",
        "title": "Between Lines of Code: Unraveling the Distinct Patterns of Machine and  Human Programmers",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06462",
        "abstract_url": "http://arxiv.org/abs/2401.06462",
        "authors": [
            {
                "last_name": "Xuan",
                "first_name": "Xiwei"
            },
            {
                "last_name": "Ono",
                "first_name": "Jorge Piazentin"
            },
            {
                "last_name": "Gou",
                "first_name": "Liang"
            },
            {
                "last_name": "Ma",
                "first_name": "Kwan-Liu"
            },
            {
                "last_name": "Ren",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC"
        ],
        "abstract": "  Data slice-finding is an emerging technique for evaluating machine learning models. It works by identifying subgroups within a specified dataset that exhibit poor performance, often defined by distinct feature sets or meta-information. However, in the context of unstructured image data, data slice-finding poses two notable challenges: it requires additional metadata -- a laborious and costly requirement, and also demands non-trivial efforts for interpreting the root causes of the underperformance within data slices. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for data-slicing-based machine learning (ML) model validation. Our approach excels in identifying interpretable data slices, employing explainable features extracted through the lens of Explainable AI (XAI) techniques, and removing the necessity for additional metadata of textual annotations or cross-model embeddings. AttributionScanner demonstrates proficiency in pinpointing critical model issues, including spurious correlations and mislabeled data. Our novel VA interface visually summarizes data slices, enabling users to gather insights into model behavior patterns effortlessly. Furthermore, our framework closes the ML Development Cycle by empowering domain experts to address model issues by using a cutting-edge neural network regularization technique. The efficacy of AttributionScanner is underscored through two prototype use cases, elucidating its substantial effectiveness in model validation for vision-centric tasks. Our approach paves the way for ML researchers and practitioners to drive interpretable model validation in a data-efficient way, ultimately leading to more reliable and accurate models. ",
        "title": "AttributionScanner: A Visual Analytics System for Metadata-Free  Data-Slicing Based Model Validation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06465",
        "abstract_url": "http://arxiv.org/abs/2401.06465",
        "authors": [
            {
                "last_name": "Hedstr\u00f6m",
                "first_name": "Anna"
            },
            {
                "last_name": "Weber",
                "first_name": "Leander"
            },
            {
                "last_name": "Lapuschkin",
                "first_name": "Sebastian"
            },
            {
                "last_name": "H\u00f6hne",
                "first_name": "Marina MC"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the eXplainable Artificial Intelligence (XAI) community for its well-motivated evaluative principle: that the explanation function should be sensitive to changes in the parameters of the model function. However, recent works have identified several methodological caveats for the empirical interpretation of MPRT. To address these caveats, we introduce two adaptations to the original MPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact that noise has on the evaluation results through sampling and the latter circumvents the need for biased similarity measurements by re-interpreting the test through the explanation's rise in complexity, after full parameter randomisation. Our experimental results demonstrate that these proposed variants lead to improved metric reliability, thus enabling a more trustworthy application of XAI methods. ",
        "title": "Sanity Checks Revisited: An Exploration to Repair the Model Parameter  Randomisation Test",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06466",
        "abstract_url": "http://arxiv.org/abs/2401.06466",
        "authors": [
            {
                "last_name": "Rostami",
                "first_name": "Pedram"
            },
            {
                "last_name": "Salemi",
                "first_name": "Ali"
            },
            {
                "last_name": "Dousti",
                "first_name": "Mohammad Javad"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another. ",
        "title": "PersianMind: A Cross-Lingual Persian-English Large Language Model",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06468",
        "abstract_url": "http://arxiv.org/abs/2401.06468",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Minghao"
            },
            {
                "last_name": "Vu",
                "first_name": "Thuy-Trang"
            },
            {
                "last_name": "Qu",
                "first_name": "Lizhen"
            },
            {
                "last_name": "Foster",
                "first_name": "George"
            },
            {
                "last_name": "Haffari",
                "first_name": "Gholamreza"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as translation errors, the scaling law of parallel documents, out-of-domain generalization, and the impact of zero-shot crosslingual transfer. The findings of this research not only shed light on the strengths and limitations of LLM-based DocMT models but also provide a foundation for future research in DocMT. ",
        "title": "Adapting Large Language Models for Document-Level Machine Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06469",
        "abstract_url": "http://arxiv.org/abs/2401.06469",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Kaiyi"
            },
            {
                "last_name": "Lv",
                "first_name": "Ang"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Ha",
                "first_name": "Hansen"
            },
            {
                "last_name": "Xu",
                "first_name": "Tao"
            },
            {
                "last_name": "Yan",
                "first_name": "Rui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple \"epochs\" of meta-optimization. This variant implicitly explores permutations of ICL examples, further enhancing ICL performance. ",
        "title": "Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06470",
        "abstract_url": "http://arxiv.org/abs/2401.06470",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Gengrui"
            },
            {
                "last_name": "Wang",
                "first_name": "Yao"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoshuang"
            },
            {
                "last_name": "Qian",
                "first_name": "Hongyi"
            },
            {
                "last_name": "Zhan",
                "first_name": "Kaiqiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Ben"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In recent years, there has been a growing interest in utilizing reinforcement learning (RL) to optimize long-term rewards in recommender systems. Since industrial recommender systems are typically designed as multi-stage systems, RL methods with a single agent face challenges when optimizing multiple stages simultaneously. The reason is that different stages have different observation spaces, and thus cannot be modeled by a single agent. To address this issue, we propose a novel UNidirectional-EXecution-based multi-agent Reinforcement Learning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage recommender systems. We show that the unidirectional execution is a key feature of multi-stage recommender systems, bringing new challenges to the applications of multi-agent reinforcement learning (MARL), namely the observation dependency and the cascading effect. To tackle these challenges, we provide a cascading information chain (CIC) method to separate the independent observations from action-dependent observations and use CIC to train UNEX-RL effectively. We also discuss practical variance reduction techniques for UNEX-RL. Finally, we show the effectiveness of UNEX-RL on both public datasets and an online recommender system with over 100 million users. Specifically, UNEX-RL reveals a 0.558% increase in users' usage time compared with single-agent RL algorithms in online A/B experiments, highlighting the effectiveness of UNEX-RL in industrial recommender systems. ",
        "title": "UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender  Systems with UNidirectional EXecution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06471",
        "abstract_url": "http://arxiv.org/abs/2401.06471",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yuwei"
            },
            {
                "last_name": "Zeng",
                "first_name": "Yi"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Concept learning is a fundamental aspect of human cognition and plays a critical role in mental processes such as categorization, reasoning, memory, and decision-making. Researchers across various disciplines have shown consistent interest in the process of concept acquisition in individuals. To elucidate the mechanisms involved in human concept learning, this study examines the findings from computational neuroscience and cognitive psychology. These findings indicate that the brain's representation of concepts relies on two essential components: multisensory representation and text-derived representation. These two types of representations are coordinated by a semantic control system, ultimately leading to the acquisition of concepts. Drawing inspiration from this mechanism, the study develops a human-like computational model for concept learning based on spiking neural networks. By effectively addressing the challenges posed by diverse sources and imbalanced dimensionality of the two forms of concept representations, the study successfully attains human-like concept representations. Tests involving similar concepts demonstrate that our model, which mimics the way humans learn concepts, yields representations that closely align with human cognition. ",
        "title": "A Brain-inspired Computational Model for Human-like Concept Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06473",
        "abstract_url": "http://arxiv.org/abs/2401.06473",
        "authors": [
            {
                "last_name": "Kats",
                "first_name": "Eytan"
            },
            {
                "last_name": "Hirsch",
                "first_name": "Jochen G."
            },
            {
                "last_name": "Heinrich",
                "first_name": "Mattias P."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper demonstrates a self-supervised framework for learning voxel-wise coarse-to-fine representations tailored for dense downstream tasks. Our approach stems from the observation that existing methods for hierarchical representation learning tend to prioritize global features over local features due to inherent architectural bias. To address this challenge, we devise a training strategy that balances the contributions of features from multiple scales, ensuring that the learned representations capture both coarse and fine-grained details. Our strategy incorporates 3-fold improvements: (1) local data augmentations, (2) a hierarchically balanced architecture, and (3) a hybrid contrastive-restorative loss function. We evaluate our method on CT and MRI data and demonstrate that our new approach particularly beneficial for fine-tuning with limited annotated data and consistently outperforms the baseline counterpart in linear evaluation settings. ",
        "title": "Self-supervised Learning of Dense Hierarchical Representations for  Medical Image Segmentation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06477",
        "abstract_url": "http://arxiv.org/abs/2401.06477",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Guo",
                "first_name": "Shuyue"
            },
            {
                "last_name": "Qu",
                "first_name": "Xingwei"
            },
            {
                "last_name": "Guo",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weixu"
            },
            {
                "last_name": "Du",
                "first_name": "Xinrun"
            },
            {
                "last_name": "Lin",
                "first_name": "Chenghua"
            },
            {
                "last_name": "Huang",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenhu"
            },
            {
                "last_name": "Fu",
                "first_name": "Jie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ge"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a scalable and efficient solution for improving the instruction-following capabilities of LLMs, with significant implications for their application across diverse fields. The code and dataset can be found at https://github.com/Zheng0428/COIG-Kun ",
        "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction  Back-Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06481",
        "abstract_url": "http://arxiv.org/abs/2401.06481",
        "authors": [
            {
                "last_name": "Holland",
                "first_name": "Kieran"
            },
            {
                "last_name": "Ipp",
                "first_name": "Andreas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "David I."
            },
            {
                "last_name": "Wenger",
                "first_name": "Urs"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Fixed point lattice actions are designed to have continuum classical properties unaffected by discretization effects and reduced lattice artifacts at the quantum level. They provide a possible way to extract continuum physics with coarser lattices, thereby allowing to circumvent problems with critical slowing down and topological freezing toward the continuum limit. A crucial ingredient for practical applications is to find an accurate and compact parametrization of a fixed point action, since many of its properties are only implicitly defined. Here we use machine learning methods to revisit the question of how to parametrize fixed point actions. In particular, we obtain a fixed point action for four-dimensional SU(3) gauge theory using convolutional neural networks with exact gauge invariance. The large operator space allows us to find superior parametrizations compared to previous studies, a necessary first step for future Monte Carlo simulations. ",
        "title": "Machine learning a fixed point action for SU(3) gauge theory with a  gauge equivariant convolutional neural network",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06482",
        "abstract_url": "http://arxiv.org/abs/2401.06482",
        "authors": [
            {
                "last_name": "Rani",
                "first_name": "Pooja"
            },
            {
                "last_name": "Zellweger",
                "first_name": "Jonas"
            },
            {
                "last_name": "Kousadianos",
                "first_name": "Veronika"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "Kehrer",
                "first_name": "Timo"
            },
            {
                "last_name": "Bacchelli",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "PF"
        ],
        "abstract": "  As the energy footprint generated by software is increasing at an alarming rate, understanding how to develop energy-efficient applications has become a necessity. Previous work has introduced catalogs of coding practices, also known as energy patterns. These patterns are yet limited to Mobile or third-party libraries. In this study, we focus on the Web domain--a main source of energy consumption. First, we investigated whether and how Mobile energy patterns could be ported to this domain and found that 20 patterns could be ported. Then, we interviewed six expert web developers from different companies to challenge the ported patterns. Most developers expressed concerns for antipatterns, specifically with functional antipatterns, and were able to formulate guidelines to locate these patterns in the source code. Finally, to quantify the effect of Web energy patterns on energy consumption, we set up an automated pipeline to evaluate two ported patterns: 'Dynamic Retry Delay' (DRD) and 'Open Only When Necessary' (OOWN). With this, we found no evidence that the DRD pattern consumes less energy than its antipattern, while the opposite is true for OOWN. Data and Material: https://doi.org/10.5281/zenodo.8404487 ",
        "title": "Energy Patterns for Web: An Exploratory Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06484",
        "abstract_url": "http://arxiv.org/abs/2401.06484",
        "authors": [
            {
                "last_name": "Khadem",
                "first_name": "Mina"
            },
            {
                "last_name": "Zeinali",
                "first_name": "Farshad"
            },
            {
                "last_name": "Mokari",
                "first_name": "Nader"
            },
            {
                "last_name": "Saeedi",
                "first_name": "Hamid"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In this paper, we present a quality of service (QoS)-aware priority-based spectrum management scheme to guarantee the minimum required bit rate of vertical sector players (VSPs) in the 5G and beyond generation, including the 6th generation (6G). VSPs are considered as spectrum leasers to optimize the overall spectrum efficiency of the network from the perspective of the mobile network operator (MNO) as the spectrum licensee and auctioneer. We exploit a modified Vickrey-Clarke-Groves (VCG) auction mechanism to allocate the spectrum to them where the QoS and the truthfulness of bidders are considered as two important parameters for prioritization of VSPs. The simulation is done with the help of deep deterministic policy gradient (DDPG) as a deep reinforcement learning (DRL)-based algorithm. Simulation results demonstrate that deploying the DDPG algorithm results in significant advantages. In particular, the efficiency of the proposed spectrum management scheme is about %85 compared to the %35 efficiency in traditional auction methods. ",
        "title": "AI-enabled Priority and Auction-Based Spectrum Management for 6G",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06485",
        "abstract_url": "http://arxiv.org/abs/2401.06485",
        "authors": [
            {
                "last_name": "Xi",
                "first_name": "Yu"
            },
            {
                "last_name": "Yang",
                "first_name": "Baochen"
            },
            {
                "last_name": "Li",
                "first_name": "Hao"
            },
            {
                "last_name": "Guo",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Yu",
                "first_name": "Kai"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Customizable keyword spotting (KWS) in continuous speech has attracted increasing attention due to its real-world application potential. While contrastive learning (CL) has been widely used to extract keyword representations, previous CL approaches all operate on pre-segmented isolated words and employ only audio-text representations matching strategy. However, for KWS in continuous speech, co-articulation and streaming word segmentation can easily yield similar audio patterns for different texts, which may consequently trigger false alarms. To address this issue, we propose a novel CL with Audio Discrimination (CLAD) approach to learning keyword representation with both audio-text matching and audio-audio discrimination ability. Here, an InfoNCE loss considering both audio-audio and audio-text CL data pairs is employed for each sliding window during training. Evaluations on the open-source LibriPhrase dataset show that the use of sliding-window level InfoNCE loss yields comparable performance compared to previous CL approaches. Furthermore, experiments on the continuous speech dataset LibriSpeech demonstrate that, by incorporating audio discrimination, CLAD achieves significant performance gain over CL without audio discrimination. Meanwhile, compared to two-stage KWS approaches, the end-to-end KWS with CLAD achieves not only better performance, but also significant speed-up. ",
        "title": "Contrastive Learning With Audio Discrimination For Customizable Keyword  Spotting In Continuous Speech",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06486",
        "abstract_url": "http://arxiv.org/abs/2401.06486",
        "authors": [
            {
                "last_name": "Brunner",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Praetorius",
                "first_name": "Dirk"
            },
            {
                "last_name": "Streitberger",
                "first_name": "Julian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider scalar semilinear elliptic PDEs, where the nonlinearity is strongly monotone, but only locally Lipschitz continuous. To linearize the arising discrete nonlinear problem, we employ a damped Zarantonello iteration, which leads to a linear Poisson-type equation that is symmetric and positive definite. The resulting system is solved by a contractive algebraic solver such as a multigrid method with local smoothing. We formulate a fully adaptive algorithm that equibalances the various error components coming from mesh refinement, iterative linearization, and algebraic solver. We prove that the proposed adaptive iteratively linearized finite element method (AILFEM) guarantees convergence with optimal complexity, where the rates are understood with respect to the overall computational cost (i.e., the computational time). Numerical experiments investigate the involved adaptivity parameters. ",
        "title": "Cost-optimal adaptive FEM with linearization and algebraic solver for  semilinear elliptic PDEs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06492",
        "abstract_url": "http://arxiv.org/abs/2401.06492",
        "authors": [
            {
                "last_name": "D\u00f6rich",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Nikoli\u0107",
                "first_name": "Vanja"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Kuznetsov equation is a classical wave model of acoustics that incorporates quadratic gradient nonlinearities. When its strong damping vanishes, it undergoes a singular behavior change, switching from a parabolic-like to a hyperbolic quasilinear evolution. In this work, we establish for the first time the optimal error bounds for its finite element approximation as well as a semi-implicit fully discrete approximation that are robust with respect to the vanishing damping parameter. The core of the new arguments lies in devising energy estimates directly for the error equation where one can more easily exploit the polynomial structure of the nonlinearities and compensate inverse estimates with smallness conditions on the error. Numerical experiments are included to illustrate the theoretical results. ",
        "title": "Robust fully discrete error bounds for the Kuznetsov equation in the  inviscid limit",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06493",
        "abstract_url": "http://arxiv.org/abs/2401.06493",
        "authors": [
            {
                "last_name": "Karmakar",
                "first_name": "Pratik"
            },
            {
                "last_name": "Monet",
                "first_name": "Mika\u00ebl"
            },
            {
                "last_name": "Senellart",
                "first_name": "Pierre"
            },
            {
                "last_name": "Bressan",
                "first_name": "St\u00e9phane"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "CC"
        ],
        "abstract": "  Shapley values, originating in game theory and increasingly prominent in explainable AI, have been proposed to assess the contribution of facts in query answering over databases, along with other similar power indices such as Banzhaf values. In this work we adapt these Shapley-like scores to probabilistic settings, the objective being to compute their expected value. We show that the computations of expected Shapley values and of the expected values of Boolean functions are interreducible in polynomial time, thus obtaining the same tractability landscape. We investigate the specific tractable case where Boolean functions are represented as deterministic decomposable circuits, designing a polynomial-time algorithm for this setting. We present applications to probabilistic databases through database provenance, and an effective implementation of this algorithm within the ProvSQL system, which experimentally validates its feasibility over a standard benchmark. ",
        "title": "Expected Shapley-Like Scores of Boolean Functions: Complexity and  Applications to Probabilistic Databases",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06495",
        "abstract_url": "http://arxiv.org/abs/2401.06495",
        "authors": [
            {
                "last_name": "Leteno",
                "first_name": "Thibaud"
            },
            {
                "last_name": "Gourru",
                "first_name": "Antoine"
            },
            {
                "last_name": "Laclau",
                "first_name": "Charlotte"
            },
            {
                "last_name": "Gravier",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY",
            "LG"
        ],
        "abstract": "  In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate gender bias (e.g. is DistilBERT more prone to gender bias than its uncompressed version, BERT)? Our findings are the following: (I) one cannot identify a specific layer that produces bias; (II) every attention head uniformly encodes bias; except in the context of underrepresented classes with a high imbalance of the sensitive attribute; (III) this subset of heads is different as we re-fine tune the network; (IV) bias is more homogeneously produced by the heads in the distilled model. ",
        "title": "An investigation of structures responsible for gender bias in BERT and  DistilBERT",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06498",
        "abstract_url": "http://arxiv.org/abs/2401.06498",
        "authors": [
            {
                "last_name": "Glandorf",
                "first_name": "Dominik"
            },
            {
                "last_name": "Lee",
                "first_name": "Hye Rin"
            },
            {
                "last_name": "Orona",
                "first_name": "Gabe Avakian"
            },
            {
                "last_name": "Pumptow",
                "first_name": "Marina"
            },
            {
                "last_name": "Yu",
                "first_name": "Renzhe"
            },
            {
                "last_name": "Fischer",
                "first_name": "Christian"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  Large-scale administrative data is a common input in early warning systems for college dropout in higher education. Still, the terminology and methodology vary significantly across existing studies, and the implications of different modeling decisions are not fully understood. This study provides a systematic evaluation of contributing factors and predictive performance of machine learning models over time and across different student groups. Drawing on twelve years of administrative data at a large public university in the US, we find that dropout prediction at the end of the second year has a 20% higher AUC than at the time of enrollment in a Random Forest model. Also, most predictive factors at the time of enrollment, including demographics and high school performance, are quickly superseded in predictive importance by college performance and in later stages by enrollment behavior. Regarding variability across student groups, college GPA has more predictive value for students from traditionally disadvantaged backgrounds than their peers. These results can help researchers and administrators understand the comparative value of different data sources when building early warning systems and optimizing decisions under specific policy goals. ",
        "title": "Temporal and Between-Group Variability in College Dropout Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06499",
        "abstract_url": "http://arxiv.org/abs/2401.06499",
        "authors": [
            {
                "last_name": "Pandey",
                "first_name": "Sumit"
            },
            {
                "last_name": "Changdar",
                "first_name": "Satyasaran"
            },
            {
                "last_name": "Perslev",
                "first_name": "Mathias"
            },
            {
                "last_name": "Dam",
                "first_name": "Erik B"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Automated segmentation of distinct tumor regions is critical for accurate diagnosis and treatment planning in pediatric brain tumors. This study evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in segmenting different tumor subregions across three challenging datasets: Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse scenarios and anatomical variations, making them suitable for assessing the robustness and generalization capabilities of the MPUnet model. By utilizing multi-planar information, the MPUnet architecture aims to enhance segmentation accuracy. Our results show varying performance levels across the evaluated challenges, with the tumor core (TC) class demonstrating relatively higher segmentation accuracy. However, variability is observed in the segmentation of other classes, such as the edema and enhancing tumor (ET) regions. These findings emphasize the complexity of brain tumor segmentation and highlight the potential for further refinement of the MPUnet approach and inclusion of MRI more data and preprocessing. ",
        "title": "Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner  UNet",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06503",
        "abstract_url": "http://arxiv.org/abs/2401.06503",
        "authors": [
            {
                "last_name": "Doloriel",
                "first_name": "Chandler Timm C."
            },
            {
                "last_name": "Cajote",
                "first_name": "Rhandley D."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Small oriented objects that represent tiny pixel-area in large-scale aerial images are difficult to detect due to their size and orientation. Existing oriented aerial detectors have shown promising results but are mainly focused on orientation modeling with less regard to the size of the objects. In this work, we proposed a method to accurately detect small oriented objects in aerial images by enhancing the classification and regression tasks of the oriented object detection model. We designed the Attention-Points Network consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss (BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn the attention features needed to improve the detection of small objects. These attention features are then used to predict box points for BPLoss, which determines the points' position relative to the target oriented bounding box. Experimental results show the effectiveness of our Attention-Points Network on a standard oriented aerial dataset with small object instances (DOTA-v1.5) and on a maritime-related dataset (HRSC2016). The code is publicly available. ",
        "title": "Improving the Detection of Small Oriented Objects in Aerial Images",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06508",
        "abstract_url": "http://arxiv.org/abs/2401.06508",
        "authors": [
            {
                "last_name": "Aljafar",
                "first_name": "Muayad J."
            },
            {
                "last_name": "Azais",
                "first_name": "Florence"
            },
            {
                "last_name": "Flottes",
                "first_name": "Marie-Lise"
            },
            {
                "last_name": "Pagliarini",
                "first_name": "Samuel"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "AR"
        ],
        "abstract": "  While numerous obfuscation techniques are available for securing digital assets in the digital domain, there has been a notable lack of focus on protecting Intellectual Property (IP) in the analog domain. This is primarily due to the relatively smaller footprint of analog components within an Integrated Circuit (IC), with the majority of the surface dedicated to digital elements. However, despite their smaller nature, analog components are highly valuable IP and warrant effective protection. In this paper, we present a groundbreaking method for safeguarding analog IP by harnessing layout-based effects that are typically considered undesirable in IC design. Specifically, we exploit the impact of Length of Oxide Diffusion and Well Proximity Effect on transistors to fine-tune critical parameters such as transconductance (gm) and threshold voltage (Vth). These parameters remain concealed behind key inputs, akin to the logic locking approach employed in digital ICs. Our research explores the application of layout-based effects in two commercial CMOS technologies, namely a 28nm and a 65nm node. To demonstrate the efficacy of our proposed technique, we implement it for locking an Operational Transconductance Amplifier. Extensive simulations are performed, evaluating the obfuscation strength by applying a large number of key sets (over 50,000 and 300,000). The results exhibit a significant degradation in performance metrics, such as open-loop gain (up to 130dB), phase margin (up to 50 degrees), 3dB bandwidth (approximately 2.5MHz), and power consumption (around 1mW) when incorrect keys are employed. Our findings highlight the advantages of our approach as well as the associated overhead. ",
        "title": "Utilizing Layout Effects for Analog Logic Locking",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06509",
        "abstract_url": "http://arxiv.org/abs/2401.06509",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Yuanzhi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Linchao"
            },
            {
                "last_name": "Yang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions, we introduce the Agent interaction Evaluation framework (AntEval), targeting the qualitative evaluation of interaction informativeness and expressiveness. Specifically, we propose two novel evaluation metrics: Information Exchanging Precision (IEP) and Interaction Expressiveness Gap (IEG). These metrics are designed to assess interactions in scenarios focused on information exchange and intention expression, respectively. Our experimental results demonstrate the effectiveness of these metrics in evaluating interaction quality. Notably, we identify significant areas for improvement in LLMs regarding social interactions, as highlighted by our metrics. We believe AntEval will guide further exploration in complex agent interactions, bringing them closer to emulating real human behavior and enhancing their integration and utility in real-world applications. ",
        "title": "AntEval: Quantitatively Evaluating Informativeness and Expressiveness of  Agent Social Interactions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06512",
        "abstract_url": "http://arxiv.org/abs/2401.06512",
        "authors": [
            {
                "last_name": "Dallant",
                "first_name": "Justin"
            },
            {
                "last_name": "Haagensen",
                "first_name": "Frederik"
            },
            {
                "last_name": "Jacob",
                "first_name": "Riko"
            },
            {
                "last_name": "Kozma",
                "first_name": "L\u00e1szl\u00f3"
            },
            {
                "last_name": "Wild",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "DS"
        ],
        "abstract": "  A \\emph{saddlepoint} of an $n \\times n$ matrix is an entry that is the maximum of its row and the minimum of its column. Saddlepoints give the \\emph{value} of a two-player zero-sum game, corresponding to its pure-strategy Nash equilibria; efficiently finding a saddlepoint is thus a natural and fundamental algorithmic task.   For finding a \\emph{strict saddlepoint} (an entry that is the strict maximum of its row and the strict minimum of its column) we recently gave an $O({n\\log^*{n}})$-time algorithm, improving the $O({n\\log{n}})$ bounds from 1991 of Bienstock, Chung, Fredman, Sch\\\"affer, Shor, Suri and of Byrne and Vaserstein.   In this paper we present an optimal $O({n})$-time algorithm for finding a strict saddlepoint based on random sampling. Our algorithm, like earlier approaches, accesses matrix entries only via unit-cost binary comparisons. For finding a (non-strict) saddlepoint, we extend an existing lower bound to randomized algorithms, showing that the trivial $O(n^2)$ runtime cannot be improved even with the use of randomness. ",
        "title": "An Optimal Randomized Algorithm for Finding the Saddlepoint",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06513",
        "abstract_url": "http://arxiv.org/abs/2401.06513",
        "authors": [
            {
                "last_name": "Abdelkader",
                "first_name": "Hala"
            },
            {
                "last_name": "Abdelrazek",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Barnett",
                "first_name": "Scott"
            },
            {
                "last_name": "Schneider",
                "first_name": "Jean-Guy"
            },
            {
                "last_name": "Rani",
                "first_name": "Priya"
            },
            {
                "last_name": "Vasa",
                "first_name": "Rajesh"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in production. ",
        "title": "ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A  Case Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06514",
        "abstract_url": "http://arxiv.org/abs/2401.06514",
        "authors": [
            {
                "last_name": "Ivanov",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Ben-Porat",
                "first_name": "Omer"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Personalization in machine learning (ML) tailors models' decisions to the individual characteristics of users. While this approach has seen success in areas like recommender systems, its expansion into high-stakes fields such as healthcare and autonomous driving is hindered by the extensive regulatory approval processes involved. To address this challenge, we propose a novel framework termed represented Markov Decision Processes (r-MDPs) that is designed to balance the need for personalization with the regulatory constraints. In an r-MDP, we cater to a diverse user population, each with unique preferences, through interaction with a small set of representative policies. Our objective is twofold: efficiently match each user to an appropriate representative policy and simultaneously optimize these policies to maximize overall social welfare. We develop two deep reinforcement learning algorithms that efficiently solve r-MDPs. These algorithms draw inspiration from the principles of classic K-means clustering and are underpinned by robust theoretical foundations. Our empirical investigations, conducted across a variety of simulated environments, showcase the algorithms' ability to facilitate meaningful personalization even under constrained policy budgets. Furthermore, they demonstrate scalability, efficiently adapting to larger policy budgets. ",
        "title": "Personalized Reinforcement Learning with a Budget of Policies",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06518",
        "abstract_url": "http://arxiv.org/abs/2401.06518",
        "authors": [
            {
                "last_name": "S\u00e1nchez",
                "first_name": "Jos\u00e9 Manuel Gaspar"
            },
            {
                "last_name": "Bruns",
                "first_name": "Leonard"
            },
            {
                "last_name": "Tumova",
                "first_name": "Jana"
            },
            {
                "last_name": "Jensfelt",
                "first_name": "Patric"
            },
            {
                "last_name": "T\u00f6rngren",
                "first_name": "Martin"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Autonomous agents rely on sensor data to construct representations of their environment, essential for predicting future events and planning their own actions. However, sensor measurements suffer from limited range, occlusions, and sensor noise. These challenges become more evident in dynamic environments, where efficiently inferring the state of the environment based on sensor readings from different times is still an open problem. This work focuses on inferring the state of the dynamic part of the environment, i.e., where dynamic objects might be, based on previous observations and constraints on their dynamics. We formalize the problem and introduce Transitional Grid Maps (TGMs), an efficient analytical solution. TGMs are based on a set of novel assumptions that hold in many practical scenarios. They significantly reduce the complexity of the problem, enabling continuous prediction and updating of the entire dynamic map based on the known static map (see Fig.1), differentiating them from other alternatives. We compare our approach with a state-of-the-art particle filter, obtaining more prudent predictions in occluded scenarios and on-par results on unoccluded tracking. ",
        "title": "Transitional Grid Maps: Efficient Analytical Inference of Dynamic  Environments under Limited Sensing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06519",
        "abstract_url": "http://arxiv.org/abs/2401.06519",
        "authors": [
            {
                "last_name": "Ahvonen",
                "first_name": "Veeti"
            },
            {
                "last_name": "Heiman",
                "first_name": "Damian"
            },
            {
                "last_name": "Kuusisto",
                "first_name": "Antti"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "DC"
        ],
        "abstract": "  We examine the relationship of graded (multi)modal logic to counting (multichannel) message passing automata with applications to the Weisfeiler-Leman algorithm. We introduce the notion of graded multimodal types, which are formulae of graded multimodal logic that encode the local information of a pointed Kripke-model. We also introduce message passing automata that carry out a generalization of the Weisfeiler-Leman algorithm for distinguishing non-isomorphic graph nodes. We show that the classes of pointed Kripke-models recognizable by these automata are definable by a countable (possibly infinite) disjunction of graded multimodal formulae and vice versa. In particular, this equivalence also holds between recursively enumerable disjunctions and recursively enumerable automata. We also show a way of carrying out the Weisfeiler-Leman algorithm with a formula of first order logic that has been augmented with H\\\"artig's quantifier and greatest fixed points. ",
        "title": "Graded modal logic and counting message passing automata",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06521",
        "abstract_url": "http://arxiv.org/abs/2401.06521",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Mu",
                "first_name": "Junxian"
            },
            {
                "last_name": "Zhu",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Hu",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Open set recognition (OSR) requires the model to classify samples that belong to closed sets while rejecting unknown samples during test. Currently, generative models often perform better than discriminative models in OSR, but recent studies show that generative models may be computationally infeasible or unstable on complex tasks. In this paper, we provide insights into OSR and find that learning supplementary representations can theoretically reduce the open space risk. Based on the analysis, we propose a new model, namely Multi-Expert Diverse Attention Fusion (MEDAF), that learns diverse representations in a discriminative way. MEDAF consists of multiple experts that are learned with an attention diversity regularization term to ensure the attention maps are mutually different. The logits learned by each expert are adaptively fused and used to identify the unknowns through the score function. We show that the differences in attention maps can lead to diverse representations so that the fused representations can well handle the open space. Extensive experiments are conducted on standard and OSR large-scale benchmarks. Results show that the proposed discriminative method can outperform existing generative models by up to 9.5% on AUROC and achieve new state-of-the-art performance with little computational cost. Our method can also seamlessly integrate existing classification models. Code is available at https://github.com/Vanixxz/MEDAF. ",
        "title": "Exploring Diverse Representations for Open Set Recognition",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06523",
        "abstract_url": "http://arxiv.org/abs/2401.06523",
        "authors": [
            {
                "last_name": "Kertel",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Klein",
                "first_name": "Nadja"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present a boosting-based method to learn additive Structural Equation Models (SEMs) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. We introduce a family of score functions based on arbitrary regression techniques, for which we establish necessary conditions to consistently favor the true causal ordering. Our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. To address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive SEMs. Our simulation study underlines our theoretical results for lower dimensions and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. In addition, it exhibits robustness with respect to the choice of the hyperparameters making the procedure easy to tune. ",
        "title": "Boosting Causal Additive Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06524",
        "abstract_url": "http://arxiv.org/abs/2401.06524",
        "authors": [
            {
                "last_name": "Khanal",
                "first_name": "Subina"
            },
            {
                "last_name": "Tirupathi",
                "first_name": "Seshu"
            },
            {
                "last_name": "Zizzo",
                "first_name": "Giulio"
            },
            {
                "last_name": "Rawat",
                "first_name": "Ambrish"
            },
            {
                "last_name": "Pedersen",
                "first_name": "Torben Bach"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the \\emph{One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with diverse time series instances. We then fine-tune the pre-trained model using a gradual unfreezing technique. This helps enhance the model's performance in time series prediction for domains with limited data. Extensive experimental results on two real-world datasets show that our approach improves over the state-of-the-art baselines by 4.35% and 11.54% for indoor temperature and wind power prediction, respectively. ",
        "title": "Domain Adaptation for Time series Transformers using One-step  fine-tuning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06526",
        "abstract_url": "http://arxiv.org/abs/2401.06526",
        "authors": [
            {
                "last_name": "Piot",
                "first_name": "Paloma"
            },
            {
                "last_name": "Mart\u00edn-Rodilla",
                "first_name": "Patricia"
            },
            {
                "last_name": "Parapar",
                "first_name": "Javier"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SI"
        ],
        "abstract": "  Hate speech represents a pervasive and detrimental form of online discourse, often manifested through an array of slurs, from hateful tweets to defamatory posts. As such speech proliferates, it connects people globally and poses significant social, psychological, and occasionally physical threats to targeted individuals and communities. Current computational linguistic approaches for tackling this phenomenon rely on labelled social media datasets for training. For unifying efforts, our study advances in the critical need for a comprehensive meta-collection, advocating for an extensive dataset to help counteract this problem effectively. We scrutinized over 60 datasets, selectively integrating those pertinent into MetaHate. This paper offers a detailed examination of existing collections, highlighting their strengths and limitations. Our findings contribute to a deeper understanding of the existing datasets, paving the way for training more robust and adaptable models. These enhanced models are essential for effectively combating the dynamic and complex nature of hate speech in the digital realm. ",
        "title": "MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06528",
        "abstract_url": "http://arxiv.org/abs/2401.06528",
        "authors": [
            {
                "last_name": "Arbash",
                "first_name": "Elias"
            },
            {
                "last_name": "Fuchs",
                "first_name": "Margret"
            },
            {
                "last_name": "Rasti",
                "first_name": "Behnood"
            },
            {
                "last_name": "Lorenz",
                "first_name": "Sandra"
            },
            {
                "last_name": "Ghamisi",
                "first_name": "Pedram"
            },
            {
                "last_name": "Gloaguen",
                "first_name": "Richard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Addressing the critical theme of recycling electronic waste (E-waste), this contribution is dedicated to developing advanced automated data processing pipelines as a basis for decision-making and process control. Aligning with the broader goals of the circular economy and the United Nations (UN) Sustainable Development Goals (SDG), our work leverages non-invasive analysis methods utilizing RGB and hyperspectral imaging data to provide both quantitative and qualitative insights into the E-waste stream composition for optimizing recycling efficiency. In this paper, we introduce 'PCB-Vision'; a pioneering RGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53 RGB images of high spatial resolution paired with their corresponding high spectral resolution hyperspectral data cubes in the visible and near-infrared (VNIR) range. Grounded in open science principles, our dataset provides a comprehensive resource for researchers through high-quality ground truths, focusing on three primary PCB components: integrated circuits (IC), capacitors, and connectors. We provide extensive statistical investigations on the proposed dataset together with the performance of several state-of-the-art (SOTA) models, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and DeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the baseline codes, we hope to foster transparent, traceable, and comparable developments of advanced data processing across various scientific communities, including, but not limited to, computer vision and remote sensing. Emphasizing our commitment to supporting a collaborative and inclusive scientific community, all materials, including code, data, ground truth, and masks, will be accessible at https://github.com/hifexplo/PCBVision. ",
        "title": "PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed  Circuit Boards",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06529",
        "abstract_url": "http://arxiv.org/abs/2401.06529",
        "authors": [
            {
                "last_name": "Moy\u00f3n",
                "first_name": "Fabiola"
            },
            {
                "last_name": "Angermeir",
                "first_name": "Florian"
            },
            {
                "last_name": "Mendez",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The intersection between security and continuous software engineering has been of great interest since the early years of the agile development movement, and it remains relevant as software development processes are more frequently guided by agility and the adoption of DevOps. Several authors have contributed studies about the framing of secure agile development and secure DevOps, motivating academic contributions to methods and practices, but also discussions around benefits and challenges. Especially the challenges captured also our interest since, for the last few years, we are conducting research on secure continuous software engineering from a more applied, practical perspective with the overarching aim to introduce solutions that can be adopted at scale. The short positioning at hands summarizes a relevant part of our endeavors in which we validated challenges with several practitioners of different roles. More than framing a set of challenges, we conclude by presenting four key research directions we identified for practitioners and researchers to delineate future work. ",
        "title": "Industrial Challenges in Secure Continuous Development",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06532",
        "abstract_url": "http://arxiv.org/abs/2401.06532",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Yutao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peitian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yifei"
            },
            {
                "last_name": "Xie",
                "first_name": "Binyu"
            },
            {
                "last_name": "Dou",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly available LLMs, such as LLaMA, Mistral, and Phi, in search-related tasks. Furthermore, we conduct a comprehensive analysis to ascertain the effects of base model selection, instruction design, volume of instructions, and task variety on performance. We make our dataset and the models fine-tuned on it publicly accessible at https://github.com/DaoD/INTERS. ",
        "title": "INTERS: Unlocking the Power of Large Language Models in Search with  Instruction Tuning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06538",
        "abstract_url": "http://arxiv.org/abs/2401.06538",
        "authors": [
            {
                "last_name": "Moreira",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Silva",
                "first_name": "Flavio de Oliveira"
            },
            {
                "last_name": "Carvalho",
                "first_name": "Tereza Cristina Melo de Brito"
            },
            {
                "last_name": "Martins",
                "first_name": "Joberto S. B."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG"
        ],
        "abstract": "  Network slicing is a crucial enabler and a trend for the Next Generation Mobile Network (NGMN) and various other new systems like the Internet of Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning are key elements with a crucial role in the network-slicing processes since the NS process needs to orchestrate resources and functionalities, and machine learning can potentially optimize the orchestration process. However, existing network-slicing architectures lack the ability to define intelligent approaches to orchestrate features and resources in the slicing process. This paper discusses machine learning-based orchestration of features and capabilities in network slicing architectures. Initially, the slice resource orchestration and allocation in the slicing planning, configuration, commissioning, and operation phases are analyzed. In sequence, we highlight the need for optimized architectural feature orchestration and recommend using ML-embed agents, federated learning intrinsic mechanisms for knowledge acquisition, and a data-driven approach embedded in the network slicing architecture. We further develop an architectural features orchestration case embedded in the SFI2 network slicing architecture. An attack prevention security mechanism is developed for the SFI2 architecture using distributed embedded and cooperating ML agents. The case presented illustrates the architectural feature's orchestration process and benefits, highlighting its importance for the network slicing process. ",
        "title": "Intelligent Data-Driven Architectural Features Orchestration for Network  Slicing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06541",
        "abstract_url": "http://arxiv.org/abs/2401.06541",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Kaishuai"
            },
            {
                "last_name": "Hou",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Medical dialogue systems have attracted growing research attention as they have the potential to provide rapid diagnoses, treatment plans, and health consultations. In medical dialogues, a proper diagnosis is crucial as it establishes the foundation for future consultations. Clinicians typically employ both intuitive and analytic reasoning to formulate a differential diagnosis. This reasoning process hypothesizes and verifies a variety of possible diseases and strives to generate a comprehensive and rigorous diagnosis. However, recent studies on medical dialogue generation have overlooked the significance of modeling a differential diagnosis, which hinders the practical application of these systems. To address the above issue, we propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced analytic procedure. The resulting differential diagnosis is then used to retrieve medical knowledge and guide response generation. Experimental results on two datasets validate the efficacy of our method. Besides, we demonstrate how our framework assists both clinicians and patients in understanding the diagnostic process, for instance, by producing intermediate results and graph-based diagnosis paths. ",
        "title": "Medical Dialogue Generation via Intuitive-then-Analytical Differential  Diagnosis",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06542",
        "abstract_url": "http://arxiv.org/abs/2401.06542",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Jia",
                "first_name": "Feiyang"
            },
            {
                "last_name": "Luo",
                "first_name": "Yadan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the realm of modern autonomous driving, the perception system is indispensable for accurately assessing the state of the surrounding environment, thereby enabling informed prediction and planning. Key to this system is 3D object detection methods, that utilize vehicle-mounted sensors such as LiDAR and cameras to identify the size, category, and location of nearby objects. Despite the surge in 3D object detection methods aimed at enhancing detection precision and efficiency, there is a gap in the literature that systematically examines their resilience against environmental variations, noise, and weather changes. This study emphasizes the importance of robustness, alongside accuracy and latency, in evaluating perception systems under practical scenarios. Our work presents an extensive survey of camera-based, LiDAR-based, and multimodal 3D object detection algorithms, thoroughly evaluating their trade-off between accuracy, latency, and robustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair comparisons. Among these,multimodal 3D detection approaches exhibit superior robustness and a novel taxonomy is introduced to reorganize its literature for enhanced clarity. This survey aims to offer a more practical perspective on the current capabilities and constraints of 3D object detection algorithms in real-world applications, thus steering future research towards robustness-centric advancements ",
        "title": "Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and  Outlook",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06546",
        "abstract_url": "http://arxiv.org/abs/2401.06546",
        "authors": [
            {
                "last_name": "Imani",
                "first_name": "Vandad"
            },
            {
                "last_name": "Moradi",
                "first_name": "Elaheh"
            },
            {
                "last_name": "Sevilla-Salcedo",
                "first_name": "Carlos"
            },
            {
                "last_name": "Fortino",
                "first_name": "Vittorio"
            },
            {
                "last_name": "Tohka",
                "first_name": "Jussi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "NE"
        ],
        "abstract": "  Feature selection in noisy label scenarios remains an understudied topic. We propose a novel genetic algorithm-based approach, the Noise-Aware Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting optimal feature subsets in binary classification with noisy labels. NMFS-GA offers a unified framework for selecting feature subsets that are both accurate and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise, a Breast Cancer dataset enriched with noisy features, and a real-world ADNI dataset for dementia conversion prediction. Our results indicate that NMFS-GA can effectively select feature subsets that improve the accuracy and interpretability of binary classifiers in scenarios with noisy labels. ",
        "title": "Optimizing Feature Selection for Binary Classification with Noisy  Labels: A Genetic Algorithm Approach",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06547",
        "abstract_url": "http://arxiv.org/abs/2401.06547",
        "authors": [
            {
                "last_name": "Magen",
                "first_name": "Roey"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  The missing item problem, as introduced by Stoeckl in his work at SODA 23, focuses on continually identifying a missing element $e$ in a stream of elements ${e_1, ..., e_{\\ell}}$ from the set $\\{1,2,...,n\\}$, such that $e \\neq e_i$ for any $i \\in \\{1,...,n\\}$. Stoeckl's investigation primarily delves into scenarios with $\\ell<n$, providing bounds for the (i) deterministic case, (ii) the static case -- where the algorithm might be randomized but the stream is fixed in advanced) and (iii) the adversarially robust case -- where the algorithm is randomized and each stream element can be chosen depending on earlier algorithm outputs. Building upon this foundation, our paper addresses previously unexplored aspects of the missing item problem.   In the first segment, we examine the static setting with a long stream, where the length of the steam $\\ell$ is close to or even exceeds the size of the universe $n$. We present an algorithm demonstrating that even when $\\ell$ is very close to $n$ (say $\\ell=n-1$), polylog($n$) bits of memory suffice to identify the missing item. Additionally, we establish tight bounds of $\\tilde{\\Theta(k)}$ for the scenario of $\\ell = n+k$.   The second segment of this part of our work focuses on the {\\em adversarially robust setting}. We show a lower bound for a pseudo-deterministic error-zero (where the algorithm reports its errors) algorithm of approximating $\\Omega(\\ell)$, up to polylog factors. Based on Stoeckl's work, we establish a lower bound for a random-start (only use randomness at initialization) error-zero streaming algorithm.   In the final segment, we explore streaming algorithms with randomness-on-the-fly, where the random bits that are saved for future use are included in the space cost. For streams with length $\\ell = O(\\sqrt{n})$, we provide an upper bound of $O(log n)$. This establishes a gap between randomness-on-the-fly to random-start. ",
        "title": "Are We Still Missing an Item?",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06548",
        "abstract_url": "http://arxiv.org/abs/2401.06548",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chenyang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Hu",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            },
            {
                "last_name": "Ji",
                "first_name": "Xiangyang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning systems are prone to catastrophic forgetting when learning from a sequence of tasks, where old data from experienced tasks is unavailable when learning from a new task. To mitigate the problem, a line of methods propose to replay the data of experienced tasks when learning new tasks. These methods usually adopt an extra memory to store the data for replay. However, it is not expected in practice considering the memory constraint or data privacy issue. As a replacement, data-free data replay methods are proposed by inverting samples from the classification model. Though achieving good results, these methods still suffer from the inconsistency of the inverted and real training data, which is neglected in the inversion stage in recent works. To that effect, we propose to measure the data consistency quantitatively by some simplification and assumptions. Using the measurement, we analyze existing techniques for inverting samples and get some insightful information that inspires a novel loss function to reduce the inconsistency. Specifically, the loss minimizes the KL divergence of the distributions of inverted and real data under the tied multivariate Gaussian assumption, which is easy to implement in continual learning. In addition, we observe that the norms of old class weights turn to decrease continually as learning progresses. We thus analyze the underlying reasons and propose a simple regularization term to balance the class weights so that the samples of old classes are more distinguishable. To conclude, we propose the Consistency enhanced data replay with debiased classifier for Class Incremental Learning (CCIL). Extensive experiments on CIFAR-100, Tiny-ImageNet, and ImageNet100 show consistently improved performance of CCIL compared to previous approaches. ",
        "title": "Enhancing Consistency and Mitigating Bias: A Data Replay Approach for  Incremental Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06550",
        "abstract_url": "http://arxiv.org/abs/2401.06550",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Chuanji"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yingying"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiaotuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Qiqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road nodes, human mobility, and logistics addresses to build a multimodal detection model based on transformer encoder-decoder architecture, titled AOITR. In the model, in addition to the remote sensing images, multi-semantic information including core POI and road nodes is embedded and reorganized as the query content part for the transformer decoder to generate the AOI polygon. Meanwhile, relatively dynamic distribution features of human mobility, nearby POIs, and logistics addresses are used for AOI reliability evaluation through a cascaded feedforward network. The experimental results demonstrate that our algorithm significantly outperforms two existing methods. ",
        "title": "Multimodal Learning for detecting urban functional zones using remote  sensing image and multi-semantic information",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06557",
        "abstract_url": "http://arxiv.org/abs/2401.06557",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Ziqiang"
            },
            {
                "last_name": "Tang",
                "first_name": "Xing"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yang"
            },
            {
                "last_name": "He",
                "first_name": "Bowei"
            },
            {
                "last_name": "Chen",
                "first_name": "Liang"
            },
            {
                "last_name": "He",
                "first_name": "Xiuqiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Chen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Estimating the individual treatment effect (ITE) from observational data is a crucial research topic that holds significant value across multiple domains. How to identify hidden confounders poses a key challenge in ITE estimation. Recent studies have incorporated the structural information of social networks to tackle this challenge, achieving notable advancements. However, these methods utilize graph neural networks to learn the representation of hidden confounders in Euclidean space, disregarding two critical issues: (1) the social networks often exhibit a scalefree structure, while Euclidean embeddings suffer from high distortion when used to embed such graphs, and (2) each ego-centric network within a social network manifests a treatment-related characteristic, implying significant patterns of hidden confounders. To address these issues, we propose a novel method called Treatment-Aware Hyperbolic Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic space to encode the social networks, thereby effectively reducing the distortion of confounder representation caused by Euclidean embeddings. Secondly, we design a treatment-aware relationship identification module that enhances the representation of hidden confounders by identifying whether an individual and her neighbors receive the same treatment. Extensive experiments on two benchmark datasets are conducted to demonstrate the superiority of our method. ",
        "title": "Treatment-Aware Hyperbolic Representation Learning for Causal Effect  Estimation with Social Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06559",
        "abstract_url": "http://arxiv.org/abs/2401.06559",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yusen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Dynamic graph learning is crucial for modeling real-world systems with evolving relationships and temporal dynamics. However, the lack of a unified benchmark framework in current research has led to inaccurate evaluations of dynamic graph models. This paper highlights the significance of dynamic graph learning and its applications in various domains. It emphasizes the need for a standardized benchmark framework that captures temporal dynamics, evolving graph structures, and downstream task requirements. Establishing a unified benchmark will help researchers understand the strengths and limitations of existing models, foster innovation, and advance dynamic graph learning. In conclusion, this paper identifies the lack of a standardized benchmark framework as a current limitation in dynamic graph learning research . Such a framework will facilitate accurate model evaluation, drive advancements in dynamic graph learning techniques, and enable the development of more effective models for real-world applications. ",
        "title": "A General Benchmark Framework is Dynamic Graph Neural Network Need",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06561",
        "abstract_url": "http://arxiv.org/abs/2401.06561",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lefei"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alphadl/SafeLLM_with_IntentionAnalysis ",
        "title": "Intention Analysis Prompting Makes Large Language Models A Good  Jailbreak Defender",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06563",
        "abstract_url": "http://arxiv.org/abs/2401.06563",
        "authors": [
            {
                "last_name": "Safa",
                "first_name": "Ali"
            },
            {
                "last_name": "Mommen",
                "first_name": "Wout"
            },
            {
                "last_name": "Keuninckx",
                "first_name": "Lars"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC"
        ],
        "abstract": "  This work proposes a novel approach for hand gesture recognition using an inexpensive, low-resolution (24 x 32) thermal sensor processed by a Spiking Neural Network (SNN) followed by Sparse Segmentation and feature-based gesture classification via Robust Principal Component Analysis (R-PCA). Compared to the use of standard RGB cameras, the proposed system is insensitive to lighting variations while being significantly less expensive compared to high-frequency radars, time-of-flight cameras and high-resolution thermal sensors previously used in literature. Crucially, this paper shows that the innovative use of the recently proposed Monostable Multivibrator (MMV) neural networks as a new class of SNN achieves more than one order of magnitude smaller memory and compute complexity compared to deep learning approaches, while reaching a top gesture recognition accuracy of 93.9% using a 5-class thermal camera dataset acquired in a car cabin, within an automotive context. Our dataset is released for helping future research. ",
        "title": "Resource-Efficient Gesture Recognition using Low-Resolution Thermal  Camera via Spiking Neural Networks and Sparse Segmentation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06566",
        "abstract_url": "http://arxiv.org/abs/2401.06566",
        "authors": [
            {
                "last_name": "Anahtarci",
                "first_name": "Berkay"
            },
            {
                "last_name": "Kariksiz",
                "first_name": "Can Deha"
            },
            {
                "last_name": "Saldi",
                "first_name": "Naci"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) for the forward RL problem. This method is employed to produce data for a numerical example. We note that this novel algorithm is also applicable to general MFE computations. ",
        "title": "Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field  Games",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06568",
        "abstract_url": "http://arxiv.org/abs/2401.06568",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Xu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhirui"
            },
            {
                "last_name": "Geng",
                "first_name": "Xiang"
            },
            {
                "last_name": "Du",
                "first_name": "Yichao"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks. ",
        "title": "Lost in the Source Language: How Large Language Models Evaluate the  Quality of Machine Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06574",
        "abstract_url": "http://arxiv.org/abs/2401.06574",
        "authors": [
            {
                "last_name": "Badings",
                "first_name": "Thom"
            },
            {
                "last_name": "Volk",
                "first_name": "Matthias"
            },
            {
                "last_name": "Junges",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Stoelinga",
                "first_name": "Marielle"
            },
            {
                "last_name": "Jansen",
                "first_name": "Nils"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Labeled continuous-time Markov chains (CTMCs) describe processes subject to random timing and partial observability. In applications such as runtime monitoring, we must incorporate past observations. The timing of these observations matters but may be uncertain. Thus, we consider a setting in which we are given a sequence of imprecisely timed labels called the evidence. The problem is to compute reachability probabilities, which we condition on this evidence. Our key contribution is a method that solves this problem by unfolding the CTMC states over all possible timings for the evidence. We formalize this unfolding as a Markov decision process (MDP) in which each timing for the evidence is reflected by a scheduler. This MDP has infinitely many states and actions in general, making a direct analysis infeasible. Thus, we abstract the continuous MDP into a finite interval MDP (iMDP) and develop an iterative refinement scheme to upper-bound conditional probabilities in the CTMC. We show the feasibility of our method on several numerical benchmarks and discuss key challenges to further enhance the performance. ",
        "title": "CTMCs with Imprecisely Timed Observations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06576",
        "abstract_url": "http://arxiv.org/abs/2401.06576",
        "authors": [
            {
                "last_name": "Theisel",
                "first_name": "Holger"
            },
            {
                "last_name": "Motejat",
                "first_name": "Michael"
            },
            {
                "last_name": "Zimmermann",
                "first_name": "Janos"
            },
            {
                "last_name": "R\u00f6ssl",
                "first_name": "Christian"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  We introduce a representation of a 2D steady vector field ${{\\mathbf v}}$ by two scalar fields $a$, $b$, such that the isolines of $a$ correspond to stream lines of ${{\\mathbf v}}$, and $b$ increases with constant speed under integration of ${{\\mathbf v}}$. This way, we get a direct encoding of stream lines, i.e., a numerical integration of ${{\\mathbf v}}$ can be replaced by a local isoline extraction of $a$. To guarantee a solution in every case, gradient-preserving cuts are introduced such that the scalar fields are allowed to be discontinuous in the values but continuous in the gradient. Along with a piecewise linear discretization and a proper placement of the cuts, the fields $a$ and $b$ can be computed. We show several evaluations on non-trivial vector fields. ",
        "title": "Scalar Representation of 2D Steady Vector Fields",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06578",
        "abstract_url": "http://arxiv.org/abs/2401.06578",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qian"
            },
            {
                "last_name": "Li",
                "first_name": "Weiqi"
            },
            {
                "last_name": "Mou",
                "first_name": "Chong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xinhua"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  360-degree panoramic videos recently attract more interest in both studies and applications, courtesy of the heightened immersive experiences they engender. Due to the expensive cost of capturing 360-degree panoramic videos, generating desirable panoramic videos by given prompts is urgently required. Recently, the emerging text-to-video (T2V) diffusion methods demonstrate notable effectiveness in standard video generation. However, due to the significant gap in content and motion patterns between panoramic and standard videos, these methods encounter challenges in yielding satisfactory 360-degree panoramic videos. In this paper, we propose a controllable panorama video generation pipeline named 360-Degree Video Diffusion model (360DVD) for generating panoramic videos based on the given prompts and motion conditions. Concretely, we introduce a lightweight module dubbed 360-Adapter and assisted 360 Enhancement Techniques to transform pre-trained T2V models for 360-degree video generation. We further propose a new panorama dataset named WEB360 consisting of 360-degree video-text pairs for training 360DVD, addressing the absence of captioned panoramic video datasets. Extensive experiments demonstrate the superiority and effectiveness of 360DVD for panorama video generation. The code and dataset will be released soon. ",
        "title": "360DVD: Controllable Panorama Video Generation with 360-Degree Video  Diffusion Model",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06579",
        "abstract_url": "http://arxiv.org/abs/2401.06579",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Yaoxu"
            },
            {
                "last_name": "Li",
                "first_name": "Hongyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Peng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Time-Triggered Ethernet (TTEthernet) has been widely applied in many scenarios such as industrial internet, automotive electronics, and aerospace, where offline routing and scheduling for TTEthernet has been largely investigated. However, predetermined routes and schedules cannot meet the demands in some agile scenarios, such as smart factories, autonomous driving, and satellite network switching, where the transmission requests join in and leave the network frequently. Thus, we study the online joint routing and scheduling problem for TTEthernet. However, balancing efficient and effective routing and scheduling in an online environment can be quite challenging. To ensure high-quality and fast routing and scheduling, we first design a time-slot expanded graph (TSEG) to model the available resources of TTEthernet over time. The fine-grained representation of TSEG allows us to select a time slot via selecting an edge, thus transforming the scheduling problem into a simple routing problem. Next, we design a dynamic weighting method for each edge in TSEG and further propose an algorithm to co-optimize the routing and scheduling. Our scheme enhances the TTEthernet throughput by co-optimizing the routing and scheduling to eliminate potential conflicts among flow requests, as compared to existing methods. The extensive simulation results show that our scheme runs >400 times faster than standard solutions (i.e., ILP solver), while the gap is only 2% to the optimally scheduled number of flow requests. Besides, as compared to existing schemes, our method can improve the successfully scheduled number of flows by more than 18%. ",
        "title": "Enhancing Throughput for TTEthernet via Co-optimizing Routing and  Scheduling: An Online Time-Varying Graph-based Method",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06580",
        "abstract_url": "http://arxiv.org/abs/2401.06580",
        "authors": [
            {
                "last_name": "Sapozhnikov",
                "first_name": "Arkadii"
            },
            {
                "last_name": "Olsthoorn",
                "first_name": "Mitchell"
            },
            {
                "last_name": "Panichella",
                "first_name": "Annibale"
            },
            {
                "last_name": "Kovalenko",
                "first_name": "Vladimir"
            },
            {
                "last_name": "Derakhshanfar",
                "first_name": "Pouria"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces TestSpark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo ",
        "title": "TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06582",
        "abstract_url": "http://arxiv.org/abs/2401.06582",
        "authors": [
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Robertson",
                "first_name": "Dawn C."
            },
            {
                "last_name": "Carley",
                "first_name": "Kathleen M."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Social media platforms are a key ground of information consumption and dissemination. Key figures like politicians, celebrities and activists have leveraged on its wide user base for strategic communication. Strategic communications, or StratCom, is the deliberate act of information creation and distribution. Its techniques are used by these key figures for establishing their brand and amplifying their messages. Automated scripts are used on top of personal touches to quickly and effectively perform these tasks. The combination of automation and manual online posting creates a Cyborg social media profile, which is a hybrid between bot and human. In this study, we establish a quantitative definition for a Cyborg account, which is an account that are detected as bots in one time window, and identified as humans in another. This definition makes use of frequent changes of bot classification labels and large differences in bot likelihood scores to identify Cyborgs. We perform a large-scale analysis across over 3.1 million users from Twitter collected from two key events, the 2020 Coronavirus pandemic and 2020 US Elections. We extract Cyborgs from two datasets and employ tools from network science, natural language processing and manual annotation to characterize Cyborg accounts. Our analyses identify Cyborg accounts are mostly constructed for strategic communication uses, have a strong duality in their bot/human classification and are tactically positioned in the social media network, aiding these accounts to promote their desired content. Cyborgs are also discovered to have long online lives, indicating their ability to evade bot detectors, or the graciousness of platforms to allow their operations. ",
        "title": "Cyborgs for strategic communication on social media",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06583",
        "abstract_url": "http://arxiv.org/abs/2401.06583",
        "authors": [
            {
                "last_name": "Tashu",
                "first_name": "Tsegaye Misikir"
            },
            {
                "last_name": "Kontos",
                "first_name": "Eduard-Raul"
            },
            {
                "last_name": "Sabatelli",
                "first_name": "Matthia"
            },
            {
                "last_name": "Valdenegro-Toro",
                "first_name": "Matias"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR",
            "LG"
        ],
        "abstract": "  Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding beyond language connections, between two specific languages. ",
        "title": "Mapping Transformer Leveraged Embeddings for Cross-Lingual Document  Representation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06588",
        "abstract_url": "http://arxiv.org/abs/2401.06588",
        "authors": [
            {
                "last_name": "Salvi",
                "first_name": "Giampiero"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "SD"
        ],
        "abstract": "  This paper describes the use of connectionist techniques in phonetic speech recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular attention has been paid to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency. ",
        "title": "Dynamic Behaviour of Connectionist Speech Recognition with Strong  Latency Constraints",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06591",
        "abstract_url": "http://arxiv.org/abs/2401.06591",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Seongyun"
            },
            {
                "last_name": "Kim",
                "first_name": "Seungone"
            },
            {
                "last_name": "Park",
                "first_name": "Sue Hyun"
            },
            {
                "last_name": "Kim",
                "first_name": "Geewook"
            },
            {
                "last_name": "Seo",
                "first_name": "Minjoon"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision ",
        "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained  Evaluation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06592",
        "abstract_url": "http://arxiv.org/abs/2401.06592",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Hang"
            },
            {
                "last_name": "Li",
                "first_name": "Song"
            },
            {
                "last_name": "Lin",
                "first_name": "Junhong"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We study deterministic matrix completion problem, i.e., recovering a low-rank matrix from a few observed entries where the sampling set is chosen as the edge set of a Ramanujan graph. We first investigate projected gradient descent (PGD) applied to a Burer-Monteiro least-squares problem and show that it converges linearly to the incoherent ground-truth with respect to the condition number \\k{appa} of ground-truth under a benign initialization and large samples. We next apply the scaled variant of PGD to deal with the ill-conditioned case when \\k{appa} is large, and we show the algorithm converges at a linear rate independent of the condition number \\k{appa} under similar conditions. Finally, we provide numerical experiments to corroborate our results. ",
        "title": "Nonconvex Deterministic Matrix Completion by Projected Gradient Descent  Methods",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06595",
        "abstract_url": "http://arxiv.org/abs/2401.06595",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Wang",
                "first_name": "Qian"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Li",
                "first_name": "Jialu"
            },
            {
                "last_name": "Hu",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Attributed graph clustering is an unsupervised task that partitions nodes into different groups. Self-supervised learning (SSL) shows great potential in handling this task, and some recent studies simultaneously learn multiple SSL tasks to further boost performance. Currently, different SSL tasks are assigned the same set of weights for all graph nodes. However, we observe that some graph nodes whose neighbors are in different groups require significantly different emphases on SSL tasks. In this paper, we propose to dynamically learn the weights of SSL tasks for different nodes and fuse the embeddings learned from different SSL tasks to boost performance. We design an innovative graph clustering approach, namely Dynamically Fusing Self-Supervised Learning (DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks using distinct weights derived from a gating network. To effectively learn the gating network, we design a dual-level self-supervised strategy that incorporates pseudo labels and the graph structure. Extensive experiments on five datasets show that DyFSS outperforms the state-of-the-art multi-task SSL methods by up to 8.66% on the accuracy metric. The code of DyFSS is available at: https://github.com/q086/DyFSS. ",
        "title": "Every Node is Different: Dynamically Fusing Self-Supervised Tasks for  Attributed Graph Clustering",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06596",
        "abstract_url": "http://arxiv.org/abs/2401.06596",
        "authors": [
            {
                "last_name": "L\u00f6ding",
                "first_name": "Christof"
            },
            {
                "last_name": "Thomas",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  The class of Boolean combinations of tree languages recognized by deterministic top-down tree automata (also known as deterministic root-to-frontier automata) is studied. The problem of determining for a given regular tree language whether it belongs to this class is open. We provide some progress by two results: First, a characterization of this class by a natural extension of deterministic top-down tree automata is presented, and as an application we obtain a convenient method to show that certain regular tree languages are outside this class. In the second result, it is shown that, for fixed $k$, it is decidable whether a regular tree language is a Boolean combination of $k$ tree languages recognized by deterministic top-down tree automata. ",
        "title": "On the Boolean Closure of Deterministic Top-Down Tree Automata",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06601",
        "abstract_url": "http://arxiv.org/abs/2401.06601",
        "authors": [
            {
                "last_name": "Nunes",
                "first_name": "Henry C."
            },
            {
                "last_name": "da Silva",
                "first_name": "Marlon P."
            },
            {
                "last_name": "Neu",
                "first_name": "Charles V."
            },
            {
                "last_name": "Zorzo",
                "first_name": "Avelino F."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "DB"
        ],
        "abstract": "  This paper presents ongoing research focused on improving the utility of data protected by Global Differential Privacy(DP) in the scenario of summary statistics. Our approach is based on predictions on how an analyst will use statistics released under DP protection, so that a developer can optimise data utility on further usage of the data in the privacy budget allocation. This novel approach can potentially improve the utility of data without compromising privacy constraints. We also propose a metric that can be used by the developer to optimise the budget allocation process. ",
        "title": "A proposal to increase data utility on Global Differential Privacy data  based on data use predictions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06602",
        "abstract_url": "http://arxiv.org/abs/2401.06602",
        "authors": [
            {
                "last_name": "Voggenreiter",
                "first_name": "Markus"
            },
            {
                "last_name": "Angermeir",
                "first_name": "Florian"
            },
            {
                "last_name": "Moy\u00f3n",
                "first_name": "Fabiola"
            },
            {
                "last_name": "Sch\u00f6pp",
                "first_name": "Ulrich"
            },
            {
                "last_name": "Bonvin",
                "first_name": "Pierre"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  In recent years, DevOps, the unification of development and operation workflows, has become a trend for the industrial software development lifecycle. Security activities turned into an essential field of application for DevOps principles as they are a fundamental part of secure software development in the industry. A common practice arising from this trend is the automation of security tests that analyze a software product from several perspectives. To effectively improve the security of the analyzed product, the identified security findings must be managed and looped back to the project team for stakeholders to take action. This management must cope with several challenges ranging from low data quality to a consistent prioritization of findings while following DevOps aims. To manage security findings with the same efficiency as other activities in DevOps projects, a methodology for the management of industrial security findings minding DevOps principles is essential.   In this paper, we propose a methodology for the management of security findings in industrial DevOps projects, summarizing our research in this domain and presenting the resulting artifact. As an instance of the methodology, we developed the Security Flama, a semantic knowledge base for the automated management of security findings. To analyze the impact of our methodology on industrial practice, we performed a case study on two DevOps projects of a multinational industrial enterprise. The results emphasize the importance of using such an automated methodology in industrial DevOps projects, confirm our approach's usefulness and positive impact on the studied projects, and identify the communication strategy as a crucial factor for usability in practice. ",
        "title": "Automated Security Findings Management: A Case Study in Industrial  DevOps",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06603",
        "abstract_url": "http://arxiv.org/abs/2401.06603",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Shangding"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as \"I help you help I help.\" The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization, exploration, and mutual improvement for both agents, enabling them to accomplish increasingly challenging tasks. Remarkably, we propose a practical algorithm to address the problem and conduct empirical experiments to evaluate the effectiveness of our method. ",
        "title": "Mutual Enhancement of Large Language and Reinforcement Learning Models  through Bi-Directional Feedback Mechanisms: A Case Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06610",
        "abstract_url": "http://arxiv.org/abs/2401.06610",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jingyi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper addresses the planar finger kinematics for seeking optimized manipulation strategies. The first step is to model based on geometric features of linear and rotation motion so that the robot can select the fingers configurations. This kinematic model considers the motion between hands and object. Based on 2-finger manipulation cases, this model can output the strategies for bimanual manipulation. For executing strategies, the second step is to seek the appropriate values of finger joints according to the ending orientation of fingers. The simulation shows that the computed solutions can complete the relative rotation and linear motion of unknown objects. ",
        "title": "The Hand-object Kinematic Model for Bimanual Manipulation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06612",
        "abstract_url": "http://arxiv.org/abs/2401.06612",
        "authors": [
            {
                "last_name": "AlQahtani",
                "first_name": "Ali Abdullah S."
            },
            {
                "last_name": "Alshayeb",
                "first_name": "Thamraa"
            },
            {
                "last_name": "Nabil",
                "first_name": "Mahmoud"
            },
            {
                "last_name": "Patooghy",
                "first_name": "Ahmad"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The traditional two-factor authentication (2FA) methods primarily rely on the user manually entering a code or token during the authentication process. This can be burdensome and time-consuming, particularly for users who must be authenticated frequently. To tackle this challenge, we present a novel 2FA approach replacing the user's input with decisions made by Machine Learning (ML) that continuously verifies the user's identity with zero effort. Our system exploits unique environmental features associated with the user, such as beacon frame characteristics and Received Signal Strength Indicator (RSSI) values from Wi-Fi Access Points (APs). These features are gathered and analyzed in real-time by our ML algorithm to ascertain the user's identity. For enhanced security, our system mandates that the user's two devices (i.e., a login device and a mobile device) be situated within a predetermined proximity before granting access. This precaution ensures that unauthorized users cannot access sensitive information or systems, even with the correct login credentials. Through experimentation, we have demonstrated our system's effectiveness in determining the location of the user's devices based on beacon frame characteristics and RSSI values, achieving an accuracy of 92.4%. Additionally, we conducted comprehensive security analysis experiments to evaluate the proposed 2FA system's resilience against various cyberattacks. Our findings indicate that the system exhibits robustness and reliability in the face of these threats. The scalability, flexibility, and adaptability of our system render it a promising option for organizations and users seeking a secure and convenient authentication system. ",
        "title": "Leveraging Machine Learning for Wi-Fi-based Environmental Continuous  Two-Factor Authentication",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06614",
        "abstract_url": "http://arxiv.org/abs/2401.06614",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Wei"
            },
            {
                "last_name": "Luo",
                "first_name": "Chang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Biao"
            },
            {
                "last_name": "Nie\u00dfner",
                "first_name": "Matthias"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based prior enables more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent vector sets instead of using a global latent. This novel 4D representation allows us to learn local surface shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporal-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid the computational overhead, we design an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against the state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations, notably achieving a 19% improvement in Intersection over Union (IoU) compared to CaDex for reconstructing unseen individuals from sparse point clouds on the DeformingThings4D-Animals dataset. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/. ",
        "title": "Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape  Reconstruction and Tracking",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06618",
        "abstract_url": "http://arxiv.org/abs/2401.06618",
        "authors": [
            {
                "last_name": "Ball",
                "first_name": "Simeon"
            },
            {
                "last_name": "Moreno",
                "first_name": "Edgar"
            },
            {
                "last_name": "Simoens",
                "first_name": "Robin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We prove that the natural isomorphism between GF(2^h) and GF(2)^h induces a bijection between stabiliser codes on n quqits with local dimension q=2^h and binary stabiliser codes on hn qubits. This allows us to describe these codes geometrically: a stabiliser code over a field of even order corresponds to a so-called quantum set of symplectic polar spaces. Moreover, equivalent stabiliser codes have a similar geometry, which can be used to prove the uniqueness of a [[4,0,3]]_4 stabiliser code and the nonexistence of both a [[7,1,4]]_4 and an [[8,0,5]]_4 stabiliser code. ",
        "title": "Stabiliser codes over fields of even order",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06619",
        "abstract_url": "http://arxiv.org/abs/2401.06619",
        "authors": [
            {
                "last_name": "Chow",
                "first_name": "Yiu Wai"
            },
            {
                "last_name": "Di Grazia",
                "first_name": "Luca"
            },
            {
                "last_name": "Pradel",
                "first_name": "Michael"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice. ",
        "title": "PyTy: Repairing Static Type Errors in Python",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06620",
        "abstract_url": "http://arxiv.org/abs/2401.06620",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yihong"
            },
            {
                "last_name": "Ma",
                "first_name": "Chunlan"
            },
            {
                "last_name": "Ye",
                "first_name": "Haotian"
            },
            {
                "last_name": "Sch\u00fctze",
                "first_name": "Hinrich"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\\%) of its training data, and refer to the resulting model as Furina. We show that Furina not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages are highly related but use different scripts. We make our code and models publicly available. ",
        "title": "TransliCo: A Contrastive Learning Framework to Address the Script  Barrier in Multilingual Pretrained Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06626",
        "abstract_url": "http://arxiv.org/abs/2401.06626",
        "authors": [
            {
                "last_name": "Bursuc",
                "first_name": "Sergiu"
            },
            {
                "last_name": "Gil-Pons",
                "first_name": "Reynaldo"
            },
            {
                "last_name": "Mauw",
                "first_name": "Sjouke"
            },
            {
                "last_name": "Trujillo-Rasua",
                "first_name": "Rolando"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  A Proof of Secure Erasure (PoSE) is a communication protocol where a verifier seeks evidence that a prover has erased its memory within the time frame of the protocol execution. Designers of PoSE protocols have long been aware that, if a prover can outsource the computation of the memory erasure proof to another device, then their protocols are trivially defeated. As a result, most software-based PoSE protocols in the literature assume that provers are isolated during the protocol execution, that is, provers cannot receive help from a network adversary. Our main contribution is to show that this assumption is not necessary. We introduce formal models for PoSE protocols playing against provers aided by external conspirators and develop three PoSE protocols that we prove secure in this context. We reduce the requirement of isolation to the more realistic requirement that the communication with the external conspirator is relatively slow. Software-based protocols with such relaxed isolation assumptions are especially pertinent for low-end devices, where it is too costly to deploy sophisticated protection methods. ",
        "title": "Software-Based Memory Erasure with relaxed isolation requirements:  Extended Version",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06628",
        "abstract_url": "http://arxiv.org/abs/2401.06628",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            },
            {
                "last_name": "Luo",
                "first_name": "Yong"
            },
            {
                "last_name": "Du",
                "first_name": "Bo"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field. Our benchmark and scripts are publicly released at: https://github.com/alphadl/OOP-eval. ",
        "title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language  Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06633",
        "abstract_url": "http://arxiv.org/abs/2401.06633",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Lei"
            },
            {
                "last_name": "Lian",
                "first_name": "Jianxun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiao"
            },
            {
                "last_name": "Xie",
                "first_name": "Xing"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We perform experiments on three widely used public datasets, incorporating five powerful sequential recommenders as backbone models. Our results demonstrate that Ada-Retrieval significantly enhances the performance of various base models, with consistent improvements observed across different datasets. Our code and data are publicly available at: https://github.com/ll0ruc/Ada-Retrieval. ",
        "title": "Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential  Recommendations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06634",
        "abstract_url": "http://arxiv.org/abs/2401.06634",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Jie"
            },
            {
                "last_name": "Liu",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhong-Yuan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCFC also shows superior performance in handling device failures from a practical viewpoint. ",
        "title": "CCFC: Bridging Federated Clustering and Contrastive Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06635",
        "abstract_url": "http://arxiv.org/abs/2401.06635",
        "authors": [
            {
                "last_name": "Iserles",
                "first_name": "Arieh"
            },
            {
                "last_name": "Kropielnicka",
                "first_name": "Karolina"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Using elementary means, we derive the three most popular splittings of $e^{(A+B)}$ and their error bounds in the case when $A$ and $B$ are (possibly unbounded) operators in a Hilbert space, generating strongly continuous semigroups, $e^{tA}$, $e^{tB}$ and $e^{t(A+B)}$. The error of these splittings is bounded in terms of the norm of the commutators $[A, B]$, $[A, [A, B]]$ and $[B, [A, B]]$. ",
        "title": "An elementary approach to splittings of unbounded operators",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06638",
        "abstract_url": "http://arxiv.org/abs/2401.06638",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Manish"
            },
            {
                "last_name": "Shrivastava",
                "first_name": "Pramsu"
            },
            {
                "last_name": "Harshan",
                "first_name": "J."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CR"
        ],
        "abstract": "  In Vehicle-to-Everything (V2X) networks that involve multi-hop communication, the Road Side Units (RSUs) typically desire to gather the location information of the participating vehicles to provide security and network-diagnostics features. Although Global Positioning System (GPS) based localization is widely used by vehicles for navigation; they may not forward their exact GPS coordinates to the RSUs due to privacy issues. Therefore, to balance the high-localization requirements of RSU and the privacy of the vehicles, we demonstrate a new spatial-provenance framework wherein the vehicles agree to compromise their privacy to a certain extent and share a low-precision variant of its coordinates in agreement with the demands of the RSU. To study the deployment feasibility of the proposed framework in state-of-the-art wireless standards, we propose a testbed of ZigBee and LoRa devices and implement the underlying protocols on their stack using correlated Bloom filters and Rake compression algorithms. Our demonstrations reveal that low-to-moderate precision localization can be achieved in fewer packets, thus making an appealing case for next-generation vehicular networks to include our methods for providing real-time security and network-diagnostics features. ",
        "title": "A Prototype on the Feasibility of Learning Spatial Provenance in XBee  and LoRa Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06640",
        "abstract_url": "http://arxiv.org/abs/2401.06640",
        "authors": [
            {
                "last_name": "Misra",
                "first_name": "Kanishka"
            },
            {
                "last_name": "Ettinger",
                "first_name": "Allyson"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs. ",
        "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference  in Language Models, but Inconsistently",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06643",
        "abstract_url": "http://arxiv.org/abs/2401.06643",
        "authors": [
            {
                "last_name": "Cegin",
                "first_name": "Jan"
            },
            {
                "last_name": "Pecher",
                "first_name": "Branislav"
            },
            {
                "last_name": "Simko",
                "first_name": "Jakub"
            },
            {
                "last_name": "Srba",
                "first_name": "Ivan"
            },
            {
                "last_name": "Bielikova",
                "first_name": "Maria"
            },
            {
                "last_name": "Brusilovsky",
                "first_name": "Peter"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune the model. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs and 6 datasets. We show that diversity is most increased by taboo words, while downstream model performance is highest when previously created paraphrases are used as hints. ",
        "title": "Effects of diversity incentives on sample diversity and downstream model  performance in LLM-based text augmentation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06644",
        "abstract_url": "http://arxiv.org/abs/2401.06644",
        "authors": [
            {
                "last_name": "Saeizadeh",
                "first_name": "Ali"
            },
            {
                "last_name": "Schonholtz",
                "first_name": "Douglas"
            },
            {
                "last_name": "Uvaydov",
                "first_name": "Daniel"
            },
            {
                "last_name": "Guida",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Demirors",
                "first_name": "Emrecan"
            },
            {
                "last_name": "Johari",
                "first_name": "Pedram"
            },
            {
                "last_name": "Jimenez",
                "first_name": "Jorge M."
            },
            {
                "last_name": "Neimat",
                "first_name": "Joseph S."
            },
            {
                "last_name": "Melodia",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we introduce SeizNet, a closed-loop system for predicting epileptic seizures through the use of Deep Learning (DL) method and implantable sensor networks. While pharmacological treatment is effective for some epilepsy patients (with ~65M people affected worldwide), one out of three suffer from drug-resistant epilepsy. To alleviate the impact of seizure, predictive systems have been developed that can notify such patients of an impending seizure, allowing them to take precautionary measures. SeizNet leverages DL techniques and combines data from multiple recordings, specifically intracranial electroencephalogram (iEEG) and electrocardiogram (ECG) sensors, that can significantly improve the specificity of seizure prediction while preserving very high levels of sensitivity. SeizNet DL algorithms are designed for efficient real-time execution at the edge, minimizing data privacy concerns, data transmission overhead, and power inefficiencies associated with cloud-based solutions. Our results indicate that SeizNet outperforms traditional single-modality and non-personalized prediction systems in all metrics, achieving up to 99% accuracy in predicting seizure, offering a promising new avenue in refractory epilepsy treatment. ",
        "title": "SeizNet: An AI-enabled Implantable Sensor Network System for Seizure  Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06646",
        "abstract_url": "http://arxiv.org/abs/2401.06646",
        "authors": [
            {
                "last_name": "Hien",
                "first_name": "Le Thi Khanh"
            },
            {
                "last_name": "Leplat",
                "first_name": "Valentin"
            },
            {
                "last_name": "Gillis",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a Block Majorization Minimization method with Extrapolation (BMMe) for solving a class of multi-convex optimization problems. The extrapolation parameters of BMMe are updated using a novel adaptive update rule. By showing that block majorization minimization can be reformulated as a block mirror descent method, with the Bregman divergence adaptively updated at each iteration, we establish subsequential convergence for BMMe. We use this method to design efficient algorithms to tackle nonnegative matrix factorization problems with the $\\beta$-divergences ($\\beta$-NMF) for $\\beta\\in [1,2]$. These algorithms, which are multiplicative updates with extrapolation, benefit from our novel results that offer convergence guarantees. We also empirically illustrate the significant acceleration of BMMe for $\\beta$-NMF through extensive experiments. ",
        "title": "Block Majorization Minimization with Extrapolation and Application to  $\\beta$-NMF",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06648",
        "abstract_url": "http://arxiv.org/abs/2401.06648",
        "authors": [
            {
                "last_name": "Allamaa",
                "first_name": "Jean Pierre"
            },
            {
                "last_name": "Patrinos",
                "first_name": "Panagiotis"
            },
            {
                "last_name": "Ohtsuka",
                "first_name": "Toshiyuki"
            },
            {
                "last_name": "Son",
                "first_name": "Tong Duy"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The autonomous driving industry is continuously dealing with more safety-critical scenarios, and nonlinear model predictive control (NMPC) is a powerful control strategy for handling such situations. However, standard safety constraints are not scalable and require a long NMPC horizon. Moreover, the adoption of NMPC in the automotive industry is limited by the heavy computation of numerical optimization routines. To address those issues, this paper presents a real-time capable NMPC for automated driving in urban environments, using control barrier functions (CBFs). Furthermore, the designed NMPC is based on a novel collocation transcription approach, named RESAFE/COL, that allows to reduce the number of optimization variables while still guaranteeing the continuous time (nonlinear) inequality constraints satisfaction, through regional convex hull approximation. RESAFE/COL is proven to be 5 times faster than multiple shooting and more tractable for embedded hardware without a decrease in the performance, nor accuracy and safety of the numerical solution. We validate our NMPC-CBF with RESAFE/COL approach with highly accurate digital twins of the vehicle and the urban environment and show the safe controller's ability to improve crash avoidance by 91%. ",
        "title": "Real-time MPC with Control Barrier Functions for Autonomous Driving  using Safety Enhanced Collocation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06649",
        "abstract_url": "http://arxiv.org/abs/2401.06649",
        "authors": [
            {
                "last_name": "Heidari",
                "first_name": "Arash"
            },
            {
                "last_name": "Gonzalez",
                "first_name": "Sebastian Rojas"
            },
            {
                "last_name": "Dhaene",
                "first_name": "Tom"
            },
            {
                "last_name": "Couckuyt",
                "first_name": "Ivo"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Multi-objective optimization is a widely studied problem in diverse fields, such as engineering and finance, that seeks to identify a set of non-dominated solutions that provide optimal trade-offs among competing objectives. However, the computation of the entire Pareto front can become prohibitively expensive, both in terms of computational resources and time, particularly when dealing with a large number of objectives. In practical applications, decision-makers (DMs) will select a single solution of the Pareto front that aligns with their preferences to be implemented; thus, traditional multi-objective algorithms invest a lot of budget sampling solutions that are not interesting for the DM. In this paper, we propose two novel algorithms that employ Gaussian Processes and advanced discretization methods to efficiently locate the most preferred region of the Pareto front in expensive-to-evaluate problems. Our approach involves interacting with the decision-maker to guide the optimization process towards their preferred trade-offs. Our experimental results demonstrate that our proposed algorithms are effective in finding non-dominated solutions that align with the decision-maker's preferences while maintaining computational efficiency. ",
        "title": "Data-Efficient Interactive Multi-Objective Optimization Using ParEGO",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06650",
        "abstract_url": "http://arxiv.org/abs/2401.06650",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Zilin"
            },
            {
                "last_name": "Georgiou",
                "first_name": "Anastasis"
            },
            {
                "last_name": "Yu",
                "first_name": "Min"
            },
            {
                "last_name": "Evangelou",
                "first_name": "Simos A."
            },
            {
                "last_name": "Jaimoukha",
                "first_name": "Imad M"
            },
            {
                "last_name": "Dini",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a robust model predictive control-based solution for the recently introduced series active variable geometry suspension (SAVGS) to improve the ride comfort and road holding of a quarter car. In order to close the gap between the nonlinear multi-body SAVGS model and its linear equivalent, a new uncertain system characterization is proposed that captures unmodeled dynamics, parameter variation, and external disturbances. Based on the newly proposed linear uncertain model for the quarter car SAVGS system, a constrained optimal control problem (OCP) is presented in the form of a linear matrix inequality (LMI) optimization. More specifically, utilizing semidefinite relaxation techniques a state-feedback robust model predictive control (RMPC) scheme is presented and integrated with the nonlinear multi-body SAVGS model, where state-feedback gain and control perturbation are computed online to optimise performance, while physical and design constraints are preserved. Numerical simulation results with different ISO-defined road events demonstrate the robustness and significant performance improvement in terms of ride comfort and road holding of the proposed approach, as compared to the conventional passive suspension, as well as, to actively controlled SAVGS by a previously developed conventional H-infinity control scheme. ",
        "title": "LMI-based robust model predictive control for a quarter car with series  active variable geometry suspension",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06653",
        "abstract_url": "http://arxiv.org/abs/2401.06653",
        "authors": [
            {
                "last_name": "Georgescu",
                "first_name": "Calin"
            },
            {
                "last_name": "Olsthoorn",
                "first_name": "Mitchell"
            },
            {
                "last_name": "Derakhshanfar",
                "first_name": "Pouria"
            },
            {
                "last_name": "Akhin",
                "first_name": "Marat"
            },
            {
                "last_name": "Panichella",
                "first_name": "Annibale"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Compiler correctness is a cornerstone of reliable software development. However, systematic testing of compilers is infeasible, given the vast space of possible programs and the complexity of modern programming languages. In this context, differential testing offers a practical methodology as it addresses the oracle problem by comparing the output of alternative compilers given the same set of programs as input. In this paper, we investigate the effectiveness of differential testing in finding bugs within the Kotlin compilers developed at JetBrains. We propose a black-box generative approach that creates input programs for the K1 and K2 compilers. First, we build workable models of Kotlin semantic (semantic interface) and syntactic (enriched context-free grammar) language features, which are subsequently exploited to generate random code snippets. Second, we extend random sampling by introducing two genetic algorithms (GAs) that aim to generate more diverse input programs. Our case study shows that the proposed approach effectively detects bugs in K1 and K2; these bugs have been confirmed and (some) fixed by JetBrains developers. While we do not observe a significant difference w.r.t. the number of defects uncovered by the different search algorithms, random search and GAs are complementary as they find different categories of bugs. Finally, we provide insights into the relationships between the size, complexity, and fault detection capability of the generated input programs. ",
        "title": "Evolutionary Generative Fuzzing for Differential Testing of the Kotlin  Compiler",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06654",
        "abstract_url": "http://arxiv.org/abs/2401.06654",
        "authors": [
            {
                "last_name": "Bl\u00fccher",
                "first_name": "Stefan"
            },
            {
                "last_name": "Vielhaben",
                "first_name": "Johanna"
            },
            {
                "last_name": "Strodthoff",
                "first_name": "Nils"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-based explanations (Shapley values) as well as their evaluation (pixel flipping, PF). However, occlusion strategies can vary significantly from simple mean replacement up to inpainting with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-based approaches. For example, PF benchmarks lead to contradicting rankings. This is amplified by competing PF measures: Features are either removed starting with most influential first (MIF) or least influential first (LIF). This study proposes two complementary perspectives to resolve this disagreement problem. Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead to unreliable model evaluations. We propose to measure the reliability by the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of occlusion strategies and resolves the disagreement problem by grouping consistent PF rankings. Secondly, we show that the insightfulness of MIF and LIF is conversely dependent on the R-OMS score. To leverage this, we combine the MIF and LIF measures into the symmetric relevance gain (SRG) measure. This breaks the inherent connection to the underlying occlusion strategy and leads to consistent rankings. This resolves the disagreement problem, which we verify for a set of 40 different occlusion strategies. ",
        "title": "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI  Benchmarks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06656",
        "abstract_url": "http://arxiv.org/abs/2401.06656",
        "authors": [
            {
                "last_name": "Opschoor",
                "first_name": "Joost A. A."
            },
            {
                "last_name": "Schwab",
                "first_name": "Christoph"
            },
            {
                "last_name": "Xenophontos",
                "first_name": "Christos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We prove deep neural network (DNN for short) expressivity rate bounds for solution sets of a model class of singularly perturbed, elliptic two-point boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We assume that the given source term and reaction coefficient are analytic in $[-1,1]$.   We establish expression rate bounds in Sobolev norms in terms of the NN size which are uniform with respect to the singular perturbation parameter for several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and $\\tanh$- and sigmoid-activated NNs. The latter activations can represent ``exponential boundary layer solution features'' explicitly, in the last hidden layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust expression rate bounds in terms of the NN size.   We prove that all DNN architectures allow robust exponential solution expression in so-called `energy' as well as in `balanced' Sobolev norms, for analytic input data. ",
        "title": "Neural Networks for Singular Perturbations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06657",
        "abstract_url": "http://arxiv.org/abs/2401.06657",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Jayasree"
            },
            {
                "last_name": "Dey",
                "first_name": "Debasmita"
            },
            {
                "last_name": "Ferlin",
                "first_name": "Simone"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Nirnay"
            },
            {
                "last_name": "Bajpai",
                "first_name": "Vaibhav"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  The Tactile Internet paradigm is set to revolutionize human society by enabling skill-set delivery and haptic communication over ultra-reliable, low-latency networks. The emerging sixth-generation (6G) mobile communication systems are envisioned to underpin this Tactile Internet ecosystem at the network edge by providing ubiquitous global connectivity. However, apart from a multitude of opportunities of the Tactile Internet, security and privacy challenges emerge at the forefront. We believe that the recently standardized QUIC protocol, characterized by end-to-end encryption and reduced round-trip delay would serve as the backbone of Tactile Internet. In this article, we envision a futuristic scenario where a QUIC-enabled network uses the underlying 6G communication infrastructure to achieve the requirements for Tactile Internet. Interestingly this requires a deeper investigation of a wide range of security and privacy challenges in QUIC, that need to be mitigated for its adoption in Tactile Internet. Henceforth, this article reviews the existing security and privacy attacks in QUIC and their implication on users. Followed by that, we discuss state-of-the-art attack mitigation strategies and investigate some of their drawbacks with possible directions for future work ",
        "title": "Accelerating Tactile Internet with QUIC: A Security and Privacy  Perspective",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06658",
        "abstract_url": "http://arxiv.org/abs/2401.06658",
        "authors": [
            {
                "last_name": "Nasuto",
                "first_name": "Andrea"
            },
            {
                "last_name": "Rowe",
                "first_name": "Francisco"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Immigration is one of the most salient topics in public debate. Social media heavily influences opinions on immigration, often sparking polarized debates and offline tensions. Studying 220,870 immigration-related tweets in the UK, we assessed the extent of polarization, key content creators and disseminators, and the speed of content dissemination. We identify a high degree of online polarization between pro and anti-immigration communities. We found that the anti-migration community is small but denser and more active than the pro-immigration community with the top 1% of users responsible for over 23% of anti-immigration tweets and 21% of retweets. We also discovered that anti-immigration content spreads also 1.66 times faster than pro-immigration messages and bots have minimal impact on content dissemination. Our findings suggest that identifying and tracking highly active users could curb anti-immigration sentiment, potentially easing social polarization and shaping broader societal attitudes toward migration. ",
        "title": "Exposing Hate -- Understanding Anti-Immigration Sentiment Spreading on  Twitter",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06659",
        "abstract_url": "http://arxiv.org/abs/2401.06659",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wenbin"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            },
            {
                "last_name": "Luo",
                "first_name": "Yong"
            },
            {
                "last_name": "Hu",
                "first_name": "Han"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (brings an average +1.89 F1 score among five advanced methods) over several state-of-the-art methods. Code will be released. ",
        "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual  World Knowledge",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06665",
        "abstract_url": "http://arxiv.org/abs/2401.06665",
        "authors": [
            {
                "last_name": "Consolaro",
                "first_name": "Gianpietro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhen"
            },
            {
                "last_name": "Razanajato",
                "first_name": "Harenome"
            },
            {
                "last_name": "Lossing",
                "first_name": "Nelson"
            },
            {
                "last_name": "Tchoulak",
                "first_name": "Nassim"
            },
            {
                "last_name": "Susungi",
                "first_name": "Adilla"
            },
            {
                "last_name": "Alves",
                "first_name": "Artur Cesar Araujo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Renwei"
            },
            {
                "last_name": "Barthou",
                "first_name": "Denis"
            },
            {
                "last_name": "Ancourt",
                "first_name": "Corinne"
            },
            {
                "last_name": "Bastoul",
                "first_name": "Cedric"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "CL",
            "PF"
        ],
        "abstract": "  Polyhedral techniques have been widely used for automatic code optimization in low-level compilers and higher-level processes. Loop optimization is central to this technique, and several polyhedral schedulers like Feautrier, Pluto, isl and Tensor Scheduler have been proposed, each of them targeting a different architecture, parallelism model, or application scenario. The need for scenario-specific optimization is growing due to the heterogeneity of architectures. One of the most critical cases is represented by NPUs (Neural Processing Units) used for AI, which may require loop optimization with different objectives. Another factor to be considered is the framework or compiler in which polyhedral optimization takes place. Different scenarios, depending on the target architecture, compilation environment, and application domain, may require different kinds of optimization to best exploit the architecture feature set.   We introduce a new configurable polyhedral scheduler, PolyTOPS, that can be adjusted to various scenarios with straightforward, high-level configurations. This scheduler allows the creation of diverse scheduling strategies that can be both scenario-specific (like state-of-the-art schedulers) and kernel-specific, breaking the concept of a one-size-fits-all scheduler approach. PolyTOPS has been used with isl and CLooG as code generators and has been integrated in MindSpore AKG deep learning compiler. Experimental results in different scenarios show good performance: a geomean speedup of 7.66x on MindSpore (for the NPU Ascend architecture) hybrid custom operators over isl scheduling, a geomean speedup up to 1.80x on PolyBench on different multicore architectures over Pluto scheduling. Finally, some comparisons with different state-of-the-art tools are presented in the PolyMage scenario. ",
        "title": "PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06667",
        "abstract_url": "http://arxiv.org/abs/2401.06667",
        "authors": [
            {
                "last_name": "Arazzi",
                "first_name": "Marco"
            },
            {
                "last_name": "Nocera",
                "first_name": "Antonino"
            },
            {
                "last_name": "Storti",
                "first_name": "Emanuele"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recently, the Industry 5.0 is gaining attention as a novel paradigm, defining the next concrete steps toward more and more intelligent, green-aware and user-centric digital systems. In an era in which smart devices typically adopted in the industry domain are more and more sophisticated and autonomous, the Internet of Things and its evolution, known as the Internet of Everything (IoE, for short), involving also people, robots, processes and data in the network, represent the main driver to allow industries to put the experiences and needs of human beings at the center of their ecosystems. However, due to the extreme heterogeneity of the involved entities, their intrinsic need and capability to cooperate, and the aim to adapt to a dynamic user-centric context, special attention is required for the integration and processing of the data produced by such an IoE. This is the objective of the present paper, in which we propose a novel semantic model that formalizes the fundamental actors, elements and information of an IoE, along with their relationships. In our design, we focus on state-of-the-art design principles, in particular reuse, and abstraction, to build ``SemIoE'', a lightweight ontology inheriting and extending concepts from well-known and consolidated reference ontologies. The defined semantic layer represents a core data model that can be extended to embrace any modern industrial scenario. It represents the base of an IoE Knowledge Graph, on top of which, as an additional contribution, we analyze and define some essential services for an IoE-based industry. ",
        "title": "The SemIoE Ontology: A Semantic Model Solution for an IoE-based Industry",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06669",
        "abstract_url": "http://arxiv.org/abs/2401.06669",
        "authors": [
            {
                "last_name": "G\u00f6ttsch",
                "first_name": "Fabian"
            },
            {
                "last_name": "Caire",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Xu",
                "first_name": "Wen"
            },
            {
                "last_name": "Schubert",
                "first_name": "Martin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper presents a comprehensive communication theoretic model for the physical layer of a cell-free user-centric network, formed by user equipments (UEs), radio units (RUs), and decentralized units (DUs), uniformly spatially distributed over a given coverage area. We consider RUs equipped with multiple antennas, and focus on the regime where the UE, RU, and DU densities are constant and therefore the number of such nodes grows with the coverage area. A system is said scalable if the computing load and information rate at any node in the network converges to a constant as the network size (coverage area) grows to infinity. This imposes that each UE must be processed by a (user-centric) finite-size cluster of RUs, and that such cluster processors are dynamically allocated to the DUs (e.g., as software defined virtual network functions) in order to achieve a balanced computation load. We also assume that the RUs are connected to the DUs through a packet switching network, in order to achieve adaptive routing and load balance. For this model, we define in details the dynamic cluster formation and uplink pilot allocation. As a consequence of the pilot allocation and the scalability constraint, each cluster processor has a partial view of the network channel state information. We define the condition of ``ideal partial CSI'' when the channel vectors that can be estimated are perfectly known (while the ones that cannot be estimated are not know at all). We develop two attractive cluster-based linear receiver schemes for the uplink, and an uplink-downlink duality that allows to reuse such vectors as precoders for the downlink. ",
        "title": "User-Centric Cell-Free Wireless Networks for 6G: Communication Theoretic  Models and Research Challenges",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06671",
        "abstract_url": "http://arxiv.org/abs/2401.06671",
        "authors": [
            {
                "last_name": "Razmjoo",
                "first_name": "Amirreza"
            },
            {
                "last_name": "Brecelj",
                "first_name": "Tilen"
            },
            {
                "last_name": "Savevska",
                "first_name": "Kristina"
            },
            {
                "last_name": "Ude",
                "first_name": "Ale\u0161"
            },
            {
                "last_name": "Petri\u010d",
                "first_name": "Tadej"
            },
            {
                "last_name": "Calinon",
                "first_name": "Sylvain"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper presents a study on the use of the Talos humanoid robot for performing assistive sit-to-stand or stand-to-sit tasks. In such tasks, the human exerts a large amount of force (100--200 N) within a very short time (2--8 s), posing significant challenges in terms of human unpredictability and robot stability control. To address these challenges, we propose an approach for finding a spatial reference for the robot, which allows the robot to move according to the force exerted by the human and control its stability during the task. Specifically, we focus on the problem of finding a 1D manifold for the robot, while assuming a simple controller to guide its movement on this manifold. To achieve this, we use a functional representation to parameterize the manifold and solve an optimization problem that takes into account the robot's stability and the unpredictability of human behavior. We demonstrate the effectiveness of our approach through simulations and experiments with the Talos robot, showing robustness and adaptability. ",
        "title": "Learning Joint Space Reference Manifold for Reliable Physical Assistance",
        "date": "2023-08-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06672",
        "abstract_url": "http://arxiv.org/abs/2401.06672",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Sangung"
            },
            {
                "last_name": "Xue",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Ukkusuri",
                "first_name": "Satish V."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Frequent and intensive disasters make the repeated and uncertain post-disaster recovery process. Despite the importance of the successful recovery process, previous simulation studies on the post-disaster recovery process did not explore the sufficient number of household return decision model types, population sizes, and the corresponding critical transition conditions of the system. This paper simulates the recovery process in the agent-based model with multilayer networks to reveal the impact of household return decision model types and population sizes in a toy network. After that, this paper applies the agent-based model to the five selected counties affected by Hurricane Harvey in 2017 to check the urban-rural recovery differences by types of household return decision models. The agent-based model yields three conclusions. First, the threshold model can successfully substitute the binary logit model. Second, high thresholds and less than 1,000 populations perturb the recovery process, yielding critical transitions during the recovery process. Third, this study checks the urban-rural recovery value differences by different decision model types. This study highlights the importance of the threshold models and population sizes to check the critical transitions and urban-rural differences in the recovery process. ",
        "title": "Finding critical transitions of the post-disaster recovery using the  sensitivity analysis of agent-based models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06676",
        "abstract_url": "http://arxiv.org/abs/2401.06676",
        "authors": [
            {
                "last_name": "John",
                "first_name": "Angela"
            },
            {
                "last_name": "Aidoo",
                "first_name": "Theophilus"
            },
            {
                "last_name": "Behmanush",
                "first_name": "Hamayoon"
            },
            {
                "last_name": "Gunduz",
                "first_name": "Irem B."
            },
            {
                "last_name": "Shrestha",
                "first_name": "Hewan"
            },
            {
                "last_name": "Rahman",
                "first_name": "Maxx Richard"
            },
            {
                "last_name": "Maa\u00df",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recommendation systems are ubiquitous, from Spotify playlist suggestions to Amazon product suggestions. Nevertheless, depending on the methodology or the dataset, these systems typically fail to capture user preferences and generate general recommendations. Recent advancements in Large Language Models (LLM) offer promising results for analyzing user queries. However, employing these models to capture user preferences and efficiency remains an open question. In this paper, we propose LLMRS, an LLM-based zero-shot recommender system where we employ pre-trained LLM to encode user reviews into a review score and generate user-tailored recommendations. We experimented with LLMRS on a real-world dataset, the Amazon product reviews, for software purchase use cases. The results show that LLMRS outperforms the ranking-based baseline model while successfully capturing meaningful information from product reviews, thereby providing more reliable recommendations. ",
        "title": "LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for  Software Purchase",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06683",
        "abstract_url": "http://arxiv.org/abs/2401.06683",
        "authors": [
            {
                "last_name": "Cambrin",
                "first_name": "Daniele Rege"
            },
            {
                "last_name": "Cagliero",
                "first_name": "Luca"
            },
            {
                "last_name": "Garza",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL",
            "LG"
        ],
        "abstract": "  Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark. ",
        "title": "DQNC2S: DQN-based Cross-stream Crisis event Summarizer",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06684",
        "abstract_url": "http://arxiv.org/abs/2401.06684",
        "authors": [
            {
                "last_name": "Frommer",
                "first_name": "Andreas"
            },
            {
                "last_name": "Ramirez-Hidalgo",
                "first_name": "Gustavo"
            },
            {
                "last_name": "Schweitzer",
                "first_name": "Marcel"
            },
            {
                "last_name": "Tsolakis",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  While preconditioning is a long-standing concept to accelerate iterative methods for linear systems, generalizations to matrix functions are still in their infancy. We go a further step in this direction, introducing polynomial preconditioning for Krylov subspace methods which approximate the action of the matrix square root and inverse square root on a vector. Preconditioning reduces the subspace size and therefore avoids the storage problem together with -- for non-Hermitian matrices -- the increased computational cost per iteration that arises in the unpreconditioned case. Polynomial preconditioning is an attractive alternative to current restarting or sketching approaches since it is simpler and computationally more efficient. We demonstrate this for several numerical examples. ",
        "title": "Polynomial Preconditioning for the Action of the Matrix Square Root and  Inverse Square Root",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06686",
        "abstract_url": "http://arxiv.org/abs/2401.06686",
        "authors": [
            {
                "last_name": "Pilli",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Heuristics and cognitive biases are an integral part of human decision-making. Automatically detecting a particular cognitive bias could enable intelligent tools to provide better decision-support. Detecting the presence of a cognitive bias currently requires a hand-crafted experiment and human interpretation. Our research aims to explore conversational agents as an effective tool to measure various cognitive biases in different domains. Our proposed conversational agent incorporates a bias measurement mechanism that is informed by the existing experimental designs and various experimental tasks identified in the literature. Our initial experiments to measure framing and loss-aversion biases indicate that the conversational agents can be effectively used to measure the biases. ",
        "title": "Exploring Conversational Agents as an Effective Tool for Measuring  Cognitive Biases in Decision-Making",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06687",
        "abstract_url": "http://arxiv.org/abs/2401.06687",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jacob M."
            },
            {
                "last_name": "Bhattacharya",
                "first_name": "Rohit"
            },
            {
                "last_name": "Keith",
                "first_name": "Katherine A."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-shot classifiers is novel (to our knowledge) and expands the set of text-specific causal methods available to practitioners. ",
        "title": "Proximal Causal Inference With Text Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06688",
        "abstract_url": "http://arxiv.org/abs/2401.06688",
        "authors": [
            {
                "last_name": "Vernikos",
                "first_name": "Giorgos"
            },
            {
                "last_name": "Popescu-Belis",
                "first_name": "Andrei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5-200). Furthermore, we empirically establish that QE-fusion scales linearly with the number of candidates in the pool. QE-fusion proves effective in enhancing LLM-based translation without the need for costly retraining of LLMs. ",
        "title": "Don't Rank, Combine! Combining Machine Translation Hypotheses Using  Quality Estimation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06690",
        "abstract_url": "http://arxiv.org/abs/2401.06690",
        "authors": [
            {
                "last_name": "Y\u00fccel",
                "first_name": "M. Erkin"
            },
            {
                "last_name": "Topalo\u011flu",
                "first_name": "Serkan"
            },
            {
                "last_name": "\u00dcnsalan",
                "first_name": "Cem"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The retail sector presents several open and challenging problems that could benefit from advanced pattern recognition and computer vision techniques. One such critical challenge is planogram compliance control. In this study, we propose a complete embedded system to tackle this issue. Our system consists of four key components as image acquisition and transfer via stand-alone embedded camera module, object detection via computer vision and deep learning methods working on single board computers, planogram compliance control method again working on single board computers, and energy harvesting and power management block to accompany the embedded camera modules. The image acquisition and transfer block is implemented on the ESP-EYE camera module. The object detection block is based on YOLOv5 as the deep learning method and local feature extraction. We implement these methods on Raspberry Pi 4, NVIDIA Jetson Orin Nano, and NVIDIA Jetson AGX Orin as single board computers. The planogram compliance control block utilizes sequence alignment through a modified Needleman-Wunsch algorithm. This block is also working along with the object detection block on the same single board computers. The energy harvesting and power management block consists of solar and RF energy harvesting modules with suitable battery pack for operation. We tested the proposed embedded planogram compliance control system on two different datasets to provide valuable insights on its strengths and weaknesses. The results show that our method achieves F1 scores of 0.997 and 1.0 in object detection and planogram compliance control blocks, respectively. Furthermore, we calculated that the complete embedded system can work in stand-alone form up to two years based on battery. This duration can be further extended with the integration of the proposed solar and RF energy harvesting options. ",
        "title": "Embedded Planogram Compliance Control System",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06692",
        "abstract_url": "http://arxiv.org/abs/2401.06692",
        "authors": [
            {
                "last_name": "Bhatt",
                "first_name": "Gantavya"
            },
            {
                "last_name": "Chen",
                "first_name": "Yifang"
            },
            {
                "last_name": "Das",
                "first_name": "Arnav M."
            },
            {
                "last_name": "Zhang",
                "first_name": "Jifan"
            },
            {
                "last_name": "Truong",
                "first_name": "Sang T."
            },
            {
                "last_name": "Mussmann",
                "first_name": "Stephen"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yinglun"
            },
            {
                "last_name": "Bilmes",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Du",
                "first_name": "Simon S."
            },
            {
                "last_name": "Jamieson",
                "first_name": "Kevin"
            },
            {
                "last_name": "Ash",
                "first_name": "Jordan T."
            },
            {
                "last_name": "Nowak",
                "first_name": "Robert D."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimental design techniques and find that these methods consistently yield significant gains in label efficiency with little computational overhead. On generative tasks, our methods achieve the same generalization performance with only $50\\%$ of annotation cost required by random sampling. ",
        "title": "An Experimental Design Framework for Label-Efficient Supervised  Finetuning of Large Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06697",
        "abstract_url": "http://arxiv.org/abs/2401.06697",
        "authors": [
            {
                "last_name": "Akpinar",
                "first_name": "Emine"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Alzheimer's disease (AD) is the most prevalent neurodegenerative brain disorder, which results in significant cognitive impairments, especially in the elderly population. Cognitive impairments can manifest as a decline in various mental faculties, such as concentration, memory, and other higher-order cognitive abilities. These deficits can significantly impact an individual's capacity to comprehend information, acquire new knowledge, and communicate effectively. One of the affected activities due to cognitive impairments is handwriting. By analyzing different aspects of handwriting, including pressure, velocity, and spatial organization, researchers can detect subtle alterations that might indicate early-stage cognitive impairments, especially AD. Recently, several classical artificial intelligence (AI) approaches have been proposed for detecting AD in elderly individuals through handwriting analysis. However, advanced AI methods require more computational power as the size of the data increases. Additionally, diagnoses can be influenced by factors such as limited relevant classical vector space and correlations between features. Recent studies have shown that using quantum computing technologies in healthcare can not only address these problems but also accelerate complex data analysis and process large datasets more efficiently. In this study, we introduced a variational quantum classifier with fewer circuit elements to facilitate the early diagnosis of AD in elderly individuals based on handwriting data. We employed ZZFeatureMap for encoding features. To classify AD, a parameterized quantum circuit consisting of repeated Ry and Rz rotation gates, as well as CY and CZ two-qubit entangling gates, was designed and implemented. The proposed model achieved an accuracy of 0.75 in classifying AD. ",
        "title": "Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease  Study",
        "date": "2023-09-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06699",
        "abstract_url": "http://arxiv.org/abs/2401.06699",
        "authors": [
            {
                "last_name": "Tomic",
                "first_name": "Slavisa"
            },
            {
                "last_name": "Matos-Carvalho",
                "first_name": "Jo\u00e3o Pedro"
            },
            {
                "last_name": "Beko",
                "first_name": "Marko"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carried out in parallel to optimize all weights in a given layer simultaneously. Furthermore, its running time is deterministic in the sense that one can obtain the exact number of computations necessary to optimize the weights in all network layers (per iteration, in the case of non-injective mapping). Our simulation and empirical results show that the proposed scheme, BPLS, works well and is competitive with existing ones in terms of accuracy, but significantly surpasses them in terms of running time. To summarize, the new method is straightforward to implement, is competitive and computationally more efficient than the existing ones, and is well-tailored for parallel implementation. ",
        "title": "A Closed-form Solution for Weight Optimization in Fully-connected  Feed-forward Neural Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06703",
        "abstract_url": "http://arxiv.org/abs/2401.06703",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Puxuan"
            },
            {
                "last_name": "Mallia",
                "first_name": "Antonio"
            },
            {
                "last_name": "Petri",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems. ",
        "title": "Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06704",
        "abstract_url": "http://arxiv.org/abs/2401.06704",
        "authors": [
            {
                "last_name": "Robert",
                "first_name": "Damien"
            },
            {
                "last_name": "Raguet",
                "first_name": "Hugo"
            },
            {
                "last_name": "Landrieu",
                "first_name": "Loic"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a highly efficient method for panoptic segmentation of large 3D point clouds by redefining this task as a scalable graph clustering problem. This approach can be trained using only local auxiliary tasks, thereby eliminating the resource-intensive instance-matching step during training. Moreover, our formulation can easily be adapted to the superpoint paradigm, further increasing its efficiency. This allows our model to process scenes with millions of points and thousands of objects in a single inference. Our method, called SuperCluster, achieves a new state-of-the-art panoptic segmentation performance for two indoor scanning datasets: $50.1$ PQ ($+7.8$) for S3DIS Area~5, and $58.7$ PQ ($+25.2$) for ScanNetV2. We also set the first state-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and DALES. With only $209$k parameters, our model is over $30$ times smaller than the best-competing method and trains up to $15$ times faster. Our code and pretrained models are available at https://github.com/drprojects/superpoint_transformer. ",
        "title": "Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06706",
        "abstract_url": "http://arxiv.org/abs/2401.06706",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Sen"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            },
            {
                "last_name": "Dai",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding. ",
        "title": "Multi-Candidate Speculative Decoding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06709",
        "abstract_url": "http://arxiv.org/abs/2401.06709",
        "authors": [
            {
                "last_name": "Garg",
                "first_name": "Muskan"
            },
            {
                "last_name": "Sathvik",
                "first_name": "MSVPJ"
            },
            {
                "last_name": "Chadha",
                "first_name": "Amrit"
            },
            {
                "last_name": "Raza",
                "first_name": "Shaina"
            },
            {
                "last_name": "Sohn",
                "first_name": "Sunghwan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The social NLP research community witness a recent surge in the computational advancements of mental health analysis to build responsible AI models for a complex interplay between language use and self-perception. Such responsible AI models aid in quantifying the psychological concepts from user-penned texts on social media. On thinking beyond the low-level (classification) task, we advance the existing binary classification dataset, towards a higher-level task of reliability analysis through the lens of explanations, posing it as one of the safety measures. We annotate the LoST dataset to capture nuanced textual cues that suggest the presence of low self-esteem in the posts of Reddit users. We further state that the NLP models developed for determining the presence of low self-esteem, focus more on three types of textual cues: (i) Trigger: words that triggers mental disturbance, (ii) LoST indicators: text indicators emphasizing low self-esteem, and (iii) Consequences: words describing the consequences of mental disturbance. We implement existing classifiers to examine the attention mechanism in pre-trained language models (PLMs) for a domain-specific psychology-grounded task. Our findings suggest the need of shifting the focus of PLMs from Trigger and Consequences to a more comprehensive explanation, emphasizing LoST indicators while determining low self-esteem in Reddit posts. ",
        "title": "Reliability Analysis of Psychological Concept Extraction and  Classification in User-penned Text",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06710",
        "abstract_url": "http://arxiv.org/abs/2401.06710",
        "authors": [
            {
                "last_name": "Iyengar",
                "first_name": "Garud"
            },
            {
                "last_name": "Singal",
                "first_name": "Raghav"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  The flexibility of choosing the ad action as a function of the consumer state is critical for modern-day marketing campaigns. We study the problem of identifying the optimal sequential personalized interventions that maximize the adoption probability for a new product. We model consumer behavior by a conversion funnel that captures the state of each consumer (e.g., interaction history with the firm) and allows the consumer behavior to vary as a function of both her state and firm's sequential interventions. We show our model captures consumer behavior with very high accuracy (out-of-sample AUC of over 0.95) in a real-world email marketing dataset. However, it results in a very large-scale learning problem, where the firm must learn the state-specific effects of various interventions from consumer interactions. We propose a novel attribution-based decision-making algorithm for this problem that we call model-free approximate Bayesian learning. Our algorithm inherits the interpretability and scalability of Thompson sampling for bandits and maintains an approximate belief over the value of each state-specific intervention. The belief is updated as the algorithm interacts with the consumers. Despite being an approximation to the Bayes update, we prove the asymptotic optimality of our algorithm and analyze its convergence rate. We show that our algorithm significantly outperforms traditional approaches on extensive simulations calibrated to a real-world email marketing dataset. ",
        "title": "Model-Free Approximate Bayesian Learning for Large-Scale Conversion  Funnel Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06712",
        "abstract_url": "http://arxiv.org/abs/2401.06712",
        "authors": [
            {
                "last_name": "Soto",
                "first_name": "Rafael Rivera"
            },
            {
                "last_name": "Koch",
                "first_name": "Kailin"
            },
            {
                "last_name": "Khan",
                "first_name": "Aleem"
            },
            {
                "last_name": "Chen",
                "first_name": "Barry"
            },
            {
                "last_name": "Bishop",
                "first_name": "Marcus"
            },
            {
                "last_name": "Andrews",
                "first_name": "Nicholas"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally different approach not relying on samples from language models of concern at training time. Instead, we propose to leverage representations of writing style estimated from human-authored text. Indeed, we find that features effective at distinguishing among human authors are also effective at distinguishing human from machine authors, including state of the art large language models like Llama 2, ChatGPT, and GPT-4. Furthermore, given a handful of examples composed by each of several specific language models of interest, our approach affords the ability to predict which model generated a given document. ",
        "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06713",
        "abstract_url": "http://arxiv.org/abs/2401.06713",
        "authors": [
            {
                "last_name": "Ferdous",
                "first_name": "S M"
            },
            {
                "last_name": "Neff",
                "first_name": "Reece"
            },
            {
                "last_name": "Peng",
                "first_name": "Bo"
            },
            {
                "last_name": "Shuvo",
                "first_name": "Salman"
            },
            {
                "last_name": "Minutoli",
                "first_name": "Marco"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Sayak"
            },
            {
                "last_name": "Kowalski",
                "first_name": "Karol"
            },
            {
                "last_name": "Becchi",
                "first_name": "Michela"
            },
            {
                "last_name": "Halappanavar",
                "first_name": "Mahantesh"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  A \\emph{coloring} of a graph is an assignment of colors to vertices such that no two neighboring vertices have the same color. The need for memory-efficient coloring algorithms is motivated by their application in computing clique partitions of graphs arising in quantum computations where the objective is to map a large set of Pauli strings into a compact set of unitaries. We present \\texttt{Picasso}, a randomized memory-efficient iterative parallel graph coloring algorithm with theoretical sublinear space guarantees under practical assumptions. The parameters of our algorithm provide a trade-off between coloring quality and resource consumption. To assist the user, we also propose a machine learning model to predict the coloring algorithm's parameters considering these trade-offs. We provide a sequential and a parallel implementation of the proposed algorithm.   We perform an experimental evaluation on a 64-core AMD CPU equipped with 512 GB of memory and an Nvidia A100 GPU with 40GB of memory. For a small dataset where existing coloring algorithms can be executed within the 512 GB memory budget, we show up to {\\bf 68$\\times$} memory savings. On massive datasets we demonstrate that GPU-accelerated \\pic{} can process inputs with {\\bf 49.5$\\times$} more Pauli strings (vertex set in our graph) and {\\bf 2,478$\\times$} more edges than state-of-the-art parallel approaches. ",
        "title": "\\texttt{Picasso}: Memory-Efficient Graph Coloring Using Palettes With  Applications in Quantum Computing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06714",
        "abstract_url": "http://arxiv.org/abs/2401.06714",
        "authors": [
            {
                "last_name": "Jaiswal",
                "first_name": "Ragesh"
            },
            {
                "last_name": "Kumar",
                "first_name": "Amit"
            },
            {
                "last_name": "Yadav",
                "first_name": "Jatin"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We consider the capacitated clustering problem in general metric spaces where the goal is to identify $k$ clusters and minimize the sum of the radii of the clusters (we call this the Capacitated-$k$-sumRadii problem). We are interested in fixed-parameter tractable (FPT) approximation algorithms where the running time is of the form $f(k) \\cdot \\text{poly}(n)$, where $f(k)$ can be an exponential function of $k$ and $n$ is the number of points in the input. In the uniform capacity case, Bandyapadhyay et al. recently gave a $4$-approximation algorithm for this problem. Our first result improves this to an FPT $3$-approximation and extends to a constant factor approximation for any $L_p$ norm of the cluster radii. In the general capacities version, Bandyapadhyay et al. gave an FPT $15$-approximation algorithm. We extend their framework to give an FPT $(4 + \\sqrt{13})$-approximation algorithm for this problem. Our framework relies on a novel idea of identifying approximations to optimal clusters by carefully pruning points from an initial candidate set of points. This is in contrast to prior results that rely on guessing suitable points and building balls of appropriate radii around them.   On the hardness front, we show that assuming the Exponential Time Hypothesis, there is a constant $c > 1$ such that any $c$-approximation algorithm for the non-uniform capacity version of this problem requires running time $2^{\\Omega \\left(\\frac{k}{polylog(k)} \\right)}$. ",
        "title": "FPT Approximation for Capacitated Sum of Radii",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06715",
        "abstract_url": "http://arxiv.org/abs/2401.06715",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Xinrui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ming"
            },
            {
                "last_name": "Weir",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Van Durme",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Holzenberger",
                "first_name": "Nils"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Statutory reasoning refers to the application of legislative provisions to a series of case facts described in natural language. We re-frame statutory reasoning as an analogy task, where each instance of the analogy task involves a combination of two instances of statutory reasoning. This increases the dataset size by two orders of magnitude, and introduces an element of interpretability. We show that this task is roughly as difficult to Natural Language Processing models as the original task. Finally, we come back to statutory reasoning, solving it with a combination of a retrieval mechanism and analogy models, and showing some progress on prior comparable work. ",
        "title": "Reframing Tax Law Entailment as Analogical Reasoning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06717",
        "abstract_url": "http://arxiv.org/abs/2401.06717",
        "authors": [
            {
                "last_name": "Costa",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Duarte",
                "first_name": "Pedro"
            },
            {
                "last_name": "Coelho",
                "first_name": "Andr\u00e9"
            },
            {
                "last_name": "Campos",
                "first_name": "Rui"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The 6G paradigm and the massive usage of interconnected wireless devices introduced the need for flexible wireless networks. A promising approach lies in employing Mobile Robotic Platforms (MRPs) to create communications cells on-demand. The challenge consists in positioning the MRPs to improve the wireless connectivity offered. This is exacerbated in millimeter wave (mmWave), Terahertz (THz), and visible light-based networks, which imply the establishment of short-range, Line of Sight (LoS) wireless links to take advantage of the ultra-high bandwidth channels available.   This paper proposes a solution to enable the obstacle-aware, autonomous positioning of MRPs and provide LoS wireless connectivity to communications devices. It consists of 1) a Vision Module that uses video data gathered by the MRP to determine the location of obstacles, wireless devices and users, and 2) a Control Module, which autonomously positions the MRP based on the information provided by the Vision Module. The proposed solution was validated in simulation and through experimental testing, showing that it is able to position an MRP while ensuring LoS wireless links between a mobile communications cell and wireless devices or users. ",
        "title": "Obstacle-Aware Positioning of a Mobile Robotic Platform for 6G Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06721",
        "abstract_url": "http://arxiv.org/abs/2401.06721",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Bowen"
            },
            {
                "last_name": "Iannelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The goal of this article is to study fundamental mechanisms behind so-called indirect and direct data-driven control for unknown systems. Specifically, we consider policy iteration applied to the linear quadratic regulator problem. Two iterative procedures, where data collected from the system are repeatedly used to compute new estimates of the desired optimal controller, are considered. In indirect policy iteration, data are used to obtain an updated model estimate through a recursive identification scheme, which is used in a certainty-equivalent fashion to perform the classic policy iteration update. By casting the concurrent model identification and control design as a feedback interconnection between two algorithmic systems, we provide a closed-loop analysis that shows convergence and robustness properties for arbitrary levels of excitation in the data. In direct policy iteration, data are used to approximate the value function and design the associated controller without requiring the intermediate identification step. After proposing an extension to a recently proposed scheme that overcomes potential identifiability issues, we establish under which conditions this procedure is guaranteed to deliver the optimal controller. Based on these analyses we are able to compare the strengths and limitations of the two approaches, highlighting aspects such as the required samples, convergence properties, and excitation requirement. Simulations are also provided to illustrate the results. ",
        "title": "The Role of Identification in Data-driven Policy Iteration: A System  Theoretic Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06723",
        "abstract_url": "http://arxiv.org/abs/2401.06723",
        "authors": [
            {
                "last_name": "Ferreira",
                "first_name": "Diogo"
            },
            {
                "last_name": "Coelho",
                "first_name": "Andr\u00e9"
            },
            {
                "last_name": "Campos",
                "first_name": "Rui"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The emerging 6G paradigm and the proliferation of wireless devices require flexible network infrastructures capable of meeting the increasing Quality of Service (QoS) requirements. Mobile Robotic Platforms (MRPs) acting as mobile communications cells are a promising solution to provide on-demand wireless connectivity in dynamic networking scenarios. However, the energy consumption of MRPs is a challenge that must be considered, in order to maximize the availability of the wireless networks created.   The main contribution of this paper is the experimental evaluation of the energy consumption of an MRP acting as a mobile communications cell. The evaluation considers different actions performed by a real MRP, showing that the energy consumption varies significantly with the type of action performed. The obtained results pave the way for optimizing the MRP movement in dynamic networking scenarios so that the wireless network's availability is maximized while minimizing the MRP's energy consumption. ",
        "title": "Evaluation of the Energy Consumption of a Mobile Robotic Platform for  Sustainable 6G Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06725",
        "abstract_url": "http://arxiv.org/abs/2401.06725",
        "authors": [
            {
                "last_name": "Kallaugher",
                "first_name": "John"
            },
            {
                "last_name": "Parekh",
                "first_name": "Ojas"
            },
            {
                "last_name": "Thompson",
                "first_name": "Kevin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yipu"
            },
            {
                "last_name": "Yirka",
                "first_name": "Justin"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  Product states, unentangled tensor products of single qubits, are a ubiquitous ansatz in quantum computation, including for state-of-the-art Hamiltonian approximation algorithms. A natural question is whether we should expect to efficiently solve product state problems on any interesting families of Hamiltonians.   We completely classify the complexity of finding minimum-energy product states for Hamiltonians defined by any fixed set of allowed 2-qubit interactions. Our results follow a line of work classifying the complexity of solving Hamiltonian problems and classical constraint satisfaction problems based on the allowed constraints. We prove that estimating the minimum energy of a product state is in P if and only if all allowed interactions are 1-local, and NP-complete otherwise. Equivalently, any family of non-trivial two-body interactions generates Hamiltonians with NP-complete product-state problems. Our hardness constructions only require coupling strengths of constant magnitude.   A crucial component of our proofs is a collection of hardness results for a new variant of the Vector Max-Cut problem, which should be of independent interest. Our definition involves sums of distances rather than squared distances and allows linear stretches.   A corollary of our classification is a new proof that optimizing product states in the Quantum Max-Cut model (the quantum Heisenberg model) is NP-complete. ",
        "title": "Complexity Classification of Product State Problems for Local  Hamiltonians",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06727",
        "abstract_url": "http://arxiv.org/abs/2401.06727",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Bozhen"
            },
            {
                "last_name": "Zang",
                "first_name": "Zelin"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Wu",
                "first_name": "Lirong"
            },
            {
                "last_name": "Tan",
                "first_name": "Cheng"
            },
            {
                "last_name": "Li",
                "first_name": "Stan Z."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Representing graph data in a low-dimensional space for subsequent tasks is the purpose of attributed graph embedding. Most existing neural network approaches learn latent representations by minimizing reconstruction errors. Rare work considers the data distribution and the topological structure of latent codes simultaneously, which often results in inferior embeddings in real-world graph data. This paper proposes a novel Deep Manifold (Variational) Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improve the stability and quality of learned representations to tackle the crowding problem. The node-to-node geodesic similarity is preserved between the original and latent space under a pre-defined distribution. The proposed method surpasses state-of-the-art baseline algorithms by a significant margin on different downstream tasks across popular datasets, which validates our solutions. We promise to release the code after acceptance. ",
        "title": "Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06730",
        "abstract_url": "http://arxiv.org/abs/2401.06730",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Kaitlyn"
            },
            {
                "last_name": "Hwang",
                "first_name": "Jena D."
            },
            {
                "last_name": "Ren",
                "first_name": "Xiang"
            },
            {
                "last_name": "Sap",
                "first_name": "Maarten"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward. ",
        "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to  Express Uncertainty",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06738",
        "abstract_url": "http://arxiv.org/abs/2401.06738",
        "authors": [
            {
                "last_name": "Dang",
                "first_name": "Anh"
            },
            {
                "last_name": "Babanezhad",
                "first_name": "Reza"
            },
            {
                "last_name": "Vaswani",
                "first_name": "Sharan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\\left(\\exp(-\\frac{T}{\\sqrt{\\kappa}}) + \\sigma \\right)$ convergence rate, where $T$ is the number of iterations and $\\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\\left(\\exp\\left(-\\frac{T}{\\sqrt{\\kappa}} \\right) + \\frac{\\sigma}{T}\\right)$ rate. For general strongly-convex functions, we use the averaging interpretation of SHB along with exponential step-sizes to prove an $O\\left(\\exp\\left(-\\frac{T}{\\kappa} \\right) + \\frac{\\sigma^2}{T} \\right)$ convergence to the minimizer in a noise-adaptive manner. Finally, we empirically demonstrate the effectiveness of the proposed algorithms. ",
        "title": "Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06740",
        "abstract_url": "http://arxiv.org/abs/2401.06740",
        "authors": [
            {
                "last_name": "Georgoulis",
                "first_name": "Emmanuil H."
            },
            {
                "last_name": "Papapantoleon",
                "first_name": "Antonis"
            },
            {
                "last_name": "Smaragdakis",
                "first_name": "Costas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model. ",
        "title": "A deep implicit-explicit minimizing movement method for option pricing  in jump-diffusion models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06742",
        "abstract_url": "http://arxiv.org/abs/2401.06742",
        "authors": [
            {
                "last_name": "DeLucia",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Zhao",
                "first_name": "Mengjie"
            },
            {
                "last_name": "Maeda",
                "first_name": "Yoshinori"
            },
            {
                "last_name": "Yoda",
                "first_name": "Makoto"
            },
            {
                "last_name": "Yamada",
                "first_name": "Keiichi"
            },
            {
                "last_name": "Wakaki",
                "first_name": "Hiromi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the \"real\" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new setting. We draw inspiration from the literature of dialog natural language inference (NLI), and devise NLI-reranking methods to extract structured persona information from dialogue. Compared to existing persona extraction models, our method returns higher-quality extracted persona and requires less human annotation. ",
        "title": "Using Natural Language Inference to Improve Persona Extraction from  Dialogue in a New Domain",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06744",
        "abstract_url": "http://arxiv.org/abs/2401.06744",
        "authors": [
            {
                "last_name": "K\u00e4mper",
                "first_name": "Niklas"
            },
            {
                "last_name": "Chizhov",
                "first_name": "Vassillen"
            },
            {
                "last_name": "Weickert",
                "first_name": "Joachim"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years inpainting-based compression methods have been shown to be a viable alternative to classical codecs such as JPEG and JPEG2000. Unlike transform-based codecs, which store coefficients in the transform domain, inpainting-based approaches store a small subset of the original image pixels and reconstruct the image from those by using a suitable inpainting operator. A good candidate for such an inpainting operator is homogeneous diffusion inpainting, as it is simple, theoretically well-motivated, and can achieve good reconstruction quality for optimized data. However, a major challenge has been to design fast solvers for homogeneous diffusion inpainting that scale to 4K image resolution ($3840 \\times 2160$ pixels) and are real-time capable. We overcome this with a careful adaptation and fusion of two of the most efficient concept from numerical analysis: multigrid and domain decomposition. Our domain decomposition algorithm efficiently utilizes GPU parallelism by solving inpainting problems on small overlapping blocks. Unlike simple block decomposition strategies such as the ones in JPEG, our approach yields block artifact-free reconstructions. Furthermore, embedding domain decomposition in a full multigrid scheme provides global interactions and allows us to achieve optimal convergence by reducing both low- and high-frequency errors at the same rate. We are able to achieve 4K color image reconstruction at more than $60$ frames per second even from very sparse data - something which has been previously unfeasible. ",
        "title": "Efficient Parallel Algorithms for Inpainting-Based Representations of 4K  Images -- Part I: Homogeneous Diffusion Inpainting",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06747",
        "abstract_url": "http://arxiv.org/abs/2401.06747",
        "authors": [
            {
                "last_name": "K\u00e4mper",
                "first_name": "Niklas"
            },
            {
                "last_name": "Chizhov",
                "first_name": "Vassillen"
            },
            {
                "last_name": "Weickert",
                "first_name": "Joachim"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Homogeneous diffusion inpainting can reconstruct missing image areas with high quality from a sparse subset of known pixels, provided that their location as well as their gray or color values are well optimized. This property is exploited in inpainting-based image compression, which is a promising alternative to classical transform-based codecs such as JPEG and JPEG2000. However, optimizing the inpainting data is a challenging task. Current approaches are either quite slow or do not produce high quality results. As a remedy we propose fast spatial and tonal optimization algorithms for homogeneous diffusion inpainting that efficiently utilize GPU parallelism, with a careful adaptation of some of the most successful numerical concepts. We propose a densification strategy using ideas from error-map dithering combined with a Delaunay triangulation for the spatial optimization. For the tonal optimization we design a domain decomposition solver that solves the corresponding normal equations in a matrix-free fashion and supplement it with a Voronoi-based initialization strategy. With our proposed methods we are able to generate high quality inpainting masks for homogeneous diffusion and optimized tonal values in a runtime that outperforms prior state-of-the-art by a wide margin. ",
        "title": "Efficient Parallel Algorithms for Inpainting-Based Representations of 4K  Images -- Part II: Spatial and Tonal Data Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06748",
        "abstract_url": "http://arxiv.org/abs/2401.06748",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qingsong"
            },
            {
                "last_name": "Ma",
                "first_name": "Guanquan"
            },
            {
                "last_name": "Sridharamurthy",
                "first_name": "Raghavendra"
            },
            {
                "last_name": "Wang",
                "first_name": "Bei"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  A Reeb graph is a graphical representation of a scalar function $f: X \\to \\mathbb{R}$ on a topological space $X$ that encodes the topology of the level sets. A Reeb space is a generalization of the Reeb graph to a multivariate function $f: X \\to \\mathbb{R}^d$. In this paper, we propose novel constructions of Reeb graphs and Reeb spaces that incorporate the use of a measure. Specifically, we introduce measure theoretic Reeb graphs and Reeb spaces when the domain or the range is modeled as a metric measure space (i.e.,~a metric space equipped with a measure). Our main goal is to enhance the robustness of the Reeb graph and Reeb space in representing the topological features of a scalar field while accounting for the distribution of the measure. We first prove the stability of our measure theoretic constructions with respect to the interleaving distance. We then prove their stability with respect to the measure, defined using the distance to a measure or the kernel distance to a measure, respectively. ",
        "title": "Measure Theoretic Reeb Graphs and Reeb Spaces",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06749",
        "abstract_url": "http://arxiv.org/abs/2401.06749",
        "authors": [
            {
                "last_name": "Garcia-Archilla",
                "first_name": "Bosco"
            },
            {
                "last_name": "Li",
                "first_name": "Xuejian"
            },
            {
                "last_name": "Novo",
                "first_name": "Julia"
            },
            {
                "last_name": "Rebholz",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider nonlinear solvers for the incompressible, steady (or at a fixed time step for unsteady) Navier-Stokes equations in the setting where partial measurement data of the solution is available. The measurement data is incorporated/assimilated into the solution through a nudging term addition to the the Picard iteration that penalized the difference between the coarse mesh interpolants of the true solution and solver solution, analogous to how continuous data assimilation (CDA) is implemented for time dependent PDEs. This was considered in the paper [Li et al. {\\it CMAME} 2023], and we extend the methodology by improving the analysis to be in the $L^2$ norm instead of a weighted $H^1$ norm where the weight depended on the coarse mesh width, and to the case of noisy measurement data. For noisy measurement data, we prove that the CDA-Picard method is stable and convergent, up to the size of the noise. Numerical tests illustrate the results, and show that a very good strategy when using noisy data is to use CDA-Picard to generate an initial guess for the classical Newton iteration. ",
        "title": "Enhancing nonlinear solvers for the Navier-Stokes equations with  continuous (noisy) data assimilation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06751",
        "abstract_url": "http://arxiv.org/abs/2401.06751",
        "authors": [
            {
                "last_name": "Hase",
                "first_name": "Peter"
            },
            {
                "last_name": "Bansal",
                "first_name": "Mohit"
            },
            {
                "last_name": "Clark",
                "first_name": "Peter"
            },
            {
                "last_name": "Wiegreffe",
                "first_name": "Sarah"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as \"oracle\" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied, suggesting the scalable oversight problem may be easier than previously thought. Our code is available at https://github.com/allenai/easy-to-hard-generalization ",
        "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06752",
        "abstract_url": "http://arxiv.org/abs/2401.06752",
        "authors": [
            {
                "last_name": "Zamir",
                "first_name": "Muhammad Tayyab"
            },
            {
                "last_name": "Ayub",
                "first_name": "Muhammad Asif"
            },
            {
                "last_name": "Gul",
                "first_name": "Asma"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Nasir"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Kashif"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent years, the increasing use of Artificial Intelligence based text generation tools has posed new challenges in document provenance, authentication, and authorship detection. However, advancements in stylometry have provided opportunities for automatic authorship and author change detection in multi-authored documents using style analysis techniques. Style analysis can serve as a primary step toward document provenance and authentication through authorship detection. This paper investigates three key tasks of style analysis: (i) classification of single and multi-authored documents, (ii) single change detection, which involves identifying the point where the author switches, and (iii) multiple author-switching detection in multi-authored documents. We formulate all three tasks as classification problems and propose a merit-based fusion framework that integrates several state-of-the-art natural language processing (NLP) algorithms and weight optimization techniques. We also explore the potential of special characters, which are typically removed during pre-processing in NLP applications, on the performance of the proposed methods for these tasks by conducting extensive experiments on both cleaned and raw datasets. Experimental results demonstrate significant improvements over existing solutions for all three tasks on a benchmark dataset. ",
        "title": "Stylometry Analysis of Multi-authored Documents for Authorship and  Author Style Change Detection",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06755",
        "abstract_url": "http://arxiv.org/abs/2401.06755",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Boyang"
            },
            {
                "last_name": "Heaney",
                "first_name": "Claire E."
            },
            {
                "last_name": "Gomes",
                "first_name": "Jefferson L. M. A."
            },
            {
                "last_name": "Matar",
                "first_name": "Omar K."
            },
            {
                "last_name": "Pain",
                "first_name": "Christopher C."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper solves the multiphase flow equations with interface capturing using the AI4PDEs approach (Artificial Intelligence for Partial Differential Equations). The solver within AI4PDEs uses tools from machine learning (ML) libraries to solve (exactly) partial differential equations (PDEs) that have been discretised using numerical methods. Convolutional layers can be used to express the discretisations as a neural network, whose weights are determined by the numerical method, rather than by training. To solve the system, a multigrid solver is implemented through a neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier-Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov-Galerkin for accuracy and designed with AI4PDEs in mind. High-order finite-element based schemes are chosen to model a collapsing water column and a rising bubble. Results compare well with experimental data and other numerical results from the literature, demonstrating that, for the first time, finite element discretisations of multiphase flows can be solved using the neural network solver from the AI4PDEs approach. A benefit of expressing numerical discretisations as neural networks is that the code can run, without modification, on CPUs, GPUs or the latest accelerators designed especially to run AI codes. ",
        "title": "Solving the Discretised Multiphase Flow Equations with Interface  Capturing on Structured Grids Using Machine Learning Libraries",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06757",
        "abstract_url": "http://arxiv.org/abs/2401.06757",
        "authors": [
            {
                "last_name": "Riaz",
                "first_name": "Muhammad Naveed"
            },
            {
                "last_name": "Wielgosz",
                "first_name": "Maciej"
            },
            {
                "last_name": "Romera",
                "first_name": "Abel Garcia"
            },
            {
                "last_name": "Lopez",
                "first_name": "Antonio M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to predict crossing intentions. ",
        "title": "Synthetic Data Generation Framework, Dataset, and Efficient Deep Model  for Pedestrian Intention Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06760",
        "abstract_url": "http://arxiv.org/abs/2401.06760",
        "authors": [
            {
                "last_name": "Kocmi",
                "first_name": "Tom"
            },
            {
                "last_name": "Zouhar",
                "first_name": "Vil\u00e9m"
            },
            {
                "last_name": "Federmann",
                "first_name": "Christian"
            },
            {
                "last_name": "Post",
                "first_name": "Matt"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the \"dynamic range\" of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask what point difference X in metric Y is required between two systems for humans to notice? We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size. Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness. ",
        "title": "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06761",
        "abstract_url": "http://arxiv.org/abs/2401.06761",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mingdao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Aohan"
            },
            {
                "last_name": "Wang",
                "first_name": "Bowen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peng"
            },
            {
                "last_name": "Tang",
                "first_name": "Jie"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuxiao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks. ",
        "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06762",
        "abstract_url": "http://arxiv.org/abs/2401.06762",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Caleb"
            },
            {
                "last_name": "Corley",
                "first_name": "Isaac"
            },
            {
                "last_name": "Ortiz",
                "first_name": "Anthony"
            },
            {
                "last_name": "Dodhia",
                "first_name": "Rahul"
            },
            {
                "last_name": "Ferres",
                "first_name": "Juan M. Lavista"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to segment roads from background in aerial imagery achieves an 84% recall on unoccluded roads, but just 63.5% recall on roads covered by tree canopy despite being trained to model both the same way. We further analyze how the performance of models changes as the relevant context for a decision (unoccluded roads in our case) varies in distance. We release the code to reproduce our experiments and dataset of imagery and masks to encourage future research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC. ",
        "title": "Seeing the roads through the trees: A benchmark for modeling spatial  dependencies with aerial imagery",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06763",
        "abstract_url": "http://arxiv.org/abs/2401.06763",
        "authors": [
            {
                "last_name": "Zaman",
                "first_name": "Md Mahabub Uz"
            },
            {
                "last_name": "Tao",
                "first_name": "Liangde"
            },
            {
                "last_name": "Maldonado",
                "first_name": "Mark"
            },
            {
                "last_name": "Liu",
                "first_name": "Chang"
            },
            {
                "last_name": "Sunny",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Xu",
                "first_name": "Shouhuai"
            },
            {
                "last_name": "Chen",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CC"
        ],
        "abstract": "  Honeypot is an important cyber defense technique that can expose attackers new attacks. However, the effectiveness of honeypots has not been systematically investigated, beyond the rule of thumb that their effectiveness depends on how they are deployed. In this paper, we initiate a systematic study on characterizing the cybersecurity effectiveness of a new paradigm of deploying honeypots: blending honeypot computers (or IP addresses) into production computers. This leads to the following Honeypot Deployment (HD) problem, How should the defender blend honeypot computers into production computers to maximize the utility in forcing attackers to expose their new attacks while minimizing the loss to the defender in terms of the digital assets stored in the compromised production computers? We formalize HD as a combinatorial optimization problem, prove its NP hardness, provide a near optimal algorithm (i.e., polynomial time approximation scheme). We also conduct simulations to show the impact of attacker capabilities. ",
        "title": "Optimally Blending Honeypots into Production Networks: Hardness and  Algorithms",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06765",
        "abstract_url": "http://arxiv.org/abs/2401.06765",
        "authors": [
            {
                "last_name": "Yaraghi",
                "first_name": "Ahmadreza Saboor"
            },
            {
                "last_name": "Holden",
                "first_name": "Darren"
            },
            {
                "last_name": "Kahani",
                "first_name": "Nafiseh"
            },
            {
                "last_name": "Briand",
                "first_name": "Lionel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. The need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. Further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers' time. To address this challenge, we present TaRGet (Test Repair GEneraTor), a novel approach leveraging pre-trained code language models for automated test case repair. TaRGet treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. To evaluate our approach, we introduce TaRBench, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. Our results demonstrate TaRGet's effectiveness, achieving a 66.1% exact match accuracy. Furthermore, our study examines the effectiveness of TaRGet across different test repair scenarios. We provide a practical guide to predict situations where the generated test repairs might be less reliable. We also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects. ",
        "title": "Automated Test Case Repair Using Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06766",
        "abstract_url": "http://arxiv.org/abs/2401.06766",
        "authors": [
            {
                "last_name": "Voronov",
                "first_name": "Anton"
            },
            {
                "last_name": "Wolf",
                "first_name": "Lena"
            },
            {
                "last_name": "Ryabinin",
                "first_name": "Max"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates. ",
        "title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning  Improvements",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06769",
        "abstract_url": "http://arxiv.org/abs/2401.06769",
        "authors": [
            {
                "last_name": "Wastl",
                "first_name": "Michelle"
            },
            {
                "last_name": "Vamvas",
                "first_name": "Jannis"
            },
            {
                "last_name": "Sennrich",
                "first_name": "Rico"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82-96% for NMT-produced translations, and 60-81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection ",
        "title": "Machine Translation Models are Zero-Shot Detectors of Translation  Direction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06771",
        "abstract_url": "http://arxiv.org/abs/2401.06771",
        "authors": [
            {
                "last_name": "Chadi",
                "first_name": "Mohamed-Amine"
            },
            {
                "last_name": "Mousannif",
                "first_name": "Hajar"
            },
            {
                "last_name": "Aamouche",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, deep learning has demonstrated promising results in de novo drug design. However, the proposed techniques still lack an efficient exploration of the large chemical space. Most of these methods explore a small fragment of the chemical space of known drugs, if the desired molecules were not found, the process ends. In this work, we introduce a curiosity-driven method to force the model to navigate many parts of the chemical space, therefore, achieving higher desirability and diversity as well. At first, we train a recurrent neural network-based general molecular generator (G), then we fine-tune G to maximize curiosity and desirability. We define curiosity as the Tanimoto similarity between two generated molecules, a first molecule generated by G, and a second one generated by a copy of G (Gcopy). We only backpropagate the loss through G while keeping Gcopy unchanged. We benchmarked our approach against two desirable chemical properties related to drug-likeness and showed that the discovered chemical space can be significantly expanded, thus, discovering a higher number of desirable molecules with more diversity and potentially easier to synthesize. All Code and data used in this paper are available at https://github.com/amine179/Curiosity-RL-for-Drug-Design. ",
        "title": "Curiosity as a Self-Supervised Method to Improve Exploration in De novo  Drug Design",
        "date": "2023-09-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06772",
        "abstract_url": "http://arxiv.org/abs/2401.06772",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Sijia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenwen"
            },
            {
                "last_name": "Li",
                "first_name": "Qisong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we introduce a novel method named \"graph-to-segment\" for question answering over knowledge graphs, focusing on understanding question utterances. This method centers on semantic parsing, a key approach for interpreting these utterances. Our primary challenge lies in comprehending implicit entities, relationships, and complex constraints like time, ordinality, and aggregation within questions, contextualized by the knowledge graph. Our framework employs a combination of rule-based and neural-based techniques to parse and construct highly accurate and comprehensive semantic segment sequences. These sequences form semantic query graphs, effectively representing question utterances. We approach question semantic parsing as a sequence generation task, utilizing an encoder-decoder neural network to transform natural language questions into semantic segments. Moreover, to enhance the parsing of implicit entities and relations, we incorporate a graph neural network that leverages the context of the knowledge graph to better understand question representations. Our experimental evaluations on two datasets demonstrate the effectiveness and superior performance of our model in semantic parsing for question answering. ",
        "title": "Semantic Segment Based Semantic Parsing for Question Answering over  Knowledge Graphs",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06774",
        "abstract_url": "http://arxiv.org/abs/2401.06774",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Rumeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xun"
            },
            {
                "last_name": "Yu",
                "first_name": "Hong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is underexplored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge, which guides LLMs to generate synthetic data following two different directions: \"data-to-label\", which labels sentences from a public EHR collection with AD-related signs and symptoms; and \"label-to-data\", which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method. We find that using the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality. ",
        "title": "Two Directions for Clinical Data Generation with Large Language Models:  Data-to-Label and Label-to-Data",
        "date": "2023-12-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06775",
        "abstract_url": "http://arxiv.org/abs/2401.06775",
        "authors": [
            {
                "last_name": "Nazi",
                "first_name": "Zabir Al"
            },
            {
                "last_name": "Peng",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The deployment of large language models (LLMs) within the healthcare sector has sparked both enthusiasm and apprehension. These models exhibit the remarkable capability to provide proficient responses to free-text queries, demonstrating a nuanced understanding of professional medical knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for healthcare applications, elucidating the trajectory of their development, starting from traditional Pretrained Language Models (PLMs) to the present state of LLMs in healthcare sector. First, we explore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare applications, particularly focusing on clinical language understanding tasks. These tasks encompass a wide spectrum, ranging from named entity recognition and relation extraction to natural language inference, multi-modal medical applications, document classification, and question-answering. Additionally, we conduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain, while also assessing the utilization of various open-source LLMs and highlighting their significance in healthcare applications. Furthermore, we present the essential performance metrics employed to evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations. Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings. This review provides a comprehensive exploration of the current landscape of LLMs in healthcare, addressing their role in transforming medical applications and the areas that warrant further research and development. ",
        "title": "Large language models in healthcare and medical domain: A review",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06777",
        "abstract_url": "http://arxiv.org/abs/2401.06777",
        "authors": [
            {
                "last_name": "Vo",
                "first_name": "Jamie"
            },
            {
                "last_name": "Sharif",
                "first_name": "Naeha"
            },
            {
                "last_name": "Hassan",
                "first_name": "Ghulam Mubashar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The early detection of Alzheimer's Disease is imperative to ensure early treatment and improve patient outcomes. There has consequently been extenstive research into detecting AD and its intermediate phase, mild cognitive impairment (MCI). However, there is very small literature in predicting the conversion to AD and MCI from normal cognitive condition. Recently, multiple studies have applied convolutional neural networks (CNN) which integrate Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) to classify MCI and AD. However, in these works, the fusion of MRI and PET features are simply achieved through concatenation, resulting in a lack of cross-modal interactions. In this paper, we propose a novel multimodal neuroimaging attention-based CNN architecture, MNA-net, to predict whether cognitively normal (CN) individuals will develop MCI or AD within a period of 10 years. To address the lack of interactions across neuroimaging modalities seen in previous works, MNA-net utilises attention mechanisms to form shared representations of the MRI and PET images. The proposed MNA-net is tested in OASIS-3 dataset and is able to predict CN individuals who converted to MCI or AD with an accuracy of 83%, true negative rate of 80%, and true positive rate of 86%. The new state of the art results improved by 5% and 10% for accuracy and true negative rate by the use of attention mechanism. These results demonstrate the potential of the proposed model to predict cognitive impairment and attention based mechanisms in the fusion of different neuroimaging modalities to improve the prediction of cognitive decline. ",
        "title": "Multimodal Neuroimaging Attention-Based architecture for Cognitive  Decline Prediction",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06779",
        "abstract_url": "http://arxiv.org/abs/2401.06779",
        "authors": [
            {
                "last_name": "El-Awady",
                "first_name": "Khalid"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We investigate the construction of generative models capable of encoding physical constraints that can be hard to express explicitly. For the problem of inverse material design, where one seeks to design a material with a prescribed set of properties, a significant challenge is ensuring synthetic viability of a proposed new material. We encode an implicit dataset relationships, namely that certain materials can be decomposed into other ones in the dataset, and present a VAE model capable of preserving this property in the latent space and generating new samples with the same. This is particularly useful in sequential inverse material design, an emergent research area that seeks to design a material with specific properties by sequentially adding (or removing) elements using policies trained through deep reinforcement learning. ",
        "title": "VAE for Modified 1-Hot Generative Materials Modeling, A Step Towards  Inverse Material Design",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06780",
        "abstract_url": "http://arxiv.org/abs/2401.06780",
        "authors": [
            {
                "last_name": "Shen",
                "first_name": "Xiongri"
            },
            {
                "last_name": "Song",
                "first_name": "Zhenxi"
            },
            {
                "last_name": "Li",
                "first_name": "Linling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            },
            {
                "last_name": "Liu",
                "first_name": "Lingyan Liang Honghai"
            },
            {
                "last_name": "Deng",
                "first_name": "Demao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhiguo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Early diagnosis of mild cognitive impairment (MCI) and subjective cognitive decline (SCD) utilizing multi-modal magnetic resonance imaging (MRI) is a pivotal area of research. While various regional and connectivity features from functional MRI (fMRI) and diffusion tensor imaging (DTI) have been employed to develop diagnosis models, most studies integrate these features without adequately addressing their alignment and interactions. This limits the potential to fully exploit the synergistic contributions of combined features and modalities. To solve this gap, our study introduces a novel Hierarchical Alignments and Hierarchical Interactions (HA-HI) method for MCI and SCD classification, leveraging the combined strengths of fMRI and DTI. HA-HI efficiently learns significant MCI- or SCD- related regional and connectivity features by aligning various feature types and hierarchically maximizing their interactions. Furthermore, to enhance the interpretability of our approach, we have developed the Synergistic Activation Map (SAM) technique, revealing the critical brain regions and connections that are indicative of MCI/SCD. Comprehensive evaluations on the ADNI dataset and our self-collected data demonstrate that HA-HI outperforms other existing methods in diagnosing MCI and SCD, making it a potentially vital and interpretable tool for early detection. The implementation of this method is publicly accessible at https://github.com/ICI-BCI/Dual-MRI-HA-HI.git. ",
        "title": "HA-HI: Synergising fMRI and DTI through Hierarchical Alignments and  Hierarchical Interactions for Mild Cognitive Impairment Diagnosis",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06781",
        "abstract_url": "http://arxiv.org/abs/2401.06781",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Cao",
                "first_name": "Yanbo"
            },
            {
                "last_name": "Wen",
                "first_name": "Yinlong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yanru"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Poker, also known as Texas Hold'em, has always been a typical research target within imperfect information games (IIGs). IIGs have long served as a measure of artificial intelligence (AI) development. Representative prior works, such as DeepStack and Libratus heavily rely on counterfactual regret minimization (CFR) to tackle heads-up no-limit Poker. However, it is challenging for subsequent researchers to learn CFR from previous models and apply it to other real-world applications due to the expensive computational cost of CFR iterations. Additionally, CFR is difficult to apply to multi-player games due to the exponential growth of the game tree size. In this work, we introduce PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number of players and gaining high win rates, established on a lightweight large language model (LLM). PokerGPT only requires simple textual information of Poker games for generating decision-making advice, thus guaranteeing the convenient interaction between AI and humans. We mainly transform a set of textual records acquired from real games into prompts, and use them to fine-tune a lightweight pre-trained LLM using reinforcement learning human feedback technique. To improve fine-tuning performance, we conduct prompt engineering on raw data, including filtering useful information, selecting behaviors of players with high win rates, and further processing them into textual instruction using multiple prompt engineering techniques. Through the experiments, we demonstrate that PokerGPT outperforms previous approaches in terms of win rate, model size, training time, and response speed, indicating the great potential of LLMs in solving IIGs. ",
        "title": "PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas  Hold'em via Large Language Model",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06782",
        "abstract_url": "http://arxiv.org/abs/2401.06782",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Liqiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Bo"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Che",
                "first_name": "Chang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the realm of patent document analysis, assessing semantic similarity between phrases presents a significant challenge, notably amplifying the inherent complexities of Cooperative Patent Classification (CPC) research. Firstly, this study addresses these challenges, recognizing early CPC work while acknowledging past struggles with language barriers and document intricacy. Secondly, it underscores the persisting difficulties of CPC research.   To overcome these challenges and bolster the CPC system, This paper presents two key innovations. Firstly, it introduces an ensemble approach that incorporates four BERT-related models, enhancing semantic similarity accuracy through weighted averaging. Secondly, a novel text preprocessing method tailored for patent documents is introduced, featuring a distinctive input structure with token scoring that aids in capturing semantic relationships during CPC context training, utilizing BCELoss. Our experimental findings conclusively establish the effectiveness of both our Ensemble Model and novel text processing strategies when deployed on the U.S. Patent Phrase to Phrase Matching dataset. ",
        "title": "Semantic Similarity Matching for Patent Documents Using Ensemble  BERT-related Model and Novel Text Processing Method",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06783",
        "abstract_url": "http://arxiv.org/abs/2401.06783",
        "authors": [
            {
                "last_name": "Bhoi",
                "first_name": "Sudhanshu"
            },
            {
                "last_name": "Markhedkar",
                "first_name": "Swapnil"
            },
            {
                "last_name": "Phadke",
                "first_name": "Shruti"
            },
            {
                "last_name": "Agrawal",
                "first_name": "Prashant"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  Social media accounts post increasingly similar content, creating a chaotic experience across platforms, which makes accessing desired information difficult. These posts can be organized by categorizing and grouping duplicates across social handles and accounts. There can be more than one duplicate of a post, however, a conventional Siamese neural network only considers a pair of inputs for duplicate text detection. In this paper, we first propose a multiple-input Siamese network, MultiSiam. This condensed network is then used to propose another model, SMCD (Social Media Classification and Duplication Model) to perform both duplicate text grouping and categorization. The MultiSiam network, just like the Siamese, can be used in multiple applications by changing the sub-network appropriately. ",
        "title": "MultiSiam: A Multiple Input Siamese Network For Social Media Text  Classification And Duplicate Text Detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06785",
        "abstract_url": "http://arxiv.org/abs/2401.06785",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongyi"
            },
            {
                "last_name": "Yao",
                "first_name": "Yuanshun"
            },
            {
                "last_name": "Shen",
                "first_name": "Wei"
            },
            {
                "last_name": "Wei",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaoying"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaoran"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aligning large language models (LLMs) with human values is a vital task for LLM practitioners. Current alignment techniques have several limitations: (1) requiring a large amount of annotated data; (2) demanding heavy human involvement; (3) lacking a systematic mechanism to continuously improve. In this work, we study aligning LLMs to a new domain with limited samples (e.g. < 100). We propose an algorithm that can self-align LLMs iteratively without active human involvement. Unlike existing works, our algorithm relies on neither human-crafted instructions nor labeled rewards, significantly reducing human involvement. In addition, our algorithm can self-improve the alignment continuously. The key idea is to first retrieve high-quality samples related to the target domain and use them as In-context Learning examples to generate more samples. Then we use the self-generated samples to finetune the LLM iteratively. We show that our method can unlock the LLMs' self-generalization ability to perform alignment with near-zero human supervision. We test our algorithm on three benchmarks in safety, truthfulness, and instruction-following, and show good performance in alignment, domain adaptability, and scalability. ",
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06786",
        "abstract_url": "http://arxiv.org/abs/2401.06786",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Yifei"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xumiao"
            },
            {
                "last_name": "Lin",
                "first_name": "Xianshang"
            },
            {
                "last_name": "Hu",
                "first_name": "Pan"
            },
            {
                "last_name": "Ma",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Lu",
                "first_name": "Songwu"
            },
            {
                "last_name": "Du",
                "first_name": "Wan"
            },
            {
                "last_name": "Mao",
                "first_name": "Zhuoqing"
            },
            {
                "last_name": "Zhai",
                "first_name": "Ennan"
            },
            {
                "last_name": "Cai",
                "first_name": "Dennis"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Among the thriving ecosystem of cloud computing and the proliferation of Large Language Model (LLM)-based code generation tools, there is a lack of benchmarking for code generation in cloud-native applications. In response to this need, we present CloudEval-YAML, a practical benchmark for cloud configuration generation. CloudEval-YAML tackles the diversity challenge by focusing on YAML, the de facto standard of numerous cloud-native tools. We develop the CloudEval-YAML benchmark with practicality in mind: the dataset consists of hand-written problems with unit tests targeting practical scenarios. We further enhanced the dataset to meet practical needs by rephrasing questions in a concise, abbreviated, and bilingual manner. The dataset consists of 1011 problems that take more than 1200 human hours to complete. To improve practicality during evaluation, we build a scalable evaluation platform for CloudEval-YAML that achieves a 20 times speedup over a single machine. To the best of our knowledge, the CloudEval-YAML dataset is the first hand-written dataset targeting cloud-native applications. We present an in-depth evaluation of 12 LLMs, leading to a deeper understanding of the problems and LLMs, as well as effective methods to improve task performance and reduce cost. ",
        "title": "CloudEval-YAML: A Practical Benchmark for Cloud Configuration Generation",
        "date": "2023-11-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06787",
        "abstract_url": "http://arxiv.org/abs/2401.06787",
        "authors": [
            {
                "last_name": "Nath",
                "first_name": "Sristy Shidul"
            },
            {
                "last_name": "Karim",
                "first_name": "Razuan"
            },
            {
                "last_name": "Miraz",
                "first_name": "Mahdi H."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  The Internet is currently the largest platform for global communication including expressions of opinions, reviews, contents, images, videos and so forth. Moreover, social media has now become a very broad and highly engaging platform due to its immense popularity and swift adoption trend. Increased social networking, however, also has detrimental impacts on the society leading to a range of unwanted phenomena, such as online assault, intimidation, digital bullying, criminality and trolling. Hence, cyberbullying has become a pervasive and worrying problem that poses considerable psychological and emotional harm to the people, particularly amongst the teens and the young adults. In order to lessen its negative effects and provide victims with prompt support, a great deal of research to identify cyberbullying instances at various online platforms is emerging. In comparison to other languages, Bangla (also known as Bengali) has fewer research studies in this domain. This study demonstrates a deep learning strategy for identifying cyberbullying in Bengali, using a dataset of 12282 versatile comments from multiple social media sites. In this study, a two-layer bidirectional long short-term memory (Bi-LSTM) model has been built to identify cyberbullying, using a variety of optimisers as well as 5-fold cross validation. To evaluate the functionality and efficacy of the proposed system, rigorous assessment and validation procedures have been employed throughout the project. The results of this study reveals that the proposed model's accuracy, using momentum-based stochastic gradient descent (SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a F1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31% in 5-fold cross validation. ",
        "title": "Deep Learning Based Cyberbullying Detection in Bangla Language",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06788",
        "abstract_url": "http://arxiv.org/abs/2401.06788",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Guo",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Pan"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual Speech Recognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks of Single-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multi-scale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, comprising a ResNet3D visual frontend, an E-Branchformer encoder, and a Transformer decoder. Experiments show that our system achieves 34.76% CER for the Single-Speaker Task and 41.06% CER for the Multi-Speaker Task after multi-system fusion, ranking first place in all three tracks we participate. ",
        "title": "The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in  CNVSRC 2023",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06789",
        "abstract_url": "http://arxiv.org/abs/2401.06789",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Tingting"
            },
            {
                "last_name": "Tian",
                "first_name": "Shubo"
            },
            {
                "last_name": "Daly",
                "first_name": "Jordan"
            },
            {
                "last_name": "Geiger",
                "first_name": "Melissa"
            },
            {
                "last_name": "Jia",
                "first_name": "Minna"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jinfeng"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL",
            "LG"
        ],
        "abstract": "  For an approaching disaster, the tracking of time-sensitive critical information such as hurricane evacuation notices is challenging in the United States. These notices are issued and distributed rapidly by numerous local authorities that may spread across multiple states. They often undergo frequent updates and are distributed through diverse online portals lacking standard formats. In this study, we developed an approach to timely detect and track the locally issued hurricane evacuation notices. The text data were collected mainly with a spatially targeted web scraping method. They were manually labeled and then classified using natural language processing techniques with deep learning models. The classification of mandatory evacuation notices achieved a high accuracy (recall = 96%). We used Hurricane Ian (2022) to illustrate how real-time evacuation notices extracted from local government sources could be redistributed with a Web GIS system. Our method applied to future hurricanes provides live data for situation awareness to higher-level government agencies and news media. The archived data helps scholars to study government responses toward weather warnings and individual behaviors influenced by evacuation history. The framework may be applied to other types of disasters for rapid and targeted retrieval, classification, redistribution, and archiving of real-time government orders and notifications. ",
        "title": "Information Retrieval and Classification of Real-Time Multi-Source  Hurricane Evacuation Notices",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06790",
        "abstract_url": "http://arxiv.org/abs/2401.06790",
        "authors": [
            {
                "last_name": "Moraes",
                "first_name": "Daniel de S."
            },
            {
                "last_name": "Santos",
                "first_name": "Pedro T. C."
            },
            {
                "last_name": "da Costa",
                "first_name": "Polyana B."
            },
            {
                "last_name": "Pinto",
                "first_name": "Matheus A. S."
            },
            {
                "last_name": "Pinto",
                "first_name": "Ivan de J. P."
            },
            {
                "last_name": "da Veiga",
                "first_name": "\u00c1lvaro M. G."
            },
            {
                "last_name": "Colcher",
                "first_name": "Sergio"
            },
            {
                "last_name": "Busson",
                "first_name": "Antonio J. G."
            },
            {
                "last_name": "Rocha",
                "first_name": "Rafael H."
            },
            {
                "last_name": "Gaio",
                "first_name": "Rennan"
            },
            {
                "last_name": "Miceli",
                "first_name": "Rafael"
            },
            {
                "last_name": "Tourinho",
                "first_name": "Gabriela"
            },
            {
                "last_name": "Rabaioli",
                "first_name": "Marcos"
            },
            {
                "last_name": "Santos",
                "first_name": "Leandro"
            },
            {
                "last_name": "Marques",
                "first_name": "Fellipe"
            },
            {
                "last_name": "Favaro",
                "first_name": "David"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This work presents an unsupervised method for automatically constructing and expanding topic taxonomies by using instruction-based fine-tuned LLMs (Large Language Models). We apply topic modeling and keyword extraction techniques to create initial topic taxonomies and LLMs to post-process the resulting terms and create a hierarchy. To expand an existing taxonomy with new terms, we use zero-shot prompting to find out where to add new nodes, which, to our knowledge, is the first work to present such an approach to taxonomy tasks. We use the resulting taxonomies to assign tags that characterize merchants from a retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a two-part form in which we first assessed the quality of the taxonomies created and then the tags assigned to merchants based on that taxonomy. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies, while the average coherence for merchant tagging surpassed 80%. ",
        "title": "Using Zero-shot Prompting in the Automatic Creation and Expansion of  Topic Taxonomies for Tagging Retail Banking Transactions",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06791",
        "abstract_url": "http://arxiv.org/abs/2401.06791",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Gongbo"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiliang"
            },
            {
                "last_name": "Hu",
                "first_name": "Yan"
            },
            {
                "last_name": "Xu",
                "first_name": "Hua"
            },
            {
                "last_name": "Weng",
                "first_name": "Chunhua"
            },
            {
                "last_name": "Peng",
                "first_name": "Yifan"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Objectives Extraction of PICO (Populations, Interventions, Comparison, and Outcomes) entities is fundamental to evidence retrieval. We present a novel method PICOX to extract overlapping PICO entities.   Materials and Methods PICOX first identifies entities by assessing whether a word marks the beginning or conclusion of an entity. Then it uses a multi-label classifier to assign one or more PICO labels to a span candidate. PICOX was evaluated using one of the best-performing baselines, EBM-NLP, and three more datasets, i.e., PICO-Corpus, and RCT publications on Alzheimer's Disease or COVID-19, using entity-level precision, recall, and F1 scores.   Results PICOX achieved superior precision, recall, and F1 scores across the board, with the micro F1 score improving from 45.05 to 50.87 (p << 0.01). On the PICO-Corpus, PICOX obtained higher recall and F1 scores than the baseline and improved the micro recall score from 56.66 to 67.33. On the COVID-19 dataset, PICOX also outperformed the baseline and improved the micro F1 score from 77.10 to 80.32. On the AD dataset, PICOX demonstrated comparable F1 scores with higher precision when compared to the baseline.   Conclusion PICOX excels in identifying overlapping entities and consistently surpasses a leading baseline across multiple datasets. Ablation studies reveal that its data augmentation strategy effectively minimizes false positives and improves precision. ",
        "title": "A Span-based Model for Extracting Overlapping PICO Entities from RCT  Publications",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06793",
        "abstract_url": "http://arxiv.org/abs/2401.06793",
        "authors": [
            {
                "last_name": "Durdymyradov",
                "first_name": "Kerven"
            },
            {
                "last_name": "Moshkov",
                "first_name": "Mikhail"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Decision trees and decision rule systems play important roles as classifiers, knowledge representation tools, and algorithms. They are easily interpretable models for data analysis, making them widely used and studied in computer science. Understanding the relationships between these two models is an important task in this field. There are well-known methods for converting decision trees into systems of decision rules. In this paper, we consider the inverse transformation problem, which is not so simple. Instead of constructing an entire decision tree, our study focuses on a greedy polynomial time algorithm that simulates the operation of a decision tree on a given tuple of attribute values. ",
        "title": "Greedy Algorithm for Inference of Decision Trees from Decision Rule  Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06794",
        "abstract_url": "http://arxiv.org/abs/2401.06794",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yurui"
            },
            {
                "last_name": "Ma",
                "first_name": "Langtian"
            },
            {
                "last_name": "Tian",
                "first_name": "Chaolin"
            },
            {
                "last_name": "Jiang",
                "first_name": "Xunyi"
            },
            {
                "last_name": "Sinatra",
                "first_name": "Roberta"
            },
            {
                "last_name": "Ma",
                "first_name": "Yifang"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  Human behaviors, including scientific activities, are shaped by the hierarchical divisions of geography. As a result, researchers' mobility patterns vary across regions, influencing several aspects of the scientific community. These aspects encompass career trajectories, knowledge transfer, international collaborations, talent circulation, innovation diffusion, resource distribution, and policy development. However, our understanding of the relationship between the hierarchical regional scale and scientific movements is limited. This study aims to understand the subtle role of the geographical scales on scientists' mobility patterns across cities, countries, and continents. To this end, we analyzed 2.03 million scientists from 1960 to 2021, spanning institutions, cities, countries, and continents. We built a model based on hierarchical regions with different administrative levels and assessed the tendency for mobility from one region to another and the attractiveness of each region. Our findings reveal distinct nested hierarchies of regional scales and the dynamic of scientists' relocation patterns. This study sheds light on the complex dynamics of scientists' mobility and offers insights into how geographical scale and administrative divisions influence career decisions. ",
        "title": "Quantifying the hierarchical scales of scientists'mobility",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06795",
        "abstract_url": "http://arxiv.org/abs/2401.06795",
        "authors": [
            {
                "last_name": "Glickman",
                "first_name": "Mark"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simulate abductive reasoning, which provides researchers the ability to make connections among related technical topics, which can also be used for research discovery. We review the developments in AI and generative AI for research discovery and summarization, and propose directions where these types of tools are likely to head in the future that may be of interest to statistician and data scientists. ",
        "title": "AI and Generative AI for Research Discovery and Summarization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06796",
        "abstract_url": "http://arxiv.org/abs/2401.06796",
        "authors": [
            {
                "last_name": "Maleki",
                "first_name": "Negar"
            },
            {
                "last_name": "Padmanabhan",
                "first_name": "Balaji"
            },
            {
                "last_name": "Dutta",
                "first_name": "Kaushik"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As large language models continue to advance in Artificial Intelligence (AI), text generation systems have been shown to suffer from a problematic phenomenon termed often as \"hallucination.\" However, with AI's increasing presence across various domains including medicine, concerns have arisen regarding the use of the term itself. In this study, we conducted a systematic review to identify papers defining \"AI hallucination\" across fourteen databases. We present and analyze definitions obtained across all databases, categorize them based on their applications, and extract key points within each category. Our results highlight a lack of consistency in how the term is used, but also help identify several alternative terms in the literature. We discuss implications of these and call for a more unified effort to bring consistency to an important contemporary AI issue that can affect multiple domains significantly. ",
        "title": "AI Hallucinations: A Misnomer Worth Clarifying",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06799",
        "abstract_url": "http://arxiv.org/abs/2401.06799",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Youngjae"
            },
            {
                "last_name": "Bae",
                "first_name": "HeeSun"
            },
            {
                "last_name": "Shin",
                "first_name": "Seungjae"
            },
            {
                "last_name": "Youn",
                "first_name": "Yeo Dong"
            },
            {
                "last_name": "Joo",
                "first_name": "Weonyoung"
            },
            {
                "last_name": "Moon",
                "first_name": "Il-Chul"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Recent Vision-Language Pretrained (VLP) models have become the backbone for many downstream tasks, but they are utilized as frozen model without learning. Prompt learning is a method to improve the pre-trained VLP model by adding a learnable context vector to the inputs of the text encoder. In a few-shot learning scenario of the downstream task, MLE training can lead the context vector to over-fit dominant image features in the training data. This overfitting can potentially harm the generalization ability, especially in the presence of a distribution shift between the training and test dataset. This paper presents a Bayesian-based framework of prompt learning, which could alleviate the overfitting issues on few-shot learning application and increase the adaptability of prompts on unseen instances. Specifically, modeling data-dependent prior enhances the adaptability of text features for both seen and unseen image features without the trade-off of performance between them. Based on the Bayesian framework, we utilize the Wasserstein Gradient Flow in the estimation of our target posterior distribution, which enables our prompt to be flexible in capturing the complex modes of image features. We demonstrate the effectiveness of our method on benchmark datasets for several experiments by showing statistically significant improvements on performance compared to existing methods. The code is available at https://github.com/youngjae-cho/APP. ",
        "title": "Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt  Learning with Data-Dependent Prior",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06800",
        "abstract_url": "http://arxiv.org/abs/2401.06800",
        "authors": [
            {
                "last_name": "Kulkarni",
                "first_name": "Mandar"
            },
            {
                "last_name": "Tangarajan",
                "first_name": "Praveen"
            },
            {
                "last_name": "Kim",
                "first_name": "Kyung"
            },
            {
                "last_name": "Trivedi",
                "first_name": "Anusua"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline. ",
        "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06801",
        "abstract_url": "http://arxiv.org/abs/2401.06801",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ye"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow's potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development. ",
        "title": "Graph-of-Thought: Utilizing Large Language Models to Solve Complex and  Dynamic Business Problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06802",
        "abstract_url": "http://arxiv.org/abs/2401.06802",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Quan"
            },
            {
                "last_name": "Jing",
                "first_name": "Shixiong"
            },
            {
                "last_name": "Chen",
                "first_name": "Lingwei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  The popularization of social media increases user engagements and generates a large amount of user-oriented data. Among them, text data (e.g., tweets, blogs) significantly attracts researchers and speculators to infer user attributes (e.g., age, gender, location) for fulfilling their intents. Generally, this line of work casts attribute inference as a text classification problem, and starts to leverage graph neural networks (GNNs) to utilize higher-level representations of source texts. However, these text graphs are constructed over words, suffering from high memory consumption and ineffectiveness on few labeled texts. To address this challenge, we design a text-graph-based few-shot learning model for attribute inferences on social media text data. Our model first constructs and refines a text graph using manifold learning and message passing, which offers a better trade-off between expressiveness and complexity. Afterwards, to further use cross-domain texts and unlabeled texts to improve few-shot performance, a hierarchical knowledge distillation is devised over text graph to optimize the problem, which derives better text representations, and advances model generalization ability. Experiments on social media datasets demonstrate the state-of-the-art performance of our model on attribute inferences with considerably fewer labeled texts. ",
        "title": "Hierarchical Knowledge Distillation on Text Graph for Data-limited  Attribute Inference",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06803",
        "abstract_url": "http://arxiv.org/abs/2401.06803",
        "authors": [
            {
                "last_name": "Grassucci",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Park",
                "first_name": "Jihong"
            },
            {
                "last_name": "Barbarossa",
                "first_name": "Sergio"
            },
            {
                "last_name": "Kim",
                "first_name": "Seong-Lyun"
            },
            {
                "last_name": "Choi",
                "first_name": "Jinho"
            },
            {
                "last_name": "Comminiello",
                "first_name": "Danilo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  While deep generative models are showing exciting abilities in computer vision and natural language processing, their adoption in communication frameworks is still far underestimated. These methods are demonstrated to evolve solutions to classic communication problems such as denoising, restoration, or compression. Nevertheless, generative models can unveil their real potential in semantic communication frameworks, in which the receiver is not asked to recover the sequence of bits used to encode the transmitted (semantic) message, but only to regenerate content that is semantically consistent with the transmitted message. Disclosing generative models capabilities in semantic communication paves the way for a paradigm shift with respect to conventional communication systems, which has great potential to reduce the amount of data traffic and offers a revolutionary versatility to novel tasks and applications that were not even conceivable a few years ago. In this paper, we present a unified perspective of deep generative models in semantic communication and we unveil their revolutionary role in future communication frameworks, enabling emerging applications and tasks. Finally, we analyze the challenges and opportunities to face to develop generative models specifically tailored for communication systems. ",
        "title": "Generative AI Meets Semantic Communication: Evolution and Revolution of  Communication Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06804",
        "abstract_url": "http://arxiv.org/abs/2401.06804",
        "authors": [
            {
                "last_name": "Shahin",
                "first_name": "Nada"
            },
            {
                "last_name": "Ismail",
                "first_name": "Leila"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  ChatGPT is a language model based on Generative AI. Existing research work on ChatGPT focused on its use in various domains. However, its potential for Sign Language Translation (SLT) is yet to be explored. This paper addresses this void. Therefore, we present GPT's evolution aiming a retrospective analysis of the improvements to its architecture for SLT. We explore ChatGPT's capabilities in translating different sign languages in paving the way to better accessibility for deaf and hard-of-hearing community. Our experimental results indicate that ChatGPT can accurately translate from English to American (ASL), Australian (AUSLAN), and British (BSL) sign languages and from Arabic Sign Language (ArSL) to English with only one prompt iteration. However, the model failed to translate from Arabic to ArSL and ASL, AUSLAN, and BSL to Arabic. Consequently, we present challenges and derive insights for future research directions. ",
        "title": "ChatGPT, Let us Chat Sign Language: Experiments, Architectural Elements,  Challenges and Research Directions",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06805",
        "abstract_url": "http://arxiv.org/abs/2401.06805",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yiqi"
            },
            {
                "last_name": "Chen",
                "first_name": "Wentao"
            },
            {
                "last_name": "Han",
                "first_name": "Xiaotian"
            },
            {
                "last_name": "Lin",
                "first_name": "Xudong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiteng"
            },
            {
                "last_name": "Liu",
                "first_name": "Yongfei"
            },
            {
                "last_name": "Zhai",
                "first_name": "Bohan"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jianbo"
            },
            {
                "last_name": "You",
                "first_name": "Quanzeng"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning. ",
        "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models  (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06806",
        "abstract_url": "http://arxiv.org/abs/2401.06806",
        "authors": [
            {
                "last_name": "Jung",
                "first_name": "Jee-weon"
            },
            {
                "last_name": "Sharma",
                "first_name": "Roshan"
            },
            {
                "last_name": "Chen",
                "first_name": "William"
            },
            {
                "last_name": "Raj",
                "first_name": "Bhiksha"
            },
            {
                "last_name": "Watanabe",
                "first_name": "Shinji"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Abstractive speech summarization (SSUM) aims to generate human-like summaries from speech. Given variations in information captured and phrasing, recordings can be summarized in multiple ways. Therefore, it is more reasonable to consider a probabilistic distribution of all potential summaries rather than a single summary. However, conventional SSUM models are mostly trained and evaluated with a single ground-truth (GT) human-annotated deterministic summary for every recording. Generating multiple human references would be ideal to better represent the distribution statistically, but is impractical because annotation is expensive. We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation. First, we explore prompting strategies to generate synthetic summaries from ChatGPT. We validate the quality of synthetic summaries using multiple metrics including human evaluation, where we find that summaries generated using AugSumm are perceived as more valid to humans. Second, we develop methods to utilize synthetic summaries in training and evaluation. Experiments on How2 demonstrate that pre-training on synthetic summaries and fine-tuning on GT summaries improves ROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries are available at https://github.com/Jungjee/AugSumm. ",
        "title": "AugSumm: towards generalizable speech summarization using synthetic  labels from large language model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06807",
        "abstract_url": "http://arxiv.org/abs/2401.06807",
        "authors": [
            {
                "last_name": "Tomar",
                "first_name": "Mohit"
            },
            {
                "last_name": "Tiwari",
                "first_name": "Abhisek"
            },
            {
                "last_name": "Saha",
                "first_name": "Tulika"
            },
            {
                "last_name": "Jha",
                "first_name": "Prince"
            },
            {
                "last_name": "Saha",
                "first_name": "Sriparna"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent times, there has been an increasing awareness about imminent environmental challenges, resulting in people showing a stronger dedication to taking care of the environment and nurturing green life. The current $19.6 billion indoor gardening industry, reflective of this growing sentiment, not only signifies a monetary value but also speaks of a profound human desire to reconnect with the natural world. However, several recent surveys cast a revealing light on the fate of plants within our care, with more than half succumbing primarily due to the silent menace of improper care. Thus, the need for accessible expertise capable of assisting and guiding individuals through the intricacies of plant care has become paramount more than ever. In this work, we make the very first attempt at building a plant care assistant, which aims to assist people with plant(-ing) concerns through conversations. We propose a plant care conversational dataset named Plantational, which contains around 1K dialogues between users and plant care experts. Our end-to-end proposed approach is two-fold : (i) We first benchmark the dataset with the help of various large language models (LLMs) and visual language model (VLM) by studying the impact of instruction tuning (zero-shot and few-shot prompting) and fine-tuning techniques on this task; (ii) finally, we build EcoSage, a multi-modal plant care assisting dialogue generation framework, incorporating an adapter-based modality infusion using a gated mechanism. We performed an extensive examination (both automated and manual evaluation) of the performance exhibited by various LLMs and VLM in the generation of the domain-specific dialogue responses to underscore the respective strengths and weaknesses of these diverse models. ",
        "title": "An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue  Assistant",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06808",
        "abstract_url": "http://arxiv.org/abs/2401.06808",
        "authors": [
            {
                "last_name": "Lewis",
                "first_name": "Martha"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "NE"
        ],
        "abstract": "  Categorical compositional distributional semantics is an approach to modelling language that combines the success of vector-based models of meaning with the compositional power of formal semantics. However, this approach was developed without an eye to cognitive plausibility. Vector representations of concepts and concept binding are also of interest in cognitive science, and have been proposed as a way of representing concepts within a biologically plausible spiking neural network. This work proposes a way for compositional distributional semantics to be implemented within a spiking neural network architecture, with the potential to address problems in concept binding, and give a small implementation. We also describe a means of training word representations using labelled images. ",
        "title": "Grounded learning for compositional vector semantics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06810",
        "abstract_url": "http://arxiv.org/abs/2401.06810",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Srishti"
            },
            {
                "last_name": "Garg",
                "first_name": "Piyush Kumar"
            },
            {
                "last_name": "Dandapat",
                "first_name": "Sourav Kumar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Emotions have played an important part in many sectors, including psychology, medicine, mental health, computer science, and so on, and categorizing them has proven extremely useful in separating one emotion from another. Emotions can be classified using the following two methods: (1) The supervised method's efficiency is strongly dependent on the size and domain of the data collected. A categorization established using relevant data from one domain may not work well in another. (2) An unsupervised method that uses either domain expertise or a knowledge base of emotion types already exists. Though this second approach provides a suitable and generic categorization of emotions and is cost-effective, the literature doesn't possess a publicly available knowledge base that can be directly applied to any emotion categorization-related task. This pushes us to create a knowledge base that can be used for emotion classification across domains, and ontology is often used for this purpose. In this study, we provide TONE, an emotion-based ontology that effectively creates an emotional hierarchy based on Dr. Gerrod Parrot's group of emotions. In addition to ontology development, we introduce a semi-automated vocabulary construction process to generate a detailed collection of terms for emotions at each tier of the hierarchy. We also demonstrate automated methods for establishing three sorts of dependencies in order to develop linkages between different emotions. Our human and automatic evaluation results show the ontology's quality. Furthermore, we describe three distinct use cases that demonstrate the applicability of our ontology. ",
        "title": "TONE: A 3-Tiered ONtology for Emotion analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06811",
        "abstract_url": "http://arxiv.org/abs/2401.06811",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Zhongtian"
            },
            {
                "last_name": "Chen",
                "first_name": "Yangqi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Meng"
            },
            {
                "last_name": "Li",
                "first_name": "Ronghan"
            },
            {
                "last_name": "Wang",
                "first_name": "Lifang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Knowledge-based dialogue systems with internet retrieval have recently attracted considerable attention from researchers. The dialogue systems overcome a major limitation of traditional knowledge dialogue systems, where the timeliness of knowledge cannot be assured, hence providing greater practical application value. Knowledge-based dialogue systems with internet retrieval can be typically segmented into three tasks: Retrieval Decision, Query Generation, and Response Generation. However, many of studies assumed that all conversations require external knowledge to continue, neglecting the critical step of determining when retrieval is necessary. This assumption often leads to an over-dependence on external knowledge, even when it may not be required. Our work addresses this oversight by employing a single unified model facilitated by prompt and multi-task learning approaches. This model not only decides whether retrieval is necessary but also generates retrieval queries and responses. By integrating these functions, our system leverages the full potential of pre-trained models and reduces the complexity and costs associated with deploying multiple models. We conducted extensive experiments to investigate the mutual enhancement among the three tasks in our system. What is more, the experiment results on the Wizint and Dusinc datasets not only demonstrate that our unified model surpasses the baseline performance for individual tasks, but also reveal that it achieves comparable results when contrasted with SOTA systems that deploy separate, specialized models for each task. ",
        "title": "UniRQR: A Unified Model for Retrieval Decision, Query, and Response  Generation in Internet-Based Knowledge Dialogue Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06816",
        "abstract_url": "http://arxiv.org/abs/2401.06816",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Qinghan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiyong"
            },
            {
                "last_name": "Huang",
                "first_name": "Jihao"
            },
            {
                "last_name": "Li",
                "first_name": "Guiquan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  ChatGPT has been evidenced to enhance human performance in creative tasks. Yet, it is still unclear if this boosting effect sustains with and without ChatGPT. In a pre-registered seven-day lab experiment and a follow-up survey after 30 days of experiment completion, we examined the impacts of ChatGPT presence and absence on sustained creativity using a text dataset of 3302 creative ideas and 427 creative solutions from 61 college students. Participants in the treatment group used ChatGPT in creative tasks, while those in the control group completed the tasks by themselves. The findings show that although the boosting effect of ChatGPT was consistently observed over a five-day creative journey, human creative performance reverted to baseline when ChatGPT was down on the 7th and the 30th day. More critically, the use of ChatGPT in creative tasks resulted in increasingly homogenized contents, and this homogenization effect persisted even when ChatGPT was absence. These findings pose a challenge to the prevailing argument that ChatGPT can enhance human creativity. In fact, generative AI like ChatGPT lends to human with a temporary rise in creative performance but boxes human creative capability in the long run, highlighting the imperative for cautious generative AI integration in creative endeavors. ",
        "title": "When ChatGPT is gone: Creativity reverts and homogeneity persists",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06817",
        "abstract_url": "http://arxiv.org/abs/2401.06817",
        "authors": [
            {
                "last_name": "Mallick",
                "first_name": "Tanwi"
            },
            {
                "last_name": "Murphy",
                "first_name": "John"
            },
            {
                "last_name": "Bergerson",
                "first_name": "Joshua David"
            },
            {
                "last_name": "Verner",
                "first_name": "Duane R."
            },
            {
                "last_name": "Hutchison",
                "first_name": "John K"
            },
            {
                "last_name": "Levy",
                "first_name": "Leslie-Anne"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Understanding the multifaceted effects of climate change across diverse geographic locations is crucial for timely adaptation and the development of effective mitigation strategies. As the volume of scientific literature on this topic continues to grow exponentially, manually reviewing these documents has become an immensely challenging task. Utilizing Natural Language Processing (NLP) techniques to analyze this wealth of information presents an efficient and scalable solution. By gathering extensive amounts of peer-reviewed articles and studies, we can extract and process critical information about the effects of climate change in specific regions. We employ BERT (Bidirectional Encoder Representations from Transformers) for Named Entity Recognition (NER), which enables us to efficiently identify specific geographies within the climate literature. This, in turn, facilitates location-specific analyses. We conduct region-specific climate trend analyses to pinpoint the predominant themes or concerns related to climate change within a particular area, trace the temporal progression of these identified issues, and evaluate their frequency, severity, and potential development over time. These in-depth examinations of location-specific climate data enable the creation of more customized policy-making, adaptation, and mitigation strategies, addressing each region's unique challenges and providing more effective solutions rooted in data-driven insights. This approach, founded on a thorough exploration of scientific texts, offers actionable insights to a wide range of stakeholders, from policymakers to engineers to environmentalists. By proactively understanding these impacts, societies are better positioned to prepare, allocate resources wisely, and design tailored strategies to cope with future climate conditions, ensuring a more resilient future for all. ",
        "title": "Analyzing Regional Impacts of Climate Change using Natural Language  Processing Techniques",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06820",
        "abstract_url": "http://arxiv.org/abs/2401.06820",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Sihan"
            },
            {
                "last_name": "Kim",
                "first_name": "Youngdae"
            },
            {
                "last_name": "Ren",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Kim",
                "first_name": "Kibaek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  At the heart of power system operations, alternating current optimal power flow (ACOPF) studies the generation of electric power in the most economical way under network-wide load requirement, and can be formulated as a highly structured non-convex quadratically constrained quadratic program (QCQP). Optimization-based solutions to ACOPF (such as ADMM or interior-point method), as the classic approach, require large amount of computation and cannot meet the need to repeatedly solve the problem as load requirement frequently changes. On the other hand, learning-based methods that directly predict the ACOPF solution given the load input incur little computational cost but often generates infeasible solutions (i.e. violate the constraints of ACOPF). In this work, we combine the best of both worlds -- we propose an innovated framework for learning ACOPF, where the input load is mapped to the ACOPF solution through a neural network in a computationally efficient and reliable manner. Key to our innovation is a specific-purpose \"activation function\" defined implicitly by a QCQP and a novel loss, which enforce constraint satisfaction. We show through numerical simulations that our proposed method achieves superior feasibility rate and generation cost in situations where the existing learning-based approaches fail. ",
        "title": "QCQP-Net: Reliably Learning Feasible Alternating Current Optimal Power  Flow Solutions Under Constraints",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06821",
        "abstract_url": "http://arxiv.org/abs/2401.06821",
        "authors": [
            {
                "last_name": "Ducoffe",
                "first_name": "M\u00e9lanie"
            },
            {
                "last_name": "Pov\u00e9da",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Galametz",
                "first_name": "Audrey"
            },
            {
                "last_name": "Boumazouza",
                "first_name": "Ryma"
            },
            {
                "last_name": "Martin",
                "first_name": "Marion-C\u00e9cile"
            },
            {
                "last_name": "Baris",
                "first_name": "Julien"
            },
            {
                "last_name": "Daverschot",
                "first_name": "Derk"
            },
            {
                "last_name": "O'Higgins",
                "first_name": "Eugene"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Surrogate Neural Networks (NN) now routinely serve as substitutes for computationally demanding simulations (e.g., finite element). They enable faster analyses in industrial applications e.g., manufacturing processes, performance assessment. The verification of surrogate models is a critical step to assess their robustness under different scenarios. We explore the combination of empirical and formal methods in one NN verification pipeline. We showcase its efficiency on an industrial use case of aircraft predictive maintenance. We assess the local stability of surrogate NN designed to predict the stress sustained by an aircraft part from external loads. Our contribution lies in the complete verification of the surrogate models that possess a high-dimensional input and output space, thus accommodating multi-objective constraints. We also demonstrate the pipeline effectiveness in substantially decreasing the runtime needed to assess the targeted property. ",
        "title": "Surrogate Neural Networks Local Stability for Aircraft Predictive  Maintenance",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06822",
        "abstract_url": "http://arxiv.org/abs/2401.06822",
        "authors": [
            {
                "last_name": "Sammany",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Steef",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Agami",
                "first_name": "Nedaa"
            },
            {
                "last_name": "Medhat",
                "first_name": "T."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  It is well known over the recent years that measuring the success of projects under the umbrella of project management is inextricably linked with the associated cost, time, and quality. Most of the previous researches in the field assigned a separate mathematical model for each criterion, then numerical methods or search techniques were applied to obtain the optimal trade-off between the three criteria. However in this paper, the problem was addressed by linear multi-objective optimization using only one fuzzy mathematical model. The three criteria were merged in a single non-linear membership function to find the optimal trade-off. Finally, the proposed model is tested and validated using numerical examples. ",
        "title": "Fuzzy Mathematical Model For Optimizing Success Criteria Of Projects: A  Project Management Application",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06823",
        "abstract_url": "http://arxiv.org/abs/2401.06823",
        "authors": [
            {
                "last_name": "Wagle",
                "first_name": "Manoj M"
            },
            {
                "last_name": "Long",
                "first_name": "Siqu"
            },
            {
                "last_name": "Chen",
                "first_name": "Carissa"
            },
            {
                "last_name": "Liu",
                "first_name": "Chunlei"
            },
            {
                "last_name": "Yang",
                "first_name": "Pengyi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recent developments in single-cell omics technologies have enabled the quantification of molecular profiles in individual cells at an unparalleled resolution. Deep learning, a rapidly evolving sub-field of machine learning, has instilled a significant interest in single-cell omics research due to its remarkable success in analysing heterogeneous high-dimensional single-cell omics data. Nevertheless, the inherent multi-layer nonlinear architecture of deep learning models often makes them `black boxes' as the reasoning behind predictions is often unknown and not transparent to the user. This has stimulated an increasing body of research for addressing the lack of interpretability in deep learning models, especially in single-cell omics data analyses, where the identification and understanding of molecular regulators are crucial for interpreting model predictions and directing downstream experimental validations. In this work, we introduce the basics of single-cell omics technologies and the concept of interpretable deep learning. This is followed by a review of the recent interpretable deep learning models applied to various single-cell omics research. Lastly, we highlight the current limitations and discuss potential future directions. We anticipate this review to bring together the single-cell and machine learning research communities to foster future development and application of interpretable deep learning in single-cell omics research. ",
        "title": "Interpretable deep learning in single-cell omics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06824",
        "abstract_url": "http://arxiv.org/abs/2401.06824",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Zheng",
                "first_name": "Xiaoqing"
            },
            {
                "last_name": "Huang",
                "first_name": "Xuanjing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Getting large language models (LLMs) to refuse to answer hostile toxicity questions is a core issue under the theme of LLMs security. Previous approaches have used prompts engineering to jailbreak LLMs and answer some toxicity questions. These approaches can easily fail after the model manufacturer makes additional fine-tuning to the model. To promote the further understanding of model jailbreaking by researchers, we are inspired by Representation Engineering to propose a jailbreaking method that does not require elaborate construction prompts, is not affected by model fine-tuning, and can be widely applied to any open-source LLMs in a pluggable manner. We have evaluated this method on multiple mainstream LLMs on carefully supplemented toxicity datasets, and the experimental results demonstrate the significant effectiveness of our approach. After being surprised by some interesting jailbreaking cases, we did extensive in-depth research to explore the techniques behind this method. ",
        "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation  Engineering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06825",
        "abstract_url": "http://arxiv.org/abs/2401.06825",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Jiangming"
            },
            {
                "last_name": "Yin",
                "first_name": "Xiangbo"
            },
            {
                "last_name": "Chen",
                "first_name": "Yeyun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yachao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhizhong"
            },
            {
                "last_name": "Xie",
                "first_name": "Yuan"
            },
            {
                "last_name": "Qu",
                "first_name": "Yanyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a promising yet challenging retrieval task. The key challenges in USL-VI-ReID are to effectively generate pseudo-labels and establish pseudo-label correspondences across modalities without relying on any prior annotations. Recently, clustered pseudo-label methods have gained more attention in USL-VI-ReID. However, previous methods fell short of fully exploiting the individual nuances, as they simply utilized a single memory that represented an identity to establish cross-modality correspondences, resulting in ambiguous cross-modality correspondences. To address the problem, we propose a Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a Cross-Modality Clustering (CMC) module to generate the pseudo-labels through clustering together both two modality samples. To associate cross-modality clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM) module, ensuring that optimization explicitly focuses on the nuances of individual perspectives and establishes reliable cross-modality correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module to narrow the modality gap while mitigating the effect of noise pseudo-labels through a soft many-to-many alignment strategy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the reliability of the established cross-modality correspondences and the effectiveness of our MMM. The source codes will be released. ",
        "title": "Multi-Memory Matching for Unsupervised Visible-Infrared Person  Re-Identification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06826",
        "abstract_url": "http://arxiv.org/abs/2401.06826",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Jialiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuo"
            },
            {
                "last_name": "Niu",
                "first_name": "Gang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongyuan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Joey Tianyi"
            },
            {
                "last_name": "Gong",
                "first_name": "Chen"
            },
            {
                "last_name": "Sugiyama",
                "first_name": "Masashi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Knowledge Distillation (KD) aims to learn a compact student network using knowledge from a large pre-trained teacher network, where both networks are trained on data from the same distribution. However, in practical applications, the student network may be required to perform in a new scenario (i.e., the target domain), which usually exhibits significant differences from the known scenario of the teacher network (i.e., the source domain). The traditional domain adaptation techniques can be integrated with KD in a two-stage process to bridge the domain gap, but the ultimate reliability of two-stage approaches tends to be limited due to the high computational consumption and the additional errors accumulated from both stages. To solve this problem, we propose a new one-stage method dubbed ``Direct Distillation between Different Domains\" (4Ds). We first design a learnable adapter based on the Fourier transform to separate the domain-invariant knowledge from the domain-specific knowledge. Then, we build a fusion-activation mechanism to transfer the valuable domain-invariant knowledge to the student network, while simultaneously encouraging the adapter within the teacher network to learn the domain-specific knowledge of the target data. As a result, the teacher network can effectively transfer categorical knowledge that aligns with the target domain of the student network. Intensive experiments on various benchmark datasets demonstrate that our proposed 4Ds method successfully produces reliable student networks and outperforms state-of-the-art approaches. ",
        "title": "Direct Distillation between Different Domains",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06827",
        "abstract_url": "http://arxiv.org/abs/2401.06827",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Guiming"
            },
            {
                "last_name": "Shi",
                "first_name": "Kaize"
            },
            {
                "last_name": "Fu",
                "first_name": "Hong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Huaiwen"
            },
            {
                "last_name": "Xu",
                "first_name": "Guandong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses the challenges in V-L models to promote prompt learning across both modalities, which indicates a competitive generalization performance in line with the state-of-the-art. Preeminently, APLe shows robustness and favourable performance in prompt-length experiments with an absolute advantage in adopting the V-L models. ",
        "title": "APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06829",
        "abstract_url": "http://arxiv.org/abs/2401.06829",
        "authors": [
            {
                "last_name": "Baldassini",
                "first_name": "Folco Bertini"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Chang",
                "first_name": "Ching-Chung"
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  A new approach to linguistic watermarking of language models is presented in which information is imperceptibly inserted into the output text while preserving its readability and original meaning. A cross-attention mechanism is used to embed watermarks in the text during inference. Two methods using cross-attention are presented that minimize the effect of watermarking on the performance of a pretrained model. Exploration of different training strategies for optimizing the watermarking and of the challenges and implications of applying this approach in real-world scenarios clarified the tradeoff between watermark robustness and text quality. Watermark selection substantially affects the generated output for high entropy sentences. This proactive watermarking approach has potential application in future model development. ",
        "title": "Cross-Attention Watermarking of Large Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06830",
        "abstract_url": "http://arxiv.org/abs/2401.06830",
        "authors": [
            {
                "last_name": "Manderlier",
                "first_name": "Maxime"
            },
            {
                "last_name": "Lecron",
                "first_name": "Fabian"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG"
        ],
        "abstract": "  The RecSys Challenge 2023, presented by ShareChat, consists to predict if an user will install an application on his smartphone after having seen advertising impressions in ShareChat & Moj apps. This paper presents the solution of 'Team UMONS' to this challenge, giving accurate results (our best score is 6.622686) with a relatively small model that can be easily implemented in different production configurations. Our solution scales well when increasing the dataset size and can be used with datasets containing missing values. ",
        "title": "RecSys Challenge 2023: From data preparation to prediction, a simple,  efficient, robust and scalable solution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06831",
        "abstract_url": "http://arxiv.org/abs/2401.06831",
        "authors": [
            {
                "last_name": "Shoaib",
                "first_name": "Mohamed R."
            },
            {
                "last_name": "Emara",
                "first_name": "Heba M."
            },
            {
                "last_name": "Zhao",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This survey paper explores the transformative influence of frontier AI, foundation models, and Large Language Models (LLMs) in the realm of Intelligent Transportation Systems (ITS), emphasizing their integral role in advancing transportation intelligence, optimizing traffic management, and contributing to the realization of smart cities. Frontier AI refers to the forefront of AI technology, encompassing the latest advancements, innovations, and experimental techniques in the field, especially AI foundation models and LLMs. Foundation models, like GPT-4, are large, general-purpose AI models that provide a base for a wide range of applications. They are characterized by their versatility and scalability. LLMs are obtained from finetuning foundation models with a specific focus on processing and generating natural language. They excel in tasks like language understanding, text generation, translation, and summarization. By leveraging vast textual data, including traffic reports and social media interactions, LLMs extract critical insights, fostering the evolution of ITS. The survey navigates the dynamic synergy between LLMs and ITS, delving into applications in traffic management, integration into autonomous vehicles, and their role in shaping smart cities. It provides insights into ongoing research, innovations, and emerging trends, aiming to inspire collaboration at the intersection of language, intelligence, and mobility for safer, more efficient, and sustainable transportation systems. The paper further surveys interactions between LLMs and various aspects of ITS, exploring roles in traffic management, facilitating autonomous vehicles, and contributing to smart city development, while addressing challenges brought by frontier AI and foundation models. This paper offers valuable inspiration for future research and innovation in the transformative domain of intelligent transportation. ",
        "title": "A Survey on the Applications of Frontier AI, Foundation Models, and  Large Language Models to Intelligent Transportation Systems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06832",
        "abstract_url": "http://arxiv.org/abs/2401.06832",
        "authors": [
            {
                "last_name": "Arisaputra",
                "first_name": "Panji"
            },
            {
                "last_name": "Handoyo",
                "first_name": "Alif Tri"
            },
            {
                "last_name": "Zahra",
                "first_name": "Amalia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  This research paper focuses on the development and evaluation of Automatic Speech Recognition (ASR) technology using the XLS-R 300m model. The study aims to improve ASR performance in converting spoken language into written text, specifically for Indonesian, Javanese, and Sundanese languages. The paper discusses the testing procedures, datasets used, and methodology employed in training and evaluating the ASR systems. The results show that the XLS-R 300m model achieves competitive Word Error Rate (WER) measurements, with a slight compromise in performance for Javanese and Sundanese languages. The integration of a 5-gram KenLM language model significantly reduces WER and enhances ASR accuracy. The research contributes to the advancement of ASR technology by addressing linguistic diversity and improving performance across various languages. The findings provide insights into optimizing ASR accuracy and applicability for diverse linguistic contexts. ",
        "title": "XLS-R Deep Learning Model for Multilingual ASR on Low- Resource  Languages: Indonesian, Javanese, and Sundanese",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06833",
        "abstract_url": "http://arxiv.org/abs/2401.06833",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xue-Fang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Jingjing"
            },
            {
                "last_name": "Chen",
                "first_name": "Wen-Hua"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a comprehensive hierarchical control framework for autonomous decision-making arising in robotics and autonomous systems. In a typical hierarchical control architecture, high-level decision making is often characterised by discrete state and decision/control sets. However, a rational decision is usually affected by not only the discrete states of the autonomous system, but also the underlying continuous dynamics even the evolution of its operational environment. This paper proposes a holistic and comprehensive design process and framework for this type of challenging problems, from new modelling and design problem formulation to control design and stability analysis. It addresses the intricate interplay between traditional continuous systems dynamics utilized at the low levels for control design and discrete Markov decision processes (MDP) for facilitating high-level decision making. We model the decision making system in complex environments as a hybrid system consisting of a controlled MDP and autonomous (i.e. uncontrolled) continuous dynamics. Consequently, the new formulation is called as hybrid Markov decision process (HMDP). The design problem is formulated with a focus on ensuring both safety and optimality while taking into account the influence of both the discrete and continuous state variables of different levels. With the help of the model predictive control (MPC) concept, a decision maker design scheme is proposed for the proposed hybrid decision making model. By carefully designing key ingredients involved in this scheme, it is shown that the recursive feasibility and stability of the proposed autonomous decision making scheme are guaranteed. The proposed framework is applied to develop an autonomous lane changing system for intelligent vehicles. ",
        "title": "A hierarchical control framework for autonomous decision-making systems:  Integrating HMDP and MPC",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06834",
        "abstract_url": "http://arxiv.org/abs/2401.06834",
        "authors": [
            {
                "last_name": "Beinarovich",
                "first_name": "Andrei"
            },
            {
                "last_name": "Stepanov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Zaslavsky",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  The problem is considered of optimizing discrete parameters in the presence of constraints. We use the stochastic sigmoid with temperature and put forward the new adaptive gradient method CONGA. The search for an optimal solution is carried out by a population of individuals. Each of them varies according to gradients of the 'environment' and is characterized by two temperature parameters with different annealing schedules. Unadapted individuals die, and optimal ones interbreed, the result is directed evolutionary dynamics. The proposed method is illustrated using the well-known combinatorial problem for optimal packing of a backpack (0-1 KP). ",
        "title": "Optimization of Discrete Parameters Using the Adaptive Gradient Method  and Directed Evolution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06836",
        "abstract_url": "http://arxiv.org/abs/2401.06836",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zaijing"
            },
            {
                "last_name": "Chen",
                "first_name": "Gongwei"
            },
            {
                "last_name": "Shao",
                "first_name": "Rui"
            },
            {
                "last_name": "Jiang",
                "first_name": "Dongmei"
            },
            {
                "last_name": "Nie",
                "first_name": "Liqiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The Emotional Generation is a subset of emotional intelligence, which aims to output an emotional response based on emotional conditions as input. Emotion generation has a wide range of applications, including emotion chat, emotional visual caption, and emotional rewriting. However, it faces challenges such as a lack of interpretability and poor evaluability. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of Large Language Models (LLMs) on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called EGS. Extensive experimental results demonstrate the effectiveness of ECoT and EGS. Further,we discuss the promise of LLMs in the field of sentiment analysis and present key insights into the LLMs with the ECoT in emotional generation tasks. ",
        "title": "Enhancing the Emotional Generation Capability of Large Language Models  via Emotional Chain-of-Thought",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06837",
        "abstract_url": "http://arxiv.org/abs/2401.06837",
        "authors": [
            {
                "last_name": "Jain",
                "first_name": "Parag"
            },
            {
                "last_name": "Marzoca",
                "first_name": "Andreea"
            },
            {
                "last_name": "Piccinno",
                "first_name": "Francesco"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of +37pp (79%) for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy. ",
        "title": "Structsum Generation for Faster Text Comprehension",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06838",
        "abstract_url": "http://arxiv.org/abs/2401.06838",
        "authors": [
            {
                "last_name": "She",
                "first_name": "Shuaijie"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            },
            {
                "last_name": "Zou",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Geng",
                "first_name": "Xiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in a pivot language is superior to other languages due to the imbalance of multilingual training data.To enhance reasoning abilities in non-pivot languages, we propose an alignment-as-preference optimization framework. Specifically, we adopt an open-source translation model to estimate the consistency between answers in non-pivot and pivot languages. We further adopt the answer consistency as the preference for DPO or PPO thus optimizing the lesser reasoning. Experiments show that our method significantly improves the model's multilingual reasoning, with better reasoning consistency across languages. Our framework achieved a 13.7% accuracy improvement on out-of-domain datasets MSVAMP while preserving the competitive performance on MGSM. Moreover, we find that iterative DPO is helpful for further alignment and improvement of the model's multilingual mathematical reasoning ability, further pushing the improvement to 16.7% ",
        "title": "MAPO: Advancing Multilingual Reasoning through Multilingual  Alignment-as-Preference Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06839",
        "abstract_url": "http://arxiv.org/abs/2401.06839",
        "authors": [
            {
                "last_name": "Gussman",
                "first_name": "Jude"
            },
            {
                "last_name": "Rice",
                "first_name": "Malena"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The properties of exoplanet host stars are traditionally characterized through a detailed forward-modeling analysis of high-resolution spectra. However, many exoplanet radial velocity surveys employ iodine-cell-calibrated spectrographs, such that the vast majority of spectra obtained include an imprinted forest of iodine absorption lines. For surveys that use iodine cells, iodine-free \"template\" spectra must be separately obtained for precise stellar characterization. These template spectra often require extensive additional observing time to obtain, and they are not always feasible to obtain for faint stars. In this paper, we demonstrate that machine learning methods can be applied to infer stellar parameters and chemical abundances from iodine-imprinted spectra with high accuracy and precision. The methods presented in this work are broadly applicable to any iodine-cell-calibrated spectrograph. We make publicly available our spectroscopic pipeline, the Cannon HIRES Iodine Pipeline (CHIP), which derives stellar parameters and 15 chemical abundances from iodine-imprinted spectra of FGK stars and which has been set up for ease of use with Keck/HIRES spectra. Our proof-of-concept offers an efficient new avenue to rapidly estimate a large number of stellar parameters even in the absence of an iodine-free template spectrum. ",
        "title": "Inferring Stellar Parameters from Iodine-Imprinted Keck/HIRES Spectra  with Machine Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06853",
        "abstract_url": "http://arxiv.org/abs/2401.06853",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Siheng"
            },
            {
                "last_name": "Payani",
                "first_name": "Ali"
            },
            {
                "last_name": "Kompella",
                "first_name": "Ramana"
            },
            {
                "last_name": "Fekri",
                "first_name": "Faramarz"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) learn temporal concepts from the co-occurrence of related tokens in a sequence. Compared with conventional text generation, temporal reasoning, which reaches a conclusion based on mathematical, logical and commonsense knowledge, is more challenging. In this paper, we propose TempGraph-LLM, a new paradigm towards text-based temporal reasoning. To be specific, we first teach LLMs to translate the context into a temporal graph. A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for pre-training on this task. We prove in experiments that LLMs benefit from the pre-training on other tasks. On top of that, we guide LLMs to perform symbolic reasoning with the strategies of Chain of Thoughts (CoTs) bootstrapping and special data augmentation. We observe that CoTs with symbolic reasoning bring more consistent and reliable results than those using free text. ",
        "title": "Large Language Models Can Learn Temporal Reasoning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06857",
        "abstract_url": "http://arxiv.org/abs/2401.06857",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jason"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  We show that finding rank-1, rank-2, and rank-3 decompositions of a 3D tensor over a fixed finite field can be done in polynomial time. However, if some cells in the tensor are allowed to have arbitrary values, then rank-2 is NP-hard over the integers modulo 2. We also explore rank-1 decomposition of a 3D tensor and of a matrix where some cells are allowed to have arbitrary values. ",
        "title": "Low-Rank Tensor Decomposition over Finite Fields",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06859",
        "abstract_url": "http://arxiv.org/abs/2401.06859",
        "authors": [
            {
                "last_name": "Atiya",
                "first_name": "Yasseen Sadoon"
            },
            {
                "last_name": "Mobini",
                "first_name": "Zahra"
            },
            {
                "last_name": "Ngo",
                "first_name": "Hien Quoc"
            },
            {
                "last_name": "Matthaiou",
                "first_name": "Michail"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we investigate joint power control and access point (AP) selection scheme in a cell-free massive multiple-input multiple-output (CF-mMIMO) system under an active eavesdropping attack, where an eavesdropper tries to overhear the signal sent to one of the legitimate users by contaminating the uplink channel estimation. We formulate a joint optimization problem to minimize the eavesdropping spectral efficiency (SE) while guaranteeing a given SE requirement at legitimate users. The challenging formulated problem is converted into a more tractable form and an efficient low-complexity accelerated projected gradient (APG)-based approach is proposed to solve it. Our findings reveal that the proposed joint optimization approach significantly outperforms the heuristic approaches in terms of secrecy SE (SSE). For instance, the $50\\%$ likely SSE performance of the proposed approach is $265\\%$ higher than that of equal power allocation and random AP selection scheme. ",
        "title": "Joint Power Optimization and AP Selection for Secure Cell-Free Massive  MIMO",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06864",
        "abstract_url": "http://arxiv.org/abs/2401.06864",
        "authors": [
            {
                "last_name": "Balgi",
                "first_name": "Sourabh"
            },
            {
                "last_name": "Daoud",
                "first_name": "Adel"
            },
            {
                "last_name": "Pe\u00f1a",
                "first_name": "Jose M."
            },
            {
                "last_name": "Wodtke",
                "first_name": "Geoffrey T."
            },
            {
                "last_name": "Zhou",
                "first_name": "Jesse"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Social science theories often postulate causal relationships among a set of variables or events. Although directed acyclic graphs (DAGs) are increasingly used to represent these theories, their full potential has not yet been realized in practice. As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships. Nevertheless, to simplify the task of empirical evaluation, researchers tend to invoke such assumptions anyway, even though they are typically arbitrary and do not reflect any theoretical content or prior knowledge. Moreover, functional form assumptions can engender bias, whenever they fail to accurately capture the complexity of the causal system under investigation. In this article, we introduce causal-graphical normalizing flows (cGNFs), a novel approach to causal inference that leverages deep neural networks to empirically evaluate theories represented as DAGs. Unlike conventional approaches, cGNFs model the full joint distribution of the data according to a DAG supplied by the analyst, without relying on stringent assumptions about functional form. In this way, the method allows for flexible, semi-parametric estimation of any causal estimand that can be identified from the DAG, including total effects, conditional effects, direct and indirect effects, and path-specific effects. We illustrate the method with a reanalysis of Blau and Duncan's (1967) model of status attainment and Zhou's (2019) model of conditional versus controlled mobility. To facilitate adoption, we provide open-source software together with a series of online tutorials for implementing cGNFs. The article concludes with a discussion of current limitations and directions for future development. ",
        "title": "Deep Learning With DAGs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06866",
        "abstract_url": "http://arxiv.org/abs/2401.06866",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Yubin"
            },
            {
                "last_name": "Xu",
                "first_name": "Xuhai"
            },
            {
                "last_name": "McDuff",
                "first_name": "Daniel"
            },
            {
                "last_name": "Breazeal",
                "first_name": "Cynthia"
            },
            {
                "last_name": "Park",
                "first_name": "Hae Won"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is important. This paper investigates the capacity of LLMs to deliver multi-modal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps, GLOBEM, AW_FB, MIT-BIH & MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance. ",
        "title": "Health-LLM: Large Language Models for Health Prediction via Wearable  Sensor Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06868",
        "abstract_url": "http://arxiv.org/abs/2401.06868",
        "authors": [
            {
                "last_name": "Campello",
                "first_name": "Betania Silva Carneiro"
            },
            {
                "last_name": "Duarte",
                "first_name": "Leonardo Tomazeli"
            },
            {
                "last_name": "Romano",
                "first_name": "Jo\u00e3o Marcos Travassos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multicriteria decision analysis (MCDA) is a widely used tool to support decisions in which a set of alternatives should be ranked or classified based on multiple criteria. Recent studies in MCDA have shown the relevance of considering not only current evaluations of each criterion but also past data. Past-data-based approaches carry new challenges, especially in time-varying environments. This study deals with this challenge via essential tools of signal processing, such as tensorial representations and adaptive prediction. More specifically, we structure the criteria' past data as a tensor and, by applying adaptive prediction, we compose signals with these prediction values of the criteria. Besides, we transform the prediction in the time domain into a most favorable decision making domain, called the feature domain. We present a novel extension of the MCDA method PROMETHEE II, aimed at addressing the tensor in the feature domain to obtain a ranking of alternatives. Numerical experiments were performed using real-world time series, and our approach is compared with other existing strategies. The results highlight the relevance and efficiency of our proposal, especially for nonstationary time series. ",
        "title": "Multicriteria decision support employing adaptive prediction in a  tensor-based feature representation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06872",
        "abstract_url": "http://arxiv.org/abs/2401.06872",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "S."
            },
            {
                "last_name": "Magpantay",
                "first_name": "F. M. G."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Edge-based percolation methods can be used to analyze disease transmission on complex social networks. This allows us to include complex social heterogeneity in our models while maintaining tractability. Here we review the seminal works on this field by Newman et al (2001); Newman (2002, 2003), and Miller et al (2012). We present a systematic discussion of the theoretical background behind these models, including an extensive derivation of the major results. We also connect these results relate back to the classical literature in random graph theory Molloy and Reed (1995, 1998). Finally, we also present an accompanying R package that takes epidemic and network parameters as input and generates estimates of the epidemic trajectory and final size. This manuscript and the R package was developed to help researchers easily understand and use network models to investigate the interaction between different community structures and disease transmission. ",
        "title": "Disease Transmission on Random Graphs Using Edge-Based Percolation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06877",
        "abstract_url": "http://arxiv.org/abs/2401.06877",
        "authors": [
            {
                "last_name": "Mehta",
                "first_name": "Maitrey"
            },
            {
                "last_name": "Pyatkin",
                "first_name": "Valentina"
            },
            {
                "last_name": "Srikumar",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants. ",
        "title": "Promptly Predicting Structures: The Return of Inference",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06883",
        "abstract_url": "http://arxiv.org/abs/2401.06883",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Qinyi"
            },
            {
                "last_name": "Khalil",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Shakya",
                "first_name": "Ronas"
            },
            {
                "last_name": "Jovanovic",
                "first_name": "Jelena"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Privacy poses a significant obstacle to the progress of learning analytics (LA), presenting challenges like inadequate anonymization and data misuse that current solutions struggle to address. Synthetic data emerges as a potential remedy, offering robust privacy protection. However, prior LA research on synthetic data lacks thorough evaluation, essential for assessing the delicate balance between privacy and data utility. Synthetic data must not only enhance privacy but also remain practical for data analytics. Moreover, diverse LA scenarios come with varying privacy and utility needs, making the selection of an appropriate synthetic data approach a pressing challenge. To address these gaps, we propose a comprehensive evaluation of synthetic data, which encompasses three dimensions of synthetic data quality, namely resemblance, utility, and privacy. We apply this evaluation to three distinct LA datasets, using three different synthetic data generation methods. Our results show that synthetic data can maintain similar utility (i.e., predictive performance) as real data, while preserving privacy. Furthermore, considering different privacy and data utility requirements in different LA scenarios, we make customized recommendations for synthetic data generation. This paper not only presents a comprehensive evaluation of synthetic data but also illustrates its potential in mitigating privacy concerns within the field of LA, thus contributing to a wider application of synthetic data in LA and promoting a better practice for open science. ",
        "title": "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data  Generation and Evaluation in Learning Analytics",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06885",
        "abstract_url": "http://arxiv.org/abs/2401.06885",
        "authors": [
            {
                "last_name": "Afifi",
                "first_name": "Salma"
            },
            {
                "last_name": "Sunny",
                "first_name": "Febin"
            },
            {
                "last_name": "Nikdast",
                "first_name": "Mahdi"
            },
            {
                "last_name": "Pasricha",
                "first_name": "Sudeep"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  In the rapidly evolving landscape of artificial intelligence, large language models (LLMs) and graph processing have emerged as transformative technologies for natural language processing (NLP), computer vision, and graph-structured data applications. However, the complex structures of these models pose challenges for acceleration on conventional electronic platforms. In this paper, we describe novel hardware accelerators based on silicon photonics to accelerate transformer neural networks that are used in LLMs and graph neural networks for graph data processing. Our analysis demonstrates that both hardware accelerators achieve at least 10.2x throughput improvement and 3.8x better energy efficiency over multiple state-of-the-art electronic hardware accelerators designed for LLMs and graph processing. ",
        "title": "Accelerating Neural Networks for Large Language Models and Graph  Processing with Silicon Photonics",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06889",
        "abstract_url": "http://arxiv.org/abs/2401.06889",
        "authors": [
            {
                "last_name": "Meluso",
                "first_name": "John"
            },
            {
                "last_name": "Casari",
                "first_name": "Amanda"
            },
            {
                "last_name": "McLaughlin",
                "first_name": "Katie"
            },
            {
                "last_name": "Trujillo",
                "first_name": "Milo Z."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Invisible labor is work that is not fully visible, not appropriately compensated, or both. In open source software (OSS) ecosystems, essential tasks that do not involve code (like content moderation) often become invisible to the detriment of individuals and organizations. However, invisible labor is so difficult to measure that we do not know how much of OSS activities are invisible. Our study addresses this challenge, demonstrating that roughly half of OSS work is invisible. We do this by developing a survey technique with cognitive anchoring that measures OSS developer self-assessments of labor visibility and attribution. Survey respondents (n=142) reported that their work is more likely to be nonvisible or partially visible (i.e. visible to at most 1 other person) than fully visible (i.e. visible to 2 or more people). Furthermore, cognitively anchoring participants to the idea of high work visibility increased perceptions of labor visibility and decreased visibility importance compared to anchoring to low work visibility. This suggests that advertising OSS activities as \"open\" may not make labor visible to most people, but rather lead contributors to overestimate labor visibility. We therefore add to a growing body of evidence that designing systems that recognize all kinds of labor as legitimate contributions is likely to improve fairness in software development while providing greater transparency into work designs that help organizations and communities achieve their goals. ",
        "title": "Invisible Labor in Open Source Software Ecosystems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06890",
        "abstract_url": "http://arxiv.org/abs/2401.06890",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Zhili"
            },
            {
                "last_name": "Moshkovitz",
                "first_name": "Michal"
            },
            {
                "last_name": "Di Castro",
                "first_name": "Dotan"
            },
            {
                "last_name": "Kolter",
                "first_name": "J. Zico"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Concept explanation is a popular approach for examining how human-interpretable concepts impact the predictions of a model. However, most existing methods for concept explanations are tailored to specific models. To address this issue, this paper focuses on model-agnostic measures. Specifically, we propose an approach to concept explanations that satisfy three natural axioms: linearity, recursivity, and similarity. We then establish connections with previous concept explanation methods, offering insight into their varying semantic meanings. Experimentally, we demonstrate the utility of the new method by applying it in different scenarios: for model selection, optimizer selection, and model improvement using a kind of prompt editing for zero-shot vision language models. ",
        "title": "An Axiomatic Approach to Model-Agnostic Concept Explanations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06893",
        "abstract_url": "http://arxiv.org/abs/2401.06893",
        "authors": [
            {
                "last_name": "Middleton",
                "first_name": "Jon"
            },
            {
                "last_name": "Bauer",
                "first_name": "Marko"
            },
            {
                "last_name": "Sheng",
                "first_name": "Kaining"
            },
            {
                "last_name": "Johansen",
                "first_name": "Jacob"
            },
            {
                "last_name": "Perslev",
                "first_name": "Mathias"
            },
            {
                "last_name": "Ingala",
                "first_name": "Silvia"
            },
            {
                "last_name": "Nielsen",
                "first_name": "Mads"
            },
            {
                "last_name": "Pai",
                "first_name": "Akshay"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The identification and localisation of pathological tissues in medical images continues to command much attention among deep learning practitioners. When trained on abundant datasets, deep neural networks can match or exceed human performance. However, the scarcity of annotated data complicates the training of these models. Data augmentation techniques can compensate for a lack of training samples. However, many commonly used augmentation methods can fail to provide meaningful samples during model fitting. We present local gamma augmentation, a technique for introducing new instances of intensities in pathological tissues. We leverage local gamma augmentation to compensate for a bias in intensities corresponding to ischemic stroke lesions in human brain MRIs. On three datasets, we show how local gamma augmentation can improve the image-level sensitivity of a deep neural network tasked with ischemic lesion segmentation on magnetic resonance images. ",
        "title": "Local Gamma Augmentation for Ischemic Stroke Lesion Segmentation on MRI",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06894",
        "abstract_url": "http://arxiv.org/abs/2401.06894",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yinbin"
            },
            {
                "last_name": "Tuninetti",
                "first_name": "Daniela"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Coded caching is a technique that leverages locally cached contents at the end users to reduce the network's peak-time communication load. Coded caching has been shown to achieve significant performance gains compared to uncoded schemes and is thus considered a promising technique to boost performance in future networks by effectively trading off bandwidth for storage. The original coded caching model introduced by Maddah-Ali and Niesen does not consider the case where some users involved in the placement phase, may be offline during the delivery phase. If so, the delivery may not start or it may be wasteful to perform the delivery with fictitious demands for the offline users. In addition, the active users may require their demand to be kept private. This paper formally defines a coded caching system where some users are offline, and investigates the optimal performance with and without demand privacy against colluding users. For this novel coded caching model with offline users, achievable and converse bounds are proposed. These bounds are shown to meet under certain conditions, and otherwise to be to within a constant multiplicative gap of one another. In addition, the proposed achievable schemes have lower subpacketization and lower load compared to baseline schemes (that trivially extend known schemes so as to accommodate for privacy) in some memory regimes. ",
        "title": "On Coded Caching Systems with Offline Users, with and without Demand  Privacy against Colluding Users",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06898",
        "abstract_url": "http://arxiv.org/abs/2401.06898",
        "authors": [
            {
                "last_name": "Heddes",
                "first_name": "Mike"
            },
            {
                "last_name": "Srinivasa",
                "first_name": "Narayan"
            },
            {
                "last_name": "Givargis",
                "first_name": "Tony"
            },
            {
                "last_name": "Nicolau",
                "first_name": "Alexandru"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on the machines that can run them. Sparsification of ANNs is often motivated by time, memory and energy savings only during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients during training. We propose an efficient, always-sparse training algorithm with excellent scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during training and inference. Moreover, our guided stochastic exploration algorithm improves over the accuracy of previous sparse training methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG, and ViT models, and compare it against a range of sparsification methods. ",
        "title": "Always-Sparse Training by Growing Connections with Guided Stochastic  Exploration",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06899",
        "abstract_url": "http://arxiv.org/abs/2401.06899",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xiaofei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This article explores the critical role of statistical analysis in precision medicine. It discusses how personalized healthcare is enhanced by statistical methods that interpret complex, multidimensional datasets, focusing on predictive modeling, machine learning algorithms, and data visualization techniques. The paper addresses challenges in data integration and interpretation, particularly with diverse data sources like electronic health records (EHRs) and genomic data. It also delves into ethical considerations such as patient privacy and data security. In addition, the paper highlights the evolution of statistical analysis in medicine, core statistical methodologies in precision medicine, and future directions in the field, emphasizing the integration of artificial intelligence (AI) and machine learning (ML). ",
        "title": "Analyses and Concerns in Precision Medicine: A Statistical Perspective",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06901",
        "abstract_url": "http://arxiv.org/abs/2401.06901",
        "authors": [
            {
                "last_name": "Schneeberger",
                "first_name": "Michael"
            },
            {
                "last_name": "Mastellone",
                "first_name": "Silvia"
            },
            {
                "last_name": "D\u00f6rfler",
                "first_name": "Florian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents a novel safety filter framework based on Control Barrier Functions (CBFs) and Control Lyapunov-like Functions (CLFs). The CBF guarantees forward invariance of the safe set, constraining system trajectories within state constraints, while the CLF guides the system away from unsafe states towards a nominal region, preserving the performance of a nominal controller. The first part of this work focuses on determining compatible CBF and CLF in the presence of linear or quadratic input constraints. This is achieved by formulating the CBF and CLF conditions, along with the input constraints, as Sum of Squares (SOS) constraints using Putinar's Positivstellensatz. For solving the resulting SOS optimization problem, we employ an alternating algorithm that simultaneously searches for a feasible controller in the class of rational functions of the state. The second part of this work details the implementation of the safety filter as a Quadratically Constrained Quadratic Program (QCQP), whose constraints encode the CBF and CLF conditions as well as the input constraints. To avoid the chattering effect and guarantee the uniqueness and Lipschitz continuity of solutions, the state-dependent inequality constraints of the QCQP are selected to be sufficiently regular. Finally, we demonstrate the method on a detailed case study involving the control of a three-phase ac/dc power converter connected to an infinite bus. ",
        "title": "Advanced safety filter based on SOS Control Barrier and Lyapunov  Functions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06908",
        "abstract_url": "http://arxiv.org/abs/2401.06908",
        "authors": [
            {
                "last_name": "Mach",
                "first_name": "Pavel"
            },
            {
                "last_name": "Becvar",
                "first_name": "Zdenek"
            },
            {
                "last_name": "Nikooroo",
                "first_name": "Mohammadsaleh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  In this paper, we focus on offloading a computing task from a user equipment (UE) to a multi-access edge computing (MEC) server via multi-hop relaying. We assume a general relaying case where relays are energy-constrained devices, such as other UEs, internet of things (IoT) devices, or unmanned aerial vehicles. To this end, we formulate the problem as a minimization of the sum energy consumed by the energy-constrained devices under the constraint on the maximum requested time of the task processing. Then, we propose a multi-hop relaying combining half and full duplexes at each individual relay involved in the offloading. We proof that the proposed multi-hop relaying is convex, thus it can be optimized by conventional convex optimization methods. We show our proposal outperforms existing multi-hop relaying schemes in terms of probability that tasks are processed within required time by up to 38\\% and, at the same time, decreases energy consumption by up to 28%. ",
        "title": "Multi-hop Relaying with Mixed Half and Full Duplex Relays for Offloading  to MEC",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06910",
        "abstract_url": "http://arxiv.org/abs/2401.06910",
        "authors": [
            {
                "last_name": "Laitz",
                "first_name": "Thiago"
            },
            {
                "last_name": "Papakostas",
                "first_name": "Konstantinos"
            },
            {
                "last_name": "Lotufo",
                "first_name": "Roberto"
            },
            {
                "last_name": "Nogueira",
                "first_name": "Rodrigo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Despite multi-billion parameter neural rankers being common components of state-of-the-art information retrieval pipelines, they are rarely used in production due to the enormous amount of compute required for inference. In this work, we propose a new method for distilling large rankers into their smaller versions focusing on out-of-domain effectiveness. We introduce InRanker, a version of monoT5 distilled from monoT5-3B with increased effectiveness on out-of-domain scenarios. Our key insight is to use language models and rerankers to generate as much as possible synthetic \"in-domain\" training data, i.e., data that closely resembles the data that will be seen at retrieval time. The pipeline consists of two distillation phases that do not require additional user queries or manual annotations: (1) training on existing supervised soft teacher labels, and (2) training on teacher soft labels for synthetic queries generated using a large language model. Consequently, models like monoT5-60M and monoT5-220M improved their effectiveness by using the teacher's knowledge, despite being 50x and 13x smaller, respectively. Models and code are available at https://github.com/unicamp-dl/InRanker. ",
        "title": "InRanker: Distilled Rankers for Zero-shot Information Retrieval",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06913",
        "abstract_url": "http://arxiv.org/abs/2401.06913",
        "authors": [
            {
                "last_name": "Ryu",
                "first_name": "Myeonghoon"
            },
            {
                "last_name": "Oh",
                "first_name": "Hongseok"
            },
            {
                "last_name": "Lee",
                "first_name": "Suji"
            },
            {
                "last_name": "Park",
                "first_name": "Han"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            "MM"
        ],
        "abstract": "  In this study, we introduce a new augmentation technique to enhance the resilience of sound event classification (SEC) systems against device variability through the use of CycleGAN. We also present a unique dataset to evaluate this method. As SEC systems become increasingly common, it is crucial that they work well with audio from diverse recording devices. Our method addresses limited device diversity in training data by enabling unpaired training to transform input spectrograms as if they are recorded on a different device. Our experiments show that our approach outperforms existing methods in generalization by 5.2% - 11.5% in weighted f1 score. Additionally, it surpasses the current methods in adaptability across diverse recording devices by achieving a 6.5% - 12.8% improvement in weighted f1 score. ",
        "title": "Microphone Conversion: Mitigating Device Variability in Sound Event  Classification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06915",
        "abstract_url": "http://arxiv.org/abs/2401.06915",
        "authors": [
            {
                "last_name": "Reddy",
                "first_name": "Varshini"
            },
            {
                "last_name": "Koncel-Kedziorski",
                "first_name": "Rik"
            },
            {
                "last_name": "Lai",
                "first_name": "Viet Dac"
            },
            {
                "last_name": "Tanner",
                "first_name": "Chris"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems. ",
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06916",
        "abstract_url": "http://arxiv.org/abs/2401.06916",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  While emerging adaptive cruise control (ACC) technologies are making their way into more vehicles, they also expose a vulnerability to potential malicious cyberattacks. Previous research has typically focused on constant or stochastic attacks without explicitly addressing their malicious and covert characteristics. As a result, these attacks may inadvertently benefit the compromised vehicles, inconsistent with real-world scenarios. In contrast, we establish an analytical framework to model and synthesize a range of candidate attacks, offering a physical interpretation from the attacker's standpoint. Specifically, we introduce a mathematical framework that describes mixed traffic scenarios, comprising ACC vehicles and human-driven vehicles (HDVs), grounded in car-following dynamics. Within this framework, we synthesize and integrate a class of false data injection attacks into ACC sensor measurements, influencing traffic flow dynamics. As a first-of-its-kind study, this work provides an analytical characterization of attacks, emphasizing their malicious and stealthy attributes while explicitly accounting for vehicle driving behavior, thereby yielding a set of candidate attacks with physical interpretability. To demonstrate the modeling process, we perform a series of numerical simulations to holistically assess the effects of attacks on car-following dynamics, traffic efficiency, and vehicular fuel consumption. The primary findings indicate that strategically synthesized candidate attacks can cause significant disruptions to the traffic flow while altering the driving behavior of ACC vehicles in a subtle fashion to remain stealthy, which is supported by a series of analytical results. ",
        "title": "An Analytical Framework for Modeling and Synthesizing Malicious Attacks  on ACC Vehicles",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06918",
        "abstract_url": "http://arxiv.org/abs/2401.06918",
        "authors": [
            {
                "last_name": "Brown",
                "first_name": "Ariana N."
            },
            {
                "last_name": "Landman",
                "first_name": "Malena Sabat\u00e9"
            },
            {
                "last_name": "Nagy",
                "first_name": "James G."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This study investigates the iterative regularization properties of two Krylov methods for solving large-scale ill-posed problems: the changing minimal residual Hessenberg method (CMRH) and a novel hybrid variant called the hybrid changing minimal residual Hessenberg method (H-CMRH). Both methods share the advantages of avoiding inner products, making them efficient and highly parallelizable, and particularly suited for implementations that exploit randomization and mixed precision arithmetic. Theoretical results and extensive numerical experiments suggest that H-CMRH exhibits comparable performance to the established hybrid GMRES method in terms of stabilizing semiconvergence, but H-CMRH has does not require any inner products, and requires less work and storage per iteration. ",
        "title": "H-CMRH: a novel inner product free hybrid Krylov method for large-scale  inverse problems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06920",
        "abstract_url": "http://arxiv.org/abs/2401.06920",
        "authors": [
            {
                "last_name": "Vergho",
                "first_name": "Tyler"
            },
            {
                "last_name": "Godbout",
                "first_name": "Jean-Francois"
            },
            {
                "last_name": "Rabbany",
                "first_name": "Reihaneh"
            },
            {
                "last_name": "Pelrine",
                "first_name": "Kellin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent large language models (LLMs) have been shown to be effective for misinformation detection. However, the choice of LLMs for experiments varies widely, leading to uncertain conclusions. In particular, GPT-4 is known to be strong in this domain, but it is closed source, potentially expensive, and can show instability between different versions. Meanwhile, alternative LLMs have given mixed results. In this work, we show that Zephyr-7b presents a consistently viable alternative, overcoming key limitations of commonly used approaches like Llama-2 and GPT-3.5. This provides the research community with a solid open-source option and shows open-source models are gradually catching up on this task. We then highlight how GPT-3.5 exhibits unstable performance, such that this very widely used model could provide misleading results in misinformation detection. Finally, we validate new tools including approaches to structured output and the latest version of GPT-4 (Turbo), showing they do not compromise performance, thus unlocking them for future research and potentially enabling more complex pipelines for misinformation mitigation. ",
        "title": "Comparing GPT-4 and Open-Source Language Models in Misinformation  Mitigation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06922",
        "abstract_url": "http://arxiv.org/abs/2401.06922",
        "authors": [
            {
                "last_name": "Lotfi",
                "first_name": "Fatemeh"
            },
            {
                "last_name": "Afghah",
                "first_name": "Fatemeh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI"
        ],
        "abstract": "  With emerging applications such as autonomous driving, smart cities, and smart factories, network slicing has become an essential component of 5G and beyond networks as a means of catering to a service-aware network. However, managing different network slices while maintaining quality of services (QoS) is a challenge in a dynamic environment. To address this issue, this paper leverages the heterogeneous experiences of distributed units (DUs) in ORAN systems and introduces a novel approach to ORAN slicing xApp using distributed deep reinforcement learning (DDRL). Additionally, to enhance the decision-making performance of the RL agent, a prediction rApp based on long short-term memory (LSTM) is incorporated to provide additional information from the dynamic environment to the xApp. Simulation results demonstrate significant improvements in network performance, particularly in reducing QoS violations. This emphasizes the importance of using the prediction rApp and distributed actors' information jointly as part of a dynamic xApp. ",
        "title": "Open RAN LSTM Traffic Prediction and Slice Management using Deep  Reinforcement Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06923",
        "abstract_url": "http://arxiv.org/abs/2401.06923",
        "authors": [
            {
                "last_name": "Lyu",
                "first_name": "Zimeng"
            },
            {
                "last_name": "Ororbia",
                "first_name": "Alexander"
            },
            {
                "last_name": "Li",
                "first_name": "Rui"
            },
            {
                "last_name": "Desell",
                "first_name": "Travis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Parameter prediction is essential for many applications, facilitating insightful interpretation and decision-making. However, in many real life domains, such as power systems, medicine, and engineering, it can be very expensive to acquire ground truth labels for certain datasets as they may require extensive and expensive laboratory testing. In this work, we introduce a semi-supervised learning approach based on topological projections in self-organizing maps (SOMs), which significantly reduces the required number of labeled data points to perform parameter prediction, effectively exploiting information contained in large unlabeled datasets. Our proposed method first trains SOMs on unlabeled data and then a minimal number of available labeled data points are ultimately assigned to key best matching units (BMU). The values estimated for newly-encountered data points are computed utilizing the average of the $n$ closest labeled data points in the SOM's U-matrix in tandem with a topological shortest path distance calculation scheme. Our results indicate that the proposed semi-supervised model significantly outperforms traditional regression techniques, including linear and polynomial regression, Gaussian process regression, K-nearest neighbors, as well as various deep neural network models. ",
        "title": "Minimally Supervised Learning using Topological Projections in  Self-Organizing Maps",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06925",
        "abstract_url": "http://arxiv.org/abs/2401.06925",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Leihao"
            },
            {
                "last_name": "Zoeter",
                "first_name": "Onno"
            },
            {
                "last_name": "Mooij",
                "first_name": "Joris M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection bias and how the conditioning operation helps with modeling of real-world problems. ",
        "title": "Modeling Latent Selection with Structural Causal Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06930",
        "abstract_url": "http://arxiv.org/abs/2401.06930",
        "authors": [
            {
                "last_name": "Diallo",
                "first_name": "Aissatou"
            },
            {
                "last_name": "Bikakis",
                "first_name": "Antonis"
            },
            {
                "last_name": "Dickens",
                "first_name": "Luke"
            },
            {
                "last_name": "Hunter",
                "first_name": "Anthony"
            },
            {
                "last_name": "Miller",
                "first_name": "Rob"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Decoding the core of procedural texts, exemplified by cooking recipes, is crucial for intelligent reasoning and instruction automation. Procedural texts can be comprehensively defined as a sequential chain of steps to accomplish a task employing resources. From a cooking perspective, these instructions can be interpreted as a series of modifications to a food preparation, which initially comprises a set of ingredients. These changes involve transformations of comestible resources. For a model to effectively reason about cooking recipes, it must accurately discern and understand the inputs and outputs of intermediate steps within the recipe. Aiming to address this, we present a new corpus of cooking recipes enriched with descriptions of intermediate steps of the recipes that explicate the input and output for each step. We discuss the data collection process, investigate and provide baseline models based on T5 and GPT-3.5. This work presents a challenging task and insight into commonsense reasoning and procedural text generation. ",
        "title": "PizzaCommonSense: Learning to Model Commonsense Reasoning about  Intermediate Steps in Cooking Recipes",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06935",
        "abstract_url": "http://arxiv.org/abs/2401.06935",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Kevin"
            },
            {
                "last_name": "Kudugunta",
                "first_name": "Sneha"
            },
            {
                "last_name": "Stella",
                "first_name": "Romina"
            },
            {
                "last_name": "Dev",
                "first_name": "Sunipa"
            },
            {
                "last_name": "Bastings",
                "first_name": "Jasmijn"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY"
        ],
        "abstract": "  Misgendering is the act of referring to someone in a way that does not reflect their gender identity. Translation systems, including foundation models capable of translation, can produce errors that result in misgendering harms. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally underpresented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both dedicated neural machine translation systems and foundation models, and show that all systems exhibit errors resulting in misgendering harms, even in high resource languages. ",
        "title": "MiTTenS: A Dataset for Evaluating Misgendering in Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06936",
        "abstract_url": "http://arxiv.org/abs/2401.06936",
        "authors": [
            {
                "last_name": "Hua",
                "first_name": "Xinru"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Rasool"
            },
            {
                "last_name": "Blanchet",
                "first_name": "Jose"
            },
            {
                "last_name": "Cai",
                "first_name": "Wei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In the field of computational physics and material science, the efficient sampling of rare events occurring at atomic scale is crucial. It aids in understanding mechanisms behind a wide range of important phenomena, including protein folding, conformal changes, chemical reactions and materials diffusion and deformation. Traditional simulation methods, such as Molecular Dynamics and Monte Carlo, often prove inefficient in capturing the timescale of these rare events by brute force. In this paper, we introduce a practical approach by combining the idea of importance sampling with deep neural networks (DNNs) that enhance the sampling of these rare events. In particular, we approximate the variance-free bias potential function with DNNs which is trained to maximize the probability of rare event transition under the importance potential function. This method is easily scalable to high-dimensional problems and provides robust statistical guarantees on the accuracy of the estimated probability of rare event transition. Furthermore, our algorithm can actively generate and learn from any successful samples, which is a novel improvement over existing methods. Using a 2D system as a test bed, we provide comparisons between results obtained from different training strategies, traditional Monte Carlo sampling and numerically solved optimal bias potential function under different temperatures. Our numerical results demonstrate the efficacy of the DNN-based importance sampling of rare events. ",
        "title": "Accelerated Sampling of Rare Events using a Neural Network Bias  Potential",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06945",
        "abstract_url": "http://arxiv.org/abs/2401.06945",
        "authors": [
            {
                "last_name": "Cachola",
                "first_name": "Isabel"
            },
            {
                "last_name": "Cucerzan",
                "first_name": "Silviu"
            },
            {
                "last_name": "Herring",
                "first_name": "Allen"
            },
            {
                "last_name": "Mijovic",
                "first_name": "Vuksan"
            },
            {
                "last_name": "Oveson",
                "first_name": "Erik"
            },
            {
                "last_name": "Jauhar",
                "first_name": "Sujay Kumar"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Authors seeking to communicate with broader audiences often compose their ideas about the same underlying knowledge in different documents and formats -- for example, as slide decks, newsletters, reports, brochures, etc. Prior work in document generation has generally considered the creation of each separate format to be different a task, developing independent methods for generation and evaluation. This approach is suboptimal for the advancement of AI-supported content authoring from both research and application perspectives because it leads to fragmented learning processes, redundancy in models and methods, and disjointed evaluation. Thus, in our work, we consider each of these documents to be templatic views of the same underlying knowledge, and we aim to unify the generation and evaluation of these templatic views of documents. We begin by introducing an LLM-powered method to extract the most important information from an input document and represent this information in a structured format. We show that this unified representation can be used to generate multiple templatic views with no supervision and with very little guidance, improving over strong baselines. We additionally introduce a unified evaluation method that is template agnostic, and can be adapted to building document generators for heterogeneous downstream applications. Finally, we conduct a human evaluation, which shows that humans prefer 82% of the downstream documents generated with our method. Furthermore, the newly proposed evaluation metric correlates more highly with human judgement than prior metrics, while providing a unified evaluation method. ",
        "title": "Knowledge-Centric Templatic Views of Documents",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06946",
        "abstract_url": "http://arxiv.org/abs/2401.06946",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Linlin"
            },
            {
                "last_name": "Yu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Aboah",
                "first_name": "Armstrong"
            },
            {
                "last_name": "Adu-Gyamfi",
                "first_name": "Yaw"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Traffic volume data collection is a crucial aspect of transportation engineering and urban planning, as it provides vital insights into traffic patterns, congestion, and infrastructure efficiency. Traditional manual methods of traffic data collection are both time-consuming and costly. However, the emergence of modern technologies, particularly Light Detection and Ranging (LiDAR), has revolutionized the process by enabling efficient and accurate data collection. Despite the benefits of using LiDAR for traffic data collection, previous studies have identified two major limitations that have impeded its widespread adoption. These are the need for multiple LiDAR systems to obtain complete point cloud information of objects of interest, as well as the labor-intensive process of annotating 3D bounding boxes for object detection tasks. In response to these challenges, the current study proposes an innovative framework that alleviates the need for multiple LiDAR systems and simplifies the laborious 3D annotation process. To achieve this goal, the study employed a single LiDAR system, that aims at reducing the data acquisition cost and addressed its accompanying limitation of missing point cloud information by developing a Point Cloud Completion (PCC) framework to fill in missing point cloud information using point density. Furthermore, we also used zero-shot learning techniques to detect vehicles and pedestrians, as well as proposed a unique framework for extracting low to high features from the object of interest, such as height, acceleration, and speed. Using the 2D bounding box detection and extracted height information, this study is able to generate 3D bounding boxes automatically without human intervention. ",
        "title": "3D Object Detection and High-Resolution Traffic Parameters Extraction  Using Low-Resolution LiDAR Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06947",
        "abstract_url": "http://arxiv.org/abs/2401.06947",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Tong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Caiming"
            },
            {
                "last_name": "Yavuz",
                "first_name": "Semih"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yingbo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy. ",
        "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06948",
        "abstract_url": "http://arxiv.org/abs/2401.06948",
        "authors": [
            {
                "last_name": "Picard",
                "first_name": "Cyril"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Faez"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In engineering design, navigating complex decision-making landscapes demands a thorough exploration of the design, performance, and constraint spaces, often impeded by resource-intensive simulations. Data-driven methods can mitigate this challenge by harnessing historical data to delineate feasible domains, accelerate optimization, or evaluate designs. However, the implementation of these methods usually demands machine-learning expertise and multiple trials to choose the right method and hyperparameters. This makes them less accessible for numerous engineering situations. Additionally, there is an inherent trade-off between training speed and accuracy, with faster methods sometimes compromising precision. In our paper, we demonstrate that a recently released general-purpose transformer-based classification model, TabPFN, is both fast and accurate. Notably, it requires no dataset-specific training to assess new tabular data. TabPFN is a Prior-Data Fitted Network, which undergoes a one-time offline training across a broad spectrum of synthetic datasets and performs in-context learning. We evaluated TabPFN's efficacy across eight engineering design classification problems, contrasting it with seven other algorithms, including a state-of-the-art AutoML method. For these classification challenges, TabPFN consistently outperforms in speed and accuracy. It is also the most data-efficient and provides the added advantage of being differentiable and giving uncertainty estimates. Our findings advocate for the potential of pre-trained models that learn from synthetic data and require no domain-specific tuning to make data-driven engineering design accessible to a broader community and open ways to efficient general-purpose models valid across applications. Furthermore, we share a benchmark problem set for evaluating new classification algorithms in engineering design. ",
        "title": "Fast and Accurate Zero-Training Classification for Tabular Engineering  Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06949",
        "abstract_url": "http://arxiv.org/abs/2401.06949",
        "authors": [
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Skreta",
                "first_name": "Marta"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yuchi"
            },
            {
                "last_name": "Yoshikawa",
                "first_name": "Naruki"
            },
            {
                "last_name": "Som",
                "first_name": "Sagnik"
            },
            {
                "last_name": "Bogdanovic",
                "first_name": "Miroslav"
            },
            {
                "last_name": "Cao",
                "first_name": "Yang"
            },
            {
                "last_name": "Hao",
                "first_name": "Han"
            },
            {
                "last_name": "Xu",
                "first_name": "Haoping"
            },
            {
                "last_name": "Aspuru-Guzik",
                "first_name": "Al\u00e1n"
            },
            {
                "last_name": "Garg",
                "first_name": "Animesh"
            },
            {
                "last_name": "Shkurti",
                "first_name": "Florian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Chemistry experimentation is often resource- and labor-intensive. Despite the many benefits incurred by the integration of advanced and special-purpose lab equipment, many aspects of experimentation are still manually conducted by chemists, for example, polishing an electrode in electrochemistry experiments. Traditional lab automation infrastructure faces challenges when it comes to flexibly adapting to new chemistry experiments. To address this issue, we propose a human-friendly and flexible robotic system, ORGANA, that automates a diverse set of chemistry experiments. It is capable of interacting with chemists in the lab through natural language, using Large Language Models (LLMs). ORGANA keeps scientists informed by providing timely reports that incorporate statistical analyses. Additionally, it actively engages with users when necessary for disambiguation or troubleshooting. ORGANA can reason over user input to derive experiment goals, and plan long sequences of both high-level tasks and low-level robot actions while using feedback from the visual perception of the environment. It also supports scheduling and parallel execution for experiments that require resource allocation and coordination between multiple robots and experiment stations. We show that ORGANA successfully conducts a diverse set of chemistry experiments, including solubility assessment, pH measurement, recrystallization, and electrochemistry experiments. For the latter, we show that ORGANA robustly executes a long-horizon plan, comprising 19 steps executed in parallel, to characterize the electrochemical properties of quinone derivatives, a class of molecules used in rechargeable flow batteries. Our user study indicates that ORGANA significantly improves many aspects of user experience while reducing their physical workload. More details about ORGANA can be found at https://ac-rad.github.io/organa/. ",
        "title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and  Characterization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06951",
        "abstract_url": "http://arxiv.org/abs/2401.06951",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Bai",
                "first_name": "Zhiqi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuanxing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chenchen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ge"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiakai"
            },
            {
                "last_name": "Que",
                "first_name": "Haoran"
            },
            {
                "last_name": "Chen",
                "first_name": "Yukang"
            },
            {
                "last_name": "Su",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Ge",
                "first_name": "Tiezheng"
            },
            {
                "last_name": "Fu",
                "first_name": "Jie"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenhu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we introduce two different augmentation methods on the scale and position index parameters for different samples in training. It aims to make the model more robust to the different relative differences when directly interpolating the arbitrary context length at inference. Comprehensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on challenging long-context tasks. ",
        "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06952",
        "abstract_url": "http://arxiv.org/abs/2401.06952",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Peng"
            },
            {
                "last_name": "Jin",
                "first_name": "Yaochu"
            },
            {
                "last_name": "Dai",
                "first_name": "Xuewu"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhenhua"
            },
            {
                "last_name": "Cui",
                "first_name": "Dongliang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Train timetable rescheduling (TTR) aims to promptly restore the original operation of trains after unexpected disturbances or disruptions. Currently, this work is still done manually by train dispatchers, which is challenging to maintain performance under various problem instances. To mitigate this issue, this study proposes a reinforcement learning-based approach to TTR, which makes the following contributions compared to existing work. First, we design a simple directed graph to represent the TTR problem, enabling the automatic extraction of informative states through graph neural networks. Second, we reformulate the construction process of TTR's solution, not only decoupling the decision model from the problem size but also ensuring the generated scheme's feasibility. Third, we design a learning curriculum for our model to handle the scenarios with different levels of delay. Finally, a simple local search method is proposed to assist the learned decision model, which can significantly improve solution quality with little additional computation cost, further enhancing the practical value of our method. Extensive experimental results demonstrate the effectiveness of our method. The learned decision model can achieve better performance for various problems with varying degrees of train delay and different scales when compared to handcrafted rules and state-of-the-art solvers. ",
        "title": "Reinforcement Learning for Scalable Train Timetable Rescheduling with  Graph Representation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06953",
        "abstract_url": "http://arxiv.org/abs/2401.06953",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Lin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Scoring the driving performance of various drivers on a unified scale, based on how safe or economical they drive on their daily trips, is essential for the driver profile task. Connected vehicles provide the opportunity to collect real-world driving data, which is advantageous for constructing scoring models. However, the lack of pre-labeled scores impede the use of supervised regression models and the data privacy issues hinder the way of traditionally data-centralized learning on the cloud side for model training. To address them, an unsupervised scoring method is presented without the need for labels while still preserving fairness and objectiveness compared to subjective scoring strategies. Subsequently, a federated learning framework based on vehicle-cloud collaboration is proposed as a privacy-friendly alternative to centralized learning. This framework includes a consistently federated version of the scoring method to reduce the performance degradation of the global scoring model caused by the statistical heterogeneous challenge of local data. Theoretical and experimental analysis demonstrate that our federated scoring model is consistent with the utility of the centrally learned counterpart and is effective in evaluating driving performance. ",
        "title": "FedDriveScore: Federated Scoring Driving Behavior with a Mixture of  Metric Distributions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06954",
        "abstract_url": "http://arxiv.org/abs/2401.06954",
        "authors": [
            {
                "last_name": "Ke",
                "first_name": "Zixuan"
            },
            {
                "last_name": "Kong",
                "first_name": "Weize"
            },
            {
                "last_name": "Li",
                "first_name": "Cheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Mei",
                "first_name": "Qiaozhu"
            },
            {
                "last_name": "Bendersky",
                "first_name": "Michael"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, while retrieval has long been established as an effective means of obtaining task-relevant information for humans. Retrieval-augmented Generation (RAG) are known for their effectiveness in knowledge-intensive tasks by locating relevant information and placing it within the context window of the LLM. However, the relationship between retrievers and LLMs is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-friendly information and assembling a LLM-friendly context. In this work, we examine a novel bridge model, validate the ranking and selection assumptions in retrievers in the context of RAG, and propose a training framework that chains together supervised and reinforcement learning to learn a bridge model. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks. ",
        "title": "Bridging the Preference Gap between Retrievers and LLMs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06957",
        "abstract_url": "http://arxiv.org/abs/2401.06957",
        "authors": [
            {
                "last_name": "Nadeem",
                "first_name": "Maryam"
            },
            {
                "last_name": "Imam",
                "first_name": "Raza"
            },
            {
                "last_name": "Al-Refai",
                "first_name": "Rouqaiah"
            },
            {
                "last_name": "Chkir",
                "first_name": "Meriem"
            },
            {
                "last_name": "Hoda",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Saddik",
                "first_name": "Abdulmotaleb El"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars. ",
        "title": "EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge  Distillation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06960",
        "abstract_url": "http://arxiv.org/abs/2401.06960",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Mang"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuoyi"
            },
            {
                "last_name": "Li",
                "first_name": "Chenyue"
            },
            {
                "last_name": "Zheng",
                "first_name": "Wei-Shi"
            },
            {
                "last_name": "Crandall",
                "first_name": "David"
            },
            {
                "last_name": "Du",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Object Re-Identification (Re-ID) aims to identify and retrieve specific objects from varying viewpoints. For a prolonged period, this field has been predominantly driven by deep convolutional neural networks. In recent years, the Transformer has witnessed remarkable advancements in computer vision, prompting an increasing body of research to delve into the application of Transformer in Re-ID. This paper provides a comprehensive review and in-depth analysis of the Transformer-based Re-ID. In categorizing existing works into Image/Video-Based Re-ID, Re-ID with limited data/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly elucidate the advantages demonstrated by the Transformer in addressing a multitude of challenges across these domains. Considering the trending unsupervised Re-ID, we propose a new Transformer baseline, UntransReID, achieving state-of-the-art performance on both single-/cross modal tasks. Besides, this survey also covers a wide range of Re-ID research objects, including progress in animal Re-ID. Given the diversity of species in animal Re-ID, we devise a standardized experimental benchmark and conduct extensive experiments to explore the applicability of Transformer for this task to facilitate future research. Finally, we discuss some important yet under-investigated open issues in the big foundation model era, we believe it will serve as a new handbook for researchers in this field. ",
        "title": "Transformer for Object Re-Identification: A Survey",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06961",
        "abstract_url": "http://arxiv.org/abs/2401.06961",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Yujun"
            },
            {
                "last_name": "Kim",
                "first_name": "Yoon"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yilun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems. And while self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting) have been shown to be helpful, whether LLMs can make use of helpful side information such as problem-specific hints has not been investigated before. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problem-specific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. We further annotate model-generated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle. The dataset and code are available on the project website. ",
        "title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'  Mathematical Reasoning Capabilities",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06962",
        "abstract_url": "http://arxiv.org/abs/2401.06962",
        "authors": [
            {
                "last_name": "Baltag",
                "first_name": "Alexandru"
            },
            {
                "last_name": "van Benthem",
                "first_name": "Johan"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We study knowable informational dependence between empirical questions, modeled as continuous functional dependence between variables in a topological setting. We also investigate epistemic independence in topological terms and show that it is compatible with functional (but non-continuous) dependence. We then proceed to study a stronger notion of knowability based on uniformly continuous dependence. On the technical logical side, we determine the complete logics of languages that combine general functional dependence, continuous dependence, and uniformly continuous dependence. ",
        "title": "Knowability as continuity: a topological account of informational  dependence",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06967",
        "abstract_url": "http://arxiv.org/abs/2401.06967",
        "authors": [
            {
                "last_name": "Katz",
                "first_name": "B. Ross"
            },
            {
                "last_name": "Khan",
                "first_name": "Abdul"
            },
            {
                "last_name": "York-Winegar",
                "first_name": "James"
            },
            {
                "last_name": "Titus",
                "first_name": "Alexander J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Summary: NHANES, the National Health and Nutrition Examination Survey, is a program of studies led by the Centers for Disease Control and Prevention (CDC) designed to assess the health and nutritional status of adults and children in the United States (U.S.). NHANES data is frequently used by biostatisticians and clinical scientists to study health trends across the U.S., but every analysis requires extensive data management and cleaning before use and this repetitive data engineering collectively costs valuable research time and decreases the reproducibility of analyses. Here, we introduce NHANES-GCP, a Cloud Development Kit for Terraform (CDKTF) Infrastructure-as-Code (IaC) and Data Build Tool (dbt) resources built on the Google Cloud Platform (GCP) that automates the data engineering and management aspects of working with NHANES data. With current GCP pricing, NHANES-GCP costs less than $2 to run and less than $15/yr of ongoing costs for hosting the NHANES data, all while providing researchers with clean data tables that can readily be integrated for large-scale analyses. We provide examples of leveraging BigQuery ML to carry out the process of selecting data, integrating data, training machine learning and statistical models, and generating results all from a single SQL-like query. NHANES-GCP is designed to enhance the reproducibility of analyses and create a well-engineered NHANES data resource for statistics, machine learning, and fine-tuning Large Language Models (LLMs).   Availability and implementation\" NHANES-GCP is available at https://github.com/In-Vivo-Group/NHANES-GCP ",
        "title": "NHANES-GCP: Leveraging the Google Cloud Platform and BigQuery ML for  reproducible machine learning with data from the National Health and  Nutrition Examination Survey",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06969",
        "abstract_url": "http://arxiv.org/abs/2401.06969",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Kai"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            },
            {
                "last_name": "Shao",
                "first_name": "Ling"
            },
            {
                "last_name": "Lu",
                "first_name": "Shijian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Large-vocabulary object detectors (LVDs) aim to detect objects of many categories, which learn super objectness features and can locate objects accurately while applied to various downstream data. However, LVDs often struggle in recognizing the located objects due to domain discrepancy in data distribution and object vocabulary. At the other end, recent vision-language foundation models such as CLIP demonstrate superior open-vocabulary recognition capability. This paper presents KGD, a Knowledge Graph Distillation technique that exploits the implicit knowledge graphs (KG) in CLIP for effectively adapting LVDs to various downstream domains. KGD consists of two consecutive stages: 1) KG extraction that employs CLIP to encode downstream domain data as nodes and their feature distances as edges, constructing KG that inherits the rich semantic relations in CLIP explicitly; and 2) KG encapsulation that transfers the extracted KG into LVDs to enable accurate cross-domain object classification. In addition, KGD can extract both visual and textual KG independently, providing complementary vision and language knowledge for object localization and object classification in detection tasks over various downstream domains. Experiments over multiple widely adopted detection benchmarks show that KGD outperforms the state-of-the-art consistently by large margins. ",
        "title": "Domain Adaptation for Large-Vocabulary Object Detectors",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06970",
        "abstract_url": "http://arxiv.org/abs/2401.06970",
        "authors": [
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Zekios",
                "first_name": "Constantinos L."
            },
            {
                "last_name": "Asadizanjani",
                "first_name": "Navid"
            },
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "HC"
        ],
        "abstract": "  Ensemble modeling has been widely used to solve complex problems as it helps to improve overall performance and generalization. In this paper, we propose a novel TemporalAugmenter approach based on ensemble modeling for augmenting the temporal information capturing for long-term and short-term dependencies in data integration of two variations of recurrent neural networks in two learning streams to obtain the maximum possible temporal extraction. Thus, the proposed model augments the extraction of temporal dependencies. In addition, the proposed approach reduces the preprocessing and prior stages of feature extraction, which reduces the required energy to process the models built upon the proposed TemporalAugmenter approach, contributing towards green AI. Moreover, the proposed model can be simply integrated into various domains including industrial, medical, and human-computer interaction applications. Our proposed approach empirically evaluated the speech emotion recognition, electrocardiogram signal, and signal quality examination tasks as three different signals with varying complexity and different temporal dependency features. ",
        "title": "TemporalAugmenter: An Ensemble Recurrent Based Deep Learning Approach  for Signal Classification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06974",
        "abstract_url": "http://arxiv.org/abs/2401.06974",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Cain",
                "first_name": "Amelia"
            },
            {
                "last_name": "De Guzman",
                "first_name": "Erica"
            },
            {
                "last_name": "Chiu",
                "first_name": "Claudia"
            },
            {
                "last_name": "Winstein",
                "first_name": "Carolee J."
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja J."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  An over-reliance on the less-affected limb for functional tasks at the expense of the paretic limb and in spite of recovered capacity is an often-observed phenomenon in survivors of hemispheric stroke. The difference between capacity for use and actual spontaneous use is referred to as arm nonuse. Obtaining an ecologically valid evaluation of arm nonuse is challenging because it requires the observation of spontaneous arm choice for different tasks, which can easily be influenced by instructions, presumed expectations, and awareness that one is being tested. To better quantify arm nonuse, we developed the Bimanual Arm Reaching Test with a Robot (BARTR) for quantitatively assessing arm nonuse in chronic stroke survivors. The BARTR is an instrument that utilizes a robot arm as a means of remote and unbiased data collection of nuanced spatial data for clinical evaluations of arm nonuse. This approach shows promise for determining the efficacy of interventions designed to reduce paretic arm nonuse and enhance functional recovery after stroke. We show that the BARTR satisfies the criteria of an appropriate metric for neurorehabilitative contexts: it is valid, reliable, and simple to use. ",
        "title": "A metric for characterizing the arm nonuse workspace in poststroke  individuals using a robot arm",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06975",
        "abstract_url": "http://arxiv.org/abs/2401.06975",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mengtian"
            },
            {
                "last_name": "Lin",
                "first_name": "Shaohui"
            },
            {
                "last_name": "Wang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Shen",
                "first_name": "Yunhang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baochang"
            },
            {
                "last_name": "Ma",
                "first_name": "Lizhuang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semi-supervised learning (SSL), thanks to the significant reduction of data annotation costs, has been an active research topic for large-scale 3D scene understanding. However, the existing SSL-based methods suffer from severe training bias, mainly due to class imbalance and long-tail distributions of the point cloud data. As a result, they lead to a biased prediction for the tail class segmentation. In this paper, we introduce a new decoupling optimization framework, which disentangles feature representation learning and classifier in an alternative optimization manner to shift the bias decision boundary effectively. In particular, we first employ two-round pseudo-label generation to select unlabeled points across head-to-tail classes. We further introduce multi-class imbalanced focus loss to adaptively pay more attention to feature learning across head-to-tail classes. We fix the backbone parameters after feature learning and retrain the classifier using ground-truth points to update its parameters. Extensive experiments demonstrate the effectiveness of our method outperforming previous state-of-the-art methods on both indoor and outdoor 3D point cloud datasets (i.e., S3DIS, ScanNet-V2, Semantic3D, and SemanticKITTI) using 1% and 1pt evaluation. ",
        "title": "Class-Imbalanced Semi-Supervised Learning for Large-Scale Point Cloud  Semantic Segmentation via Decoupling Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06977",
        "abstract_url": "http://arxiv.org/abs/2401.06977",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Users develop mental models of robots to conceptualize what kind of interactions they can have with those robots. The conceptualizations are often formed before interactions with the robot and are based only on observing the robot's physical design. As a result, understanding conceptualizations formed from physical design is necessary to understand how users intend to interact with the robot. We propose to use multimodal features of robot embodiments to predict what kinds of expectations users will have about a given robot's social and physical capabilities. We show that using such features provides information about general mental models of the robots that generalize across socially interactive robots. We describe how these models can be incorporated into interaction design and physical design for researchers working with socially interactive robots. ",
        "title": "Singing the Body Electric: The Impact of Robot Embodiment on User  Expectations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06978",
        "abstract_url": "http://arxiv.org/abs/2401.06978",
        "authors": [
            {
                "last_name": "Lau",
                "first_name": "Yuen-Fui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianjia"
            },
            {
                "last_name": "Rao",
                "first_name": "Zhefan"
            },
            {
                "last_name": "Chen",
                "first_name": "Qifeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module. ",
        "title": "ENTED: Enhanced Neural Texture Extraction and Distribution for  Reference-based Blind Face Restoration",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06979",
        "abstract_url": "http://arxiv.org/abs/2401.06979",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yang"
            },
            {
                "last_name": "Jia",
                "first_name": "Ya-Hui"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei-Neng"
            },
            {
                "last_name": "Mei",
                "first_name": "Yi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural solvers based on attention mechanism have demonstrated remarkable effectiveness in solving vehicle routing problems. However, in the generalization process from small scale to large scale, we find a phenomenon of the dispersion of attention scores in existing neural solvers, which leads to poor performance. To address this issue, this paper proposes a distance-aware attention reshaping method, assisting neural solvers in solving large-scale vehicle routing problems. Specifically, without the need for additional training, we utilize the Euclidean distance information between current nodes to adjust attention scores. This enables a neural solver trained on small-scale instances to make rational choices when solving a large-scale problem. Experimental results show that the proposed method significantly outperforms existing state-of-the-art neural solvers on the large-scale CVRPLib dataset. ",
        "title": "Distance-aware Attention Reshaping: Enhance Generalization of Neural  Solver for Large-scale Vehicle Routing Problems",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06980",
        "abstract_url": "http://arxiv.org/abs/2401.06980",
        "authors": [
            {
                "last_name": "Saif",
                "first_name": "A F M"
            },
            {
                "last_name": "Cui",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Shen",
                "first_name": "Han"
            },
            {
                "last_name": "Lu",
                "first_name": "Songtao"
            },
            {
                "last_name": "Kingsbury",
                "first_name": "Brian"
            },
            {
                "last_name": "Chen",
                "first_name": "Tianyi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy. ",
        "title": "Joint Unsupervised and Supervised Training for Automatic Speech  Recognition via Bilevel Optimization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06981",
        "abstract_url": "http://arxiv.org/abs/2401.06981",
        "authors": [
            {
                "last_name": "Hathcock",
                "first_name": "Daniel"
            },
            {
                "last_name": "Jin",
                "first_name": "Billy"
            },
            {
                "last_name": "Patton",
                "first_name": "Kalen"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Sherry"
            },
            {
                "last_name": "Zlatin",
                "first_name": "Michael"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We study two problems in online matroid intersection. First, we consider the problem of maximizing the size of a common independent set between a general matroid and a partition matroid whose parts arrive online. This captures the classic online bipartite matching problem when both matroids are partition matroids. Our main result is a $(1 - \\frac{1}{e})$-competitive algorithm for the fractional version of this problem. This applies even for the poly-matroid setting, where the rank function of the offline matroid is replaced with a general monotone submodular function. The key new ingredient for this result is the construction of a ''water level'' vector for poly-matroids, which allows us to generalize the classic water-filling algorithm for online bipartite matching. This construction reveals connections to submodular utility allocation markets and principal partition sequences of matroids.   Our second result concerns the Online Submodular Welfare Maximization (OSWM) problem, in which items arriving online are allocated among a set of agents with the goal of maximizing their overall utility. If the utility function of each agent is a monotone, submodular function over the set of available items, then a simple greedy algorithm achieves a competitive ratio of $\\frac{1}{2}$. Kapralov, Post, and Vondr\\'ak showed that in this case, no polynomial time algorithm achieves a competitive ratio of $\\frac{1}{2} + \\varepsilon$ for any $\\varepsilon > 0$ unless NP = RP (SODA, 2013). We extend the RANKING algorithm of Karp, Vazirani, and Vazirani (STOC, 1990) to achieve an optimal $(1-\\frac{1}{e})$-competitive algorithm for OSWM in the case that the utility function of each agent is the rank function of a matroid. ",
        "title": "Online Matroid Intersection: Submodular Water-Filling and Matroidal  Welfare Maximization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06986",
        "abstract_url": "http://arxiv.org/abs/2401.06986",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Learning fingerprint-like driving style representations is crucial to accurately identify who is behind the wheel in open driving situations. This study explores the learning of driving styles with GPS signals that are currently available in connected vehicles for short-term driver identification. First, an input driving trajectory is windowed into subtrajectories with fixed time lengths. Then, each subtrajectory is further divided into overlapping dynamic segments. For each segment, the local features are obtained by combining statistical and state transitional patterns. Finally, the driving style embedded in each subtrajectory is learned with the proposed regularized recurrent neural network (RNN) for short-term driver identification. We evaluate the impacts of key factors and the effectiveness of the proposed approach on the identification performance of 5 and 10 drivers. The results show that our proposed neural network structure, which complements movement statistics (MS) with state transitions (ST), provides better prediction performance than existing deep learning methods. ",
        "title": "Learning driving style embedding from GPS-derived moving patterns for  driver identification",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06987",
        "abstract_url": "http://arxiv.org/abs/2401.06987",
        "authors": [
            {
                "last_name": "Loutchko",
                "first_name": "Dimitri"
            },
            {
                "last_name": "Sughiyama",
                "first_name": "Yuki"
            },
            {
                "last_name": "Kobayashi",
                "first_name": "Tetsuya J."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Chemical reaction networks (CRN) comprise an important class of models to understand biological functions such as cellular information processing, the robustness and control of metabolic pathways, circadian rhythms, and many more. However, any CRN describing a certain function does not act in isolation but is a part of a much larger network and as such is constantly subject to external changes. In [Shinar, Alon, and Feinberg. \"Sensitivity and robustness in chemical reaction networks.\" SIAM J App Math (2009): 977-998.], the responses of CRN to changes in the linear conserved quantities, called sensitivities, were studied in and the question of how to construct absolute, i.e., basis-independent, sensitivities was raised. In this article, by applying information geometric methods, such a construction is provided. The idea is to track how concentration changes in a particular chemical propagate to changes of all the other chemicals within a steady state. This is encoded in the matrix of absolute sensitivites. A linear algebraic characterization of the matrix of absolute sensitivities for quasi-thermostatic CRN is derived via a Cramer-Rao bound for CRN, which is based on the the analogy between quasi-thermostatic steady states and the exponential family of probability distributions. ",
        "title": "Cramer-Rao bound and absolute sensitivity in chemical reaction networks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06989",
        "abstract_url": "http://arxiv.org/abs/2401.06989",
        "authors": [
            {
                "last_name": "Sivasubramanian",
                "first_name": "Durga"
            },
            {
                "last_name": "Nagalapatti",
                "first_name": "Lokesh"
            },
            {
                "last_name": "Iyer",
                "first_name": "Rishabh"
            },
            {
                "last_name": "Ramakrishnan",
                "first_name": "Ganesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated Learning (FL) is used to learn machine learning models with data that is partitioned across multiple clients, including resource-constrained edge devices. It is therefore important to devise solutions that are efficient in terms of compute, communication, and energy consumption, while ensuring compliance with the FL framework's privacy requirements. Conventional approaches to these problems select a weighted subset of the training dataset, known as coreset, and learn by fitting models on it. Such coreset selection approaches are also known to be robust to data noise. However, these approaches rely on the overall statistics of the training data and are not easily extendable to the FL setup.   In this paper, we propose an algorithm called Gradient based Coreset for Robust and Efficient Federated Learning (GCFL) that selects a coreset at each client, only every $K$ communication rounds and derives updates only from it, assuming the availability of a small validation dataset at the server. We demonstrate that our coreset selection technique is highly effective in accounting for noise in clients' data. We conduct experiments using four real-world datasets and show that GCFL is (1) more compute and energy efficient than FL, (2) robust to various kinds of noise in both the feature space and labels, (3) preserves the privacy of the validation dataset, and (4) introduces a small communication overhead but achieves significant gains in performance, particularly in cases when the clients' data is noisy. ",
        "title": "Gradient Coreset for Federated Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06992",
        "abstract_url": "http://arxiv.org/abs/2401.06992",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Kaiqun"
            },
            {
                "last_name": "Jiang",
                "first_name": "Xiaoling"
            },
            {
                "last_name": "Yu",
                "first_name": "Rui"
            },
            {
                "last_name": "Luo",
                "first_name": "Yonggang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Tian"
            },
            {
                "last_name": "Wu",
                "first_name": "Xi"
            },
            {
                "last_name": "Wei",
                "first_name": "Peng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image compression has been applied in the fields of image storage and video broadcasting. However, it's formidably tough to distinguish the subtle quality differences between those distorted images generated by different algorithms. In this paper, we propose a new image quality assessment framework to decide which image is better in an image group. To capture the subtle differences, a fine-grained network is adopted to acquire multi-scale features. Subsequently, we design a cross subtract block for separating and gathering the information within positive and negative image pairs. Enabling image comparison in feature space. After that, a progressive feature fusion block is designed, which fuses multi-scale features in a novel progressive way. Hierarchical spatial 2D features can thus be processed gradually. Experimental results show that compared with the current mainstream image quality assessment methods, the proposed network can achieve more accurate image quality assessment and ranks second in the benchmark of CLIC in the image perceptual model track. ",
        "title": "Progressive Feature Fusion Network for Enhancing Image Quality  Assessment",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06994",
        "abstract_url": "http://arxiv.org/abs/2401.06994",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Yu"
            },
            {
                "last_name": "Liu",
                "first_name": "Qian"
            },
            {
                "last_name": "Cheng",
                "first_name": "Huayuan"
            },
            {
                "last_name": "Ma",
                "first_name": "Danjiao"
            },
            {
                "last_name": "Dai",
                "first_name": "Hang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Cao",
                "first_name": "Guangzhi"
            },
            {
                "last_name": "Ding",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The past few years have witnessed the rapid development of vision-centric 3D perception in autonomous driving. Although the 3D perception models share many structural and conceptual similarities, there still exist gaps in their feature representations, data formats, and objectives, posing challenges for unified and efficient 3D perception framework design. In this paper, we present UniVision, a simple and efficient framework that unifies two major tasks in vision-centric 3D perception, \\ie, occupancy prediction and object detection. Specifically, we propose an explicit-implicit view transform module for complementary 2D-3D feature transformation. We propose a local-global feature extraction and fusion module for efficient and adaptive voxel and BEV feature extraction, enhancement, and interaction. Further, we propose a joint occupancy-detection data augmentation strategy and a progressive loss weight adjustment strategy which enables the efficiency and stability of the multi-task framework training. We conduct extensive experiments for different perception tasks on four public benchmarks, including nuScenes LiDAR segmentation, nuScenes detection, OpenOccupancy, and Occ3D. UniVision achieves state-of-the-art results with +1.5 mIoU, +1.8 NDS, +1.5 mIoU, and +1.8 mIoU gains on each benchmark, respectively. We believe that the UniVision framework can serve as a high-performance baseline for the unified vision-centric 3D perception task. The code will be available at \\url{https://github.com/Cc-Hy/UniVision}. ",
        "title": "UniVision: A Unified Framework for Vision-Centric 3D Perception",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06995",
        "abstract_url": "http://arxiv.org/abs/2401.06995",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Vishwakarma",
                "first_name": "Dinesh Kumar"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image splice manipulation presents a severe challenge in today's society. With easy access to image manipulation tools, it is easier than ever to modify images that can mislead individuals, organizations or society. In this work, a novel, \"Visually Attentive Splice Localization Network with Multi-Domain Feature Extractor and Multi-Receptive Field Upsampler\" has been proposed. It contains a unique \"visually attentive multi-domain feature extractor\" (VA-MDFE) that extracts attentional features from the RGB, edge and depth domains. Next, a \"visually attentive downsampler\" (VA-DS) is responsible for fusing and downsampling the multi-domain features. Finally, a novel \"visually attentive multi-receptive field upsampler\" (VA-MRFU) module employs multiple receptive field-based convolutions to upsample attentional features by focussing on different information scales. Experimental results conducted on the public benchmark dataset CASIA v2.0 prove the potency of the proposed model. It comfortably beats the existing state-of-the-arts by achieving an IoU score of 0.851, pixel F1 score of 0.9195 and pixel AUC score of 0.8989. ",
        "title": "A Visually Attentive Splice Localization Network with Multi-Domain  Feature Extractor and Multi-Receptive Field Upsampler",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06998",
        "abstract_url": "http://arxiv.org/abs/2401.06998",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Vishwakarma",
                "first_name": "Dinesh Kumar"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Splice detection models are the need of the hour since splice manipulations can be used to mislead, spread rumors and create disharmony in society. However, there is a severe lack of image splicing datasets, which restricts the capabilities of deep learning models to extract discriminative features without overfitting. This manuscript presents two-fold contributions toward splice detection. Firstly, a novel splice detection dataset is proposed having two variants. The two variants include spliced samples generated from code and through manual editing. Spliced images in both variants have corresponding binary masks to aid localization approaches. Secondly, a novel Spatio-Compression Lightweight Splice Detection Framework is proposed for accurate splice detection with minimum computational cost. The proposed dual-branch framework extracts discriminative spatial features from a lightweight spatial branch. It uses original resolution compression data to extract double compression artifacts from the second branch, thereby making it 'information preserving.' Several CNNs are tested in combination with the proposed framework on a composite dataset of images from the proposed dataset and the CASIA v2.0 dataset. The best model accuracy of 0.9382 is achieved and compared with similar state-of-the-art methods, demonstrating the superiority of the proposed framework. ",
        "title": "Towards Effective Image Forensics via A Novel Computationally Efficient  Framework and A New Image Splice Dataset",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06999",
        "abstract_url": "http://arxiv.org/abs/2401.06999",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Vishwakarma",
                "first_name": "Dinesh Kumar"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the large chunks of social media data being created daily and the parallel rise of realistic multimedia tampering methods, detecting and localising tampering in images and videos has become essential. This survey focusses on approaches for tampering detection in multimedia data using deep learning models. Specifically, it presents a detailed analysis of benchmark datasets for malicious manipulation detection that are publicly available. It also offers a comprehensive list of tampering clues and commonly used deep learning architectures. Next, it discusses the current state-of-the-art tampering detection methods, categorizing them into meaningful types such as deepfake detection methods, splice tampering detection methods, copy-move tampering detection methods, etc. and discussing their strengths and weaknesses. Top results achieved on benchmark datasets, comparison of deep learning approaches against traditional methods and critical insights from the recent tampering detection methods are also discussed. Lastly, the research gaps, future direction and conclusion are discussed to provide an in-depth understanding of the tampering detection research arena. ",
        "title": "Datasets, Clues and State-of-the-Arts for Multimedia Forensics: An  Extensive Review",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07001",
        "abstract_url": "http://arxiv.org/abs/2401.07001",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zao"
            },
            {
                "last_name": "Xu",
                "first_name": "Lianming"
            },
            {
                "last_name": "Hou",
                "first_name": "Luyang"
            },
            {
                "last_name": "Li",
                "first_name": "Ruoguang"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  UAV-assisted integrated sensing and communication (ISAC) network is crucial for post-disaster emergency rescue. The speed of UAV deployment will directly impact rescue results. However, the ISAC UAV deployment in emergency scenarios is difficult to solve, which contradicts the rapid deployment. In this paper, we propose a two-stage deployment framework to achieve rapid ISAC UAV deployment in emergency scenarios, which consists of an offline stage and an online stage. Specifically, in the offline stage, we first formulate the ISAC UAV deployment problem and define the ISAC utility as the objective function, which integrates communication rate and localization accuracy. Secondly, we develop a dynamic particle swarm optimization (DPSO) algorithm to construct an optimized UAV deployment dataset. Finally, we train a convolutional neural network (CNN) model with this dataset, which replaces the time-consuming DPSO algorithm. In the online stage, the trained CNN model can be used to make quick decisions for the ISAC UAV deployment. The simulation results indicate that the trained CNN model achieves superior ISAC performance compared to the classic particle swarm optimization algorithm. Additionally, it significantly reduces the deployment time by more than 96%. ",
        "title": "UAV-assisted Emergency Integrated Sensing and Communication Networks: A  CNN-based Rapid Deployment Approach",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07003",
        "abstract_url": "http://arxiv.org/abs/2401.07003",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Jie"
            },
            {
                "last_name": "Xu",
                "first_name": "Yuesheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We studied the use of deep neural networks (DNNs) in the numerical solution of the oscillatory Fredholm integral equation of the second kind. It is known that the solution of the equation exhibits certain oscillatory behaviors due to the oscillation of the kernel. It was pointed out recently that standard DNNs favour low frequency functions, and as a result, they often produce poor approximation for functions containing high frequency components. We addressed this issue in this study. We first developed a numerical method for solving the equation with DNNs as an approximate solution by designing a numerical quadrature that tailors to computing oscillatory integrals involving DNNs. We proved that the error of the DNN approximate solution of the equation is bounded by the training loss and the quadrature error. We then proposed a multi-grade deep learning (MGDL) model to overcome the spectral bias issue of neural networks. Numerical experiments demonstrate that the MGDL model is effective in extracting multiscale information of the oscillatory solution and overcoming the spectral bias issue from which a standard DNN model suffers. ",
        "title": "Deep Neural Network Solutions for Oscillatory Fredholm Integral  Equations",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07004",
        "abstract_url": "http://arxiv.org/abs/2401.07004",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yikai"
            },
            {
                "last_name": "Li",
                "first_name": "Junlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Pengfei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF. ",
        "title": "Extending LLMs' Context Window with 100 Samples",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07009",
        "abstract_url": "http://arxiv.org/abs/2401.07009",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Fan"
            },
            {
                "last_name": "Qi",
                "first_name": "Quan"
            },
            {
                "last_name": "Qin",
                "first_name": "Huaibin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Medical knowledge extraction methods based on edge computing deploy deep learning models on edge devices to achieve localized entity and relation extraction. This approach avoids transferring substantial sensitive data to cloud data centers, effectively safeguarding the privacy of healthcare services. However, existing relation extraction methods mainly employ a sequential pipeline approach, which classifies relations between determined entities after entity recognition. This mode faces challenges such as error propagation between tasks, insufficient consideration of dependencies between the two subtasks, and the neglect of interrelations between different relations within a sentence. To address these challenges, a joint extraction model with parameter sharing in edge computing is proposed, named CoEx-Bert. This model leverages shared parameterization between two models to jointly extract entities and relations. Specifically, CoEx-Bert employs two models, each separately sharing hidden layer parameters, and combines these two loss functions for joint backpropagation to optimize the model parameters. Additionally, it effectively resolves the issue of entity overlapping when extracting knowledge from unstructured Uyghur medical texts by considering contextual relations. Finally, this model is deployed on edge devices for real-time extraction and inference of Uyghur medical knowledge. Experimental results demonstrate that CoEx-Bert outperforms existing state-of-the-art methods, achieving accuracy, recall, and F1 scores of 90.65\\%, 92.45\\%, and 91.54\\%, respectively, in the Uyghur traditional medical literature dataset. These improvements represent a 6.45\\% increase in accuracy, a 9.45\\% increase in recall, and a 7.95\\% increase in F1 score compared to the baseline. ",
        "title": "Joint Extraction of Uyghur Medicine Knowledge with Edge Computing",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07010",
        "abstract_url": "http://arxiv.org/abs/2401.07010",
        "authors": [
            {
                "last_name": "M",
                "first_name": "Shankar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This review article discusses current technological advances in biomedical devices,emphasizing cardiovascular and ophthalmic application diagnostic,monitoring, and prosthetic instruments and systems. The scope encompasses various aspects, including implantable retinal prosthetic devices, portable device for carotid stiffness measurement, automatic identification algorithms for arteries, cuffless evaluation of carotid pulse pressure, wearable neural recording systems, and arterial compliance probes. Additionally, the paper explores advancements in pulse wave velocity measurement, real time heart rate estimation from wrist type signals, and the clinical significance of non invasive pulse wave velocity measurement in assessing arterial stiffness. The synthesis of these studies provides insights into the evolving landscape of biomedical devices, their validation, reproducibility, and potential clinical implications, emphasizing their role in enhancing diagnostics and therapeutic interventions in cardiovascular and ophthalmic domains. ",
        "title": "Advances in Biomedical Devices_A comprehensive Exploration of  Cardiovascular and Ophthalmic Applications",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07012",
        "abstract_url": "http://arxiv.org/abs/2401.07012",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jinli"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ye"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  High-dimensional and incomplete (HDI) matrix contains many complex interactions between numerous nodes. A stochastic gradient descent (SGD)-based latent factor analysis (LFA) model is remarkably effective in extracting valuable information from an HDI matrix. However, such a model commonly encounters the problem of slow convergence because a standard SGD algorithm only considers the current learning error to compute the stochastic gradient without considering the historical and future state of the learning error. To address this critical issue, this paper innovatively proposes an ADRC-incorporated SGD (ADS) algorithm by refining the instance learning error by considering the historical and future state by following the principle of an ADRC controller. With it, an ADS-based LFA model is further achieved for fast and accurate latent factor analysis on an HDI matrix. Empirical studies on two HDI datasets demonstrate that the proposed model outperforms the state-of-the-art LFA models in terms of computational efficiency and accuracy for predicting the missing data of an HDI matrix. ",
        "title": "An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent  Factor Analysis",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07013",
        "abstract_url": "http://arxiv.org/abs/2401.07013",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hongzhan"
            },
            {
                "last_name": "Quan",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hehong"
            },
            {
                "last_name": "Yan",
                "first_name": "Ming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ji"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Closed-source language models such as GPT-4 have achieved remarkable performance. Many recent studies focus on enhancing the capabilities of smaller models through knowledge distillation from closed-source language models. However, due to the incapability to directly access the weights, hidden states, and output distributions of these closed-source models, the distillation can only be performed by fine-tuning smaller models with data samples generated by closed-source language models, which constrains the effectiveness of knowledge distillation. In this paper, we propose to estimate the output distributions of closed-source language models within a Bayesian estimation framework, involving both prior and posterior estimation. The prior estimation aims to derive a prior distribution by utilizing the corpus generated by closed-source language models, while the posterior estimation employs a proxy model to update the prior distribution and derive a posterior distribution. By leveraging the estimated output distribution of closed-source language models, traditional knowledge distillation can be executed. Experimental results demonstrate that our method surpasses the performance of current models directly fine-tuned on data generated by closed-source language models. ",
        "title": "Knowledge Distillation for Closed-Source Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07014",
        "abstract_url": "http://arxiv.org/abs/2401.07014",
        "authors": [
            {
                "last_name": "Hacheme",
                "first_name": "Gilles Quentin"
            },
            {
                "last_name": "Zaytar",
                "first_name": "Akram"
            },
            {
                "last_name": "Tadesse",
                "first_name": "Girmaw Abebe"
            },
            {
                "last_name": "Robinson",
                "first_name": "Caleb"
            },
            {
                "last_name": "Dodhia",
                "first_name": "Rahul"
            },
            {
                "last_name": "Ferres",
                "first_name": "Juan M. Lavista"
            },
            {
                "last_name": "Wood",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cropland mapping can play a vital role in addressing environmental, agricultural, and food security challenges. However, in the context of Africa, practical applications are often hindered by the limited availability of high-resolution cropland maps. Such maps typically require extensive human labeling, thereby creating a scalability bottleneck. To address this, we propose an approach that utilizes unsupervised object clustering to refine existing weak labels, such as those obtained from global cropland maps. The refined labels, in conjunction with sparse human annotations, serve as training data for a semantic segmentation network designed to identify cropland areas. We conduct experiments to demonstrate the benefits of the improved weak labels generated by our method. In a scenario where we train our model with only 33 human-annotated labels, the F_1 score for the cropland category increases from 0.53 to 0.84 when we add the mined negative labels. ",
        "title": "Weak Labeling for Cropland Mapping in Africa",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07017",
        "abstract_url": "http://arxiv.org/abs/2401.07017",
        "authors": [
            {
                "last_name": "Aliabadi",
                "first_name": "Zohreh"
            },
            {
                "last_name": "Kalayc\u0131",
                "first_name": "Tekg\u00fcl"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We study the asymptotic behavior of double and four circulant codes, which are quasi-cyclic codes of index two and four respectively. Exact enumeration results are derived for these families of codes with the prescribed hull dimension. These formulas, in turn, are the most used tools to prove the good behavior of double circulant and four circulant codes asymptotically. Computational results on the code families in consideration are provided as well. ",
        "title": "Asymptotic performance of double and four circulant codes with small  hull dimension",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07020",
        "abstract_url": "http://arxiv.org/abs/2401.07020",
        "authors": [
            {
                "last_name": "Mobarakeh",
                "first_name": "Sayed Amir Mousavi"
            },
            {
                "last_name": "Kazemi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Aarabi",
                "first_name": "Ardalan"
            },
            {
                "last_name": "Danyal",
                "first_name": "Habibollah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Since 2019, the global dissemination of the Coronavirus and its novel strains has resulted in a surge of new infections. The use of X-ray and computed tomography (CT) imaging techniques is critical in diagnosing and managing COVID-19. Incorporating artificial intelligence (AI) into the field of medical imaging is a powerful combination that can provide valuable support to healthcare professionals.This paper focuses on the methodological approach of using machine learning (ML) to enhance medical imaging for COVID-19 diagnosis.For example, deep learning can accurately distinguish lesions from other parts of the lung without human intervention in a matter of minutes.Moreover, ML can enhance performance efficiency by assisting radiologists in making more precise clinical decisions, such as detecting and distinguishing Covid-19 from different respiratory infections and segmenting infections in CT and X-ray images, even when the lesions have varying sizes and shapes.This article critically assesses machine learning methodologies utilized for the segmentation, classification, and detection of Covid-19 within CT and X-ray images, which are commonly employed tools in clinical and hospital settings to represent the lung in various aspects and extensive detail.There is a widespread expectation that this technology will continue to hold a central position within the healthcare sector, driving further progress in the management of the pandemic. ",
        "title": "Empowering Medical Imaging with Artificial Intelligence: A Review of  Machine Learning Approaches for the Detection, and Segmentation of COVID-19  Using Radiographic and Tomographic Images",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07022",
        "abstract_url": "http://arxiv.org/abs/2401.07022",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Fan"
            },
            {
                "last_name": "Qi",
                "first_name": "Quan"
            },
            {
                "last_name": "Qin",
                "first_name": "Huaibin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  In the rapidly advancing information era, various human behaviors are being precisely recorded in the form of data, including identity information, criminal records, and communication data. Law enforcement agencies can effectively maintain social security and precisely combat criminal activities by analyzing the aforementioned data. In comparison to traditional data analysis methods, deep learning models, relying on the robust computational power in cloud centers, exhibit higher accuracy in extracting data features and inferring data. However, within the architecture of cloud centers, the transmission of data from end devices introduces significant latency, hindering real-time inference of data. Furthermore, low-latency edge computing architectures face limitations in direct deployment due to relatively weak computing and storage capacities of nodes. To address these challenges, a lightweight distributed knowledge graph completion architecture is proposed. Firstly, we introduce a lightweight distributed knowledge graph completion architecture that utilizes knowledge graph embedding for data analysis. Subsequently, to filter out substandard data, a personnel data quality assessment method named PDQA is proposed. Lastly, we present a model pruning algorithm that significantly reduces the model size while maximizing performance, enabling lightweight deployment. In experiments, we compare the effects of 11 advanced models on completing the knowledge graph of public security personnel information. The results indicate that the RotatE model outperforms other models significantly in knowledge graph completion, with the pruned model size reduced by 70\\%, and hits@10 reaching 86.97\\%.} ",
        "title": "Edge-Enabled Anomaly Detection and Information Completion for Social  Network Knowledge Graphs",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07028",
        "abstract_url": "http://arxiv.org/abs/2401.07028",
        "authors": [
            {
                "last_name": "Bu",
                "first_name": "Tianhao"
            },
            {
                "last_name": "Lazarou",
                "first_name": "Michalis"
            },
            {
                "last_name": "Stathaki",
                "first_name": "Tania"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image classification has been a popular task due to its feasibility in real-world applications. Training neural networks by feeding them RGB images has demonstrated success over it. Nevertheless, improving the classification accuracy and computational efficiency of this process continues to present challenges that researchers are actively addressing. A widely popular embraced method to improve the classification performance of neural networks is to incorporate data augmentations during the training process. Data augmentations are simple transformations that create slightly modified versions of the training data and can be very effective in training neural networks to mitigate overfitting and improve their accuracy performance. In this study, we draw inspiration from high-boost image filtering and propose an edge enhancement-based method as means to enhance both accuracy and training speed of neural networks. Specifically, our approach involves extracting high frequency features, such as edges, from images within the available dataset and fusing them with the original images, to generate new, enriched images. Our comprehensive experiments, conducted on two distinct datasets CIFAR10 and CALTECH101, and three different network architectures ResNet-18, LeNet-5 and CNN-9 demonstrates the effectiveness of our proposed method. ",
        "title": "Image edge enhancement for effective image classification",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07031",
        "abstract_url": "http://arxiv.org/abs/2401.07031",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Nafis Tanveer"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize generating functional code without adding proper security measures, which also benefits the model by resulting in minimal loss. Therefore, in this work, for security hardening and strengthening of generated code from LLMs, we propose a reinforcement learning-based method for program-specific repair with the combination of semantic and syntactic reward mechanisms that focus heavily on adding security and functional measures in the code, respectively. ",
        "title": "Code Security Vulnerability Repair Using Reinforcement Learning with  Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07033",
        "abstract_url": "http://arxiv.org/abs/2401.07033",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lu"
            },
            {
                "last_name": "Das",
                "first_name": "Mayukh"
            },
            {
                "last_name": "Yang",
                "first_name": "Fangkai"
            },
            {
                "last_name": "Sheng",
                "first_name": "Junjie"
            },
            {
                "last_name": "Qiao",
                "first_name": "Bo"
            },
            {
                "last_name": "Dong",
                "first_name": "Hang"
            },
            {
                "last_name": "Qin",
                "first_name": "Si"
            },
            {
                "last_name": "R\u00fchle",
                "first_name": "Victor"
            },
            {
                "last_name": "Bansal",
                "first_name": "Chetan"
            },
            {
                "last_name": "Cortez",
                "first_name": "Eli"
            },
            {
                "last_name": "Goiri",
                "first_name": "\u00cd\u00f1igo"
            },
            {
                "last_name": "Rajmohan",
                "first_name": "Saravan"
            },
            {
                "last_name": "Lin",
                "first_name": "Qingwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dongmei"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Oversubscription is a prevalent practice in cloud services where the system offers more virtual resources, such as virtual cores in virtual machines, to users or applications than its available physical capacity for reducing revenue loss due to unused/redundant capacity. While oversubscription can potentially lead to significant enhancement in efficient resource utilization, the caveat is that it comes with the risks of overloading and introducing jitter at the level of physical nodes if all the co-located virtual machines have high utilization. Thus suitable oversubscription policies which maximize utilization while mitigating risks are paramount for cost-effective seamless cloud experiences. Most cloud platforms presently rely on static heuristics-driven decisions about oversubscription activation and limits, which either leads to overloading or stranded resources. Designing an intelligent oversubscription policy that can adapt to resource utilization patterns and jointly optimizes benefits and risks is, largely, an unsolved problem. We address this challenge with our proposed novel HuMan-in-the-loop Protoypical Imitation Learning (ProtoHAIL) framework that exploits approximate symmetries in utilization patterns to learn suitable policies. Also, our human-in-the-loop (knowledge-infused) training allows for learning safer policies that are robust to noise and sparsity. Our empirical investigations on real data show orders of magnitude reduction in risk and significant increase in benefits (saving stranded cores) in Microsoft cloud platform for 1st party (internal services). ",
        "title": "Risk-aware Adaptive Virtual CPU Oversubscription in Microsoft Cloud via  Prototypical Human-in-the-loop Imitation Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07035",
        "abstract_url": "http://arxiv.org/abs/2401.07035",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Nafis Tanveer"
            },
            {
                "last_name": "Parra",
                "first_name": "Gonzalo De La Torre"
            },
            {
                "last_name": "Manual",
                "first_name": "Dylan"
            },
            {
                "last_name": "Jadliwala",
                "first_name": "Murtuza"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Open Source Software (OSS) security and resilience are worldwide phenomena hampering economic and technological innovation. OSS vulnerabilities can cause unauthorized access, data breaches, network disruptions, and privacy violations, rendering any benefits worthless. While recent deep-learning techniques have shown great promise in identifying and localizing vulnerabilities in source code, it is unclear how effective these research techniques are from a usability perspective due to a lack of proper methodological analysis. Usually, these methods offload a developer's task of classifying and localizing vulnerable code; still, a reasonable study to measure the actual effectiveness of these systems to the end user has yet to be conducted. To address the challenge of proper developer training from the prior methods, we propose a system to link vulnerabilities to their root cause, thereby intuitively educating the developers to code more securely. Furthermore, we provide a comprehensive usability study to test the effectiveness of our system in fixing vulnerabilities and its capability to assist developers in writing more secure code. We demonstrate the effectiveness of our system by showing its efficacy in helping developers fix source code with vulnerabilities. Our study shows a 24% improvement in code repair capabilities compared to previous methods. We also show that, when trained by our system, on average, approximately 9% of the developers naturally tend to write more secure code with fewer vulnerabilities. ",
        "title": "Causative Insights into Open Source Software Security using Large  Language Code Embeddings and Semantic Vulnerability Graph",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07037",
        "abstract_url": "http://arxiv.org/abs/2401.07037",
        "authors": [
            {
                "last_name": "Chai",
                "first_name": "Linzheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Sun",
                "first_name": "Tao"
            },
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Bing"
            },
            {
                "last_name": "Liang",
                "first_name": "Xiannian"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Tongliang"
            },
            {
                "last_name": "Peng",
                "first_name": "Qiyao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL)) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap. ",
        "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual  Chain-of-Thought Reasoning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07039",
        "abstract_url": "http://arxiv.org/abs/2401.07039",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Chuangtao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qinglin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper introduces the Quantum Generative Diffusion Model (QGDM), a fully quantum-mechanical model for generating quantum state ensembles, inspired by Denoising Diffusion Probabilistic Models. QGDM features a diffusion process that introduces timestep-dependent noise into quantum states, paired with a denoising mechanism trained to reverse this contamination. This model efficiently evolves a completely mixed state into a target quantum state post-training. Our comparative analysis with Quantum Generative Adversarial Networks demonstrates QGDM's superiority, with fidelity metrics exceeding 0.99 in numerical simulations involving up to 4 qubits. Additionally, we present a Resource-Efficient version of QGDM (RE-QGDM), which minimizes the need for auxiliary qubits while maintaining impressive generative capabilities for tasks involving up to 8 qubits. These results showcase the proposed models' potential for tackling challenging quantum generation problems. ",
        "title": "Quantum Generative Diffusion Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07041",
        "abstract_url": "http://arxiv.org/abs/2401.07041",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Sijie"
            },
            {
                "last_name": "Su",
                "first_name": "Ruisheng"
            },
            {
                "last_name": "Su",
                "first_name": "Jianghang"
            },
            {
                "last_name": "Xin",
                "first_name": "Jingmin"
            },
            {
                "last_name": "Wu",
                "first_name": "Jiayi"
            },
            {
                "last_name": "van Zwam",
                "first_name": "Wim"
            },
            {
                "last_name": "van Doormaal",
                "first_name": "Pieter Jan"
            },
            {
                "last_name": "van der Lugt",
                "first_name": "Aad"
            },
            {
                "last_name": "Niessen",
                "first_name": "Wiro J."
            },
            {
                "last_name": "Zheng",
                "first_name": "Nanning"
            },
            {
                "last_name": "van Walsum",
                "first_name": "Theo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate automated extraction of brain vessel centerlines from CTA images plays an important role in diagnosis and therapy of cerebrovascular diseases, such as stroke. However, this task remains challenging due to the complex cerebrovascular structure, the varying imaging quality, and vessel pathology effects. In this paper, we consider automatic lumen segmentation generation without additional annotation effort by physicians and more effective use of the generated lumen segmentation for improved centerline extraction performance. We propose an automated framework for brain vessel centerline extraction from CTA images. The framework consists of four major components: (1) pre-processing approaches that register CTA images with a CT atlas and divide these images into input patches, (2) lumen segmentation generation from annotated vessel centerlines using graph cuts and robust kernel regression, (3) a dual-branch topology-aware UNet (DTUNet) that can effectively utilize the annotated vessel centerlines and the generated lumen segmentation through a topology-aware loss (TAL) and its dual-branch design, and (4) post-processing approaches that skeletonize the predicted lumen segmentation. Extensive experiments on a multi-center dataset demonstrate that the proposed framework outperforms state-of-the-art methods in terms of average symmetric centerline distance (ASCD) and overlap (OV). Subgroup analyses further suggest that the proposed framework holds promise in clinical applications for stroke treatment. Code is publicly available at https://github.com/Liusj-gh/DTUNet. ",
        "title": "An automated framework for brain vessel centerline extraction from CTA  images",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07042",
        "abstract_url": "http://arxiv.org/abs/2401.07042",
        "authors": [
            {
                "last_name": "Barbudo",
                "first_name": "Rafael"
            },
            {
                "last_name": "Ram\u00edrez",
                "first_name": "Aurora"
            },
            {
                "last_name": "Servant",
                "first_name": "Francisco"
            },
            {
                "last_name": "Romero",
                "first_name": "Jos\u00e9 Ra\u00fal"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Design patterns (DPs) are recognised as a good practice in software development. However, the lack of appropriate documentation often hampers traceability, and their benefits are blurred among thousands of lines of code. Automatic methods for DP detection have become relevant but are usually based on the rigid analysis of either software metrics or specific properties of the source code. We propose GEML, a novel detection approach based on evolutionary machine learning using software properties of diverse nature. Firstly, GEML makes use of an evolutionary algorithm to extract those characteristics that better describe the DP, formulated in terms of human-readable rules, whose syntax is conformant with a context-free grammar. Secondly, a rule-based classifier is built to predict whether new code contains a hidden DP implementation. GEML has been validated over five DPs taken from a public repository recurrently adopted by machine learning studies. Then, we increase this number up to 15 diverse DPs, showing its effectiveness and robustness in terms of detection capability. An initial parameter study served to tune a parameter setup whose performance guarantees the general applicability of this approach without the need to adjust complex parameters to a specific pattern. Finally, a demonstration tool is also provided. ",
        "title": "GEML: A Grammar-based Evolutionary Machine Learning Approach for  Design-Pattern Detection",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07043",
        "abstract_url": "http://arxiv.org/abs/2401.07043",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Hgog",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Ritz",
                "first_name": "Fabian"
            },
            {
                "last_name": "Altmann",
                "first_name": "Philipp"
            },
            {
                "last_name": "Zorn",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Stein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Quantum computing offers efficient encapsulation of high-dimensional states. In this work, we propose a novel quantum reinforcement learning approach that combines the Advantage Actor-Critic algorithm with variational quantum circuits by substituting parts of the classical components. This approach addresses reinforcement learning's scalability concerns while maintaining high performance. We empirically test multiple quantum Advantage Actor-Critic configurations with the well known Cart Pole environment to evaluate our approach in control tasks with continuous state spaces. Our results indicate that the hybrid strategy of using either a quantum actor or quantum critic with classical post-processing yields a substantial performance increase compared to pure classical and pure quantum variants with similar parameter counts. They further reveal the limits of current quantum approaches due to the hardware constraints of noisy intermediate-scale quantum computers, suggesting further research to scale hybrid approaches for larger and more complex control tasks. ",
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07044",
        "abstract_url": "http://arxiv.org/abs/2401.07044",
        "authors": [
            {
                "last_name": "Pemberton",
                "first_name": "Joseph"
            },
            {
                "last_name": "Costa",
                "first_name": "Rui Ponte"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Training recurrent neural networks typically relies on backpropagation through time (BPTT). BPTT depends on forward and backward passes to be completed, rendering the network locked to these computations before loss gradients are available. Recently, Jaderberg et al. proposed synthetic gradients to alleviate the need for full BPTT. In their implementation synthetic gradients are learned through a mixture of backpropagated gradients and bootstrapped synthetic gradients, analogous to the temporal difference (TD) algorithm in Reinforcement Learning (RL). However, as in TD learning, heavy use of bootstrapping can result in bias which leads to poor synthetic gradient estimates. Inspired by the accumulate $\\mathrm{TD}(\\lambda)$ in RL, we propose a fully online method for learning synthetic gradients which avoids the use of BPTT altogether: accumulate $BP(\\lambda)$. As in accumulate $\\mathrm{TD}(\\lambda)$, we show analytically that accumulate $\\mathrm{BP}(\\lambda)$ can control the level of bias by using a mixture of temporal difference errors and recursively defined eligibility traces. We next demonstrate empirically that our model outperforms the original implementation for learning synthetic gradients in a variety of tasks, and is particularly suited for capturing longer timescales. Finally, building on recent work we reflect on accumulate $\\mathrm{BP}(\\lambda)$ as a principle for learning in biological circuits. In summary, inspired by RL principles we introduce an algorithm capable of bias-free online learning via synthetic gradients. ",
        "title": "BP(\\lambda): Online Learning via Synthetic Gradients",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07049",
        "abstract_url": "http://arxiv.org/abs/2401.07049",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Stenzel",
                "first_name": "Gerhard"
            },
            {
                "last_name": "Stein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Zielinski",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Ommer",
                "first_name": "Bj\u00f6rn"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation. ",
        "title": "Quantum Denoising Diffusion Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07051",
        "abstract_url": "http://arxiv.org/abs/2401.07051",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lu"
            },
            {
                "last_name": "Das",
                "first_name": "Mayukh"
            },
            {
                "last_name": "Yang",
                "first_name": "Fangkai"
            },
            {
                "last_name": "Duo",
                "first_name": "Chao"
            },
            {
                "last_name": "Qiao",
                "first_name": "Bo"
            },
            {
                "last_name": "Dong",
                "first_name": "Hang"
            },
            {
                "last_name": "Qin",
                "first_name": "Si"
            },
            {
                "last_name": "Bansal",
                "first_name": "Chetan"
            },
            {
                "last_name": "Lin",
                "first_name": "Qingwei"
            },
            {
                "last_name": "Rajmohan",
                "first_name": "Saravan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dongmei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We address the challenge of learning safe and robust decision policies in presence of uncertainty in context of the real scientific problem of adaptive resource oversubscription to enhance resource efficiency while ensuring safety against resource congestion risk.   Traditional supervised prediction or forecasting models are ineffective in learning adaptive policies whereas standard online optimization or reinforcement learning is difficult to deploy on real systems. Offline methods such as imitation learning (IL) are ideal since we can directly leverage historical resource usage telemetry. But, the underlying aleatoric uncertainty in such telemetry is a critical bottleneck.   We solve this with our proposed novel chance-constrained imitation learning framework, which ensures implicit safety against uncertainty in a principled manner via a combination of stochastic (chance) constraints on resource congestion risk and ensemble value functions. This leads to substantial ($\\approx 3-4\\times$) improvement in resource efficiency and safety in many oversubscription scenarios, including resource management in cloud services. ",
        "title": "COIN: Chance-Constrained Imitation Learning for Uncertainty-aware  Adaptive Resource Oversubscription Policy",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07052",
        "abstract_url": "http://arxiv.org/abs/2401.07052",
        "authors": [
            {
                "last_name": "G\u00f3mez-D\u00e9niz",
                "first_name": "Emilio"
            },
            {
                "last_name": "Dorta-Gonz\u00e1lez",
                "first_name": "Pablo"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  When a graphical representation of the cumulative percentage of total citations to articles, ordered from most cited to least cited, is plotted against the cumulative percentage of articles, we obtain a Leimkuhler curve. In this study, we noticed that standard Leimkuhler functions may not be sufficient to provide accurate fits to various empirical informetrics data. Therefore, we introduce a new approach to Leimkuhler curves by fitting a known probability density function to the initial Leimkuhler curve, taking into account the presence of a heterogeneity factor. As a significant contribution to the existing literature, we introduce a pair of mixture distributions (called PG and PIG) to bibliometrics. In addition, we present closed-form expressions for Leimkuhler curves. {Some measures of citation concentration are examined empirically for the basic models (based on the Power {and Pareto distributions}) and the mixed models derived from {these}.} An application to two sources of informetric data was conducted to see how the mixing models outperform the standard basic models. The different models were fitted using non-linear least squares estimation. ",
        "title": "Modeling citation concentration through a mixture of Leimkuhler curves",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07053",
        "abstract_url": "http://arxiv.org/abs/2401.07053",
        "authors": [
            {
                "last_name": "Reimann",
                "first_name": "Lars"
            },
            {
                "last_name": "Kniesel-W\u00fcnsche",
                "first_name": "G\u00fcnter"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Third-party libraries are a cornerstone of fast application development. To enable efficient use, libraries must provide a well-designed API. An obscure API instead slows down the learning process and can lead to erroneous use.   The usual approach to improve the API of a library is to edit its code directly, either keeping the old API but deprecating it (temporarily increasing the API size) or dropping it (introducing breaking changes). If maintainers are unwilling to make such changes, others need to create a hard fork, which they can refactor. But then it is difficult to incorporate changes to the original library, such as bug fixes or performance improvements.   In this paper, we instead explore the use of the adapter pattern to provide a new API as a new library that calls the original library internally. This allows the new library to leverage all implementation changes to the original library, at no additional cost. We call this approach adaptoring. To make the approach practical, we identify API transformations for which adapter code can be generated automatically, and investigate which transformations can be inferred automatically, based on the documentation and usage patterns of the original library. For cases where automated inference is not possible, we present a tool that lets developers manually specify API transformations. Finally, we consider the issue of migrating the generated adapters if the original library introduces breaking changes. We implemented our approach for Python, demonstrating its effectiveness to quickly provide an alternative API even for large libraries. ",
        "title": "Adaptoring: Adapter Generation to Provide an Alternative API for a  Library",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07054",
        "abstract_url": "http://arxiv.org/abs/2401.07054",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Schubert",
                "first_name": "Tom"
            },
            {
                "last_name": "Altmann",
                "first_name": "Philipp"
            },
            {
                "last_name": "Zorn",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Stein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  With recent advancements in quantum computing technology, optimizing quantum circuits and ensuring reliable quantum state preparation have become increasingly vital. Traditional methods often demand extensive expertise and manual calculations, posing challenges as quantum circuits grow in qubit- and gate-count. Therefore, harnessing machine learning techniques to handle the growing variety of gate-to-qubit combinations is a promising approach. In this work, we introduce a comprehensive reinforcement learning environment for quantum circuit synthesis, where circuits are constructed utilizing gates from the the Clifford+T gate set to prepare specific target states. Our experiments focus on exploring the relationship between the depth of synthesized quantum circuits and the circuit depths used for target initialization, as well as qubit count. We organize the environment configurations into multiple evaluation levels and include a range of well-known quantum states for benchmarking purposes. We also lay baselines for evaluating the environment using Proximal Policy Optimization. By applying the trained agents to benchmark tests, we demonstrated their ability to reliably design minimal quantum circuits for a selection of 2-qubit Bell states. ",
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit  Synthesis",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07055",
        "abstract_url": "http://arxiv.org/abs/2401.07055",
        "authors": [
            {
                "last_name": "Bonchi",
                "first_name": "Filippo"
            },
            {
                "last_name": "Di Giorgio",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Haydon",
                "first_name": "Nathan"
            },
            {
                "last_name": "Sobocinski",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We introduce the calculus of neo-Peircean relations, a string diagrammatic extension of the calculus of binary relations that has the same expressivity as first order logic and comes with a complete axiomatisation. The axioms are obtained by combining two well known categorical structures: cartesian and linear bicategories. ",
        "title": "Diagrammatic Algebra of First Order Logic",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07056",
        "abstract_url": "http://arxiv.org/abs/2401.07056",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Erpelding",
                "first_name": "Yannick"
            },
            {
                "last_name": "Ritz",
                "first_name": "Fabian"
            },
            {
                "last_name": "Phan",
                "first_name": "Thomy"
            },
            {
                "last_name": "Illium",
                "first_name": "Steffen"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Recent advances in Multi-Agent Reinforcement Learning have prompted the modeling of intricate interactions between agents in simulated environments. In particular, the predator-prey dynamics have captured substantial interest and various simulations been tailored to unique requirements. To prevent further time-intensive developments, we introduce Aquarium, a comprehensive Multi-Agent Reinforcement Learning environment for predator-prey interaction, enabling the study of emergent behavior. Aquarium is open source and offers a seamless integration of the PettingZoo framework, allowing a quick start with proven algorithm implementations. It features physics-based agent movement on a two-dimensional, edge-wrapping plane. The agent-environment interaction (observations, actions, rewards) and the environment settings (agent speed, prey reproduction, predator starvation, and others) are fully customizable. Besides a resource-efficient visualization, Aquarium supports to record video files, providing a visual comprehension of agent behavior. To demonstrate the environment's capabilities, we conduct preliminary studies which use PPO to train multiple prey agents to evade a predator. In accordance to the literature, we find Individual Learning to result in worse performance than Parameter Sharing, which significantly improves coordination and sample-efficiency. ",
        "title": "Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics  through Multi-Agent Reinforcement Learning Algorithms",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07058",
        "abstract_url": "http://arxiv.org/abs/2401.07058",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Wang",
                "first_name": "Dakuo"
            },
            {
                "last_name": "Yin",
                "first_name": "Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of {second opinions} may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regardless whether the second opinion is generated by a peer or another AI model. However, if decision-makers have the control to decide when to solicit a peer's second opinion, we find that their active solicitations of second opinions have the potential to mitigate over-reliance on AI without inducing increased under-reliance in some cases. We conclude by discussing the implications of our findings for promoting effective human-AI collaborations in decision-making. ",
        "title": "Does More Advice Help? The Effects of Second Opinions in AI-Assisted  Decision Making",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07059",
        "abstract_url": "http://arxiv.org/abs/2401.07059",
        "authors": [
            {
                "last_name": "Ziegler",
                "first_name": "Christian"
            },
            {
                "last_name": "Miranda",
                "first_name": "Marcos"
            },
            {
                "last_name": "Cao",
                "first_name": "Guangye"
            },
            {
                "last_name": "Arentoft",
                "first_name": "Gustav"
            },
            {
                "last_name": "Nam",
                "first_name": "Doo Wan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Our study demonstrates the effective use of Large Language Models (LLMs) for automating the classification of complex datasets. We specifically target proposals of Decentralized Autonomous Organizations (DAOs), as the classification of this data requires the understanding of context and, therefore, depends on human expertise, leading to high costs associated with the task. The study applies an iterative approach to specify categories and further refine them and the prompt in each iteration, which led to an accuracy rate of 95% in classifying a set of 100 proposals. With this, we demonstrate the potential of LLMs to automate data labeling tasks that depend on textual context effectively. ",
        "title": "Classifying Proposals of Decentralized Autonomous Organizations Using  Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07060",
        "abstract_url": "http://arxiv.org/abs/2401.07060",
        "authors": [
            {
                "last_name": "Carpio",
                "first_name": "Ana"
            },
            {
                "last_name": "Cebrian",
                "first_name": "Elena"
            },
            {
                "last_name": "Vidal",
                "first_name": "Perfecto"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Biofilms are bacterial aggregates encased in a self-produced polymeric matrix which attach to moist surfaces and are extremely resistant to chemicals and antibiotics. Recent experiments show that their structure is defined by the interplay of elastic deformations and liquid transport within the biofilm, in response to the cellular activity and the interaction with the surrounding environment. We propose a poroelastic model for elastic deformation and liquid transport in three dimensional biofilms spreading on agar surfaces. The motion of the boundaries can be described by the combined use of Von Karman type approximations for the agar/biofilm interface and thin film approximations for the biofilm/air interface. Bacterial activity informs the macroscopic continuous model through source terms and residual stresses, either phenomenological or derived from microscopic models. We present a procedure to estimate the structure of such residual stresses, based on a simple cellular automata description of bacterial activity. Inspired by image processing, we show that a filtering strategy effectively smooths out the rough tensors provided by the stochastic cellular automata rules, allowing us to insert them in the macroscopic model without numerical instability. ",
        "title": "Biofilms as poroelastic materials",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07061",
        "abstract_url": "http://arxiv.org/abs/2401.07061",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hefeng"
            },
            {
                "last_name": "Ye",
                "first_name": "Guangzhi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Tian",
                "first_name": "Ling"
            },
            {
                "last_name": "Wang",
                "first_name": "Qing"
            },
            {
                "last_name": "Lin",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Learning to recognize novel concepts from just a few image samples is very challenging as the learned model is easily overfitted on the few data and results in poor generalizability. One promising but underexplored solution is to compensate the novel classes by generating plausible samples. However, most existing works of this line exploit visual information only, rendering the generated data easy to be distracted by some challenging factors contained in the few available samples. Being aware of the semantic information in the textual modality that reflects human concepts, this work proposes a novel framework that exploits semantic relations to guide dual-view data hallucination for few-shot image recognition. The proposed framework enables generating more diverse and reasonable data samples for novel classes through effective information transfer from base classes. Specifically, an instance-view data hallucination module hallucinates each sample of a novel class to generate new data by employing local semantic correlated attention and global semantic feature fusion derived from base classes. Meanwhile, a prototype-view data hallucination module exploits semantic-aware measure to estimate the prototype of a novel class and the associated distribution from the few samples, which thereby harvests the prototype as a more stable sample and enables resampling a large number of samples. We conduct extensive experiments and comparisons with state-of-the-art methods on several popular few-shot benchmarks to verify the effectiveness of the proposed framework. ",
        "title": "Dual-View Data Hallucination with Semantic Relation Guidance for  Few-Shot Image Recognition",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07062",
        "abstract_url": "http://arxiv.org/abs/2401.07062",
        "authors": [
            {
                "last_name": "Zong",
                "first_name": "Chen-Chen"
            },
            {
                "last_name": "Wang",
                "first_name": "Ye-Wen"
            },
            {
                "last_name": "Xie",
                "first_name": "Ming-Kun"
            },
            {
                "last_name": "Huang",
                "first_name": "Sheng-Jun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Learning with noisy labels can significantly hinder the generalization performance of deep neural networks (DNNs). Existing approaches address this issue through loss correction or example selection methods. However, these methods often rely on the model's predictions obtained from the softmax function, which can be over-confident and unreliable. In this study, we identify the translation invariance of the softmax function as the underlying cause of this problem and propose the \\textit{Dirichlet-based Prediction Calibration} (DPC) method as a solution. Our method introduces a calibrated softmax function that breaks the translation invariance by incorporating a suitable constant in the exponent term, enabling more reliable model predictions. To ensure stable model training, we leverage a Dirichlet distribution to assign probabilities to predicted labels and introduce a novel evidence deep learning (EDL) loss. The proposed loss function encourages positive and sufficiently large logits for the given label, while penalizing negative and small logits for other labels, leading to more distinct logits and facilitating better example selection based on a large-margin criterion. Through extensive experiments on diverse benchmark datasets, we demonstrate that DPC achieves state-of-the-art performance. The code is available at https://github.com/chenchenzong/DPC. ",
        "title": "Dirichlet-Based Prediction Calibration for Learning with Noisy Labels",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07063",
        "abstract_url": "http://arxiv.org/abs/2401.07063",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Huijia"
            },
            {
                "last_name": "Poskitt",
                "first_name": "Christopher M."
            },
            {
                "last_name": "Sun",
                "first_name": "Yang"
            },
            {
                "last_name": "Sun",
                "first_name": "Jun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuqi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The rapid progress of autonomous vehicles~(AVs) has brought the prospect of a driverless future closer than ever. Recent fatalities, however, have emphasized the importance of safety validation through large-scale testing. Multiple approaches achieve this fully automatically using high-fidelity simulators, i.e., by generating diverse driving scenarios and evaluating autonomous driving systems~(ADSs) against different test oracles. While effective at finding violations, these approaches do not identify the decisions and actions that \\emph{caused} them -- information that is critical for improving the safety of ADSs. To address this challenge, we propose ACAV, an automated framework designed to conduct causality analysis for AV accident recordings in two stages. First, we apply feature extraction schemas based on the messages exchanged between ADS modules, and use a weighted voting method to discard frames of the recording unrelated to the accident. Second, we use safety specifications to identify safety-critical frames and deduce causal events by applying CAT -- our causal analysis tool -- to a station-time graph. We evaluate ACAV on the Apollo ADS, finding that it can identify five distinct types of causal events in 93.64% of 110 accident recordings generated by an AV testing engine. We further evaluated ACAV on 1206 accident recordings collected from versions of Apollo injected with specific faults, finding that it can correctly identify causal events in 96.44% of the accidents triggered by prediction errors, and 85.73% of the accidents triggered by planning errors. ",
        "title": "ACAV: A Framework for Automatic Causality Analysis in Autonomous Vehicle  Accident Recordings",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07065",
        "abstract_url": "http://arxiv.org/abs/2401.07065",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Ling"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ye"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Dynamic graphs (DG) describe dynamic interactions between entities in many practical scenarios. Most existing DG representation learning models combine graph convolutional network and sequence neural network, which model spatial-temporal dependencies through two different types of neural networks. However, this hybrid design cannot well capture the spatial-temporal continuity of a DG. In this paper, we propose a tensor graph convolutional network to learn DG representations in one convolution framework based on the tensor product with the following two-fold ideas: a) representing the information of DG by tensor form; b) adopting tensor product to design a tensor graph convolutional network modeling spatial-temporal feature simultaneously. Experiments on real-world DG datasets demonstrate that our model obtains state-of-the-art performance. ",
        "title": "Tensor Graph Convolutional Network for Dynamic Graph Representation  Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07066",
        "abstract_url": "http://arxiv.org/abs/2401.07066",
        "authors": [
            {
                "last_name": "Rauhameri",
                "first_name": "Anton"
            },
            {
                "last_name": "Robi\u00f1os",
                "first_name": "Angelo"
            },
            {
                "last_name": "Anttalainen",
                "first_name": "Osmo"
            },
            {
                "last_name": "Salpavaara",
                "first_name": "Timo"
            },
            {
                "last_name": "Rantala",
                "first_name": "Jussi"
            },
            {
                "last_name": "Surakka",
                "first_name": "Veikko"
            },
            {
                "last_name": "Kallio",
                "first_name": "Pasi"
            },
            {
                "last_name": "Vehkaoja",
                "first_name": "Antti"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Background: Classification of volatile organic compounds (VOCs) is of interest in many fields. Examples include but are not limited to medicine, detection of explosives, and food quality control. Measurements collected with electronic noses can be used for classification and analysis of VOCs. One type of electronic noses that has seen considerable development in recent years is Differential Mobility Spectrometry (DMS). DMS yields measurements that are visualized as dispersion plots that contain traces, also known as alpha curves. Current methods used for analyzing DMS dispersion plots do not usually utilize the information stored in the continuity of these traces, which suggests that alternative approaches should be investigated.   Results: In this work, for the first time, dispersion plots were interpreted as a series of measurements evolving sequentially. Thus, it was hypothesized that time-series classification algorithms can be effective for classification and analysis of dispersion plots. An extensive dataset of 900 dispersion plots for five chemicals measured at five flow rates and two concentrations was collected. The data was used to analyze the classification performance of six algorithms. According to our hypothesis, the highest classification accuracy of 88\\% was achieved by a Long-Short Term Memory neural network, which supports our hypothesis.   Significance: A new concept for approaching classification tasks of dispersion plots is presented and compared with other well-known classification algorithms. This creates a new angle of view for analysis and classification of the dispersion plots. In addition, a new dataset of dispersion plots is openly shared to public. ",
        "title": "Classification of Volatile Organic Compounds by Differential Mobility  Spectrometry Based on Continuity of Alpha Curves",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07070",
        "abstract_url": "http://arxiv.org/abs/2401.07070",
        "authors": [
            {
                "last_name": "Supantha",
                "first_name": "Subhamon"
            },
            {
                "last_name": "Sharma",
                "first_name": "Naresh Kumar"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  We have used agent-based modeling as our numerical method to artificially simulate a dynamic real economy where agents are rational maximizers of an objective function of Cobb-Douglas type. The economy is characterised by heterogeneous agents, acting out of local or imperfect information, monopolistic competition, perfect product differentiation, allowance for increasing returns to scale technology and trade in disequilibrium. An algorithm for economic activity in each period is devised and a general purpose open source agent-based model is developed which allows for counterfactual inquiries, testing out treatments, analysing causality of various economic processes, outcomes and studying emergent properties. 10,000 simulations, with 10 firms and 80 consumers are run with varying parameters and the results show that from only a few initial conditions the economy reaches equilibrium while in most of the other cases it remains in perpetual disequilibrium. It also shows that from a few initial conditions the economy reaches a disaster where all the consumer wealth falls to zero or only a single producer remains. Furthermore, from some initial conditions, an ideal economy with high wage rate, high consumer utility and no unemployment is also reached. It was also observed that starting from an equal endowment of wealth in consumers and in producers, inequality emerged in the economy. In majority of the cases most of the firms(6-7) shut down because they were not profitable enough and only a few firms remained. Our results highlight that all these varying outcomes are possible for a decentralized market economy with rational optimizing agents. ",
        "title": "A Dynamic Agent Based Model of the Real Economy with Monopolistic  Competition, Perfect Product Differentiation, Heterogeneous Agents,  Increasing Returns to Scale and Trade in Disequilibrium",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07072",
        "abstract_url": "http://arxiv.org/abs/2401.07072",
        "authors": [
            {
                "last_name": "Delgado-P\u00e9rez",
                "first_name": "Pedro"
            },
            {
                "last_name": "Ram\u00edrez",
                "first_name": "Aurora"
            },
            {
                "last_name": "Valle-G\u00f3mez",
                "first_name": "Kevin J."
            },
            {
                "last_name": "Medina-Bulo",
                "first_name": "Inmaculada"
            },
            {
                "last_name": "Romero",
                "first_name": "Jos\u00e9 Ra\u00fal"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Automated test case generation has proven to be useful to reduce the usually high expenses of software testing. However, several studies have also noted the skepticism of testers regarding the comprehension of generated test suites when compared to manually designed ones. This fact suggests that involving testers in the test generation process could be helpful to increase their acceptance of automatically-produced test suites. In this paper, we propose incorporating interactive readability assessments made by a tester into EvoSuite, a widely-known evolutionary test generation tool. Our approach, InterEvo-TR, interacts with the tester at different moments during the search and shows different test cases covering the same coverage target for their subjective evaluation. The design of such an interactive approach involves a schedule of interaction, a method to diversify the selected targets, a plan to save and handle the readability values, and some mechanisms to customize the level of engagement in the revision, among other aspects. To analyze the potential and practicability of our proposal, we conduct a controlled experiment in which 39 participants, including academics, professional developers, and student collaborators, interact with InterEvo-TR. Our results show that the strategy to select and present intermediate results is effective for the purpose of readability assessment. Furthermore, the participants' actions and responses to a questionnaire allowed us to analyze the aspects influencing test code readability and the benefits and limitations of an interactive approach in the context of test case generation, paving the way for future developments based on interactivity. ",
        "title": "InterEvo-TR: Interactive Evolutionary Test Generation With Readability  Assessment",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07074",
        "abstract_url": "http://arxiv.org/abs/2401.07074",
        "authors": [
            {
                "last_name": "Hansen",
                "first_name": "Henri"
            },
            {
                "last_name": "Kanniainen",
                "first_name": "Juho"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In this paper, we introduce the Detachment Problem. It can be seen as a generalized Vaccination Problem. The aim is to optimally cut the individuals' ties to circles that connect them to others, to minimize the overall information transfer in a social network. When an individual is isolated from a particular circle, it leads to the elimination of the connections to all the members of that circle, yet the connections to other circles remain. This approach contrasts with the conventional vaccination problem, in which a subset of vertices is totally eliminated. In our case, the connections of individuals to their circles are selectively, rather than entirely, eliminated. Contextually, this article focuses on private information flows, specifically within networks formed by memberships in circles of insiders in companies. Our quasi-empirical study uses simulated information flows on an observable network, and the statistical properties of the simulated information flows are matched with real-world data. In a broader context, this paper presents the Detachment Problem as a versatile approach for optimal social distancing, applicable across various scenarios. We propose and define a concept of expected proportional outside influence, or EPOI, as measure of how widespread information leak is. We also implement a greedy algorithm for finding a set of detachments to minimize EPOI. For comparison, we devise a simple heuristic based on minimal cut, to separate the most influential circles from each other. We provide evidence that the greedy algorithm is not optimal, and it is sometimes outperformed by the simple heuristic minimum cut algorithm, However, the greedy algorithm outperforms the cut algorithm in most cases. Further avenues of research are discussed. ",
        "title": "Detachment Problem -- Application in Prevention of Information Leakage  in Stock Markets",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07078",
        "abstract_url": "http://arxiv.org/abs/2401.07078",
        "authors": [
            {
                "last_name": "Sravanthi",
                "first_name": "Settaluri Lakshmi"
            },
            {
                "last_name": "Doshi",
                "first_name": "Meet"
            },
            {
                "last_name": "Kalyan",
                "first_name": "Tankala Pavan"
            },
            {
                "last_name": "Murthy",
                "first_name": "Rudra"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Pushpak"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  LLMs have demonstrated remarkable capability for understanding semantics, but they often struggle with understanding pragmatics. To demonstrate this fact, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely, Implicature, Presupposition, Reference, and Deixis. We curated high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k of which have been created by us, and the rest are adapted from existing datasets. We evaluated nine models varying in the number of parameters and type of training. Our study indicates that fine-tuning for instruction-following and chat significantly enhances the pragmatics capabilities of smaller language models. However, for larger models, the base versions perform comparably with their chat-adapted counterparts. Additionally, there is a noticeable performance gap between human capabilities and model capabilities. Furthermore, unlike the consistent performance of humans across various tasks, the models demonstrate variability in their proficiency, with performance levels fluctuating due to different hints and the complexities of tasks within the same dataset. Overall, the benchmark aims to provide a comprehensive evaluation of LLM's ability to handle real-world language tasks that require pragmatic reasoning. ",
        "title": "PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics  Capabilities",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07080",
        "abstract_url": "http://arxiv.org/abs/2401.07080",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Haibin"
            },
            {
                "last_name": "Ye",
                "first_name": "Maoyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jing"
            },
            {
                "last_name": "Liu",
                "first_name": "Juhua"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we highlight a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching achieves impressive performance on two public benchmarks, e.g., setting a new record on the ICDAR15-video dataset, and one novel test set with arbitrary-shaped text, while saving considerable training budgets. The code will be released at https://github.com/Hxyz-123/GoMatching. ",
        "title": "GoMatching: A Simple Baseline for Video Text Spotting via Long and Short  Term Matching",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07081",
        "abstract_url": "http://arxiv.org/abs/2401.07081",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhichao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoxin"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yanan"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The discovery of active IPv6 addresses represents a pivotal challenge in IPv6 network survey, as it is a prerequisite for downstream tasks such as network topology measurements and security analysis. With the rapid spread of IPv6 networks in recent years, many researchers have focused on improving the hit rate, efficiency, and coverage of IPv6 scanning methods, resulting in considerable advancements. However, existing approaches remain heavily dependent on seed addresses, thereby limiting their effectiveness in unseeded prefixes. Consequently, this paper proposes 6Rover, a reinforcement learning-based model for active address discovery in unseeded environments. To overcome the reliance on seeded addresses, 6Rover constructs patterns with higher generality that reflects the actual address allocation strategies of network administrators, thereby avoiding biased transfers of patterns from seeded to unseeded prefixes. After that, 6Rover employs a multi-armed bandit model to optimize the probing resource allocation when applying patterns to unseeded spaces. It models the challenge of discovering optimal patterns in unseeded spaces as an exploration-exploitation dilemma, and progressively uncover the potential patterns applied in unseeded spaces, leading to the efficient discovery of active addresses without seed address as the prior knowledge. Experiments on large-scale unseeded datasets show that 6Rover has a higher hit rate than existing methods in the absence of any seed addresses as prior knowledge. In real network environments, 6Rover achieved a 5% - 8% hit rate in seedless spaces with 100 million budget scale, representing an approximate 200\\% improvement over the existing state-of-the-art methods. ",
        "title": "6Rover: Leveraging Reinforcement Learning-based Address Pattern Mining  Approach for Discovering Active Targets in IPv6 Unseeded Space",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07084",
        "abstract_url": "http://arxiv.org/abs/2401.07084",
        "authors": [
            {
                "last_name": "Veerendranath",
                "first_name": "Vishruth"
            },
            {
                "last_name": "Masti",
                "first_name": "Vibha"
            },
            {
                "last_name": "Gupta",
                "first_name": "Utkarsh"
            },
            {
                "last_name": "Chaudhuri",
                "first_name": "Hrishit"
            },
            {
                "last_name": "Srinivasa",
                "first_name": "Gowri"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "SD"
        ],
        "abstract": "  Film scores are considered an essential part of the film cinematic experience, but the process of film score generation is often expensive and infeasible for small-scale creators. Automating the process of film score composition would provide useful starting points for music in small projects. In this paper, we propose a two-stage pipeline for generating music from a movie script. The first phase is the Sentiment Analysis phase where the sentiment of a scene from the film script is encoded into the valence-arousal continuous space. The second phase is the Conditional Music Generation phase which takes as input the valence-arousal vector and conditionally generates piano MIDI music to match the sentiment. We study the efficacy of various music generation architectures by performing a qualitative user survey and propose methods to improve sentiment-conditioning in VAE architectures. ",
        "title": "ScripTONES: Sentiment-Conditioned Music Generation for Movie Scripts",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07085",
        "abstract_url": "http://arxiv.org/abs/2401.07085",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Yizhou"
            },
            {
                "last_name": "Ziyin",
                "first_name": "Liu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We identify and solve a hidden-layer model that is analytically tractable at any finite width and whose limits exhibit both the kernel phase and the feature learning phase. We analyze the phase diagram of this model in all possible limits of common hyperparameters including width, layer-wise learning rates, scale of output, and scale of initialization. We apply our result to analyze how and when feature learning happens in both infinite and finite-width models. Three prototype mechanisms of feature learning are identified: (1) learning by alignment, (2) learning by disalignment, and (3) learning by rescaling. In sharp contrast, neither of these mechanisms is present when the model is in the kernel regime. This discovery explains why large initialization often leads to worse performance. Lastly, we empirically demonstrate that discoveries we made for this analytical model also appear in nonlinear networks in real tasks. ",
        "title": "When Does Feature Learning Happen? Perspective from an Analytically  Solvable Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07087",
        "abstract_url": "http://arxiv.org/abs/2401.07087",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junxi"
            },
            {
                "last_name": "Dong",
                "first_name": "Junhao"
            },
            {
                "last_name": "Xie",
                "first_name": "Xiaohua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR",
            "LG"
        ],
        "abstract": "  Recently, many studies utilized adversarial examples (AEs) to raise the cost of malicious image editing and copyright violation powered by latent diffusion models (LDMs). Despite their successes, a few have studied the surrogate model they used to generate AEs. In this paper, from the perspective of adversarial transferability, we investigate how the surrogate model's property influences the performance of AEs for LDMs. Specifically, we view the time-step sampling in the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogate models. We find that the smoothness of surrogate models at different time steps differs, and we substantially improve the performance of the MC-based AEs by selecting smoother surrogate models. In the light of the theoretical framework on adversarial transferability in image classification, we also conduct a theoretical analysis to explain why smooth surrogate models can also boost AEs for LDMs. ",
        "title": "Exploring Adversarial Attacks against Latent Diffusion Model from the  Perspective of Adversarial Transferability",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07088",
        "abstract_url": "http://arxiv.org/abs/2401.07088",
        "authors": [
            {
                "last_name": "Carpio",
                "first_name": "Ana"
            },
            {
                "last_name": "Cebrian",
                "first_name": "Elena"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The dynamics of cellular aggregates is driven by the interplay of mechanochemical processes and cellular activity. Although deterministic models may capture mechanical features, local chemical fluctuations trigger random cell responses, which determine the overall evolution. Incorporating stochastic cellular behavior in macroscopic models of biological media is a challenging task. Herein, we propose hybrid models for bacterial biofilm growth, which couple a two phase solid/fluid mixture description of mechanical and chemical fields with a dynamic energy budget-based cellular automata treatment of bacterial activity. Thin film and plate approximations for the relevant interfaces allow us to obtain numerical solutions exhibiting behaviors observed in experiments, such as accelerated spread due to water intake from the environment, wrinkle formation, undulated contour development, and the appearance of inhomogeneous distributions of differentiated bacteria performing varied tasks. ",
        "title": "Incorporating Cellular Stochasticity in Solid--Fluid Mixture Biofilm  Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07091",
        "abstract_url": "http://arxiv.org/abs/2401.07091",
        "authors": [
            {
                "last_name": "Laber",
                "first_name": "Eduardo S."
            },
            {
                "last_name": "Murtinho",
                "first_name": "Lucas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DS"
        ],
        "abstract": "  Internal measures that are used to assess the quality of a clustering usually take into account intra-group and/or inter-group criteria. There are many papers in the literature that propose algorithms with provable approximation guarantees for optimizing the former. However, the optimization of inter-group criteria is much less understood.   Here, we contribute to the state-of-the-art of this literature by devising algorithms with provable guarantees for the maximization of two natural inter-group criteria, namely the minimum spacing and the minimum spanning tree spacing. The former is the minimum distance between points in different groups while the latter captures separability through the cost of the minimum spanning tree that connects all groups. We obtain results for both the unrestricted case, in which no constraint on the clusters is imposed, and for the constrained case where each group is required to have a minimum number of points. Our constraint is motivated by the fact that the popular Single Linkage, which optimizes both criteria in the unrestricted case, produces clusterings with many tiny groups.   To complement our work, we present an empirical study with 10 real datasets, providing evidence that our methods work very well in practical settings. ",
        "title": "Optimization of Inter-group Criteria for Clustering with Minimum Size  Constraints",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07096",
        "abstract_url": "http://arxiv.org/abs/2401.07096",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Bowen"
            },
            {
                "last_name": "Shi",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In the fields of statistics, machine learning, image science, and related areas, there is an increasing demand for decentralized collection or storage of large-scale datasets, as well as distributed solution methods. To tackle this challenge, the alternating direction method of multipliers (ADMM) has emerged as a widely used approach, particularly well-suited to distributed convex optimization. However, the iterative behavior of ADMM has not been well understood. In this paper, we employ dimensional analysis to derive a system of high-resolution ordinary differential equations (ODEs) for ADMM. This system captures an important characteristic of ADMM, called the $\\lambda$-correction, which causes the trajectory of ADMM to deviate from the constrained hyperplane. To explore the convergence behavior of the system of high-resolution ODEs, we utilize Lyapunov analysis and extend our findings to the discrete ADMM algorithm. Through this analysis, we identify that the numerical error resulting from the implicit scheme is a crucial factor that affects the convergence rate and monotonicity in the discrete ADMM algorithm. In addition, we further discover that if one component of the objective function is assumed to be strongly convex, the iterative average of ADMM converges strongly with a rate $O(1/N)$, where $N$ is the number of iterations. ",
        "title": "Understanding the ADMM Algorithm via High-Resolution Differential  Equations",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07098",
        "abstract_url": "http://arxiv.org/abs/2401.07098",
        "authors": [
            {
                "last_name": "Maity",
                "first_name": "Subhankar"
            },
            {
                "last_name": "Deroy",
                "first_name": "Aniket"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Sudeshna"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We introduce a multi-stage prompting approach (MSP) for the generation of multiple choice questions (MCQs), harnessing the capabilities of GPT models such as text-davinci-003 and GPT-4, renowned for their excellence across various NLP tasks. Our approach incorporates the innovative concept of chain-of-thought prompting, a progressive technique in which the GPT model is provided with a series of interconnected cues to guide the MCQ generation process. Automated evaluations consistently demonstrate the superiority of our proposed MSP method over the traditional single-stage prompting (SSP) baseline, resulting in the production of high-quality distractors. Furthermore, the one-shot MSP technique enhances automatic evaluation results, contributing to improved distractor generation in multiple languages, including English, German, Bengali, and Hindi. In human evaluations, questions generated using our approach exhibit superior levels of grammaticality, answerability, and difficulty, highlighting its efficacy in various languages. ",
        "title": "A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ  Generation using GPT",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07100",
        "abstract_url": "http://arxiv.org/abs/2401.07100",
        "authors": [
            {
                "last_name": "Javadi",
                "first_name": "Sepideh"
            },
            {
                "last_name": "Farhadi",
                "first_name": "Armin"
            },
            {
                "last_name": "Mili",
                "first_name": "Mohammad Robat"
            },
            {
                "last_name": "Jorswieck",
                "first_name": "Eduard"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) is a novel technology which enables the full-space coverage by splitting the incident signal into reflected and transmitted signals. In this letter, a multi STAR-RIS-aided system using non-orthogonal multiple access (NOMA) in an uplink transmission is considered, where the multi-order reflections among multiple STAR-RISs assist the transmission from the single-antenna users to the multi-antenna base station (BS). Specifically, the total sum rate maximization problem is solved by jointly optimizing the active beamforming, power allocation, transmission and reflection beamforming at the STAR-RIS, and user-STAR-RIS association indicator. To solve the non-convex optimization problem, a novel deep reinforcement learning algorithm is proposed which is the combination of meta-learning and deep deterministic policy gradient (DDPG), namely Meta-DDPG. Numerical results demonstrate that the proposed Meta-DDPG algorithm outperforms the conventional DDPG algorithm. ",
        "title": "Resource Allocation in Uplink Multi STAR-RIS-aided NOMA System via  Meta-Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07102",
        "abstract_url": "http://arxiv.org/abs/2401.07102",
        "authors": [
            {
                "last_name": "Hemberg",
                "first_name": "Erik"
            },
            {
                "last_name": "Moskal",
                "first_name": "Stephen"
            },
            {
                "last_name": "O'Reilly",
                "first_name": "Una-May"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM GP, a formalized LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators radically differ from GP's because they enlist an LLM, using prompting and the LLM's pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM GP and share its code. By addressing algorithms that range from the formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming. ",
        "title": "Evolving Code with A Large Language Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07103",
        "abstract_url": "http://arxiv.org/abs/2401.07103",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhen"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Shen",
                "first_name": "Tao"
            },
            {
                "last_name": "Xu",
                "first_name": "Can"
            },
            {
                "last_name": "Gu",
                "first_name": "Jia-Chen"
            },
            {
                "last_name": "Tao",
                "first_name": "Chongyang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This survey aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques. ",
        "title": "Leveraging Large Language Models for NLG Evaluation: A Survey",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07105",
        "abstract_url": "http://arxiv.org/abs/2401.07105",
        "authors": [
            {
                "last_name": "Plenz",
                "first_name": "Moritz"
            },
            {
                "last_name": "Frank",
                "first_name": "Anette"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  While Language Models have become workhorses for NLP, their interplay with textual knowledge graphs (KGs) - structured memories of general or domain knowledge - is actively researched. Current embedding methodologies for such graphs typically either (i) linearize graphs for embedding them using sequential Language Models (LMs), which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve graph structure, while GNNs cannot represent textual features as well as a pre-trained LM could. In this work we introduce a novel language model, the Graph Language Model (GLM), that integrates the strengths of both approaches, while mitigating their weaknesses. The GLM parameters are initialized from a pretrained LM, to facilitate nuanced understanding of individual concepts and triplets. Simultaneously, its architectural design incorporates graph biases, thereby promoting effective knowledge distribution within the graph. Empirical evaluations on relation classification tasks on ConceptNet subgraphs reveal that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot settings. ",
        "title": "Graph Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07106",
        "abstract_url": "http://arxiv.org/abs/2401.07106",
        "authors": [
            {
                "last_name": "Ganardi",
                "first_name": "Moses"
            },
            {
                "last_name": "Saglam",
                "first_name": "Irmak"
            },
            {
                "last_name": "Zetzsche",
                "first_name": "Georg"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "CL"
        ],
        "abstract": "  We study the problem of deciding whether a given language is directed. A language $L$ is \\emph{directed} if every pair of words in $L$ have a common (scattered) superword in $L$. Deciding directedness is a fundamental problem in connection with ideal decompositions of downward closed sets. Another motivation is that deciding whether two \\emph{directed} context-free languages have the same downward closures can be decided in polynomial time, whereas for general context-free languages, this problem is known to be coNEXP-complete.   We show that the directedness problem for regular languages, given as NFAs, belongs to $AC^1$, and thus polynomial time. Moreover, it is NL-complete for fixed alphabet sizes. Furthermore, we show that for context-free languages, the directedness problem is PSPACE-complete. ",
        "title": "Directed Regular and Context-Free Languages",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07108",
        "abstract_url": "http://arxiv.org/abs/2401.07108",
        "authors": [
            {
                "last_name": "Agouzal",
                "first_name": "Eki"
            },
            {
                "last_name": "Taddei",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present an accelerated greedy strategy for training of projection-based reduced-order models for parametric steady and unsteady partial differential equations. Our approach exploits hierarchical approximate proper orthogonal decomposition to speed up the construction of the empirical test space for least-square Petrov-Galerkin formulations, a progressive construction of the empirical quadrature rule based on a warm start of the non-negative least-square algorithm, and a two-fidelity sampling strategy to reduce the number of expensive greedy iterations. We illustrate the performance of our method for two test cases: a two-dimensional compressible inviscid flow past a LS89 blade at moderate Mach number, and a three-dimensional nonlinear mechanics problem to predict the long-time structural response of the standard section of a nuclear containment building under external loading. ",
        "title": "Accelerated construction of projection-based reduced-order models via  incremental approaches",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07114",
        "abstract_url": "http://arxiv.org/abs/2401.07114",
        "authors": [
            {
                "last_name": "Rydell",
                "first_name": "Felix"
            },
            {
                "last_name": "Torres",
                "first_name": "Ang\u00e9lica"
            },
            {
                "last_name": "Larsson",
                "first_name": "Viktor"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Many problems in computer vision can be formulated as geometric estimation problems, i.e. given a collection of measurements (e.g. point correspondences) we wish to fit a model (e.g. an essential matrix) that agrees with our observations. This necessitates some measure of how much an observation ``agrees\" with a given model. A natural choice is to consider the smallest perturbation that makes the observation exactly satisfy the constraints. However, for many problems, this metric is expensive or otherwise intractable to compute. The so-called Sampson error approximates this geometric error through a linearization scheme. For epipolar geometry, the Sampson error is a popular choice and in practice known to yield very tight approximations of the corresponding geometric residual (the reprojection error).   In this paper we revisit the Sampson approximation and provide new theoretical insights as to why and when this approximation works, as well as provide explicit bounds on the tightness under some mild assumptions. Our theoretical results are validated in several experiments on real data and in the context of different geometric estimation tasks. ",
        "title": "Revisiting Sampson Approximations for Geometric Estimation Problems",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07115",
        "abstract_url": "http://arxiv.org/abs/2401.07115",
        "authors": [
            {
                "last_name": "La Cava",
                "first_name": "Lucio"
            },
            {
                "last_name": "Costa",
                "first_name": "Davide"
            },
            {
                "last_name": "Tagarelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY",
            "HC"
        ],
        "abstract": "  The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology, leading to a proliferation of computational agents. Scholars have been studying the inherent personalities displayed by LLM agents and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by conducting a comprehensive examination of the ability of agents to emulate human personalities using Open LLMs. To achieve this, we generate a set of ten LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that: $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities; and $(iv)$ personalities typically associated with the role of teacher tend to be emulated with greater accuracy. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs. ",
        "title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human  Personalities through Open Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07118",
        "abstract_url": "http://arxiv.org/abs/2401.07118",
        "authors": [
            {
                "last_name": "Pascher",
                "first_name": "Max"
            },
            {
                "last_name": "Zinta",
                "first_name": "Kevin"
            },
            {
                "last_name": "Gerken",
                "first_name": "Jens"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  Robotic arms, integral in domestic care for individuals with motor impairments, enable them to perform Activities of Daily Living (ADLs) independently, reducing dependence on human caregivers. These collaborative robots require users to manage multiple Degrees-of-Freedom (DoFs) for tasks like grasping and manipulating objects. Conventional input devices, typically limited to two DoFs, necessitate frequent and complex mode switches to control individual DoFs. Modern adaptive controls with feed-forward multi-modal feedback reduce the overall task completion time, number of mode switches, and cognitive load. Despite the variety of input devices available, their effectiveness in adaptive settings with assistive robotics has yet to be thoroughly assessed. This study explores three different input devices by integrating them into an established XR framework for assistive robotics, evaluating them and providing empirical insights through a preliminary study for future developments. ",
        "title": "Exploring of Discrete and Continuous Input Control for AI-enhanced  Assistive Robotic Arms",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07119",
        "abstract_url": "http://arxiv.org/abs/2401.07119",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Yicheng"
            },
            {
                "last_name": "Wu",
                "first_name": "Yongji"
            },
            {
                "last_name": "Hu",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Maggs",
                "first_name": "Bruce M."
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhuo",
                "first_name": "Danyang"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "DC",
            "IR",
            "LG"
        ],
        "abstract": "  Vector databases have emerged as key enablers for bridging intelligent applications with unstructured data, providing generic search and management support for embedding vectors extracted from the raw unstructured data. As multiple data users can share the same database infrastructure, multi-tenancy support for vector databases is increasingly desirable. This hinges on an efficient filtered search operation, i.e., only querying the vectors accessible to a particular tenant. Multi-tenancy in vector databases is currently achieved by building either a single, shared index among all tenants, or a per-tenant index. The former optimizes for memory efficiency at the expense of search performance, while the latter does the opposite. Instead, this paper presents Curator, an in-memory vector index design tailored for multi-tenant queries that simultaneously achieves the two conflicting goals, low memory overhead and high performance for queries, vector insertion, and deletion. Curator indexes each tenant's vectors with a tenant-specific clustering tree and encodes these trees compactly as sub-trees of a shared clustering tree. Each tenant's clustering tree adapts dynamically to its unique vector distribution, while maintaining a low per-tenant memory footprint. Our evaluation, based on two widely used data sets, confirms that Curator delivers search performance on par with per-tenant indexing, while maintaining memory consumption at the same level as metadata filtering on a single, shared index. ",
        "title": "Curator: Efficient Indexing for Multi-Tenant Vector Databases",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07120",
        "abstract_url": "http://arxiv.org/abs/2401.07120",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Minrui"
            },
            {
                "last_name": "Niyato",
                "first_name": "Dusit"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zehui"
            },
            {
                "last_name": "Cao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Yulan"
            },
            {
                "last_name": "Ren",
                "first_name": "Chao"
            },
            {
                "last_name": "Yu",
                "first_name": "Han"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Quantum computing networks enable scalable collaboration and secure information exchange among multiple classical and quantum computing nodes while executing large-scale generative AI computation tasks and advanced quantum algorithms. Quantum computing networks overcome limitations such as the number of qubits and coherence time of entangled pairs and offer advantages for generative AI infrastructure, including enhanced noise reduction through distributed processing and improved scalability by connecting multiple quantum devices. However, efficient resource allocation in quantum computing networks is a critical challenge due to factors including qubit variability and network complexity. In this article, we propose an intelligent resource allocation framework for quantum computing networks to improve network scalability with minimized resource costs. To achieve scalability in quantum computing networks, we formulate the resource allocation problem as stochastic programming, accounting for the uncertain fidelities of qubits and entangled pairs. Furthermore, we introduce state-of-the-art reinforcement learning (RL) algorithms, from generative learning to quantum machine learning for optimal quantum resource allocation to resolve the proposed stochastic resource allocation problem efficiently. Finally, we optimize the resource allocation in heterogeneous quantum computing networks supporting quantum generative learning applications and propose a multi-agent RL-based algorithm to learn the optimal resource allocation policies without prior knowledge. ",
        "title": "Generative AI-enabled Quantum Computing Networks and Intelligent  Resource Allocation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07121",
        "abstract_url": "http://arxiv.org/abs/2401.07121",
        "authors": [
            {
                "last_name": "Parolini",
                "first_name": "Nicola"
            },
            {
                "last_name": "Poiatti",
                "first_name": "Andrea"
            },
            {
                "last_name": "Vene'",
                "first_name": "Julian"
            },
            {
                "last_name": "Verani",
                "first_name": "Marco"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we address the importance and the impact of employing structure preserving neural networks as surrogate of the analytical physics-based models typically employed to describe the rheology of non-Newtonian fluids in Stokes flows. In particular, we propose and test on real-world scenarios a novel strategy to build data-driven rheological models based on the use of Input-Output Convex Neural Networks (ICNNs), a special class of feedforward neural network scalar valued functions that are convex with respect to their inputs. Moreover, we show, through a detailed campaign of numerical experiments, that the use of ICNNs is of paramount importance to guarantee the well-posedness of the associated non-Newtonian Stokes differential problem. Finally, building upon a novel perturbation result for non-Newtonian Stokes problems, we study the impact of our data-driven ICNN based rheological model on the accuracy of the finite element approximation. ",
        "title": "Structure-preserving neural networks in data-driven rheological models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07122",
        "abstract_url": "http://arxiv.org/abs/2401.07122",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Haihui"
            },
            {
                "last_name": "Xia",
                "first_name": "Minghua"
            },
            {
                "last_name": "Wu",
                "first_name": "Peiran"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Huang",
                "first_name": "Kaibin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Federated learning (FL) enables wireless terminals to collaboratively learn a shared parameter model while keeping all the training data on devices per se. Parameter sharing consists of synchronous and asynchronous ways: the former transmits parameters as blocks or frames and waits until all transmissions finish, whereas the latter provides messages about the status of pending and failed parameter transmission requests. Whatever synchronous or asynchronous parameter sharing is applied, the learning model shall adapt to distinct network architectures as an improper learning model will deteriorate learning performance and, even worse, lead to model divergence for the asynchronous transmission in resource-limited large-scale Internet-of-Things (IoT) networks. This paper proposes a decentralized learning model and develops an asynchronous parameter-sharing algorithm for resource-limited distributed IoT networks. This decentralized learning model approaches a convex function as the number of nodes increases, and its learning process converges to a global stationary point with a higher probability than the centralized FL model. Moreover, by jointly accounting for the convergence bound of federated learning and the transmission delay of wireless communications, we develop a node scheduling and bandwidth allocation algorithm to minimize the transmission delay. Extensive simulation results corroborate the effectiveness of the distributed algorithm in terms of fast learning model convergence and low transmission delay. ",
        "title": "Decentralized Federated Learning with Asynchronous Parameter Sharing for  Large-scale IoT Networks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07123",
        "abstract_url": "http://arxiv.org/abs/2401.07123",
        "authors": [
            {
                "last_name": "Clarke",
                "first_name": "Christopher"
            },
            {
                "last_name": "Krishnamurthy",
                "first_name": "Karthik"
            },
            {
                "last_name": "Talamonti",
                "first_name": "Walter"
            },
            {
                "last_name": "Kang",
                "first_name": "Yiping"
            },
            {
                "last_name": "Tang",
                "first_name": "Lingjia"
            },
            {
                "last_name": "Mars",
                "first_name": "Jason"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CL"
        ],
        "abstract": "  Conversational agents have been gaining increasing popularity in recent years. Influenced by the widespread adoption of task-oriented agents such as Apple Siri and Amazon Alexa, these agents are being deployed into various applications to enhance user experience. Although these agents promote \"ask me anything\" functionality, they are typically built to focus on a single or finite set of expertise. Given that complex tasks often require more than one expertise, this results in the users needing to learn and adopt multiple agents. One approach to alleviate this is to abstract the orchestration of agents in the background. However, this removes the option of choice and flexibility, potentially harming the ability to complete tasks. In this paper, we explore these different interaction experiences (one agent for all) vs (user choice of agents) for conversational AI. We design prototypes for each, systematically evaluating their ability to facilitate task completion. Through a series of conducted user studies, we show that users have a significant preference for abstracting agent orchestration in both system usability and system performance. Additionally, we demonstrate that this mode of interaction is able to provide quality responses that are rated within 1% of human-selected answers. ",
        "title": "One Agent Too Many: User Perspectives on Approaches to Multi-agent  Conversational AI",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07124",
        "abstract_url": "http://arxiv.org/abs/2401.07124",
        "authors": [
            {
                "last_name": "Zadeh",
                "first_name": "Sara Shomal"
            },
            {
                "last_name": "birgani",
                "first_name": "Sina Aalipour"
            },
            {
                "last_name": "Khorshidi",
                "first_name": "Meisam"
            },
            {
                "last_name": "Kooban",
                "first_name": "Farhad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Effective crack detection is pivotal for the structural health monitoring and inspection of buildings. This task presents a formidable challenge to computer vision techniques due to the inherently subtle nature of cracks, which often exhibit low-level features that can be easily confounded with background textures, foreign objects, or irregularities in construction. Furthermore, the presence of issues like non-uniform lighting and construction irregularities poses significant hurdles for autonomous crack detection during building inspection and monitoring. Convolutional neural networks (CNNs) have emerged as a promising framework for crack detection, offering high levels of accuracy and precision. Additionally, the ability to adapt pre-trained networks through transfer learning provides a valuable tool for users, eliminating the need for an in-depth understanding of algorithm intricacies. Nevertheless, it is imperative to acknowledge the limitations and considerations when deploying CNNs, particularly in contexts where the outcomes carry immense significance, such as crack detection in buildings. In this paper, our approach to surface crack detection involves the utilization of various deep-learning models. Specifically, we employ fine-tuning techniques on pre-trained deep learning architectures: VGG19, ResNet50, Inception V3, and EfficientNetV2. These models are chosen for their established performance and versatility in image analysis tasks. We compare deep learning models using precision, recall, and F1 scores. ",
        "title": "Concrete Surface Crack Detection with Convolutional-based Deep Learning  Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07128",
        "abstract_url": "http://arxiv.org/abs/2401.07128",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Wenqi"
            },
            {
                "last_name": "Xu",
                "first_name": "Ran"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Yu",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jieyu"
            },
            {
                "last_name": "Wu",
                "first_name": "Hang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuanda"
            },
            {
                "last_name": "Ho",
                "first_name": "Joyce"
            },
            {
                "last_name": "Yang",
                "first_name": "Carl"
            },
            {
                "last_name": "Wang",
                "first_name": "May D."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent1, an LLM agent empowered with a code interface, to autonomously generate and execute code for complex clinical tasks within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on two real-world EHR datasets show that EHRAgent outperforms the strongest LLM agent baseline by 36.48% and 12.41%, respectively. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations. ",
        "title": "EHRAgent: Code Empowers Large Language Models for Complex Tabular  Reasoning on Electronic Health Records",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07132",
        "abstract_url": "http://arxiv.org/abs/2401.07132",
        "authors": [
            {
                "last_name": "Boffi",
                "first_name": "Daniele"
            },
            {
                "last_name": "Khan",
                "first_name": "Arbaz"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we discuss the optimal convergence of a standard adaptive scheme based on mixed finite element approximation to the solution of the eigenvalue problem associated with the Stokes equations. The proofs of the quasi-orthogonality and the discrete reliability are presented. Our numerical experiments confirm the efficacy of the proposed adaptive scheme. ",
        "title": "Adaptive Mixed FEM for the Stokes eigenvalue problem",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07139",
        "abstract_url": "http://arxiv.org/abs/2401.07139",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Yi"
            },
            {
                "last_name": "Yuan",
                "first_name": "Qiangqiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liangpei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent efforts have witnessed remarkable progress in Satellite Video Super-Resolution (SVSR). However, most SVSR methods usually assume the degradation is fixed and known, e.g., bicubic downsampling, which makes them vulnerable in real-world scenes with multiple and unknown degradations. To alleviate this issue, blind SR has thus become a research hotspot. Nevertheless, existing approaches are mainly engaged in blur kernel estimation while losing sight of another critical aspect for VSR tasks: temporal compensation, especially compensating for blurry and smooth pixels with vital sharpness from severely degraded satellite videos. Therefore, this paper proposes a practical Blind SVSR algorithm (BSVSR) to explore more sharp cues by considering the pixel-wise blur levels in a coarse-to-fine manner. Specifically, we employed multi-scale deformable convolution to coarsely aggregate the temporal redundancy into adjacent frames by window-slid progressive fusion. Then the adjacent features are finely merged into mid-feature using deformable attention, which measures the blur levels of pixels and assigns more weights to the informative pixels, thus inspiring the representation of sharpness. Moreover, we devise a pyramid spatial transformation module to adjust the solution space of sharp mid-feature, resulting in flexible feature adaptation in multi-level domains. Quantitative and qualitative evaluations on both simulated and real-world satellite videos demonstrate that our BSVSR performs favorably against state-of-the-art non-blind and blind SR models. Code will be available at https://github.com/XY-boy/Blind-Satellite-VSR ",
        "title": "Deep Blind Super-Resolution for Satellite Video",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07140",
        "abstract_url": "http://arxiv.org/abs/2401.07140",
        "authors": [
            {
                "last_name": "Cuesta",
                "first_name": "Carlota M."
            },
            {
                "last_name": "de la Hoz",
                "first_name": "Francisco"
            },
            {
                "last_name": "Girona",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we develop an accurate pseudospectral method to approximate numerically the Riesz-Feller operator $D_\\gamma^\\alpha$ on $\\mathbb R$, where $\\alpha\\in(0,2)$, and $|\\gamma|\\le\\min\\{\\alpha, 2 - \\alpha\\}$. This operator can be written as a linear combination of the Weyl-Marchaud derivatives $\\mathcal{D}^{\\alpha}$ and $\\overline{\\mathcal{D}^\\alpha}$, when $\\alpha\\in(0,1)$, and of $\\partial_x\\mathcal{D}^{\\alpha-1}$ and $\\partial_x\\overline{\\mathcal{D}^{\\alpha-1}}$, when $\\alpha\\in(1,2)$.   Given the so-called Higgins functions $\\lambda_k(x) = ((ix-1)/(ix+1))^k$, where $k\\in\\mathbb Z$, we compute explicitly, using complex variable techniques, $\\mathcal{D}^{\\alpha}[\\lambda_k](x)$, $\\overline{\\mathcal{D}^\\alpha}[\\lambda_k](x)$, $\\partial_x\\mathcal{D}^{\\alpha-1}[\\lambda_k](x)$, $\\partial_x\\overline{\\mathcal{D}^{\\alpha-1}}[\\lambda_k](x)$ and $D_\\gamma^\\alpha[\\lambda_k](x)$, in terms of the Gaussian hypergeometric function ${}_2F_1$, and relate these results to previous ones for the fractional Laplacian. This enables us to approximate $\\mathcal{D}^{\\alpha}[u](x)$, $\\overline{\\mathcal{D}^\\alpha}[u](x)$, $\\partial_x\\mathcal{D}^{\\alpha-1}[u](x)$, $\\partial_x\\overline{\\mathcal{D}^{\\alpha-1}}[u](x)$ and $D_\\gamma^\\alpha[u](x)$, for bounded continuous functions $u(x)$. Finally, we simulate a nonlinear Riesz-Feller fractional diffusion equation, characterized by having front propagating solutions whose speed grows exponentially in time. ",
        "title": "Numerical Approximation of Riesz-Feller Operators on $\\mathbb R$",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07141",
        "abstract_url": "http://arxiv.org/abs/2401.07141",
        "authors": [
            {
                "last_name": "Nikkhah",
                "first_name": "Ali"
            },
            {
                "last_name": "Shoushtari",
                "first_name": "Morteza"
            },
            {
                "last_name": "Akhbari",
                "first_name": "Bahareh"
            },
            {
                "last_name": "Harrison",
                "first_name": "Willie K."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we use a linear programming (LP) optimization approach to evaluate the equivocation for a wiretap channel where the main channel is noiseless, and the wiretap channel is a binary symmetric channel (BSC). Using this technique, we present an analytical limit for the achievable secrecy rate in the finite blocklength regime that is tighter than traditional fundamental limits. We also propose a secrecy coding technique that outperforms random binning codes. When there is one overhead bit, this coding technique is optimum and achieves the analytical limit. For cases with additional bits of overhead, our coding scheme can achieve equivocation rates close to the new limit. Furthermore, we evaluate the patterns of the generator matrix and the parity-check matrix for linear codes and we present binning techniques for both linear and non-linear codes using two different approaches: recursive and non-recursive. To our knowledge, this is the first optimization solution for secrecy coding obtained through linear programming. ",
        "title": "Secrecy Coding for the Binary Symmetric Wiretap Channel via Linear  Programming",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07142",
        "abstract_url": "http://arxiv.org/abs/2401.07142",
        "authors": [
            {
                "last_name": "Aksoy",
                "first_name": "Levent"
            },
            {
                "last_name": "Yasin",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Pagliarini",
                "first_name": "Samuel"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Logic locking proposed to protect integrated circuits from serious hardware threats has been studied extensively over a decade. In these years, many efficient logic locking techniques have been proven to be broken. The state-of-the-art logic locking techniques, including the prominent corrupt and correct (CAC) technique, are resilient to satisfiability (SAT)-based and removal attacks, but vulnerable to structural analysis attacks. To overcome this drawback, this paper introduces an improved version of CAC, called CAC 2.0, which increases the search space of structural analysis attacks using obfuscation. To do so, CAC 2.0 locks the original circuit twice, one after another, on different nodes with different number of protected primary inputs using CAC, while hiding original protected primary inputs among decoy primary inputs. This paper also introduces an open source logic locking tool, called HIID, equipped with well-known techniques including CAC 2.0. Our experiments show that CAC 2.0 is resilient to existing SAT-based, removal, and structural analysis attacks. To achieve this, it increases the number of key inputs at most 4x and the gate-level area between 30.2% and 0.8% on circuits with low and high complexity with respect to CAC. ",
        "title": "CAC 2.0: A Corrupt and Correct Logic Locking Technique Resilient to  Structural Analysis Attacks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07143",
        "abstract_url": "http://arxiv.org/abs/2401.07143",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Hossam O."
            },
            {
                "last_name": "Wyatt",
                "first_name": "David"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The demand for more developed and agile urban taxi drones is increasing rapidly nowadays to sustain crowded cities and their traffic issues. The critical factor for spreading such technology could be related to the safety criteria that must be considered. One of the most critical safety aspects for such vertical and/or Short Take-Off and Landing (V/STOL) drones is related to safety during the landing stage, in which most of the recent flight accidents have occurred. This paper focused on solving this issue by proposing decentralized processing cores that could improve the landing failure rate by depending on a Fuzzy Logic System (FLS) and additional Digital Signal Processing (DSP) elements. Also, the proposed system will enhance the safety factor during the landing stages by adding a self-awareness feature in case a certain sensor malfunction occurs using the proposed Adaptive Prognostic Malfunction Unit (APMU). This proposed coarse-grained Autonomous Landing Guidance Assistance System (ALGAS4) processing architecture has been optimized using different optimization techniques. The ALGAS4 architecture has been designed completely using VHDL, and the targeted FPGA was the INTEL Cyclone V 5CGXFC9D6F27C7 chip. According to the synthesis findings of the INTEL Quartus Prime software, the maximum working frequency of the ALGAS4 system is 278.24 MHz. In addition, the proposed ALGAS4 system could maintain a maximum computing performance of approximately 74.85 GOPS while using just 166.56 mW for dynamic and I/O power dissipation. ",
        "title": "Adaptive Prognostic Malfunction Based Processor for Autonomous Landing  Guidance Assistance System Using FPGA",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07145",
        "abstract_url": "http://arxiv.org/abs/2401.07145",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural networks (NNs) can achieved high performance in various fields such as computer vision, and natural language processing. However, deploying NNs in resource-constrained safety-critical systems has challenges due to uncertainty in the prediction caused by out-of-distribution data, and hardware non-idealities. To address the challenges of deploying NNs in resource-constrained safety-critical systems, this paper summarizes the (4th year) PhD thesis work that explores scalable and efficient methods for uncertainty estimation and reduction in deep learning, with a focus on Computation-in-Memory (CIM) using emerging resistive non-volatile memories. We tackle the inherent uncertainties arising from out-of-distribution inputs and hardware non-idealities, crucial in maintaining functional safety in automated decision-making systems. Our approach encompasses problem-aware training algorithms, novel NN topologies, and hardware co-design solutions, including dropout-based \\emph{binary} Bayesian Neural Networks leveraging spintronic devices and variational inference techniques. These innovations significantly enhance OOD data detection, inference accuracy, and energy efficiency, thereby contributing to the reliability and robustness of NN implementations. ",
        "title": "Scalable and Efficient Methods for Uncertainty Estimation and Reduction  in Deep Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07147",
        "abstract_url": "http://arxiv.org/abs/2401.07147",
        "authors": [
            {
                "last_name": "Pago",
                "first_name": "Benedikt"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  The search for a logic capturing PTIME is a long standing open problem in finite model theory. One of the most promising candidate logics for this is Choiceless Polynomial Time with counting (CPT). Abstractly speaking, CPT is an isomorphism-invariant computation model working with hereditarily finite sets as data structures. While it is easy to check that the evaluation of CPT-sentences is possible in polynomial time, the converse has been open for more than 20 years: Can every PTIME-decidable property of finite structures be expressed in CPT? We attempt to make progress towards a negative answer and show that Choiceless Polynomial Time cannot compute a preorder with colour classes of logarithmic size in every hypercube. The reason is that such preorders have super-polynomially many automorphic images, which makes it impossible for CPT to define them. While the computation of such a preorder is not a decision problem that would immediately separate P and CPT, it is significant for the following reason: The so-called Cai-F\\\"urer-Immerman (CFI) problem is one of the standard benchmarks for logics and maybe best known for separating fixed-point logic with counting (FPC) from P. Hence, it is natural to consider this also a potential candidate for the separation of CPT and P. The strongest known positive result in this regard says that CPT is able to solve CFI if a preorder with logarithmically sized colour classes is present in the input structure. Our result implies that this approach cannot be generalised to unordered inputs. In other words, CFI on unordered hypercubes is a PTIME-problem which provably cannot be tackled with the state-of-the-art choiceless algorithmic techniques. ",
        "title": "Choiceless Computation and Symmetry: Limitations of Definability",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07148",
        "abstract_url": "http://arxiv.org/abs/2401.07148",
        "authors": [
            {
                "last_name": "Vaidya",
                "first_name": "Ruturaj K."
            },
            {
                "last_name": "Kulkarni",
                "first_name": "Prasad A."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  Memory corruption is an important class of vulnerability that can be leveraged to craft control flow hijacking attacks. Control Flow Integrity (CFI) provides protection against such attacks. Application of type-based CFI policies requires information regarding the number and type of function arguments. Binary-level type recovery is inherently speculative, which motivates the need for an evaluation framework to assess the effectiveness of binary-level CFI techniques compared with their source-level counterparts, where such type information is fully and accurately accessible. In this work, we develop a novel, generalized and extensible framework to assess how the program analysis information we get from state-of-the-art binary analysis tools affects the efficacy of type-based CFI techniques. We introduce new and insightful metrics to quantitatively compare source independent CFI policies with their ground truth source aware counterparts. We leverage our framework to evaluate binary-level CFI policies implemented using program analysis information extracted from the IDA Pro binary analyzer and compared with the ground truth information obtained from the LLVM compiler, and present our observations. ",
        "title": "Assessing the Effectiveness of Binary-Level CFI Techniques",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07154",
        "abstract_url": "http://arxiv.org/abs/2401.07154",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Cheng"
            },
            {
                "last_name": "Kakkar",
                "first_name": "Akshay"
            },
            {
                "last_name": "Redino",
                "first_name": "Christopher"
            },
            {
                "last_name": "Rahman",
                "first_name": "Abdul"
            },
            {
                "last_name": "S",
                "first_name": "Ajinsyam"
            },
            {
                "last_name": "Clark",
                "first_name": "Ryan"
            },
            {
                "last_name": "Radke",
                "first_name": "Daniel"
            },
            {
                "last_name": "Cody",
                "first_name": "Tyler"
            },
            {
                "last_name": "Huang",
                "first_name": "Lanxiao"
            },
            {
                "last_name": "Bowen",
                "first_name": "Edward"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Command and control (C2) paths for issuing commands to malware are sometimes the only indicators of its existence within networks. Identifying potential C2 channels is often a manually driven process that involves a deep understanding of cyber tradecraft. Efforts to improve discovery of these channels through using a reinforcement learning (RL) based approach that learns to automatically carry out C2 attack campaigns on large networks, where multiple defense layers are in place serves to drive efficiency for network operators. In this paper, we model C2 traffic flow as a three-stage process and formulate it as a Markov decision process (MDP) with the objective to maximize the number of valuable hosts whose data is exfiltrated. The approach also specifically models payload and defense mechanisms such as firewalls which is a novel contribution. The attack paths learned by the RL agent can in turn help the blue team identify high-priority vulnerabilities and develop improved defense strategies. The method is evaluated on a large network with more than a thousand hosts and the results demonstrate that the agent can effectively learn attack paths while avoiding firewalls. ",
        "title": "Discovering Command and Control Channels Using Reinforcement Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07157",
        "abstract_url": "http://arxiv.org/abs/2401.07157",
        "authors": [
            {
                "last_name": "Vafiadis",
                "first_name": "Dimitris"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The problem of decoupling a nonsquare state space system by state feedback with singular input transformation is considered. The problem is solved by conducting a finite search for decouplable square systems, appropriately derived from the original. Decoupling feedback on any of these systems defines the decoupling feedback for the original. The issue of fixed poles is also considered and the possibility of selecting the uncontrollable poles is investigated. ",
        "title": "A matrix pencil approach to the Morgan's problem",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07159",
        "abstract_url": "http://arxiv.org/abs/2401.07159",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhengxin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Dan"
            },
            {
                "last_name": "Miao",
                "first_name": "Xupeng"
            },
            {
                "last_name": "Oliaro",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yong"
            },
            {
                "last_name": "Jia",
                "first_name": "Zhihao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory and none can simultaneously mitigate memory footprint for all three sources. In this paper, we present Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM's model weights into 4-bit to reduce the memory footprint of the LLM's original weights; QST also introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing backpropagation through the LLM, thus reducing the memory requirement of the intermediate activations. Furthermore, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3 $\\times$ and speed up the finetuning process by up to 3 $\\times$ while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7 $\\times$. ",
        "title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized  Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07163",
        "abstract_url": "http://arxiv.org/abs/2401.07163",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zihao"
            },
            {
                "last_name": "Hou",
                "first_name": "Yu"
            },
            {
                "last_name": "Soibelman",
                "first_name": "Lucio"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The potential energy loss of aging buildings traps building owners in a cycle of underfunding operations and overpaying maintenance costs. Energy auditors intending to generate an energy model of a target building for performance assessment may struggle to obtain accurate results as the spatial distribution of temperatures is not considered when calculating the U-value of the building envelope. This paper proposes a pixel-level method based on infrared thermography (IRT) that considers two-dimensional (2D) spatial temperature distributions of the outdoor and indoor surfaces of the target wall to generate a 2D U-value map of the wall. The result supports that the proposed method can better reflect the actual thermal insulation performance of the target wall compared to the current IRT-based methods that use a single-point room temperature as input. ",
        "title": "A New Method of Pixel-level In-situ U-value Measurement for Building  Envelopes Based on Infrared Thermography",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07164",
        "abstract_url": "http://arxiv.org/abs/2401.07164",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Shuo"
            },
            {
                "last_name": "Mielle",
                "first_name": "Malcolm"
            },
            {
                "last_name": "Lilienthal",
                "first_name": "Achim J."
            },
            {
                "last_name": "Magnusson",
                "first_name": "Martin"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Neural implicit surface representations are currently receiving a lot of interest as a means to achieve high-fidelity surface reconstruction at a low memory cost, compared to traditional explicit representations.However, state-of-the-art methods still struggle with excessive memory usage and non-smooth surfaces. This is particularly problematic in large-scale applications with sparse inputs, as is common in robotics use cases. To address these issues, we first introduce a sparse structure, \\emph{tri-quadtrees}, which represents the environment using learnable features stored in three planar quadtree projections. Secondly, we concatenate the learnable features with a Fourier feature positional encoding. The combined features are then decoded into signed distance values through a small multi-layer perceptron. We demonstrate that this approach facilitates smoother reconstruction with a higher completion ratio with fewer holes. Compared to two recent baselines, one implicit and one explicit, our approach requires only 10\\%--50\\% as much memory, while achieving competitive quality. ",
        "title": "3QFP: Efficient neural implicit surface reconstruction using  Tri-Quadtrees and Fourier feature Positional encoding",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07167",
        "abstract_url": "http://arxiv.org/abs/2401.07167",
        "authors": [
            {
                "last_name": "Mandal",
                "first_name": "Avijit"
            },
            {
                "last_name": "Brandsen",
                "first_name": "S."
            },
            {
                "last_name": "Pfister",
                "first_name": "Henry D."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper considers the design and decoding of polar codes for general classical-quantum (CQ) channels. It focuses on decoding via belief-propagation with quantum messages (BPQM) and, in particular, the idea of paired-measurement BPQM (PM-BPQM) decoding. Since the PM-BPQM decoder admits a classical density evolution (DE) analysis, one can use DE to design a polar code for any CQ channel and then efficiently compute the trade-off between code rate and error probability. We have also implemented and tested a classical simulation of our PM-BPQM decoder for polar codes. While the decoder can be implemented efficiently on a quantum computer, simulating the decoder on a classical computer actually has exponential complexity. Thus, simulation results for the decoder are somewhat limited and are included primarily to validate our theoretical results. ",
        "title": "Polar Codes for CQ Channels: Decoding via Belief-Propagation with  Quantum Messages",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07174",
        "abstract_url": "http://arxiv.org/abs/2401.07174",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Shizhou"
            },
            {
                "last_name": "Strohmer",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  We study the compatibility between the optimal statistical parity solutions and individual fairness. While individual fairness seeks to treat similar individuals similarly, optimal statistical parity aims to provide similar treatment to individuals who share relative similarity within their respective sensitive groups. The two fairness perspectives, while both desirable from a fairness perspective, often come into conflict in applications. Our goal in this work is to analyze the existence of this conflict and its potential solution. In particular, we establish sufficient (sharp) conditions for the compatibility between the optimal (post-processing) statistical parity $L^2$ learning and the ($K$-Lipschitz or $(\\epsilon,\\delta)$) individual fairness requirements. Furthermore, when there exists a conflict between the two, we first relax the former to the Pareto frontier (or equivalently the optimal trade-off) between $L^2$ error and statistical disparity, and then analyze the compatibility between the frontier and the individual fairness requirements. Our analysis identifies regions along the Pareto frontier that satisfy individual fairness requirements. (Lastly, we provide individual fairness guarantees for the composition of a trained model and the optimal post-processing step so that one can determine the compatibility of the post-processed model.) This provides practitioners with a valuable approach to attain Pareto optimality for statistical parity while adhering to the constraints of individual fairness. ",
        "title": "On the (In)Compatibility between Group Fairness and Individual Fairness",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07175",
        "abstract_url": "http://arxiv.org/abs/2401.07175",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Somya"
            },
            {
                "last_name": "Sharma",
                "first_name": "Swati"
            },
            {
                "last_name": "Padilha",
                "first_name": "Rafael"
            },
            {
                "last_name": "Kiciman",
                "first_name": "Emre"
            },
            {
                "last_name": "Chandra",
                "first_name": "Ranveer"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Monitoring organic matter is pivotal for maintaining soil health and can help inform sustainable soil management practices. While sensor-based soil information offers higher-fidelity and reliable insights into organic matter changes, sampling and measuring sensor data is cost-prohibitive. We propose a multi-modal, scalable framework that can estimate organic matter from remote sensing data, a more readily available data source while leveraging sparse soil information for improving generalization. Using the sensor data, we preserve underlying causal relations among sensor attributes and organic matter. Simultaneously we leverage inherent structure in the data and train the model to discriminate among domains using contrastive learning. This causal and contrastive constraint minimization ensures improved generalization and adaptation to other domains. We also shed light on the interpretability of the framework by identifying attributes that are important for improving generalization. Identifying these key soil attributes that affect organic matter will aid in efforts to standardize data collection efforts. ",
        "title": "Domain Adaptation for Sustainable Soil Management using Causal and  Contrastive Constraint Minimization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07179",
        "abstract_url": "http://arxiv.org/abs/2401.07179",
        "authors": [
            {
                "last_name": "Barbaglia",
                "first_name": "Luca"
            },
            {
                "last_name": "Consoli",
                "first_name": "Sergio"
            },
            {
                "last_name": "Manzan",
                "first_name": "Sebastiano"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "CL"
        ],
        "abstract": "  We evaluate the informational content of news-based sentiment indicators for forecasting Gross Domestic Product (GDP) and other macroeconomic variables of the five major European economies. Our data set includes over 27 million articles for 26 major newspapers in 5 different languages. The evidence indicates that these sentiment indicators are significant predictors to forecast macroeconomic variables and their predictive content is robust to controlling for other indicators available to forecasters in real-time. ",
        "title": "Forecasting GDP in Europe with Textual Data",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07181",
        "abstract_url": "http://arxiv.org/abs/2401.07181",
        "authors": [
            {
                "last_name": "Barj",
                "first_name": "Houda Nait El"
            },
            {
                "last_name": "Sautory",
                "first_name": "Theophile"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a method to address goal misgeneralization in reinforcement learning (RL), leveraging Large Language Model (LLM) feedback during training. Goal misgeneralization, a type of robustness failure in RL occurs when an agent retains its capabilities out-of-distribution yet pursues a proxy rather than the intended one. Our approach utilizes LLMs to analyze an RL agent's policies during training and identify potential failure scenarios. The RL agent is then deployed in these scenarios, and a reward model is learnt through the LLM preferences and feedback. This LLM-informed reward model is used to further train the RL agent on the original dataset. We apply our method to a maze navigation task, and show marked improvements in goal generalization, especially in cases where true and proxy goals are somewhat distinguishable and behavioral biases are pronounced. This study demonstrates how the LLM, despite its lack of task proficiency, can efficiently supervise RL agents, providing scalable oversight and valuable insights for enhancing goal-directed learning in RL through the use of LLMs. ",
        "title": "Reinforcement Learning from LLM Feedback to Counteract Goal  Misgeneralization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07183",
        "abstract_url": "http://arxiv.org/abs/2401.07183",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Huisheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "H. Vicky"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we study the optimal investment problem involving two agents, where the decision of one agent is influenced by the other. To measure the distance between two agents' decisions, we introduce the average deviation. We formulate the stochastic optimal control problem considering herd behavior and derive the analytical solution through the variational method. We theoretically analyze the impact of users' herd behavior on the optimal decision by decomposing it into their rational decisions, which is called the rational decision decomposition. Furthermore, to quantify the preference for their rational decision over that of the other agent, we introduce the agent's investment opinion. Our study is validated through simulations on real stock data. ",
        "title": "Herd Behavior in Optimal Investment: A Dual-Agent Approach with  Investment Opinion and Rational Decision Decomposition",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07187",
        "abstract_url": "http://arxiv.org/abs/2401.07187",
        "authors": [
            {
                "last_name": "Suh",
                "first_name": "Namjoon"
            },
            {
                "last_name": "Cheng",
                "first_name": "Guang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. In the last part, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs). The former two models are known to be the main pillars of the modern generative AI era, while ICL is a strong capability of LLMs in learning from a few examples in the context. Finally, we conclude the paper by suggesting several promising directions for deep learning theory. ",
        "title": "A Survey on Statistical Theory of Deep Learning: Approximation, Training  Dynamics, and Generative Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07188",
        "abstract_url": "http://arxiv.org/abs/2401.07188",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Hui",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Lu",
                "first_name": "Beijia"
            },
            {
                "last_name": "Lilith",
                "first_name": "Nimrod"
            },
            {
                "last_name": "Liu",
                "first_name": "Jun"
            },
            {
                "last_name": "Alam",
                "first_name": "Sameer"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Stereo matching neural networks often involve a Siamese structure to extract intermediate features from left and right images. The similarity between these intermediate left-right features significantly impacts the accuracy of disparity estimation. In this paper, we introduce a novel adversarial attack approach that generates perturbation noise specifically designed to maximize the discrepancy between left and right image features. Extensive experiments demonstrate the superior capability of our method to induce larger prediction errors in stereo neural networks, e.g. outperforming existing state-of-the-art attack methods by 219% MAE on the KITTI dataset and 85% MAE on the Scene Flow dataset. Additionally, we extend our approach to include a proxy network black-box attack method, eliminating the need for access to stereo neural network. This method leverages an arbitrary network from a different vision task as a proxy to generate adversarial noise, effectively causing the stereo network to produce erroneous predictions. Our findings highlight a notable sensitivity of stereo networks to discrepancies in shallow layer features, offering valuable insights that could guide future research in enhancing the robustness of stereo vision systems. ",
        "title": "Left-right Discrepancy for Adversarial Attack on Stereo Networks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07190",
        "abstract_url": "http://arxiv.org/abs/2401.07190",
        "authors": [
            {
                "last_name": "Vente",
                "first_name": "Blake"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This work finds limited evidence supporting the theory that using multiple tasks with sequence-to-sequence transformer language models can improve performance on some metrics. In particular, the multi-task generalist t5-small outperforms the specialist t5-small with a $F_1$ of $0.771$ up from $0.692$, which may point to underlying cross-task knowledge generalization. This further suggests that even with the same network, \"re-using\" the same data in a different way may lead to higher performance in some metrics. However, the inverse task alone is likely only an optimization strategy, since it does not yield a significant general improvement at the model sizes explored in this work. Also, adding $\\approx 4500$ LLM annotated records (interlaced with the $12800$ WebNLG training records) does not substantially change automatic metric performance compared to the same t5-small model without the synthetic data. This may be due to a learning capacity bottleneck on account of model size, and decreases observed may be due to distributional differences in the corpora. Future research using larger models or human evaluation is required to more fully explain the mechanisms contributing to performance on these tasks. ",
        "title": "Inroads to a Structured Data Natural Language Bijection and the role of  LLM annotation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07193",
        "abstract_url": "http://arxiv.org/abs/2401.07193",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Hu",
                "first_name": "Guanghui"
            },
            {
                "last_name": "Ma",
                "first_name": "Guanqiu"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper addresses a factorization method for imaging the support of a wave-number-dependent source function from multi-frequency data measured at a finite pair of symmetric receivers in opposite directions. The source function is given by the inverse Fourier transform of a compactly supported time-dependent source whose initial moment or terminal moment for radiating is unknown. Using the multi-frequency far-field data at two opposite observation directions, we provide a computational criterion for characterizing the smallest strip containing the support and perpendicular to the directions. A new parameter is incorporated into the design of test functions for indicating the unknown moment. The data from a finite pair of opposite directions can be used to recover the $\\Theta$-convex polygon of the support. Uniqueness in recovering the convex hull of the support is obtained as a by-product of our analysis using all observation directions. Similar results are also discussed with the multi-frequency near-field data from a finite pair of observation positions in three dimensions. We further comment on possible extensions to source functions with two disconnected supports. Extensive numerical tests in both two and three dimensions are implemented to show effectiveness and feasibility of the approach. The theoretical framework explored here should be seen as the frequency-domain analysis for inverse source problems in the time domain. ",
        "title": "Inverse wave-number-dependent source problems for the Helmholtz equation  with partial information on radiating period",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07194",
        "abstract_url": "http://arxiv.org/abs/2401.07194",
        "authors": [
            {
                "last_name": "Hussain",
                "first_name": "Razin Farhan"
            },
            {
                "last_name": "Salehi",
                "first_name": "Mohsen Amini"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The Industry 4.0 revolution has been made possible via AI-based applications (e.g., for automation and maintenance) deployed on the serverless edge (aka fog) computing platforms at the industrial sites -- where the data is generated. Nevertheless, fulfilling the fault-intolerant and real-time constraints of Industry 4.0 applications on resource-limited fog systems in remote industrial sites (e.g., offshore oil fields) that are uncertain, disaster-prone, and have no cloud access is challenging. It is this challenge that our research aims at addressing. We consider the inelastic nature of the fog systems, software architecture of the industrial applications (micro-service-based versus monolithic), and scarcity of human experts in remote sites. To enable cloud-like elasticity, our approach is to dynamically and seamlessly (i.e., without human intervention) federate nearby fog systems. Then, we develop serverless resource allocation solutions that are cognizant of the applications' software architecture, their latency requirements, and distributed nature of the underlying infrastructure. We propose methods to seamlessly and optimally partition micro-service-based application across the federated fog. Our experimental evaluation express that not only the elasticity is overcome in a serverless manner, but also our developed application partitioning method can serve around 20% more tasks on-time than the existing methods in the literature. ",
        "title": "Resource Allocation of Industry 4.0 Micro-Service Applications across  Serverless Fog Federation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07196",
        "abstract_url": "http://arxiv.org/abs/2401.07196",
        "authors": [
            {
                "last_name": "Goda",
                "first_name": "Takashi"
            },
            {
                "last_name": "Kazashi",
                "first_name": "Yoshihito"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Ken'ichiro"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Numerical integration over the real line for analytic functions is studied. Our main focus is on the sharpness of the error bounds. We first derive two general lower estimates for the worst-case integration error, and then apply these to establish lower bounds for various quadrature rules. These bounds turn out to be either novel or improve upon existing results, leading to lower bounds that closely match upper bounds for various formulas. Specifically, for the suitably truncated trapezoidal rule, we improve upon general lower bounds on the worst-case error obtained by Sugihara [\\textit{Numer. Math.}, 75 (1997), pp.~379--395] and provide exceptionally sharp lower bounds apart from a polynomial factor, in particular show that the worst-case error for the trapezoidal rule by Sugihara is not improvable more than a polynomial factor. Additionally, our research reveals a discrepancy between the error decay of the trapezoidal rule and Sugihara's lower bound for general numerical integration rules, introducing a new open problem. Moreover, Gauss--Hermite quadrature is proven sub-optimal under the decay conditions on integrands we consider, a result not deducible from upper-bound arguments alone. Furthermore, to establish the near-optimality of the suitably scaled Gauss--Legendre and Clenshaw--Curtis quadratures, we generalize a recent result of Trefethen [\\textit{SIAM Rev.}, 64 (2022), pp.~132--150] for the upper error bounds in terms of the decay conditions. ",
        "title": "How sharp are error bounds? --lower bounds on quadrature worst-case  errors for analytic functions",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07200",
        "abstract_url": "http://arxiv.org/abs/2401.07200",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Chen-Hsiu"
            },
            {
                "last_name": "Wu",
                "first_name": "Ja-Ling"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We propose an end-to-end learned image compression codec wherein the analysis transform is jointly trained with an object classification task. This study affirms that the compressed latent representation can predict human perceptual distance judgments with an accuracy comparable to a custom-tailored DNN-based quality metric. We further investigate various neural encoders and demonstrate the effectiveness of employing the analysis transform as a perceptual loss network for image tasks beyond quality judgments. Our experiments show that the off-the-shelf neural encoder proves proficient in perceptual modeling without needing an additional VGG network. We expect this research to serve as a valuable reference developing of a semantic-aware and coding-efficient neural encoder. ",
        "title": "Exploring Compressed Image Representation as a Perceptual Proxy: A Study",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07201",
        "abstract_url": "http://arxiv.org/abs/2401.07201",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jingyi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Bimanual manipulation needs robots to be sensitive on the grasp force which is hard to be accurately detected. This paper proposes RL framework for enhancing the grasp quality during the bimanual manipulation. This framework is based on finger configurations and its feedback. After that, the grasp quality is evaluated by the reward mechanism for the hands to determine strategies. There are 2 strategies, simultaneous and interleaved strategies, which will be determined in this framework to manipulate objects. In this paper, the contour and centroid of objects to the robot are unknown. Through the RL framework, robots can perceive hand-object relation and then optimize fingers configurations. The simulations and experiments showed that this framework can improve the success rates and finger motion accuracy. ",
        "title": "The Multi-fingered Kinematic Model for Dual-arm Manipulation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07205",
        "abstract_url": "http://arxiv.org/abs/2401.07205",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shiming"
            },
            {
                "last_name": "Ji",
                "first_name": "Zhe"
            },
            {
                "last_name": "Xiang",
                "first_name": "Liyao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinbing"
            },
            {
                "last_name": "Zhou",
                "first_name": "Chenghu"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CV",
            "LG"
        ],
        "abstract": "  With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods. ",
        "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft  on Deep Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07206",
        "abstract_url": "http://arxiv.org/abs/2401.07206",
        "authors": [
            {
                "last_name": "Mo",
                "first_name": "Yanfang"
            },
            {
                "last_name": "Qin",
                "first_name": "S. Joe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we propose a probabilistic reduced-dimensional vector autoregressive (PredVAR) model to extract low-dimensional dynamics from high-dimensional noisy data. The model utilizes an oblique projection to partition the measurement space into a subspace that accommodates the reduced-dimensional dynamics and a complementary static subspace. An optimal oblique decomposition is derived for the best predictability regarding prediction error covariance. Building on this, we develop an iterative PredVAR algorithm using maximum likelihood and the expectation-maximization (EM) framework. This algorithm alternately updates the estimates of the latent dynamics and optimal oblique projection, yielding dynamic latent variables with rank-ordered predictability and an explicit latent VAR model that is consistent with the outer projection model. The superior performance and efficiency of the proposed approach are demonstrated using data sets from a synthesized Lorenz system and an industrial process from Eastman Chemical. ",
        "title": "Probabilistic Reduced-Dimensional Vector Autoregressive Modeling with  Oblique Projections",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07207",
        "abstract_url": "http://arxiv.org/abs/2401.07207",
        "authors": [
            {
                "last_name": "Rostami",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A major technique for tackling unsupervised domain adaptation involves mapping data points from both the source and target domains into a shared embedding space. The mapping encoder to the embedding space is trained such that the embedding space becomes domain agnostic, allowing a classifier trained on the source domain to generalize well on the target domain. To further enhance the performance of unsupervised domain adaptation (UDA), we develop an additional technique which makes the internal distribution of the source domain more compact, thereby improving the model's ability to generalize in the target domain.We demonstrate that by increasing the margins between data representations for different classes in the embedding space, we can improve the model performance for UDA. To make the internal representation more compact, we estimate the internally learned multi-modal distribution of the source domain as Gaussian mixture model (GMM). Utilizing the estimated GMM, we enhance the separation between different classes in the source domain, thereby mitigating the effects of domain shift. We offer theoretical analysis to support outperofrmance of our method. To evaluate the effectiveness of our approach, we conduct experiments on widely used UDA benchmark UDA datasets. The results indicate that our method enhances model generalizability and outperforms existing techniques. ",
        "title": "Unsupervised Domain Adaptation Using Compact Internal Representations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07208",
        "abstract_url": "http://arxiv.org/abs/2401.07208",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Mingli"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zihao"
            },
            {
                "last_name": "Chen",
                "first_name": "Sihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Wu",
                "first_name": "Baoyuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Few-shot class-incremental learning (FSCIL) aims to continually fit new classes with limited training data, while maintaining the performance of previously learned classes. The main challenges are overfitting the rare new training samples and forgetting old classes. While catastrophic forgetting has been extensively studied, the overfitting problem has attracted less attention in FSCIL. To tackle overfitting challenge, we design a new ensemble model framework cooperated with data augmentation to boost generalization. In this way, the enhanced model works as a library storing abundant features to guarantee fast adaptation to downstream tasks. Specifically, the multi-input multi-output ensemble structure is applied with a spatial-aware data augmentation strategy, aiming at diversifying the feature extractor and alleviating overfitting in incremental sessions. Moreover, self-supervised learning is also integrated to further improve the model generalization. Comprehensive experimental results show that the proposed method can indeed mitigate the overfitting problem in FSCIL, and outperform the state-of-the-art methods. ",
        "title": "Enhanced Few-Shot Class-Incremental Learning via Ensemble Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07209",
        "abstract_url": "http://arxiv.org/abs/2401.07209",
        "authors": [
            {
                "last_name": "Raza",
                "first_name": "Ali"
            },
            {
                "last_name": "Penuel",
                "first_name": "William R."
            },
            {
                "last_name": "Sumner",
                "first_name": "Tamara"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Supporting equitable instruction is an important issue for teachers attending diverse STEM classrooms. Visual learning analytics along with effective student survey measures can support providing on time feedback to teachers in making instruction more culturally relevant to all students. We adopted a user-centered approach, where we engaged seven middle school science teachers in iterative testing of thirty data visualizations disaggregated over markers such as gender and race for implementation of selected displays in a visual learning analytics tool- Student Electronic Exit Ticket (SEET). This process helped us gather insights into teachers' sensemaking in identifying patterns of student data related to gender and race, selecting and improving the design of the feedback displays for the SEET [10]. ",
        "title": "Designing Visual Learning Analytics for Supporting Equity in STEM  Classrooms",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07211",
        "abstract_url": "http://arxiv.org/abs/2401.07211",
        "authors": [
            {
                "last_name": "Adenekan",
                "first_name": "Rachel A. G."
            },
            {
                "last_name": "Reyes",
                "first_name": "Alejandrina Gonzalez"
            },
            {
                "last_name": "Yoshida",
                "first_name": "Kyle T."
            },
            {
                "last_name": "Kodali",
                "first_name": "Sreela"
            },
            {
                "last_name": "Okamura",
                "first_name": "Allison M."
            },
            {
                "last_name": "Nunez",
                "first_name": "Cara M."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Tactile perception plays an important role in activities of daily living, and it can be impaired in individuals with medical conditions. The most common tools used to assess tactile sensation, the Semmes-Weinstein monofilaments and the 128 Hz tuning fork, have poor repeatability and resolution. Long term, we aim to provide a repeatable, high-resolution testing platform that can be used to assess vibrotactile perception through smartphones without the need for an experimenter to be present to conduct the test. We present a smartphone-based vibration perception measurement platform and compare its performance to measurements from standard monofilament and tuning fork tests. We conducted a user study with 36 healthy adults in which we tested each tool on the hand, wrist, and foot, to assess how well our smartphone-based vibration perception thresholds (VPTs) detect known trends obtained from standard tests. The smartphone platform detected statistically significant changes in VPT between the index finger and the foot and also between the feet of younger adults and older adults. Our smartphone-based VPT had a moderate correlation to tuning fork-based VPT. A long-term objective of this work is to develop an accessible smartphone-based platform that can be used to measure disease progression and regression. ",
        "title": "A Comparative Analysis of Smartphone and Standard Tools for Touch  Perception Assessment Across Multiple Body Sites",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07212",
        "abstract_url": "http://arxiv.org/abs/2401.07212",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Zexuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiahong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yankai"
            },
            {
                "last_name": "King",
                "first_name": "Irwin"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Existing unsupervised deep product quantization methods primarily aim for the increased similarity between different views of the identical image, whereas the delicate multi-level semantic similarities preserved between images are overlooked. Moreover, these methods predominantly focus on the Euclidean space for computational convenience, compromising their ability to map the multi-level semantic relationships between images effectively. To mitigate these shortcomings, we propose a novel unsupervised product quantization method dubbed \\textbf{Hi}erarchical \\textbf{H}yperbolic \\textbf{P}roduct \\textbf{Q}uantization (HiHPQ), which learns quantized representations by incorporating hierarchical semantic similarity within hyperbolic geometry. Specifically, we propose a hyperbolic product quantizer, where the hyperbolic codebook attention mechanism and the quantized contrastive learning on the hyperbolic product manifold are introduced to expedite quantization. Furthermore, we propose a hierarchical semantics learning module, designed to enhance the distinction between similar and non-matching images for a query by utilizing the extracted hierarchical semantics as an additional training supervision. Experiments on benchmarks show that our proposed method outperforms state-of-the-art baselines. ",
        "title": "HiHPQ: Hierarchical Hyperbolic Product Quantization for Unsupervised  Image Retrieval",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07213",
        "abstract_url": "http://arxiv.org/abs/2401.07213",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Honglei"
            },
            {
                "last_name": "Shu",
                "first_name": "Yan"
            },
            {
                "last_name": "Liu",
                "first_name": "Shaohui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single image dehazing is a challenging ill-posed problem. Existing datasets for training deep learning-based methods can be generated by hand-crafted or synthetic schemes. However, the former often suffers from small scales, while the latter forces models to learn scene depth instead of haze distribution, decreasing their dehazing ability. To overcome the problem, we propose a simple yet novel synthetic method to decouple the relationship between haze density and scene depth, by which a depth-agnostic dataset (DA-HAZE) is generated. Meanwhile, a Global Shuffle Strategy (GSS) is proposed for generating differently scaled datasets, thereby enhancing the generalization ability of the model. Extensive experiments indicate that models trained on DA-HAZE achieve significant improvements on real-world benchmarks, with less discrepancy between SOTS and DA-SOTS (the test set of DA-HAZE). Additionally, Depth-agnostic dehazing is a more complicated task because of the lack of depth prior. Therefore, an efficient architecture with stronger feature modeling ability and fewer computational costs is necessary. We revisit the U-Net-based architectures for dehazing, in which dedicatedly designed blocks are incorporated. However, the performances of blocks are constrained by limited feature fusion methods. To this end, we propose a Convolutional Skip Connection (CSC) module, allowing vanilla feature fusion methods to achieve promising results with minimal costs. Extensive experimental results demonstrate that current state-of-the-art methods. equipped with CSC can achieve better performance and reasonable computational expense, whether the haze distribution is relevant to the scene depth. ",
        "title": "Depth-agnostic Single Image Dehazing",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07216",
        "abstract_url": "http://arxiv.org/abs/2401.07216",
        "authors": [
            {
                "last_name": "Cherumanal",
                "first_name": "Sachin Pathiyan"
            },
            {
                "last_name": "Tian",
                "first_name": "Lin"
            },
            {
                "last_name": "Abushaqra",
                "first_name": "Futoon M."
            },
            {
                "last_name": "de Paula",
                "first_name": "Angel Felipe Magnossao"
            },
            {
                "last_name": "Ji",
                "first_name": "Kaixin"
            },
            {
                "last_name": "Hettiachchi",
                "first_name": "Danula"
            },
            {
                "last_name": "Trippas",
                "first_name": "Johanne R."
            },
            {
                "last_name": "Ali",
                "first_name": "Halil"
            },
            {
                "last_name": "Scholer",
                "first_name": "Falk"
            },
            {
                "last_name": "Spina",
                "first_name": "Damiano"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Creating and deploying customized applications is crucial for operational success and enriching user experiences in the rapidly evolving modern business world. A prominent facet of modern user experiences is the integration of chatbots or voice assistants. The rapid evolution of Large Language Models (LLMs) has provided a powerful tool to build conversational applications. We present Walert, a customized LLM-based conversational agent able to answer frequently asked questions about computer science degrees and programs at RMIT University. Our demo aims to showcase how conversational information-seeking researchers can effectively communicate the benefits of using best practices to stakeholders interested in developing and deploying LLM-based chatbots. These practices are well-known in our community but often overlooked by practitioners who may not have access to this knowledge. The methodology and resources used in this demo serve as a bridge to facilitate knowledge transfer from experts, address industry professionals' practical needs, and foster a collaborative environment. The data and code of the demo are available at https://github.com/rmit-ir/walert. ",
        "title": "Walert: Putting Conversational Search Knowledge into Action by Building  and Evaluating a Large Language Model-Powered Chatbot",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07218",
        "abstract_url": "http://arxiv.org/abs/2401.07218",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Junyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Lina"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bofeng"
            },
            {
                "last_name": "Wen",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Li",
                "first_name": "Wanlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  An event camera is a novel vision sensor that can capture per-pixel brightness changes and output a stream of asynchronous ``events''. It has advantages over conventional cameras in those scenes with high-speed motions and challenging lighting conditions because of the high temporal resolution, high dynamic range, low bandwidth, low power consumption, and no motion blur. Therefore, several supervised monocular depth estimation from events is proposed to address scenes difficult for conventional cameras. However, depth annotation is costly and time-consuming. In this paper, to lower the annotation cost, we propose a self-supervised event-based monocular depth estimation framework named EMoDepth. EMoDepth constrains the training process using the cross-modal consistency from intensity frames that are aligned with events in the pixel coordinate. Moreover, in inference, only events are used for monocular depth prediction. Additionally, we design a multi-scale skip-connection architecture to effectively fuse features for depth estimation while maintaining high inference speed. Experiments on MVSEC and DSEC datasets demonstrate that our contributions are effective and that the accuracy can outperform existing supervised event-based and unsupervised frame-based methods. ",
        "title": "Self-supervised Event-based Monocular Depth Estimation using Cross-modal  Consistency",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07220",
        "abstract_url": "http://arxiv.org/abs/2401.07220",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Linlin"
            },
            {
                "last_name": "Yu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Daud",
                "first_name": "Abdulateef"
            },
            {
                "last_name": "Mussah",
                "first_name": "Abdul Rashid"
            },
            {
                "last_name": "Adu-Gyamfi",
                "first_name": "Yaw"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Traffic cameras remain the primary source data for surveillance activities such as congestion and incident monitoring. To date, State agencies continue to rely on manual effort to extract data from networked cameras due to limitations of the current automatic vision systems including requirements for complex camera calibration and inability to generate high resolution data. This study implements a three-stage video analytics framework for extracting high-resolution traffic data such vehicle counts, speed, and acceleration from infrastructure-mounted CCTV cameras. The key components of the framework include object recognition, perspective transformation, and vehicle trajectory reconstruction for traffic data collection. First, a state-of-the-art vehicle recognition model is implemented to detect and classify vehicles. Next, to correct for camera distortion and reduce partial occlusion, an algorithm inspired by two-point linear perspective is utilized to extracts the region of interest (ROI) automatically, while a 2D homography technique transforms the CCTV view to bird's-eye view (BEV). Cameras are calibrated with a two-layer matrix system to enable the extraction of speed and acceleration by converting image coordinates to real-world measurements. Individual vehicle trajectories are constructed and compared in BEV using two time-space-feature-based object trackers, namely Motpy and BYTETrack. The results of the current study showed about +/- 4.5% error rate for directional traffic counts, less than 10% MSE for speed bias between camera estimates in comparison to estimates from probe data sources. Extracting high-resolution data from traffic cameras has several implications, ranging from improvements in traffic management and identify dangerous driving behavior, high-risk areas for accidents, and other safety concerns, enabling proactive measures to reduce accidents and fatalities. ",
        "title": "Application of 2D Homography for High Resolution Traffic Data Collection  using CCTV Cameras",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07222",
        "abstract_url": "http://arxiv.org/abs/2401.07222",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Kaijian"
            },
            {
                "last_name": "Liu",
                "first_name": "Tao"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents a new robust data-driven predictive control scheme for unknown linear time-invariant systems by using input-state-output or input-output data based on whether the state is measurable. To remove the need for the persistently exciting (PE) condition of a sufficiently high order on pre-collected data, a set containing all systems capable of generating such data is constructed. Then, at each time step, an upper bound of a given objective function is derived for all systems in the set, and a feedback controller is designed to minimize this bound. The optimal control gain at each time step is determined by solving a set of linear matrix inequalities. We prove that if the synthesis problem is feasible at the initial time step, it remains feasible for all future time steps. Unlike current data-driven predictive control schemes based on behavioral system theory, our approach requires less stringent conditions for the pre-collected data, facilitating easier implementation. Further, the proposed predictive control scheme features an infinite prediction horizon, potentially resulting in superior overall control performance compared to existing methods with finite prediction horizons. The effectiveness of our proposed methods is demonstrated through application to an unknown and unstable batch reactor. ",
        "title": "Robust Data-Driven Predictive Control for Unknown Linear Time-Invariant  Systems",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07224",
        "abstract_url": "http://arxiv.org/abs/2401.07224",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Qiong"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaobo"
            },
            {
                "last_name": "Fan",
                "first_name": "Pingyi"
            },
            {
                "last_name": "Fan",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Huiling"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiangzhou"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Federated learning (FL) is a promising technology for vehicular networks to protect vehicles' privacy in Internet of Vehicles (IoV). Vehicles with limited computation capacity may face a large computational burden associated with FL. Federated edge learning (FEEL) systems are introduced to solve such a problem. In FEEL systems, vehicles adopt the cellular-vehicle to everything (C-V2X) mode 4 to upload encrypted data to road side units' (RSUs)' cache queue. Then RSUs train the data transmitted by vehicles, update the locally model hyperparameters and send back results to vehicles, thus vehicles' computational burden can be released. However, each RSU has limited cache queue. To maintain the stability of cache queue and maximize the accuracy of model, it is essential to select appropriate vehicles to upload data. The vehicle selection method for FEEL systems faces challenges due to the random departure of data from the cache queue caused by the stochastic channel and the different system status of vehicles, such as remaining data amount, transmission delay, packet collision probability and survival ability. This paper proposes a vehicle selection method for FEEL systems that aims to maximize the accuracy of model while keeping the cache queue stable. Extensive simulation experiments demonstrate that our proposed method outperforms other baseline selection methods. ",
        "title": "Vehicle Selection for C-V2X Mode 4 Based Federated Edge Learning Systems",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07228",
        "abstract_url": "http://arxiv.org/abs/2401.07228",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Bo"
            },
            {
                "last_name": "Ma",
                "first_name": "Ying"
            },
            {
                "last_name": "Wang",
                "first_name": "Chushan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a Lawson-time-splitting extended Fourier pseudospectral (LTSeFP) method for the numerical integration of the Gross-Pitaevskii equation with time-dependent potential that is of low regularity in space. For the spatial discretization of low regularity potential, we use an extended Fourier pseudospectral (eFP) method, i.e., we compute the discrete Fourier transform of the low regularity potential in an extended window. For the temporal discretization, to efficiently implement the eFP method for time-dependent low regularity potential, we combine the standard time-splitting method with a Lawson-type exponential integrator to integrate potential and nonlinearity differently. The LTSeFP method is both accurate and efficient: it achieves first-order convergence in time and optimal-order convergence in space in $L^2$-norm under low regularity potential, while the computational cost is comparable to the standard time-splitting Fourier pseudospectral method. Theoretically, we also prove such convergence orders for a large class of spatially low regularity time-dependent potential. Extensive numerical results are reported to confirm the error estimates and to demonstrate the superiority of our method. ",
        "title": "A Lawson-time-splitting extended Fourier pseudospectral method for the  Gross-Pitaevskii equation with time-dependent low regularity potential",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07230",
        "abstract_url": "http://arxiv.org/abs/2401.07230",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Yue"
            },
            {
                "last_name": "He",
                "first_name": "Changyang"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "SI"
        ],
        "abstract": "  Quarantine is a widely-adopted measure during health crises caused by highly-contagious diseases like COVID-19, yet it poses critical challenges to public mental health. Given this context, emotional disclosure on social media in the form of keeping a diary emerges as a popular way for individuals to express emotions and record their mental health status. However, the exploration of emotional disclosure via diary-keeping on social media during quarantine is underexplored, understanding which could be beneficial to facilitate emotional connections and enlighten health intervention measures. Focusing on this particular form of self-disclosure, this work proposes a quantitative approach to figure out the prevalence and changing patterns of emotional disclosure during quarantine, and the possible factors contributing to the negative emotions. We collected 58, 796 posts with the \"Quarantine Diary\" keyword on Weibo, a popular social media website in China. Through text classification, we capture diverse emotion categories that characterize public emotion disclosure during quarantine, such as annoyed, anxious, boring, happy, hopeful and appreciative. Based on temporal analysis, we uncover the changing patterns of emotional disclosure from long-term perspectives and period-based perspectives (e.g., the gradual decline of all negative emotions and the upsurge of the annoyed emotion near the end of quarantine). Leveraging topic modeling, we also encapsulate the possible influencing factors of negative emotions, such as freedom restriction and solitude, and uncertainty of infection and supply. We reflect on how our findings could deepen the understanding of mental health on social media and further provide practical and design implications to mitigate mental health issues during quarantine. ",
        "title": "Understanding Emotional Disclosure via Diary-keeping in Quarantine on  Social Media",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07232",
        "abstract_url": "http://arxiv.org/abs/2401.07232",
        "authors": [
            {
                "last_name": "Sedov",
                "first_name": "Evgeny"
            },
            {
                "last_name": "Kavokin",
                "first_name": "Alexey"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  We introduce a novel neuromorphic network architecture based on a lattice of exciton-polariton condensates, intricately interconnected and energized through non-resonant optical pumping. The network employs a binary framework, where each neuron, facilitated by the spatial coherence of pairwise coupled condensates, performs binary operations. This coherence, emerging from the ballistic propagation of polaritons, ensures efficient, network-wide communication. The binary neuron switching mechanism, driven by the nonlinear repulsion through the excitonic component of polaritons, offers computational efficiency and scalability advantages over continuous weight neural networks. Our network enables parallel processing, enhancing computational speed compared to sequential or pulse-coded binary systems. The system's performance was evaluated using the MNIST dataset for handwritten digit recognition, showcasing the potential to outperform existing polaritonic neuromorphic systems, as demonstrated by its impressive predicted classification accuracy of up to 97.5%. ",
        "title": "Polariton lattices as binarized neuromorphic networks",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07234",
        "abstract_url": "http://arxiv.org/abs/2401.07234",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shuyao"
            },
            {
                "last_name": "Tay",
                "first_name": "Jordan"
            },
            {
                "last_name": "Baiz",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Credit risk forecasting plays a crucial role for commercial banks and other financial institutions in granting loans to customers and minimise the potential loss. However, traditional machine learning methods require the sharing of sensitive client information with an external server to build a global model, potentially posing a risk of security threats and privacy leakage. A newly developed privacy-preserving distributed machine learning technique known as Federated Learning (FL) allows the training of a global model without the necessity of accessing private local data directly. This investigation examined the feasibility of federated learning in credit risk assessment and showed the effects of data imbalance on model performance. Two neural network architectures, Multilayer Perceptron (MLP) and Long Short-Term Memory (LSTM), and one tree ensemble architecture, Extreme Gradient Boosting (XGBoost), were explored across three different datasets under various scenarios involving different numbers of clients and data distribution configurations. We demonstrate that federated models consistently outperform local models on non-dominant clients with smaller datasets. This trend is especially pronounced in highly imbalanced data scenarios, yielding a remarkable average improvement of 17.92% in model performance. However, for dominant clients (clients with more data), federated models may not exhibit superior performance, suggesting the need for special incentives for this type of clients to encourage their participation. ",
        "title": "The Effects of Data Imbalance Under a Federated Learning Approach for  Credit Risk Forecasting",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07237",
        "abstract_url": "http://arxiv.org/abs/2401.07237",
        "authors": [
            {
                "last_name": "Wadhwa",
                "first_name": "Somin"
            },
            {
                "last_name": "Hassanzadeh",
                "first_name": "Oktie"
            },
            {
                "last_name": "Bhattacharjya",
                "first_name": "Debarun"
            },
            {
                "last_name": "Barker",
                "first_name": "Ken"
            },
            {
                "last_name": "Ni",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex structured knowledge from pattern mining and probabilistic event models. We release our sequence generation code and evaluation framework, as well as corpus of event sequence data. ",
        "title": "Distilling Event Sequence Knowledge From Large Language Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07240",
        "abstract_url": "http://arxiv.org/abs/2401.07240",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Shuai"
            },
            {
                "last_name": "Li",
                "first_name": "Boyang"
            },
            {
                "last_name": "Fang",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Kai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, significant progress has been made in the research of 3D object detection. However, most prior studies have focused on the utilization of center-based or anchor-based label assignment schemes. Alternative label assignment strategies remain unexplored in 3D object detection. We find that the center-based label assignment often fails to generate sufficient positive samples for training, while the anchor-based label assignment tends to encounter an imbalanced issue when handling objects of varying scales. To solve these issues, we introduce a dynamic cross label assignment (DCLA) scheme, which dynamically assigns positive samples for each object from a cross-shaped region, thus providing sufficient and balanced positive samples for training. Furthermore, to address the challenge of accurately regressing objects with varying scales, we put forth a rotation-weighted Intersection over Union (RWIoU) metric to replace the widely used L1 metric in regression loss. Extensive experiments demonstrate the generality and effectiveness of our DCLA and RWIoU-based regression loss. The Code will be available at https://github.com/Say2L/DCDet.git. ",
        "title": "DCDet: Dynamic Cross-based 3D Object Detector",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07242",
        "abstract_url": "http://arxiv.org/abs/2401.07242",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Nadimpalli",
                "first_name": "Shivam"
            },
            {
                "last_name": "Randolph",
                "first_name": "Tim"
            },
            {
                "last_name": "Servedio",
                "first_name": "Rocco A."
            },
            {
                "last_name": "Zamir",
                "first_name": "Or"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  A subset $S$ of the Boolean hypercube $\\mathbb{F}_2^n$ is a *sumset* if $S = \\{a + b : a, b\\in A\\}$ for some $A \\subseteq \\mathbb{F}_2^n$. Sumsets are central objects of study in additive combinatorics, featuring in several influential results. We prove a lower bound of $\\Omega(2^{n/2})$ for the number of queries needed to test whether a Boolean function $f:\\mathbb{F}_2^n \\to \\{0,1\\}$ is the indicator function of a sumset. Our lower bound for testing sumsets follows from sharp bounds on the related problem of *shift testing*, which may be of independent interest. We also give a near-optimal {$2^{O(n/2)} \\cdot \\mathrm{poly}(n)$}-query algorithm for a smoothed analysis formulation of the sumset *refutation* problem. ",
        "title": "Testing Sumsets is Hard",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07245",
        "abstract_url": "http://arxiv.org/abs/2401.07245",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Guo",
                "first_name": "Xiaobao"
            },
            {
                "last_name": "Peng",
                "first_name": "Xiaojiang"
            },
            {
                "last_name": "Kot",
                "first_name": "Alex"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cutting-edge research in facial expression recognition (FER) currently favors the utilization of convolutional neural networks (CNNs) backbone which is supervisedly pre-trained on face recognition datasets for feature extraction. However, due to the vast scale of face recognition datasets and the high cost associated with collecting facial labels, this pre-training paradigm incurs significant expenses. Towards this end, we propose to pre-train vision Transformers (ViTs) through a self-supervised approach on a mid-scale general image dataset. In addition, when compared with the domain disparity existing between face datasets and FER datasets, the divergence between general datasets and FER datasets is more pronounced. Therefore, we propose a contrastive fine-tuning approach to effectively mitigate this domain disparity. Specifically, we introduce a novel FER training paradigm named Mask Image pre-training with MIx Contrastive fine-tuning (MIMIC). In the initial phase, we pre-train the ViT via masked image reconstruction on general images. Subsequently, in the fine-tuning stage, we introduce a mix-supervised contrastive learning process, which enhances the model with a more extensive range of positive samples by the mixing strategy. Through extensive experiments conducted on three benchmark datasets, we demonstrate that our MIMIC outperforms the previous training paradigm, showing its capability to learn better representations. Remarkably, the results indicate that the vanilla ViT can achieve impressive performance without the need for intricate, auxiliary-designed modules. Moreover, when scaling up the model size, MIMIC exhibits no performance saturation and is superior to the current state-of-the-art methods. ",
        "title": "MIMIC: Mask Image Pre-training with Mix Contrastive Fine-tuning for  Facial Expression Recognition",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07246",
        "abstract_url": "http://arxiv.org/abs/2401.07246",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Fridman",
                "first_name": "Emilia"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Recently, a constructive method was suggested for finite-dimensional observer-based control of 1D linear heat equation, which is robust to input/output delays. In this paper, we aim to extend this method to the 2D case with general time-varying input/output delays (known output delay and unknown input delay) or sawtooth delays (that correspond to network-based control). We use the modal decomposition approach and consider boundary or non-local sensing together with non-local actuation, or Neumann actuation with non-local sensing. To compensate the output delay that appears in the infinite-dimensional part of the closed-loop system, for the first time for delayed PDEs we suggest a vector Lyapunov functional combined with the recently introduced vector Halanay inequality. We provide linear matrix inequality (LMI) conditions for finding the observer dimension and upper bounds on delays that preserve the exponential stability. We prove that the LMIs are always feasible for large enough observer dimension and small enough upper bounds on delays. A numerical example demonstrates the efficiency of our method and shows that the employment of vector Halanay's inequality allows for larger delays than the classical scalar Halanay inequality for comparatively large observer dimension. ",
        "title": "Delayed finite-dimensional observer-based control of 2D linear parabolic  PDEs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07249",
        "abstract_url": "http://arxiv.org/abs/2401.07249",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Chu",
                "first_name": "Xu"
            },
            {
                "last_name": "Ma",
                "first_name": "Liantao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yasha"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wenwu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Irregularly sampled time series are ubiquitous, presenting significant challenges for analysis due to missing values. Despite existing methods address imputation, they predominantly focus on leveraging intra-series information, neglecting the potential benefits that inter-series information could provide, such as reducing uncertainty and memorization effect. To bridge this gap, we propose PRIME, a Prototype Recurrent Imputation ModEl, which integrates both intra-series and inter-series information for imputing missing values in irregularly sampled time series. Our framework comprises a prototype memory module for learning inter-series information, a bidirectional gated recurrent unit utilizing prototype information for imputation, and an attentive prototypical refinement module for adjusting imputations. We conducted extensive experiments on three datasets, and the results underscore PRIME's superiority over the state-of-the-art models by up to 26% relative improvement on mean square error. ",
        "title": "Imputation with Inter-Series Information from Prototypes for Irregular  Sampled Time Series",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07250",
        "abstract_url": "http://arxiv.org/abs/2401.07250",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Chengli"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiangshe"
            },
            {
                "last_name": "Liu",
                "first_name": "Junmin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yicheng"
            },
            {
                "last_name": "Hao",
                "first_name": "Yunda"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, sharpness-aware minimization (SAM) has attracted a lot of attention because of its surprising effectiveness in improving generalization performance.However, training neural networks with SAM can be highly unstable since the loss does not decrease along the direction of the exact gradient at the current point, but instead follows the direction of a surrogate gradient evaluated at another point nearby. To address this issue, we propose a simple renormalization strategy, dubbed StableSAM, so that the norm of the surrogate gradient maintains the same as that of the exact gradient. Our strategy is easy to implement and flexible enough to integrate with SAM and its variants, almost at no computational cost. With elementary tools from convex optimization and learning theory, we also conduct a theoretical analysis of sharpness-aware training, revealing that compared to stochastic gradient descent (SGD), the effectiveness of SAM is only assured in a limited regime of learning rate. In contrast, we show how StableSAM extends this regime of learning rate and when it can consistently perform better than SAM with minor modification. Finally, we demonstrate the improved performance of StableSAM on several representative data sets and tasks. ",
        "title": "Stabilizing Sharpness-aware Minimization Through A Simple  Renormalization Strategy",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07251",
        "abstract_url": "http://arxiv.org/abs/2401.07251",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Mao",
                "first_name": "Shuyi"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Peng",
                "first_name": "Xiaojiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  3D landmark detection plays a pivotal role in various applications such as 3D registration, pose estimation, and virtual try-on. While considerable success has been achieved in 2D human landmark detection or pose estimation, there is a notable scarcity of reported works on landmark detection in unordered 3D point clouds. This paper introduces a novel challenge, namely 3D landmark detection on human point clouds, presenting two primary contributions. Firstly, we establish a comprehensive human point cloud dataset, named HPoint103, designed to support the 3D landmark detection community. This dataset comprises 103 human point clouds created with commercial software and actors, each manually annotated with 11 stable landmarks. Secondly, we propose a Dual Cascade Point Transformer (D-CPT) model for precise point-based landmark detection. D-CPT gradually refines the landmarks through cascade Transformer decoder layers across the entire point cloud stream, simultaneously enhancing landmark coordinates with a RefineNet over local regions. Comparative evaluations with popular point-based methods on HPoint103 and the public dataset DHP19 demonstrate the dramatic outperformance of our D-CPT. Additionally, the integration of our RefineNet into existing methods consistently improves performance. ",
        "title": "3D Landmark Detection on Human Point Clouds: A Benchmark and A Dual  Cascade Point Transformer Framework",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07255",
        "abstract_url": "http://arxiv.org/abs/2401.07255",
        "authors": [
            {
                "last_name": "Tariverdi",
                "first_name": "Abbas"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "MA"
        ],
        "abstract": "  The paper begins by exploring the rationality of ethical trust as a foundational concept. This involves distinguishing between trust and trustworthiness and delving into scenarios where trust is both rational and moral. It lays the groundwork for understanding the complexities of trust dynamics in decision-making scenarios. Following this theoretical groundwork, we introduce an agent-based simulation framework that investigates these dynamics of ethical trust, specifically in the context of a disaster response scenario. These agents, utilizing emotional models like Plutchik's Wheel of Emotions and memory learning mechanisms, are tasked with allocating limited resources in disaster-affected areas. The model, which embodies the principles discussed in the first section, integrates cognitive load management, Big Five personality traits, and structured interactions within networked or hierarchical settings. It also includes feedback loops and simulates external events to evaluate their impact on the formation and evolution of trust among agents. Through our simulations, we demonstrate the intricate interplay of cognitive, emotional, and social factors in ethical decision-making. These insights shed light on the behaviors and resilience of trust networks in crisis situations, emphasizing the role of rational and moral considerations in the development of trust among autonomous agents. This study contributes to the field by offering an understanding of trust dynamics in socio-technical systems and by providing a robust, adaptable framework capable of addressing ethical dilemmas in disaster response and beyond. The implementation of the algorithms presented in this paper is available at this GitHub repository: \\url{https://github.com/abbas-tari/ethical-trust-cognitive-modeling}. ",
        "title": "Trust from Ethical Point of View: Exploring Dynamics Through  Multiagent-Driven Cognitive Modeling",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07256",
        "abstract_url": "http://arxiv.org/abs/2401.07256",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Zhihao"
            },
            {
                "last_name": "He",
                "first_name": "Jiafan"
            },
            {
                "last_name": "Hou",
                "first_name": "Luyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Lianming"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wendi"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  In emergency search and rescue scenarios, the quick location of trapped people is essential. However, disasters can render the Global Positioning System (GPS) unusable. Unmanned aerial vehicles (UAVs) with localization devices can serve as mobile anchors due to their agility and high line-of-sight (LoS) probability. Nonetheless, the number of available UAVs during the initial stages of disaster relief is limited, and innovative methods are needed to quickly plan UAV trajectories to locate non-uniformly distributed dynamic targets while ensuring localization accuracy. To address this challenge, we design a single UAV localization method without hovering, use the maximum likelihood estimation (MLE) method to estimate the location of mobile users and define the upper bound of the localization error by considering users' movement.Combining this localization method and localization error-index, we utilize the enhanced particle swarm optimization (EPSO) algorithm and edge access strategy to develop a low complexity localization-oriented adaptive trajectory planning algorithm. Simulation results demonstrate that our method outperforms other baseline algorithms, enabling faster localization without compromising localization accuracy. ",
        "title": "Emergency Localization for Mobile Ground Users: An Adaptive UAV  Trajectory Planning Method",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07257",
        "abstract_url": "http://arxiv.org/abs/2401.07257",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Hengchang"
            },
            {
                "last_name": "Liu",
                "first_name": "Qijiong"
            },
            {
                "last_name": "Li",
                "first_name": "Chuang"
            },
            {
                "last_name": "Kan",
                "first_name": "Min-Yen"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In Sequential Recommenders (SR), encoding and utilizing modalities in an end-to-end manner is costly in terms of modality encoder sizes. Two-stage approaches can mitigate such concerns, but they suffer from poor performance due to modality forgetting, where the sequential objective overshadows modality representation. We propose a lightweight knowledge distillation solution that preserves both merits: retaining modality information and maintaining high efficiency. Specifically, we introduce a novel method that enhances the learning of embeddings in SR through the supervision of modality correlations. The supervision signals are distilled from the original modality representations, including both (1) holistic correlations, which quantify their overall associations, and (2) dissected correlation types, which refine their relationship facets (honing in on specific aspects like color or shape consistency). To further address the issue of modality forgetting, we propose an asynchronous learning step, allowing the original information to be retained longer for training the representation learning module. Our approach is compatible with various backbone architectures and outperforms the top baselines by 6.8% on average. We empirically demonstrate that preserving original feature associations from modality encoders significantly boosts task-specific recommendation adaptation. Additionally, we find that larger modality encoders (e.g., Large Language Models) contain richer feature sets which necessitate more fine-grained modeling to reach their full performance potential. ",
        "title": "Lightweight Modality Adaptation to Sequential Recommendation via  Correlation Supervision",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07261",
        "abstract_url": "http://arxiv.org/abs/2401.07261",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Shoupeng"
            },
            {
                "last_name": "Tu",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Jian"
            },
            {
                "last_name": "Wu",
                "first_name": "Di"
            },
            {
                "last_name": "Ren",
                "first_name": "Kui"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  DeFi incidents stemming from various smart contract vulnerabilities have culminated in financial damages exceeding 3 billion USD. The attacks causing such incidents commonly commence with the deployment of adversarial contracts, subsequently leveraging these contracts to execute adversarial transactions that exploit vulnerabilities in victim contracts. Existing defense mechanisms leverage heuristic or machine learning algorithms to detect adversarial transactions, but they face significant challenges in detecting private adversarial transactions. Namely, attackers can send adversarial transactions directly to miners, evading visibility within the blockchain network and effectively bypassing the detection. In this paper, we propose a new direction for detecting DeFi attacks, i.e., detecting adversarial contracts instead of adversarial transactions, allowing us to proactively identify potential attack intentions, even if they employ private adversarial transactions. Specifically, we observe that most adversarial contracts follow a similar pattern, e.g., anonymous fund source, closed-source, frequent token-related function calls. Based on this observation, we build a machine learning classifier that can effectively distinguish adversarial contracts from benign ones. We build a dataset consists of features extracted from 304 adversarial contracts and 13,000 benign contracts. Based on this dataset, we evaluate different classifiers, the results of which show that our method for identifying DeFi adversarial contracts performs exceptionally well. For example, the F1-Score for LightGBM-based classifier is 0.9434, with a remarkably low false positive rate of only 0.12%. ",
        "title": "LookAhead: Preventing DeFi Attacks via Unveiling Adversarial Contracts",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07263",
        "abstract_url": "http://arxiv.org/abs/2401.07263",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jie"
            },
            {
                "last_name": "Chen",
                "first_name": "Wubing"
            },
            {
                "last_name": "Tan",
                "first_name": "Mao"
            },
            {
                "last_name": "Su",
                "first_name": "Yongxing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Despite the impressive capabilities of Deep Reinforcement Learning (DRL) agents in many challenging scenarios, their black-box decision-making process significantly limits their deployment in safety-sensitive domains. Several previous self-interpretable works focus on revealing the critical states of the agent's decision. However, they cannot pinpoint the error-prone states. To address this issue, we propose a novel self-interpretable structure, named Backbone Extract Tree (BET), to better explain the agent's behavior by identify the error-prone states. At a high level, BET hypothesizes that states in which the agent consistently executes uniform decisions exhibit a reduced propensity for errors. To effectively model this phenomenon, BET expresses these states within neighborhoods, each defined by a curated set of representative states. Therefore, states positioned at a greater distance from these representative benchmarks are more prone to error. We evaluate BET in various popular RL environments and show its superiority over existing self-interpretable models in terms of explanation fidelity. Furthermore, we demonstrate a use case for providing explanations for the agents in StarCraft II, a sophisticated multi-agent cooperative game. To the best of our knowledge, we are the first to explain such a complex scenarios using a fully transparent structure. ",
        "title": "BET: Explaining Deep Reinforcement Learning through The Error-Prone  Decisions",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07271",
        "abstract_url": "http://arxiv.org/abs/2401.07271",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Sheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Minheng"
            },
            {
                "last_name": "Wu",
                "first_name": "Junxian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziyue"
            },
            {
                "last_name": "Li",
                "first_name": "Tonglong"
            },
            {
                "last_name": "Xue",
                "first_name": "Cheng"
            },
            {
                "last_name": "Kong",
                "first_name": "Youyong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vertebrae identification in arbitrary fields-of-view plays a crucial role in diagnosing spine disease. Most spine CT contain only local regions, such as the neck, chest, and abdomen. Therefore, identification should not depend on specific vertebrae or a particular number of vertebrae being visible. Existing methods at the spine-level are unable to meet this challenge. In this paper, we propose a three-stage method to address the challenges in 3D CT vertebrae identification at vertebrae-level. By sequentially performing the tasks of vertebrae localization, segmentation, and identification, the anatomical prior information of the vertebrae is effectively utilized throughout the process. Specifically, we introduce a dual-factor density clustering algorithm to acquire localization information for individual vertebra, thereby facilitating subsequent segmentation and identification processes. In addition, to tackle the issue of interclass similarity and intra-class variability, we pre-train our identification network by using a supervised contrastive learning method. To further optimize the identification results, we estimated the uncertainty of the classification network and utilized the message fusion module to combine the uncertainty scores, while aggregating global information about the spine. Our method achieves state-of-the-art results on the VerSe19 and VerSe20 challenge benchmarks. Additionally, our approach demonstrates outstanding generalization performance on an collected dataset containing a wide range of abnormal cases. ",
        "title": "SpineCLUE: Automatic Vertebrae Identification Using Contrastive Learning  and Uncertainty Estimation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07272",
        "abstract_url": "http://arxiv.org/abs/2401.07272",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Zhengyang"
            },
            {
                "last_name": "Wang",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Super-resolution techniques are crucial in improving image granularity, particularly in complex urban scenes, where preserving geometric structures is vital for data-informed cultural heritage applications. In this paper, we propose a city scene super-resolution method via geometric error minimization. The geometric-consistent mechanism leverages the Hough Transform to extract regular geometric features in city scenes, enabling the computation of geometric errors between low-resolution and high-resolution images. By minimizing mixed mean square error and geometric align error during the super-resolution process, the proposed method efficiently restores details and geometric regularities. Extensive validations on the SET14, BSD300, Cityscapes and GSV-Cities datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, especially in urban scenes. ",
        "title": "City Scene Super-Resolution via Geometric Error Minimization",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07278",
        "abstract_url": "http://arxiv.org/abs/2401.07278",
        "authors": [
            {
                "last_name": "Luu",
                "first_name": "Vinh Quoc"
            },
            {
                "last_name": "Le",
                "first_name": "Duy Khanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy Thanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Minh Thanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Thinh Tien"
            },
            {
                "last_name": "Dinh",
                "first_name": "Vinh Quang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Artificial Intelligence (AI) in healthcare, especially in white blood cell cancer diagnosis, is hindered by two primary challenges: the lack of large-scale labeled datasets for white blood cell (WBC) segmentation and outdated segmentation methods. To address the first challenge, a semi-supervised learning framework should be brought to efficiently annotate the large dataset. In this work, we address this issue by proposing a novel self-training pipeline with the incorporation of FixMatch. We discover that by incorporating FixMatch in the self-training pipeline, the performance improves in the majority of cases. Our performance achieved the best performance with the self-training scheme with consistency on DeepLab-V3 architecture and ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC datasets, respectively. ",
        "title": "Semi-supervised Semantic Segmentation using Redesigned Self-Training for  White Blood Cel",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07282",
        "abstract_url": "http://arxiv.org/abs/2401.07282",
        "authors": [
            {
                "last_name": "Kamber",
                "first_name": "Anil"
            },
            {
                "last_name": "Yilmaz",
                "first_name": "H. Birkan"
            },
            {
                "last_name": "Pusane",
                "first_name": "Ali Emre"
            },
            {
                "last_name": "Tugcu",
                "first_name": "Tuna"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "ET"
        ],
        "abstract": "  Molecular communications is a technique emulated by researchers, which has already been used by the nature for millions of years. In Molecular Communications via Diffusion (MCvD), messenger molecules are emitted by a transmitter and propagate in the fluidic environment in a random manner. In biological systems, the environment can be considered a bounded space, surrounded by different structures, such as tissues and organs. The propagation of molecules is affected by these structures in the environment, which reflect the molecules upon collision. Hence, understanding the behavior of MCvD systems near reflecting surfaces is important for modeling molecular communication systems analytically. However, deriving the channel response of MCvD systems with an absorbing spherical receiver requires solving the diffusion equation in 3-D space in the presence of a reflecting boundary, which is extremely challenging. Therefore, derivation of the channel response in a bounded environment has remained one of the unanswered questions in the literature. In this paper, a method to model molecular communication systems near reflecting surfaces is proposed, and an analytical closed-form solution for the channel response is derived. ",
        "title": "Half-Space Modeling with Reflecting Surface in Molecular Communication",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07283",
        "abstract_url": "http://arxiv.org/abs/2401.07283",
        "authors": [
            {
                "last_name": "Miandji",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Tongbuasirilai",
                "first_name": "Tanaboon"
            },
            {
                "last_name": "Hajisharif",
                "first_name": "Saghi"
            },
            {
                "last_name": "Kavoosighafi",
                "first_name": "Behnaz"
            },
            {
                "last_name": "Unger",
                "first_name": "Jonas"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CV"
        ],
        "abstract": "  Efficient and accurate BRDF acquisition of real world materials is a challenging research problem that requires sampling millions of incident light and viewing directions. To accelerate the acquisition process, one needs to find a minimal set of sampling directions such that the recovery of the full BRDF is accurate and robust given such samples. In this paper, we formulate BRDF acquisition as a compressed sensing problem, where the sensing operator is one that performs sub-sampling of the BRDF signal according to a set of optimal sample directions. To solve this problem, we propose the Fast and Robust Optimal Sampling Technique (FROST) for designing a provably optimal sub-sampling operator that places light-view samples such that the recovery error is minimized. FROST casts the problem of designing an optimal sub-sampling operator for compressed sensing into a sparse representation formulation under the Multiple Measurement Vector (MMV) signal model. The proposed reformulation is exact, i.e. without any approximations, hence it converts an intractable combinatorial problem into one that can be solved with standard optimization techniques. As a result, FROST is accompanied by strong theoretical guarantees from the field of compressed sensing. We perform a thorough analysis of FROST-BRDF using a 10-fold cross-validation with publicly available BRDF datasets and show significant advantages compared to the state-of-the-art with respect to reconstruction quality. Finally, FROST is simple, both conceptually and in terms of implementation, it produces consistent results at each run, and it is at least two orders of magnitude faster than the prior art. ",
        "title": "FROST-BRDF: A Fast and Robust Optimal Sampling Technique for BRDF  Acquisition",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07284",
        "abstract_url": "http://arxiv.org/abs/2401.07284",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Ting"
            },
            {
                "last_name": "Huang",
                "first_name": "Shaohan"
            },
            {
                "last_name": "Luo",
                "first_name": "Shengyue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Huang",
                "first_name": "Haizhen"
            },
            {
                "last_name": "Wei",
                "first_name": "Furu"
            },
            {
                "last_name": "Deng",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Sun",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            },
            {
                "last_name": "Wang",
                "first_name": "Deqing"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Fuzhen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement exceeding 5% in domain-specific tasks. Our code will available at https://github.com/microsoft/LMOps. ",
        "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07286",
        "abstract_url": "http://arxiv.org/abs/2401.07286",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weiqi"
            },
            {
                "last_name": "Fang",
                "first_name": "Tianqing"
            },
            {
                "last_name": "Li",
                "first_name": "Chunyang"
            },
            {
                "last_name": "Shi",
                "first_name": "Haochen"
            },
            {
                "last_name": "Ding",
                "first_name": "Wenxuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Baixuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaowei"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xin"
            },
            {
                "last_name": "Cheng",
                "first_name": "Jiayang"
            },
            {
                "last_name": "Chan",
                "first_name": "Chunkit"
            },
            {
                "last_name": "Song",
                "first_name": "Yangqiu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across four downstream tasks. Our code, data, and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE. ",
        "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from  Large Language Models for Commonsense Reasoning",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07288",
        "abstract_url": "http://arxiv.org/abs/2401.07288",
        "authors": [
            {
                "last_name": "Sheshjavani",
                "first_name": "Abdollah Ghaffari"
            },
            {
                "last_name": "Khonsari",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Moradian",
                "first_name": "Masoumeh"
            },
            {
                "last_name": "Shariatpanahi",
                "first_name": "Seyed Pooya"
            },
            {
                "last_name": "Hassanpour",
                "first_name": "Seyedeh Bahereh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  To address the massive growth of data traffic over cellular networks, increasing spatial reuse of the frequency spectrum by the deployment of small base stations (SBSs) has been considered. For rapid deployment of SBSs in the networks, caching popular content along with new coded caching schemes are proposed. To maximize the cellular network's capacity, densifying it with small base stations is inevitable. In ultra-dense cellular networks, coverage of SBSs may overlap. To this aim, the multi-access caching system, where users potentially can access multiple cache nodes simultaneously, has attracted more attention in recent years. Most previous works on multi-access coded caching, only consider specific conditions such as cyclic wrap-around network topologies. In this paper, we investigate caching in ultra-dense cellular networks, where different users can access different numbers of caches under non-uniform content popularity distribution, and propose Multi-Access Hybrid coded-uncoded Caching (MAHC). We formulate the optimization problem of the proposed scheme for general network topologies and evaluate it for 2-SBS network scenarios. The numerical and simulation results show that the proposed MAHC scheme outperforms optimal conventional uncoded and previous multi-access coded caching (MACC) schemes. ",
        "title": "Hybrid Coded-Uncoded Caching in Multi-Access Networks with Non-uniform  Demands",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07290",
        "abstract_url": "http://arxiv.org/abs/2401.07290",
        "authors": [
            {
                "last_name": "Mahadevan",
                "first_name": "Ananth"
            },
            {
                "last_name": "Mathioudakis",
                "first_name": "Michael"
            },
            {
                "last_name": "M\u00e4kel\u00e4",
                "first_name": "Eetu"
            },
            {
                "last_name": "Tolonen",
                "first_name": "Mikko"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  Text reuse is a methodological element of fundamental importance in humanities research: pieces of text that re-appear across different documents, verbatim or paraphrased, provide invaluable information about the historical spread and evolution of ideas. Large modern digitized corpora enable the joint analysis of text collections that span entire centuries and the detection of large-scale patterns, impossible to detect with traditional small-scale analysis. For this opportunity to materialize, it is necessary to develop efficient data science systems that perform the corresponding analysis tasks.   In this paper, we share insights from ReceptionReader, a system for analyzing text reuse in large historical corpora. The system is built upon billions of instances of text reuses from large digitized corpora of 18th-century texts. Its main functionality is to perform downstream text reuse analysis tasks, such as finding reuses that stem from a given article or identifying the most reused quotes from a set of documents, with each task expressed as a database query. For the purposes of the paper, we discuss the related design choices including various database normalization levels and query execution frameworks, such as distributed data processing (Apache Spark), indexed row store engine (MariaDB Aria), and compressed column store engine (MariaDB Columnstore). Moreover, we present an extensive evaluation with various metrics of interest (latency, storage size, and computing costs) for varying workloads, and we offer insights from the trade-offs we observed and the choices that emerged as optimal in our setting. In summary, our results show that (1) for the workloads that are most relevant to text-reuse analysis, the MariaDB Aria framework emerges as the overall optimal choice, (2) big data processing (Apache Spark) is irreplaceable for all processing stages of the system's pipeline. ",
        "title": "Optimizing a Data Science System for Text Reuse Analysis",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07291",
        "abstract_url": "http://arxiv.org/abs/2401.07291",
        "authors": [
            {
                "last_name": "Buckwar",
                "first_name": "Evelyn"
            },
            {
                "last_name": "Djurdjevac",
                "first_name": "Ana"
            },
            {
                "last_name": "Eisenmann",
                "first_name": "Monika"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years, SPDEs have become a well-studied field in mathematics. With their increase in popularity, it becomes important to efficiently approximate their solutions. Thus, our goal is a contribution towards the development of efficient and practical time-stepping methods for SPDEs.   Operator splitting schemes are a powerful tool for deterministic and stochastic differential equations. An example is given by domain decomposition schemes, where we split the domain into sub-domains. Instead of solving one expensive problem on the entire domain, we deal with cheaper problems on the sub-domains. This is particularly useful in modern computer architectures, as the sub-problems may often be solved in parallel. While splitting methods have already been used to study domain decomposition methods for deterministic PDEs, this is a new approach for SPDEs.   We provide an abstract convergence analysis of a splitting scheme for stochastic evolution equations and state a domain decomposition scheme as an application of the setting. The theoretical results are verified through numerical experiments. ",
        "title": "A domain decomposition method for stochastic evolution equations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07298",
        "abstract_url": "http://arxiv.org/abs/2401.07298",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Yue"
            },
            {
                "last_name": "Hsieh",
                "first_name": "Cho-Jui"
            },
            {
                "last_name": "Lee",
                "first_name": "Thomas C. M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In the stochastic contextual low-rank matrix bandit problem, the expected reward of an action is given by the inner product between the action's feature matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\\Theta^*$ with rank $r \\ll \\{d_1, d_2\\}$, and an agent sequentially takes actions based on past experience to maximize the cumulative reward. In this paper, we study the generalized low-rank matrix bandit problem, which has been recently proposed in \\cite{lu2021low} under the Generalized Linear Model (GLM) framework. To overcome the computational infeasibility and theoretical restrain of existing algorithms on this problem, we first propose the G-ESTT framework that modifies the idea from \\cite{jun2019bilinear} by using Stein's method on the subspace estimation and then leverage the estimated subspaces via a regularization idea. Furthermore, we remarkably improve the efficiency of G-ESTT by using a novel exclusion idea on the estimated subspace instead, and propose the G-ESTS framework. We also show that G-ESTT can achieve the $\\tilde{O}(\\sqrt{(d_1+d_2)MrT})$ bound of regret while G-ESTS can achineve the $\\tilde{O}(\\sqrt{(d_1+d_2)^{3/2}Mr^{3/2}T})$ bound of regret under mild assumption up to logarithm terms, where $M$ is some problem dependent value. Under a reasonable assumption that $M = O((d_1+d_2)^2)$ in our problem setting, the regret of G-ESTT is consistent with the current best regret of $\\tilde{O}((d_1+d_2)^{3/2} \\sqrt{rT}/D_{rr})$~\\citep{lu2021low} ($D_{rr}$ will be defined later). For completeness, we conduct experiments to illustrate that our proposed algorithms, especially G-ESTS, are also computationally tractable and consistently outperform other state-of-the-art (generalized) linear matrix bandit methods based on a suite of simulations. ",
        "title": "Efficient Frameworks for Generalized Low-Rank Matrix Bandit Problems",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07300",
        "abstract_url": "http://arxiv.org/abs/2401.07300",
        "authors": [
            {
                "last_name": "Riva",
                "first_name": "Stefano"
            },
            {
                "last_name": "Introini",
                "first_name": "Carolina"
            },
            {
                "last_name": "Cammi",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Nowadays, interest in combining mathematical knowledge about phenomena and data from the physical system is growing. Past research was devoted to developing so-called high-fidelity models, intending to make them able to catch most of the physical phenomena occurring in the system. Nevertheless, models will always be affected by uncertainties related, for example, to the parameters and inevitably limited by the underlying simplifying hypotheses on, for example, geometry and mathematical equations; thus, in a way, there exists an upper threshold of model performance. Now, research in many engineering sectors also focuses on the so-called data-driven modelling, which aims at extracting information from available data to combine it with the mathematical model. Focusing on the nuclear field, interest in this approach is also related to the Multi-Physics modelling of nuclear reactors. Due to the multiple physics involved and their mutual and complex interactions, developing accurate and stable models both from the physical and numerical point of view remains a challenging task despite the advancements in computational hardware and software, and combining the available mathematical model with data can further improve the performance and the accuracy of the former.   This work investigates this aspect by applying two Data-Driven Reduced Order Modelling (DDROM) techniques, the Generalised Empirical Interpolation Method and the Parametrised-Background Data-Weak formulation, to literature benchmark nuclear case studies. The main goal of this work is to assess the possibility of using data to perform model bias correction, that is, verifying the reliability of DDROM approaches in improving the model performance and accuracy through the information provided by the data. The obtained numerical results are promising, foreseeing further investigation of the DDROM approach to nuclear industrial cases. ",
        "title": "Multi-Physics Model Bias Correction with Data-Driven Reduced Order  Modelling Techniques: Application to Nuclear Case Studies",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07301",
        "abstract_url": "http://arxiv.org/abs/2401.07301",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Haixia"
            },
            {
                "last_name": "Liang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Shi",
                "first_name": "Jie"
            },
            {
                "last_name": "He",
                "first_name": "Qianyu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yanghua"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Generative Language Models (LMs) such as ChatGPT have exhibited remarkable performance across various downstream tasks. Nevertheless, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. Previous studies have devised sophisticated pipelines and prompts to induce large LMs to exhibit the capability for self-correction. However, large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans. Moreover, these complex prompts are extremely challenging for small LMs to follow. In this paper, we introduce the \\underline{I}ntrinsic \\underline{S}elf-\\underline{C}orrection (ISC) in generative language models, aiming to correct the initial output of LMs in a self-triggered manner, even for those small LMs with 6 billion parameters. Specifically, we devise a pipeline for constructing self-correction data and propose Partial Answer Masking (PAM), aiming to endow the model with the capability for intrinsic self-correction through fine-tuning. We conduct experiments using LMs with parameters sizes ranging from 6 billion to 13 billion in two tasks, including commonsense reasoning and factual knowledge reasoning. Our experiments demonstrate that the outputs generated using ISC outperform those generated without self-correction. We believe that the output quality of even small LMs can be further improved by empowering them with the ability to intrinsic self-correct. ",
        "title": "Small Language Model Can Self-correct",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07305",
        "abstract_url": "http://arxiv.org/abs/2401.07305",
        "authors": [
            {
                "last_name": "Kuang",
                "first_name": "Xu"
            },
            {
                "last_name": "Mendelson",
                "first_name": "Gal"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Being able to detect service slowdowns is crucial to many operational problems. We study how to use observational congestion data to detect service slowdown in a multi-server system, and in particular, the statistical implications of running adaptive congestion control mechanisms in such settings. We show that a commonly used summary statistic that relies on the marginal congestion measured at individual servers can be highly inaccurate the presence of adaptive congestion control. We propose a new statistic based on potential routing actions, and show it provides a much more robust signal for server slowdown in these settings. Unlike the marginal statistic, potential action aims to detect changes in the {routing actions}, and is able to uncover slowdowns even when they do not reflect in marginal congestion. Our work highlights the complexity in performing observational statistical analysis for service systems in the presence of adaptive congestion control. Our results also suggest that practitioners may want to combine multiple, orthogonal statistics to achieve reliable slowdown detection. ",
        "title": "Detecting Service Slowdown using Observational Data",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07308",
        "abstract_url": "http://arxiv.org/abs/2401.07308",
        "authors": [
            {
                "last_name": "Alahmadi",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Alharbi",
                "first_name": "Salma"
            },
            {
                "last_name": "Alharbi",
                "first_name": "Talal"
            },
            {
                "last_name": "Almutairi",
                "first_name": "Nadiyah"
            },
            {
                "last_name": "Alshammari",
                "first_name": "Tuwailaa"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Anirban"
            },
            {
                "last_name": "Koutny",
                "first_name": "Maciej"
            },
            {
                "last_name": "Li",
                "first_name": "Bowen"
            },
            {
                "last_name": "Randell",
                "first_name": "Brian"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The concept of structured occurrence nets is an extension of that of occurrence nets which are directed acyclic graphs that represent causality and concurrency information concerning a single execution of a distributed system. The formalism of structured occurrence nets has been introduced to facilitate the portrayal and analysis of the behaviours, and in particular failures, of complex evolving systems. Such systems are composed of a large number of sub-systems which may proceed concurrently and interact with each other and with the external environment while their behaviour is subject to modification by other systems. The purpose of this paper is to provide an extension of structured occurrence nets to include models built up of acyclic nets rather than occurrence nets. ",
        "title": "Structured Acyclic Nets",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07310",
        "abstract_url": "http://arxiv.org/abs/2401.07310",
        "authors": [
            {
                "last_name": "Chowdhury",
                "first_name": "Ahmadul Karim"
            },
            {
                "last_name": "Sujon",
                "first_name": "Md. Saidur Rahman"
            },
            {
                "last_name": "Shafi",
                "first_name": "Md. Shirajus Salekin"
            },
            {
                "last_name": "Ahmmad",
                "first_name": "Tasin"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Sifat"
            },
            {
                "last_name": "Hasib",
                "first_name": "Khan Md"
            },
            {
                "last_name": "Shah",
                "first_name": "Faisal Muhammad"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In an era where the silent struggle of underdiagnosed depression pervades globally, our research delves into the crucial link between mental health and social media. This work focuses on early detection of depression, particularly in extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our proposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning models(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT, SahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into \"Depressive\" and \"Non-Depressive\" segments, translated into Bengali by native speakers with expertise in mental health, resulting in the creation of the Bengali Social Media Depressive Dataset (BSMDD). Our work provides full architecture details for each model and a methodical way to assess their performance in Bengali depressive text categorization using zero-shot and few-shot learning techniques. Our work demonstrates the superiority of SahajBERT and Bi-LSTM with FastText embeddings in their respective domains also tackles explainability issues with transformer models and emphasizes the effectiveness of LLMs, especially DepGPT, demonstrating flexibility and competence in a range of learning contexts. According to the experiment results, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in zero-shot and few-shot scenarios but also every other model, achieving a near-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and exceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B show relatively poorer effectiveness in zero-shot and few-shot situations. The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models. ",
        "title": "Harnessing Large Language Models Over Transformer Models for Detecting  Bengali Depressive Social Media Text: A Comprehensive Study",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07312",
        "abstract_url": "http://arxiv.org/abs/2401.07312",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "JiayiZhou. Renzhong"
            },
            {
                "last_name": "Tang",
                "first_name": "Junxiu"
            },
            {
                "last_name": "Tang",
                "first_name": "Tan"
            },
            {
                "last_name": "Li",
                "first_name": "Haotian"
            },
            {
                "last_name": "Cui",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Wu",
                "first_name": "Yingcaui"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Creative design is a nonlinear process where designers generate diverse ideas in the pursuit of an open-ended goal and converge towards consensus through iterative remixing. In contrast, AI-powered design tools often employ a linear sequence of incremental and precise instructions to approximate design objectives. Such operations violate customary creative design practices and thus hinder AI agents' ability to complete creative design tasks. To explore better human-AI co-design tools, we first summarize human designers' practices through a formative study with 12 design experts. Taking graphic design as a representative scenario, we formulate a nonlinear human-AI co-design framework and develop a proof-of-concept prototype, OptiMuse. We evaluate OptiMuse and validate the nonlinear framework through a comparative study. We notice a subconscious change in people's attitudes towards AI agents, shifting from perceiving them as mere executors to regarding them as opinionated colleagues. This shift effectively fostered the exploration and reflection processes of individual designers. ",
        "title": "Understanding Nonlinear Collaboration between Human and AI Agents: A  Co-design Framework for Creative Design",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07314",
        "abstract_url": "http://arxiv.org/abs/2401.07314",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Lin",
                "first_name": "Bingqian"
            },
            {
                "last_name": "Xu",
                "first_name": "Ran"
            },
            {
                "last_name": "Chai",
                "first_name": "Zhenhua"
            },
            {
                "last_name": "Liang",
                "first_name": "Xiaodan"
            },
            {
                "last_name": "Wong",
                "first_name": "Kwan-Yee K."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Embodied agents equipped with GPT as their brain have exhibited extraordinary thinking and decision-making abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT to handle excessive environmental information and select potential locations within localized environments, without constructing an effective ''global-view'' (e.g., a commonly-used map) for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based path-planning agent, dubbed MapGPT, for the zero-shot VLN task. Specifically, we convert a topological map constructed online into prompts to encourage map-guided global exploration, and require the agent to explicitly output and update multi-step path planning to avoid getting stuck in local exploration. Extensive experiments demonstrate that our MapGPT is effective, achieving impressive performance on both the R2R and REVERIE datasets (38.8% and 28.4% success rate, respectively) and showcasing the newly emerged global thinking and path planning capabilities of the GPT model. Unlike previous VLN agents, which require separate parameters fine-tuning or specific prompt design to accommodate various instruction styles across different datasets, our MapGPT is more unified as it can adapt to different instruction styles seamlessly, which is the first of its kind in this field. ",
        "title": "MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07316",
        "abstract_url": "http://arxiv.org/abs/2401.07316",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Feiyang"
            },
            {
                "last_name": "\u00d8stvold",
                "first_name": "Bjarte M."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Privacy code review is a critical process that enables developers and legal experts to ensure compliance with data protection regulations. However, the task is challenging due to resource constraints. To address this, we introduce the concept of privacy-relevant methods - specific methods in code that are directly involved in the processing of personal data. We then present an automated approach to assist in code review by identifying and categorizing these privacy-relevant methods in source code.   Using static analysis, we identify a set of methods based on their occurrences in 50 commonly used libraries. We then rank these methods according to their frequency of invocation with actual personal data in the top 30 GitHub applications. The highest-ranked methods are the ones we designate as privacy-relevant in practice. For our evaluation, we examined 100 open-source applications and found that our approach identifies fewer than 5% of the methods as privacy-relevant for personal data processing. This reduces the time required for code reviews. Case studies on Signal Desktop and Cal.com further validate the effectiveness of our approach in aiding code reviewers to produce enhanced reports that facilitate compliance with privacy regulations. ",
        "title": "Finding Privacy-relevant Source Code",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07319",
        "abstract_url": "http://arxiv.org/abs/2401.07319",
        "authors": [
            {
                "last_name": "Friedlander",
                "first_name": "Izzy"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The MacWilliams Identity is a well established theorem relating the weight enumerator of a code to the weight enumerator of its dual. The ability to use a known weight enumerator to generate the weight enumerator of another through a simple transform proved highly effective and efficient. An equivalent relation was also developed by Delsarte which linked the eigenvalues of any association scheme to the eigenvalues of it's dual association scheme but this was less practical to use in reality. A functional transform was developed for some specific association schemes including those based on the rank metric, the skew rank metric and Hermitian matrices. In this paper those results are unified into a single consistent theory applied to these \"Krawtchouk association schemes\" using a $b$-algebra. The derivatives formed using the $b$-algebra have also been applied to derive the moments of the weight distribution for any code within these association schemes. ",
        "title": "The MacWilliams Identity for Krawtchouk Association Schemes",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07322",
        "abstract_url": "http://arxiv.org/abs/2401.07322",
        "authors": [
            {
                "last_name": "Zunair",
                "first_name": "Hasib"
            },
            {
                "last_name": "Khan",
                "first_name": "Shakib"
            },
            {
                "last_name": "Hamza",
                "first_name": "A. Ben"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Road scene understanding is crucial in autonomous driving, enabling machines to perceive the visual environment. However, recent object detectors tailored for learning on datasets collected from certain geographical locations struggle to generalize across different locations. In this paper, we present RSUD20K, a new dataset for road scene understanding, comprised of over 20K high-resolution images from the driving perspective on Bangladesh roads, and includes 130K bounding box annotations for 13 objects. This challenging dataset encompasses diverse road scenes, narrow streets and highways, featuring objects from different viewpoints and scenes from crowded environments with densely cluttered objects and various weather conditions. Our work significantly improves upon previous efforts, providing detailed annotations and increased object complexity. We thoroughly examine the dataset, benchmarking various state-of-the-art object detectors and exploring large vision models as image annotators. ",
        "title": "RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07323",
        "abstract_url": "http://arxiv.org/abs/2401.07323",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Toyota"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  High-Definition (HD) maps are pivotal to autopilot navigation. Integrating the capability of lightweight HD map construction at runtime into a self-driving system recently emerges as a promising direction. In this surge, vision-only perception stands out, as a camera rig can still perceive the stereo information, let alone its appealing signature of portability and economy. The latest MapTR architecture solves the online HD map construction task in an end-to-end fashion but its potential is yet to be explored. In this work, we present a full-scale upgrade of MapTR and propose MapNeXt, the next generation of HD map learning architecture, delivering major contributions from the model training and scaling perspectives. After shedding light on the training dynamics of MapTR and exploiting the supervision from map elements thoroughly, MapNeXt-Tiny raises the mAP of MapTR-Tiny from 49.0% to 54.8%, without any architectural modifications. Enjoying the fruit of map segmentation pre-training, MapNeXt-Base further lifts the mAP up to 63.9% that has already outperformed the prior art, a multi-modality MapTR, by 1.4% while being $\\sim1.8\\times$ faster. Towards pushing the performance frontier to the next level, we draw two conclusions on practical model scaling: increased query favors a larger decoder network for adequate digestion; a large backbone steadily promotes the final accuracy without bells and whistles. Building upon these two rules of thumb, MapNeXt-Huge achieves state-of-the-art performance on the challenging nuScenes benchmark. Specifically, we push the mapless vision-only single-model performance to be over 78% for the first time, exceeding the best model from existing methods by 16%. ",
        "title": "MapNeXt: Revisiting Training and Scaling Practices for Online Vectorized  HD Map Construction",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07324",
        "abstract_url": "http://arxiv.org/abs/2401.07324",
        "authors": [
            {
                "last_name": "Shen",
                "first_name": "Weizhou"
            },
            {
                "last_name": "Li",
                "first_name": "Chenliang"
            },
            {
                "last_name": "Chen",
                "first_name": "Hongzhan"
            },
            {
                "last_name": "Yan",
                "first_name": "Ming"
            },
            {
                "last_name": "Quan",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hehong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ji"
            },
            {
                "last_name": "Huang",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. ",
        "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07326",
        "abstract_url": "http://arxiv.org/abs/2401.07326",
        "authors": [
            {
                "last_name": "Chung",
                "first_name": "Dat T."
            },
            {
                "last_name": "Dang",
                "first_name": "Minh-Anh"
            },
            {
                "last_name": "Vu",
                "first_name": "Mai-Anh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Minh T."
            },
            {
                "last_name": "Nguyen",
                "first_name": "Thanh-Huy"
            },
            {
                "last_name": "Dinh",
                "first_name": "Vinh Q."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Breast Ultrasound plays a vital role in cancer diagnosis as a non-invasive approach with cost-effective. In recent years, with the development of deep learning, many CNN-based approaches have been widely researched in both tumor localization and cancer classification tasks. Even though previous single models achieved great performance in both tasks, these methods have some limitations in inference time, GPU requirement, and separate fine-tuning for each model. In this study, we aim to redesign and build end-to-end multi-task architecture to conduct both segmentation and classification. With our proposed approach, we achieved outstanding performance and time efficiency, with 79.8% and 86.4% in DeepLabV3+ architecture in the segmentation task. ",
        "title": "Beyond Traditional Approaches: Multi-Task Network for Breast Ultrasound  Diagnosis",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07329",
        "abstract_url": "http://arxiv.org/abs/2401.07329",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Tong",
                "first_name": "Haonan"
            },
            {
                "last_name": "Yang",
                "first_name": "Nuocheng"
            },
            {
                "last_name": "Yin",
                "first_name": "Changchuan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  This paper studies the problem of the lightweight image semantic communication system that is deployed on Internet of Things (IoT) devices. In the considered system model, devices must use semantic communication techniques to support user behavior recognition in ultimate video service with high data transmission efficiency. However, it is computationally expensive for IoT devices to deploy semantic codecs due to the complex calculation processes of deep learning (DL) based codec training and inference. To make it affordable for IoT devices to deploy semantic communication systems, we propose an attention-based UNet enabled lightweight image semantic communication (LSSC) system, which achieves low computational complexity and small model size. In particular, we first let the LSSC system train the codec at the edge server to reduce the training computation load on IoT devices. Then, we introduce the convolutional block attention module (CBAM) to extract the image semantic features and decrease the number of downsampling layers thus reducing the floating-point operations (FLOPs). Finally, we experimentally adjust the structure of the codec and find out the optimal number of downsampling layers. Simulation results show that the proposed LSSC system can reduce the semantic codec FLOPs by 14%, and reduce the model size by 55%, with a sacrifice of 3% accuracy, compared to the baseline. Moreover, the proposed scheme can achieve a higher transmission accuracy than the traditional communication scheme in the low channel signal-to-noise (SNR) region. ",
        "title": "Attention-based UNet enabled Lightweight Image Semantic Communication  System over Internet of Things",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07331",
        "abstract_url": "http://arxiv.org/abs/2401.07331",
        "authors": [
            {
                "last_name": "Naghavi",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Wang",
                "first_name": "Haifeng"
            },
            {
                "last_name": "Fan",
                "first_name": "Lei"
            },
            {
                "last_name": "Choy",
                "first_name": "Jenny S."
            },
            {
                "last_name": "Kassab",
                "first_name": "Ghassan"
            },
            {
                "last_name": "Baek",
                "first_name": "Seungik"
            },
            {
                "last_name": "Lee",
                "first_name": "Lik-Chuan"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Physics-based computer models based on numerical solution of the governing equations generally cannot make rapid predictions, which in turn, limits their applications in the clinic. To address this issue, we developed a physics-informed neural network (PINN) model that encodes the physics of a closed-loop blood circulation system embedding a left ventricle (LV). The PINN model is trained to satisfy a system of ordinary differential equations (ODEs) associated with a lumped parameter description of the circulatory system. The model predictions have a maximum error of less than 5% when compared to those obtained by solving the ODEs numerically. An inverse modeling approach using the PINN model is also developed to rapidly estimate model parameters (in $\\sim$ 3 mins) from single-beat LV pressure and volume waveforms. Using synthetic LV pressure and volume waveforms generated by the PINN model with different model parameter values, we show that the inverse modeling approach can recover the corresponding ground truth values, which suggests that the model parameters are unique. The PINN inverse modeling approach is then applied to estimate LV contractility indexed by the end-systolic elastance $E_{es}$ using waveforms acquired from 11 swine models, including waveforms acquired before and after administration of dobutamine (an inotropic agent) in 3 animals. The estimated $E_{es}$ is about 58% to 284% higher for the data associated with dobutamine compared to those without, which implies that this approach can be used to estimate LV contractility using single-beat measurements. The PINN inverse modeling can potentially be used in the clinic to simultaneously estimate LV contractility and other physiological parameters from single-beat measurements. ",
        "title": "Rapid Estimation of Left Ventricular Contractility with a  Physics-Informed Neural Network Inverse Modeling Approach",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07333",
        "abstract_url": "http://arxiv.org/abs/2401.07333",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Yakun"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Ma",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Xie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/. ",
        "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided  Sequence Reordering",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07336",
        "abstract_url": "http://arxiv.org/abs/2401.07336",
        "authors": [
            {
                "last_name": "Ting",
                "first_name": "Zhu"
            },
            {
                "last_name": "Liangqi",
                "first_name": "Li"
            },
            {
                "last_name": "Shufei",
                "first_name": "Duan"
            },
            {
                "last_name": "Xueying",
                "first_name": "Zhang"
            },
            {
                "last_name": "Zhongzhe",
                "first_name": "Xiao"
            },
            {
                "last_name": "Hairng",
                "first_name": "Jia"
            },
            {
                "last_name": "Huizhi",
                "first_name": "Liang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  A multi-modal emotional speech Mandarin database including articulatory kinematics, acoustics, glottal and facial micro-expressions is designed and established, which is described in detail from the aspects of corpus design, subject selection, recording details and data processing. Where signals are labeled with discrete emotion labels (neutral, happy, pleasant, indifferent, angry, sad, grief) and dimensional emotion labels (pleasure, arousal, dominance). In this paper, the validity of dimension annotation is verified by statistical analysis of dimension annotation data. The SCL-90 scale data of annotators are verified and combined with PAD annotation data for analysis, so as to explore the internal relationship between the outlier phenomenon in annotation and the psychological state of annotators. In order to verify the speech quality and emotion discrimination of the database, this paper uses 3 basic models of SVM, CNN and DNN to calculate the recognition rate of these seven emotions. The results show that the average recognition rate of seven emotions is about 82% when using acoustic data alone. When using glottal data alone, the average recognition rate is about 72%. Using kinematics data alone, the average recognition rate also reaches 55.7%. Therefore, the database is of high quality and can be used as an important source for speech analysis research, especially for the task of multimodal emotional speech analysis. ",
        "title": "Construction and Evaluation of Mandarin Multimodal Emotional Speech  Database",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07337",
        "abstract_url": "http://arxiv.org/abs/2401.07337",
        "authors": [
            {
                "last_name": "Echenique",
                "first_name": "Federico"
            },
            {
                "last_name": "Pourbabaee",
                "first_name": "Farzad"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We provide a quantitative assessment of welfare in the classical model of risk-sharing and exchange under uncertainty. We prove three kinds of results. First, that in an equilibrium allocation, the scope for improving individual welfare by a given margin (an $\\ve$-improvement) vanishes as the number of states increases. Second, that the scope for a change in aggregate resources that may be distributed to enhance individual welfare by a given margin also vanishes. Equivalently: in an inefficient allocation, for a given level of resource sub-optimality (as measured by the coefficient of resource under-utilization), the possibilities for enhancing welfare by perturbing aggregate resources decrease exponentially to zero with the number of states. Finally, we consider efficient risk-sharing in standard models of uncertainty aversion with multiple priors, and show that, in an inefficient allocation, certain sets of priors shrink with the size of the state space. ",
        "title": "Individual and Collective Welfare in Risk Sharing with Many States",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07339",
        "abstract_url": "http://arxiv.org/abs/2401.07339",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Kechi"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Li",
                "first_name": "Ge"
            },
            {
                "last_name": "Shi",
                "first_name": "Xianjie"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\\% to 250\\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges. ",
        "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems  for Real-World Repo-level Coding Challenges",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07340",
        "abstract_url": "http://arxiv.org/abs/2401.07340",
        "authors": [
            {
                "last_name": "Antoniak",
                "first_name": "Maria"
            },
            {
                "last_name": "Mimno",
                "first_name": "David"
            },
            {
                "last_name": "Thalken",
                "first_name": "Rosamond"
            },
            {
                "last_name": "Walsh",
                "first_name": "Melanie"
            },
            {
                "last_name": "Wilkens",
                "first_name": "Matthew"
            },
            {
                "last_name": "Yauney",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The growth of social reading platforms such as Goodreads and LibraryThing enables us to analyze reading activity at very large scale and in remarkable detail. But twenty-first century systems give us a perspective only on contemporary readers. Meanwhile, the digitization of the lending library records of Shakespeare and Company provides a window into the reading activity of an earlier, smaller community in interwar Paris. In this article, we explore the extent to which we can make comparisons between the Shakespeare and Company and Goodreads communities. By quantifying similarities and differences, we can identify patterns in how works have risen or fallen in popularity across these datasets. We can also measure differences in how works are received by measuring similarities and differences in co-reading patterns. Finally, by examining the complete networks of co-readership, we can observe changes in the overall structures of literary reception. ",
        "title": "The Afterlives of Shakespeare and Company in Online Social Readership",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07341",
        "abstract_url": "http://arxiv.org/abs/2401.07341",
        "authors": [
            {
                "last_name": "Hochbaum",
                "first_name": "Dorit S."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We address here spanning tree problems on a graph with binary edge weights. For a general weighted graph the minimum spanning tree is solved in super-linear running time, even when the edges of the graph are pre-sorted. A related problem, of finding a spanning tree with a pre-specified sum of weights, is NP-hard. In contrast, for a graph with binary weights associated with the edges, it is shown that the minimum spanning tree and finding a spanning tree with a given total sum, are solvable in linear time with simple algorithms. ",
        "title": "Binary weights spanning trees and the $k$-red spanning tree problem in  linear time",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07342",
        "abstract_url": "http://arxiv.org/abs/2401.07342",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Anchen"
            },
            {
                "last_name": "Londono",
                "first_name": "Juan J"
            },
            {
                "last_name": "Elbaum",
                "first_name": "Batya"
            },
            {
                "last_name": "Estrada",
                "first_name": "Luis"
            },
            {
                "last_name": "Lazo",
                "first_name": "Roberto Jose"
            },
            {
                "last_name": "Vitale",
                "first_name": "Laura"
            },
            {
                "last_name": "Villasanti",
                "first_name": "Hugo Gonzalez"
            },
            {
                "last_name": "Fusaroli",
                "first_name": "Riccardo"
            },
            {
                "last_name": "Perry",
                "first_name": "Lynn K"
            },
            {
                "last_name": "Messinger",
                "first_name": "Daniel S"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2 teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was .76, with an error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for both teacher and child transcriptions was .15, meaning that 15% of words would need to be deleted, added, or changed to equate the Whisper and expert transcriptions. Moreover, speech features such as the mean length of utterances in words, the proportion of teacher and child utterances that were questions, and the proportion of utterances that were responded to within 2.5 seconds were similar when calculated separately from expert and automated transcriptions. The results suggest substantial progress in analyzing classroom speech that may support children's language development. Future research using natural language processing is underway to improve speaker classification and to analyze results from the application of the automated it framework to a larger dataset containing classroom recordings from 13 children and 4 teachers observed on 17 occasions over one year. ",
        "title": "Who Said What? An Automated Approach to Analyzing Speech in Preschool  Classrooms",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07348",
        "abstract_url": "http://arxiv.org/abs/2401.07348",
        "authors": [
            {
                "last_name": "Novelli",
                "first_name": "Claudio"
            },
            {
                "last_name": "Casolari",
                "first_name": "Federico"
            },
            {
                "last_name": "Hacker",
                "first_name": "Philipp"
            },
            {
                "last_name": "Spedicato",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Floridi",
                "first_name": "Luciano"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models, ensuring they align with the EU's evolving digital landscape and legal standards. ",
        "title": "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and  Cybersecurity",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07353",
        "abstract_url": "http://arxiv.org/abs/2401.07353",
        "authors": [
            {
                "last_name": "Gohar",
                "first_name": "Usman"
            },
            {
                "last_name": "Hunter",
                "first_name": "Michael C."
            },
            {
                "last_name": "Marczak-Czajka",
                "first_name": "Agnieszka"
            },
            {
                "last_name": "Lutz",
                "first_name": "Robyn R."
            },
            {
                "last_name": "Cohen",
                "first_name": "Myra B."
            },
            {
                "last_name": "Cleland-Huang",
                "first_name": "Jane"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversion to any AI-supported automation, highlighting the need for full transparency in automated decision-making. Results provide a societal perspective on the challenges of automating UTM flight authorization decisions and help frame the ongoing design of a solution acceptable to the broader sUAS community. ",
        "title": "Towards Engineering Fair and Equitable Software Systems for Managing  Low-Altitude Airspace Authorizations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07356",
        "abstract_url": "http://arxiv.org/abs/2401.07356",
        "authors": [
            {
                "last_name": "Pramod",
                "first_name": "K. D."
            },
            {
                "last_name": "De Silva",
                "first_name": "W. T. N."
            },
            {
                "last_name": "Thabrew",
                "first_name": "W. U. K."
            },
            {
                "last_name": "Shariffdeen",
                "first_name": "Ridwan"
            },
            {
                "last_name": "Wickramanayake",
                "first_name": "Sandareka"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Automated Program Repair (APR) improves developer productivity by saving debugging and bug-fixing time. While APR has been extensively explored for C/C++ and Java programs, there is little research on bugs in PHP programs due to the lack of a benchmark PHP bug dataset. This is surprising given that PHP has been one of the most widely used server-side languages for over two decades, being used in a variety of contexts such as e-commerce, social networking, and content management. This paper presents a benchmark dataset of PHP bugs on real-world applications called BUGSPHP, which can enable research on analysis, testing, and repair for PHP programs. The dataset consists of training and test datasets, separately curated from GitHub and processed locally. The training dataset includes more than 600,000 bug-fixing commits. The test dataset contains 513 manually validated bug-fixing commits equipped with developer-provided test cases to assess patch correctness. ",
        "title": "BUGSPHP: A dataset for Automated Program Repair in PHP",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07358",
        "abstract_url": "http://arxiv.org/abs/2401.07358",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Hao",
                "first_name": "Yizhi"
            },
            {
                "last_name": "Cong",
                "first_name": "Amando Xu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the realm of digital media, the advent of AI-generated synthetic images has introduced significant challenges in distinguishing between real and fabricated visual content. These images, often indistinguishable from authentic ones, pose a threat to the credibility of digital media, with potential implications for disinformation and fraud. Our research addresses this challenge by employing machine learning techniques to discern between AI-generated and genuine images. Central to our approach is the CIFAKE dataset, a comprehensive collection of images labeled as \"Real\" and \"Fake\". We refine and adapt advanced deep learning architectures like ResNet, VGGNet, and DenseNet, utilizing transfer learning to enhance their precision in identifying synthetic images. We also compare these with a baseline model comprising a vanilla Support Vector Machine (SVM) and a custom Convolutional Neural Network (CNN). The experimental results were significant, demonstrating that our optimized deep learning models outperform traditional methods, with DenseNet achieving an accuracy of 97.74%. Our application study contributes by applying and optimizing these advanced models for synthetic image detection, conducting a comparative analysis using various metrics, and demonstrating their superior capability in identifying AI-generated images over traditional machine learning techniques. This research not only advances the field of digital media integrity but also sets a foundation for future explorations into the ethical and technical dimensions of AI-generated content in digital media. ",
        "title": "Harnessing Machine Learning for Discerning AI-Generated Synthetic Images",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07359",
        "abstract_url": "http://arxiv.org/abs/2401.07359",
        "authors": [
            {
                "last_name": "Scorzato",
                "first_name": "Luigi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown to be language-independent. It is argued that the high epistemic complexity of DNN models hinders the estimate of their reliability and also their prospect of long-term progress. Some potential ways forward are suggested. Thirdly, this article identifies the close relation between a model's epistemic complexity and its interpretability, as introduced in the context of responsible AI. This clarifies in which sense, and to what extent, the lack of understanding of a model (black-box problem) impacts its interpretability in a way that is independent of individual skills. It also clarifies how interpretability is a precondition for assessing the reliability of any model, which cannot be based on statistical analysis alone. This article focuses on the comparison between traditional scientific models and DNN models. But, Random Forest and Logistic Regression models are also briefly considered. ",
        "title": "Reliability and Interpretability in Science and Deep Learning",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07360",
        "abstract_url": "http://arxiv.org/abs/2401.07360",
        "authors": [
            {
                "last_name": "Duarte-Torres",
                "first_name": "Sergio"
            },
            {
                "last_name": "Sen",
                "first_name": "Arunasish"
            },
            {
                "last_name": "Rana",
                "first_name": "Aman"
            },
            {
                "last_name": "Drude",
                "first_name": "Lukas"
            },
            {
                "last_name": "Gomez-Alanis",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Schwarz",
                "first_name": "Andreas"
            },
            {
                "last_name": "R\u00e4del",
                "first_name": "Leif"
            },
            {
                "last_name": "Leutnant",
                "first_name": "Volker"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  Context cues carry information which can improve multi-turn interactions in automatic speech recognition (ASR) systems. In this paper, we introduce a novel mechanism inspired by hyper-prompting to fuse textual context with acoustic representations in the attention mechanism. Results on a test set with multi-turn interactions show that our method achieves 5.9% relative word error rate reduction (rWERR) over a strong baseline. We show that our method does not degrade in the absence of context and leads to improvements even if the model is trained without context. We further show that leveraging a pre-trained sentence-piece model for context embedding generation can outperform an external BERT model. ",
        "title": "Promptformer: Prompted Conformer Transducer for ASR",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07361",
        "abstract_url": "http://arxiv.org/abs/2401.07361",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Anthony"
            },
            {
                "last_name": "Jablonowski",
                "first_name": "Christiane"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Fast summation refers to a family of techniques for approximating $O(N^2)$ sums in $O(N\\log{N})$ or $O(N)$ time. These techniques have traditionally found wide use in astrophysics and electrostatics in calculating the forces in a $N$-body problem. In this work, we present a spherical tree code, and apply it to the problem of efficiently solving the barotropic vorticity equation. ",
        "title": "Fast Summation on the Sphere with Applications to the Barotropic  Vorticity Equation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07363",
        "abstract_url": "http://arxiv.org/abs/2401.07363",
        "authors": [
            {
                "last_name": "Lotfi",
                "first_name": "Ehsan"
            },
            {
                "last_name": "De Bruyn",
                "first_name": "Maxime"
            },
            {
                "last_name": "Buhmann",
                "first_name": "Jeska"
            },
            {
                "last_name": "Daelemans",
                "first_name": "Walter"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The new wave of Large Language Models (LLM) has offered an efficient tool to curate sizeable conversational datasets. So far studies have mainly focused on task-oriented or generic open-domain dialogs, and have not fully explored the ability of LLMs in following complicated prompts. In this work, we focus on personalization, and employ LLMs to curate a dataset which is difficult and costly to crowd-source: PersonalityChat is a synthetic conversational dataset based upon the popular PersonaChat dataset, but conditioned on both personas and (Big-5) personality traits. Evaluating models fine-tuned on this dataset, we show that the personality trait labels can be used for trait-based personalization of generative dialogue models. We also perform a head-to-head comparison between PersonalityChat and PersonaChat, and show that training on the distilled dataset results in more fluent and coherent dialog agents in the small-model regime. ",
        "title": "PersonalityChat: Conversation Distillation for Personalized Dialog  Modeling with Facts and Traits",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07364",
        "abstract_url": "http://arxiv.org/abs/2401.07364",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Liu"
            },
            {
                "last_name": "Osher",
                "first_name": "Stanley J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) [1] represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated in [1]. In this paper, we explore the second question by investigating the generalization capabilities of ICON for conservation laws, a family of PDEs with temporal evolution. We show the positive answer to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. We also show how to broaden the range of problems that ICON can address, by transforming functions and equations to ICON's capability scope. We believe that the progress in this paper is a significant step towards the goal of training a foundation model for PDE-related tasks under the in-context operator learning framework. ",
        "title": "PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar  Nonlinear Conservation Laws",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07367",
        "abstract_url": "http://arxiv.org/abs/2401.07367",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xuesong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Human annotation of training samples is expensive, laborious, and sometimes challenging, especially for Natural Language Processing (NLP) tasks. To reduce the labeling cost and enhance the sample efficiency, Active Learning (AL) technique can be used to label as few samples as possible to reach a reasonable or similar results. To reduce even more costs and with the significant advances of Large Language Models (LLMs), LLMs can be a good candidate to annotate samples. This work investigates the accuracy and cost of using LLMs (GPT-3.5 and GPT-4) to label samples on 3 different datasets. A consistency-based strategy is proposed to select samples that are potentially incorrectly labeled so that human annotations can be used for those samples in AL settings, and we call it mixed annotation strategy. Then we test performance of AL under two different settings: (1) using human annotations only; (2) using the proposed mixed annotation strategy. The accuracy of AL models under 3 AL query strategies are reported on 3 text classification datasets, i.e., AG's News, TREC-6, and Rotten Tomatoes. On AG's News and Rotten Tomatoes, the models trained with the mixed annotation strategy achieves similar or better results compared to that with human annotations. The method reveals great potentials of LLMs as annotators in terms of accuracy and cost efficiency in active learning settings. ",
        "title": "Active Learning for NLP with Large Language Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07368",
        "abstract_url": "http://arxiv.org/abs/2401.07368",
        "authors": [
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Bay",
                "first_name": "Sajjad"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The integration of Internet of Things (IoT) devices in healthcare applications has revolutionized patient care, monitoring, and data management. The Global IoT in Healthcare Market value is $252.2 Billion in 2023. However, the rapid involvement of these devices brings information security concerns that pose critical threats to patient privacy and the integrity of healthcare data. This paper introduces a novel machine learning (ML) based architecture explicitly designed to address and mitigate security vulnerabilities in IoT devices within healthcare applications. By leveraging advanced convolution ML architecture, the proposed architecture aims to proactively monitor and detect potential threats, ensuring the confidentiality and integrity of sensitive healthcare information while minimizing the cost and increasing the portability specialized for healthcare and emergency environments. The experimental results underscore the accuracy of up to 93.6% for predicting various attacks based on the results demonstrate a zero-day detection accuracy simulated using the CICIoT2023 dataset and reduces the cost by a factor of x10. The significance of our approach is in fortifying the security posture of IoT devices and maintaining a robust implementation of trustful healthcare systems. ",
        "title": "A Novel Zero-Trust Machine Learning Green Architecture for Healthcare  IoT Cybersecurity: Review, Analysis, and Implementation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07369",
        "abstract_url": "http://arxiv.org/abs/2401.07369",
        "authors": [
            {
                "last_name": "Yi",
                "first_name": "Zeji"
            },
            {
                "last_name": "Pan",
                "first_name": "Chaoyi"
            },
            {
                "last_name": "He",
                "first_name": "Guanqi"
            },
            {
                "last_name": "Qu",
                "first_name": "Guannan"
            },
            {
                "last_name": "Shi",
                "first_name": "Guanya"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  Sampling-based Model Predictive Control (MPC) has been a practical and effective approach in many domains, notably model-based reinforcement learning, thanks to its flexibility and parallelizability. Despite its appealing empirical performance, the theoretical understanding, particularly in terms of convergence analysis and hyperparameter tuning, remains absent. In this paper, we characterize the convergence property of a widely used sampling-based MPC method, Model Predictive Path Integral Control (MPPI). We show that MPPI enjoys at least linear convergence rates when the optimization is quadratic, which covers time-varying LQR systems. We then extend to more general nonlinear systems. Our theoretical analysis directly leads to a novel sampling-based MPC algorithm, CoVariance-Optimal MPC (CoVo-MPC) that optimally schedules the sampling covariance to optimize the convergence rate. Empirically, CoVo-MPC significantly outperforms standard MPPI by 43-54% in both simulations and real-world quadrotor agile control tasks. Videos and Appendices are available at \\url{https://lecar-lab.github.io/CoVO-MPC/}. ",
        "title": "CoVO-MPC: Theoretical Analysis of Sampling-based MPC and Optimal  Covariance Design",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07370",
        "abstract_url": "http://arxiv.org/abs/2401.07370",
        "authors": [
            {
                "last_name": "Seib",
                "first_name": "Viktor"
            },
            {
                "last_name": "Roosen",
                "first_name": "Malte"
            },
            {
                "last_name": "Germann",
                "first_name": "Ida"
            },
            {
                "last_name": "Wirtz",
                "first_name": "Stefan"
            },
            {
                "last_name": "Paulus",
                "first_name": "Dietrich"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Creating annotated datasets demands a substantial amount of manual effort. In this proof-of-concept work, we address this issue by proposing a novel image generation pipeline. The pipeline consists of three distinct generative adversarial networks (previously published), combined in a novel way to augment a dataset for pedestrian detection. Despite the fact that the generated images are not always visually pleasant to the human eye, our detection benchmark reveals that the results substantially surpass the baseline. The presented proof-of-concept work was done in 2020 and is now published as a technical report after a three years retention period. ",
        "title": "Generation of Synthetic Images for Pedestrian Detection Using a Sequence  of GANs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07371",
        "abstract_url": "http://arxiv.org/abs/2401.07371",
        "authors": [
            {
                "last_name": "Kays",
                "first_name": "H M Imran"
            },
            {
                "last_name": "Momin",
                "first_name": "Khondhaker Al"
            },
            {
                "last_name": "Muraleetharan",
                "first_name": "K. K. \"Muralee\""
            },
            {
                "last_name": "Sadri",
                "first_name": "Arif Mohaimin"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  Roadway reconfiguration is a crucial aspect of transportation planning, aiming to enhance traffic flow, reduce congestion, and improve overall road network performance with existing infrastructure and resources. This paper presents a novel roadway reconfiguration technique by integrating optimization based Brute Force search approach and decision support framework to rank various roadway configurations for better performance. The proposed framework incorporates a multi-criteria decision analysis (MCDA) approach, combining input from generated scenarios during the optimization process. By utilizing data from optimization, the model identifies total betweenness centrality (TBC), system travel time (STT), and total link traffic flow (TLTF) as the most influential decision variables. The developed framework leverages graph theory to model the transportation network topology and apply network science metrics as well as stochastic user equilibrium traffic assignment to assess the impact of each roadway configuration on the overall network performance. To rank the roadway configurations, the framework employs machine learning algorithms, such as ridge regression, to determine the optimal weights for each criterion (i.e., TBC, STT, TLTF). Moreover, the network-based analysis ensures that the selected configurations not only optimize individual roadway segments but also enhance system-level efficiency, which is particularly helpful as the increasing frequency and intensity of natural disasters and other disruptive events underscore the critical need for resilient transportation networks. By integrating multi-criteria decision analysis, machine learning, and network science metrics, the proposed framework would enable transportation planners to make informed and data-driven decisions, leading to more sustainable, efficient, and resilient roadway configurations. ",
        "title": "A Data-driven Resilience Framework of Directionality Configuration based  on Topological Credentials in Road Networks",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07378",
        "abstract_url": "http://arxiv.org/abs/2401.07378",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Guangyu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ruyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Liu"
            },
            {
                "last_name": "Liang",
                "first_name": "Peixian"
            },
            {
                "last_name": "Liu",
                "first_name": "Fang"
            },
            {
                "last_name": "Chen",
                "first_name": "Danny"
            },
            {
                "last_name": "Niemier",
                "first_name": "Michael"
            },
            {
                "last_name": "Hu",
                "first_name": "X. Sharon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods. ",
        "title": "Efficient approximation of Earth Mover's Distance Based on Nearest  Neighbor Search",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07379",
        "abstract_url": "http://arxiv.org/abs/2401.07379",
        "authors": [
            {
                "last_name": "Mircea",
                "first_name": "Maria"
            },
            {
                "last_name": "Garlaschelli",
                "first_name": "Diego"
            },
            {
                "last_name": "Semrau",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  One of the main goals of developmental biology is to reveal the gene regulatory networks (GRNs) underlying the robust differentiation of multipotent progenitors into precisely specified cell types. Most existing methods to infer GRNs from experimental data have limited predictive power as the inferred GRNs merely reflect gene expression similarity or correlation. Here, we demonstrate, how physics-informed neural networks (PINNs) can be used to infer the parameters of predictive, dynamical GRNs that provide mechanistic understanding of biological processes. Specifically we study GRNs that exhibit bifurcation behavior and can therefore model cell differentiation. We show that PINNs outperform regular feed-forward neural networks on the parameter inference task and analyze two relevant experimental scenarios: 1. a system with cell communication for which gene expression trajectories are available and 2. snapshot measurements of a cell population in which cell communication is absent. Our analysis will inform the design of future experiments to be analyzed with PINNs and provides a starting point to explore this powerful class of neural network models further. ",
        "title": "Inference of dynamical gene regulatory networks from single-cell data  with physics informed neural networks",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07381",
        "abstract_url": "http://arxiv.org/abs/2401.07381",
        "authors": [
            {
                "last_name": "Borriello",
                "first_name": "Enrico"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In network theory, a triad census is a method designed to categorize and enumerate the various types of subgraphs with three nodes and their connecting edges within a network. Triads serve as fundamental building blocks for comprehending the structure and dynamics of networks, and the triad census offers a systematic approach to their classification. Typically, triad counts are obtained numerically, but lesser-known methods have been developed to precisely evaluate them without the need for sampling. In our study, we build upon Moody's matrix approach, presenting general diagrammatic rules that systematically and intuitively generate closed formulas for the occurrence numbers of triads in a network. ",
        "title": "Diagrammatic Rules for Triad Census",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07382",
        "abstract_url": "http://arxiv.org/abs/2401.07382",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Meng"
            },
            {
                "last_name": "Shu",
                "first_name": "Lei"
            },
            {
                "last_name": "Yu",
                "first_name": "Lei"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yun"
            },
            {
                "last_name": "Wichers",
                "first_name": "Nevan"
            },
            {
                "last_name": "Liu",
                "first_name": "Yinxiao"
            },
            {
                "last_name": "Meng",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only one reward for the entire generation. This sparsity of rewards can lead to inefficient and unstable learning. In this paper, we introduce a novel framework leveraging the critique ability of LLMs to produce dense rewards throughout the learning process. Our approach incorporates a critic language model alongside the policy model. This critic is prompted with the task description, question, policy model's output, and environment's reward signal as input, and provides token or span-level dense rewards that reflect the quality of each segment of the output. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial dense rewards in training yields consistent performance gains over the PPO baseline with holistic rewards. Furthermore, in a setting where the same model serves as both policy and critic, we demonstrate that \"self-critique\" rewards also boost learning efficiency. ",
        "title": "DRLC: Reinforcement Learning with Dense Rewards from LLM Critic",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07383",
        "abstract_url": "http://arxiv.org/abs/2401.07383",
        "authors": [
            {
                "last_name": "Faulstich",
                "first_name": "Fabian M."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This article presents an in-depth educational overview of the latest mathematical developments in coupled cluster (CC) theory, beginning with Schneider's seminal work from 2009 that introduced the first local analysis of CC theory. We offer a tutorial review of second quantization and the CC ansatz, laying the groundwork for understanding the mathematical basis of the theory. This is followed by a detailed exploration of the most recent mathematical advancements in CC theory.Our review starts with an in-depth look at the local analysis pioneered by Schneider which has since been applied to analyze various CC methods. We then move on to discuss the graph-based framework for CC methods developed by Csirik and Laestadius. This framework provides a comprehensive platform for comparing different CC methods, including multireference approaches. Next, we delve into the latest numerical analysis results analyzing the single reference CC method developed by Hassan, Maday, and Wang. This very general approach is based on the invertibility of the CC function's Fr\\'echet derivative. We conclude the article with a discussion on the recent incorporation of algebraic geometry into CC theory, highlighting how this novel and fundamentally different mathematical perspective has furthered our understanding and provides exciting pathways to new computational approaches. ",
        "title": "Recent mathematical advances in coupled cluster theory",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07386",
        "abstract_url": "http://arxiv.org/abs/2401.07386",
        "authors": [
            {
                "last_name": "Queiroz",
                "first_name": "Rubens Lacerda"
            },
            {
                "last_name": "Lima",
                "first_name": "Cabral"
            },
            {
                "last_name": "Sampaio",
                "first_name": "Fabio Ferrentini"
            },
            {
                "last_name": "Lima",
                "first_name": "Priscila Machado Vieira"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper evaluates AIcon2abs (Queiroz et al., 2021), a recently proposed method that enables awareness among the general public on machine learning. Such is possible due to the use of WiSARD, an easily understandable machine learning mechanism, thus requiring little effort and no technical background from the target users. WiSARD is adherent to digital computing; training consists of writing to RAM-type memories, and classification consists of reading from these memories. The model enables easy visualization and understanding of training and classification tasks' internal realization through ludic activities. Furthermore, the WiSARD model does not require an Internet connection for training and classification, and it can learn from a few or one example. This feature makes it easier to observe the machine, increasing its accuracy on a particular task with each new example used. WiSARD can also create \"mental images\" of what it has learned so far, evidencing key features pertaining to a given class. The assessment of the AIcon2abs method's effectiveness was conducted through the evaluation of a remote course with a workload of approximately 6 hours. It was completed by thirty-four Brazilian subjects: 5 children between 8 and 11 years old; 5 adolescents between 12 and 17 years old; and 24 adults between 21 and 72 years old. Data analysis adopted a hybrid approach. AIcon2abs was well-rated by almost 100% of the research subjects, and the data collected revealed quite satisfactory results concerning the intended outcomes. This research has been approved by the CEP/HUCFF/FM/UFRJ Human Research Ethics Committee. ",
        "title": "How do machines learn? Evaluating the AIcon2abs method",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07387",
        "abstract_url": "http://arxiv.org/abs/2401.07387",
        "authors": [
            {
                "last_name": "Manneschi",
                "first_name": "Luca"
            },
            {
                "last_name": "Vidamour",
                "first_name": "Ian T."
            },
            {
                "last_name": "Stenning",
                "first_name": "Kilian D."
            },
            {
                "last_name": "Gartside",
                "first_name": "Jack C."
            },
            {
                "last_name": "Swindells",
                "first_name": "Charles"
            },
            {
                "last_name": "Venkat",
                "first_name": "Guru"
            },
            {
                "last_name": "Griffin",
                "first_name": "David"
            },
            {
                "last_name": "Stepney",
                "first_name": "Susan"
            },
            {
                "last_name": "Branford",
                "first_name": "Will R."
            },
            {
                "last_name": "Hayward",
                "first_name": "Thomas"
            },
            {
                "last_name": "Ellis",
                "first_name": "Matt O"
            },
            {
                "last_name": "Vasilaki",
                "first_name": "Eleni"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "ET",
            "NE"
        ],
        "abstract": "  Physically implemented neural networks hold the potential to achieve the performance of deep learning models by exploiting the innate physical properties of devices as computational tools. This exploration of physical processes for computation requires to also consider their intrinsic dynamics, which can serve as valuable resources to process information. However, existing computational methods are unable to extend the success of deep learning techniques to parameters influencing device dynamics, which often lack a precise mathematical description. In this work, we formulate a universal framework to optimise interactions with dynamic physical systems in a fully data-driven fashion. The framework adopts neural stochastic differential equations as differentiable digital twins, effectively capturing both deterministic and stochastic behaviours of devices. Employing differentiation through the trained models provides the essential mathematical estimates for optimizing a physical neural network, harnessing the intrinsic temporal computation abilities of its physical nodes. To accurately model real devices' behaviours, we formulated neural-SDE variants that can operate under a variety of experimental settings. Our work demonstrates the framework's applicability through simulations and physical implementations of interacting dynamic devices, while highlighting the importance of accurately capturing system stochasticity for the successful deployment of a physically defined neural network. ",
        "title": "Optimising network interactions through device agnostic models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07389",
        "abstract_url": "http://arxiv.org/abs/2401.07389",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Hui"
            },
            {
                "last_name": "Aryani",
                "first_name": "Amir"
            },
            {
                "last_name": "Petrie",
                "first_name": "Stephen"
            },
            {
                "last_name": "Nambissan",
                "first_name": "Aishwarya"
            },
            {
                "last_name": "Astudillo",
                "first_name": "Aland"
            },
            {
                "last_name": "Cao",
                "first_name": "Shengyuan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Clustering algorithms aim to organize data into groups or clusters based on the inherent patterns and similarities within the data. They play an important role in today's life, such as in marketing and e-commerce, healthcare, data organization and analysis, and social media. Numerous clustering algorithms exist, with ongoing developments introducing new ones. Each algorithm possesses its own set of strengths and weaknesses, and as of now, there is no universally applicable algorithm for all tasks. In this work, we analyzed existing clustering algorithms and classify mainstream algorithms across five different dimensions: underlying principles and characteristics, data point assignment to clusters, dataset capacity, predefined cluster numbers and application area. This classification facilitates researchers in understanding clustering algorithms from various perspectives and helps them identify algorithms suitable for solving specific tasks. Finally, we discussed the current trends and potential future directions in clustering algorithms. We also identified and discussed open challenges and unresolved issues in the field. ",
        "title": "A Rapid Review of Clustering Algorithms",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07390",
        "abstract_url": "http://arxiv.org/abs/2401.07390",
        "authors": [
            {
                "last_name": "Wendt",
                "first_name": "Veronica"
            },
            {
                "last_name": "Yu",
                "first_name": "Byunggu"
            },
            {
                "last_name": "Kelly",
                "first_name": "Caleb"
            },
            {
                "last_name": "Kim",
                "first_name": "Junwhan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Self-attention transformers have demonstrated accuracy for image classification with smaller data sets. However, a limitation is that tests to-date are based upon single class image detection with known representation of image populations. For instances where the input image classes may be greater than one and test sets that lack full information on representation of image populations, accuracy calculations must adapt. The Receiver Operating Characteristic (ROC) accuracy thresh-old can address the instances of multi-class input images. However, this approach is unsuitable in instances where image population representation is unknown. We consider calculating accuracy using the knee method to determine threshold values on an ad-hoc basis. Results of ROC curve and knee thresholds for a multi-class data set, created from CIFAR-10 images, are discussed for multi-class image detection. ",
        "title": "Knee or ROC",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07392",
        "abstract_url": "http://arxiv.org/abs/2401.07392",
        "authors": [
            {
                "last_name": "Scilipoti",
                "first_name": "Marco"
            },
            {
                "last_name": "Fuster",
                "first_name": "Marina"
            },
            {
                "last_name": "Ramele",
                "first_name": "Rodrigo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning networks have become the de-facto standard in Computer Vision for industry and research. However, recent developments in their cousin, Natural Language Processing (NLP), have shown that there are areas where parameter-less models with strong inductive biases can serve as computationally cheaper and simpler alternatives. We propose such a model for binary image classification: a nearest neighbor classifier combined with a general purpose compressor like Gzip. We test and compare it against popular deep learning networks like Resnet, EfficientNet and Mobilenet and show that it achieves better accuracy and utilizes significantly less space, more than two order of magnitude, within a few-shot setting. As a result, we believe that this underlines the untapped potential of models with stronger inductive biases in few-shot scenarios. ",
        "title": "A Strong Inductive Bias: Gzip for binary image classification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07393",
        "abstract_url": "http://arxiv.org/abs/2401.07393",
        "authors": [
            {
                "last_name": "Aviles",
                "first_name": "Robert S."
            },
            {
                "last_name": "Beerel",
                "first_name": "Peter A."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Adiabatic Quantum-Flux-Parametron (AQFP) logic is a promising emerging device technology that promises six orders of magnitude lower power than CMOS. However, AQFP is challenged by operation at only ultra-low temperatures, has high latency and area, and requires a complex clocking scheme. In particular, every logic gate, buffer, and splitter must be clocked and each pair of connected clocked gates requires overlapping alternating current (AC) clock signals. In particular, clocked buffers need to be used to balance re-convergent logic paths, a problem that is exacerbated by every multi-node fanout needing a tree of clocked splitters. To reduce circuit area many works have proposed buffer and splitter insertion optimization algorithms and recent works have demonstrated a phase-skipping clocking scheme that reduces latency and area. This paper proposes the first algorithm to optimize buffer and splitter insertion for circuits that adopt phase-skipping and demonstrate the resulting performance improvements for a suite of AQFP benchmark circuits. ",
        "title": "A Novel Optimization Algorithm for Buffer and Splitter Minimization in  Phase-Skipping Adiabatic Quantum-Flux-Parametron Circuits",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07395",
        "abstract_url": "http://arxiv.org/abs/2401.07395",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Wei"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Ngoc Dang"
            },
            {
                "last_name": "Du",
                "first_name": "Lan"
            },
            {
                "last_name": "Buntine",
                "first_name": "Wray"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Within the scope of natural language processing, the domain of multi-label text classification is uniquely challenging due to its expansive and uneven label distribution. The complexity deepens due to the demand for an extensive set of annotated data for training an advanced deep learning model, especially in specialized fields where the labeling task can be labor-intensive and often requires domain-specific knowledge. Addressing these challenges, our study introduces a novel deep active learning strategy, capitalizing on the Beta family of proper scoring rules within the Expected Loss Reduction framework. It computes the expected increase in scores using the Beta Scoring Rules, which are then transformed into sample vector representations. These vector representations guide the diverse selection of informative samples, directly linking this process to the model's expected proper score. Comprehensive evaluations across both synthetic and real datasets reveal our method's capability to often outperform established acquisition techniques in multi-label text classification, presenting encouraging outcomes across various architectural and dataset scenarios. ",
        "title": "Harnessing the Power of Beta Scoring in Deep Active Learning for  Multi-Label Text Classification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07398",
        "abstract_url": "http://arxiv.org/abs/2401.07398",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yiqun"
            },
            {
                "last_name": "Huang",
                "first_name": "Hui"
            },
            {
                "last_name": "State",
                "first_name": "Radu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Driven by abundant satellite imagery, machine learning-based approaches have recently been promoted to generate high-resolution crop cultivation maps to support many agricultural applications. One of the major challenges faced by these approaches is the limited availability of ground truth labels. In the absence of ground truth, existing work usually adopts the \"direct transfer strategy\" that trains a classifier using historical labels collected from other regions and then applies the trained model to the target region. Unfortunately, the spectral features of crops exhibit inter-region and inter-annual variability due to changes in soil composition, climate conditions, and crop progress, the resultant models perform poorly on new and unseen regions or years. This paper presents the Crop Generative Adversarial Network (CropGAN) to address the above cross-domain issue. Our approach does not need labels from the target domain. Instead, it learns a mapping function to transform the spectral features of the target domain to the source domain (with labels) while preserving their local structure. The classifier trained by the source domain data can be directly applied to the transformed data to produce high-accuracy early crop maps of the target domain. Comprehensive experiments across various regions and years demonstrate the benefits and effectiveness of the proposed approach. Compared with the widely adopted direct transfer strategy, the F1 score after applying the proposed CropGAN is improved by 13.13% - 50.98% ",
        "title": "Cross Domain Early Crop Mapping using CropGAN and CNN Classifier",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07402",
        "abstract_url": "http://arxiv.org/abs/2401.07402",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Kexuan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Gu",
                "first_name": "Shuhang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Implicit Neural Representation (INR) as a mighty representation paradigm has achieved success in various computer vision tasks recently. Due to the low-frequency bias issue of vanilla multi-layer perceptron (MLP), existing methods have investigated advanced techniques, such as positional encoding and periodic activation function, to improve the accuracy of INR. In this paper, we connect the network training bias with the reparameterization technique and theoretically prove that weight reparameterization could provide us a chance to alleviate the spectral bias of MLP. Based on our theoretical analysis, we propose a Fourier reparameterization method which learns coefficient matrix of fixed Fourier bases to compose the weights of MLP. We evaluate the proposed Fourier reparameterization method on different INR tasks with various MLP architectures, including vanilla MLP, MLP with positional encoding and MLP with advanced activation function, etc. The superiority approximation results on different MLP architectures clearly validate the advantage of our proposed method. Armed with our Fourier reparameterization method, better INR with more textures and less artifacts can be learned from the training data. ",
        "title": "Improved Implicity Neural Representation with Fourier Bases  Reparameterized Training",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07404",
        "abstract_url": "http://arxiv.org/abs/2401.07404",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Rahul K."
            },
            {
                "last_name": "Buason",
                "first_name": "Paprapee"
            },
            {
                "last_name": "Molzahn",
                "first_name": "Daniel K."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a framework for fairly curtailing photovoltaic (PV) plants in response to the over-voltage problem in PV-rich distribution networks. The framework imposes PV generation limits to avoid overvoltages. These limits are computed a day ahead of real-time operations by solving an offline stochastic optimization problem using forecasted scenarios for PV generation and load demand. The framework minimizes the overall curtailment while considering fairness by reducing disparities in curtailments among different PV owners. We model the distribution grid constraints using a conservative linear approximation (CLA) of the AC power flow equations which is computed using a set of sampled power injections from the day-ahead predicted scenarios. The proposed framework is numerically validated on a CIGRE benchmark network interfaced with a large number of PV plants. We compare the performance of the proposed framework versus an alternative formulation that does not incorporate fairness considerations. To this end, we assess tradeoffs between fairness, as quantified with the Jain Fairness Index (JFI), and the total curtailed energy. ",
        "title": "Fairness-aware Photovoltaic Generation Limits for Voltage Regulation in  Power Distribution Networks using Conservative Linear Approximations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07408",
        "abstract_url": "http://arxiv.org/abs/2401.07408",
        "authors": [
            {
                "last_name": "Ock",
                "first_name": "Janghoon"
            },
            {
                "last_name": "Magar",
                "first_name": "Rishikesh"
            },
            {
                "last_name": "Antony",
                "first_name": "Akshay"
            },
            {
                "last_name": "Farimani",
                "first_name": "Amir Barati"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Adsorption energy, a reactivity descriptor, should be accurately assessed for efficient catalyst screening. This evaluation requires determining the lowest energy across various adsorption configurations on the catalytic surface. While graph neural networks (GNNs) have gained popularity as a machine learning approach for computing the energy of catalyst systems, they rely heavily on atomic spatial coordinates and often lack clarity in their interpretations. Recent advancements in language models have broadened their applicability to predicting catalytic properties, allowing us to bypass the complexities of graph representation. These models are adept at handling textual data, making it possible to incorporate observable features in a human-readable format. However, language models encounter challenges in accurately predicting the energy of adsorption configurations, typically showing a high mean absolute error (MAE) of about 0.71 eV. Our study addresses this limitation by introducing a self-supervised multi-modal learning approach, termed graph-assisted pretraining. This method significantly reduces the MAE to 0.35 eV through a combination of data augmentation, achieving comparable accuracy with DimeNet++ while using 0.4% of its training data size. Furthermore, the Transformer encoder at the core of the language model can provide insights into the feature focus through its attention scores. This analysis shows that our multimodal training effectively redirects the model's attention toward relevant adsorption configurations from adsorbate-related features, enhancing prediction accuracy and interpretability. ",
        "title": "Multimodal Language and Graph Learning of Adsorption Configuration in  Catalysis",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07410",
        "abstract_url": "http://arxiv.org/abs/2401.07410",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Meng"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "CR"
        ],
        "abstract": "  DNS is an essential Internet infrastructure to support network applications and services, but is also a significant tool exploited by various cyberattacks. Existing DNS security analysis techniques mostly focus on one specific task associated with one single entity (e.g., domain) via conventional feature engineering. They rely heavily on the labor-intensive feature selection and largely ignore the intrinsic correlations among the heterogeneous DNS entities (e.g., domain and IP). In this paper, I explore the potential of heterogeneous graph embedding to automatically learn the behavior features of multiple DNS entities, and to simultaneously support more than one security tasks. Considering the joint optimization of malicious domain detection and IP reputation evaluation as an example, I propose a novel joint DNS embedding (JDE) model to formulate the DNS query behavior via a similarity-enhanced graph with heterogeneous entities. The random walk technique is applied to the heterogeneous graph to comprehensively explore the hidden homogeneous and heterogeneous high-order proximities among domains and IPs. Extensive experiments on real DNS traffic demonstrate that the joint optimization of multiple tasks with the latent high-order proximities can lead to better security analysis performance for all the tasks than respectively optimizing each single task with the observable low-order proximity. ",
        "title": "Multi-Task DNS Security Analysis via High-Order Heterogeneous Graph  Embedding",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07411",
        "abstract_url": "http://arxiv.org/abs/2401.07411",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Li",
                "first_name": "Chunxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yongxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baoxian"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Short video applications have attracted billions of users on the Internet and can satisfy diverse users' fragmented spare time with content-rich and duration-short videos. To achieve fast playback at user side, existing short video systems typically enforce burst transmission of initial segment of each video when being requested for improved quality of user experiences. However, such a way of burst transmissions can cause unexpected large startup delays at user side. This is because users may frequently switch videos when sequentially watching a list of short videos recommended by the server side, which can cause excessive burst transmissions of initial segments of different short videos and thus quickly deplete the network transmission capacity. In this paper, we adopt token bucket to characterize the video transmission path between video server and each user, and accordingly study how to effectively reduce the startup delay of short videos by effectively arranging the viewing order of a video list at the server side. We formulate the optimal video ordering problem for minimizing the maximum video startup delay as a combinatorial optimization problem and prove its NP-hardness. We accordingly propose a Partially Shared Actor Critic reinforcement learning algorithm (PSAC) to learn optimized video ordering strategy. Numerical results based on a real dataset provided by a large-scale short video service provider demonstrate that the proposed PSAC algorithm can significantly reduce the video startup delay compared to baseline algorithms. ",
        "title": "Startup Delay Aware Short Video Ordering: Problem, Model, and A  Reinforcement Learning based Algorithm",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07414",
        "abstract_url": "http://arxiv.org/abs/2401.07414",
        "authors": [
            {
                "last_name": "Meque",
                "first_name": "Abdul Gafar Manuel"
            },
            {
                "last_name": "Angel",
                "first_name": "Jason"
            },
            {
                "last_name": "Sidorov",
                "first_name": "Grigori"
            },
            {
                "last_name": "Gelbukh",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent years, language models and deep learning techniques have revolutionized natural language processing tasks, including emotion detection. However, the specific emotion of guilt has received limited attention in this field. In this research, we explore the applicability of three transformer-based language models for detecting guilt in text and compare their performance for general emotion detection and guilt detection. Our proposed model outformed BERT and RoBERTa models by two and one points respectively. Additionally, we analyze the challenges in developing accurate guilt-detection models and evaluate our model's effectiveness in detecting related emotions like \"shame\" through qualitative analysis of results. ",
        "title": "Leveraging the power of transformers for guilt detection in text",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07417",
        "abstract_url": "http://arxiv.org/abs/2401.07417",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Lipeng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Decentralized applications (DApps), which are innovative blockchain-powered software systems designed to serve as the fundamental building blocks for the next generation of Internet services, have witnessed exponential growth in recent years. This paper thoroughly compares and analyzes two blockchain-based decentralized storage networks (DSNs), which are crucial foundations for DApp and blockchain ecosystems. The study examines their respective mechanisms for data persistence, strategies for enforcing data retention, and token economics. In addition to delving into technical details, the suitability of each storage solution for decentralized application development is assessed, taking into consideration network performance, storage costs, and existing use cases. By evaluating these factors, the paper aims to provide insights into the effectiveness of these technologies in supporting the desirable properties of truly decentralized blockchain applications. In conclusion, the findings of this research are discussed and synthesized, offering valuable perspectives on the capabilities of these technologies. It sheds light on their potential to facilitate the development of DApps and provides an understanding of the ongoing trends in blockchain development. ",
        "title": "A Comparative Examination of Network and Contract-Based Blockchain  Storage Solutions for Decentralized Applications",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07425",
        "abstract_url": "http://arxiv.org/abs/2401.07425",
        "authors": [
            {
                "last_name": "Carnerero",
                "first_name": "A. Daniel"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Taichi"
            },
            {
                "last_name": "Li",
                "first_name": "Mengmou"
            },
            {
                "last_name": "Hatanaka",
                "first_name": "Takeshi"
            },
            {
                "last_name": "Wasa",
                "first_name": "Yasuaki"
            },
            {
                "last_name": "Hirata",
                "first_name": "Kenji"
            },
            {
                "last_name": "Ushifusa",
                "first_name": "Yoshiaki"
            },
            {
                "last_name": "Ida",
                "first_name": "Takanori"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we explore the concept of net-Zero Energy Houses (ZEH) - houses designed to have an annual net energy consumption around zero. To achieve this, we present a constrained optimization problem whose objective is finding the optimal sizing of photovoltaic panels and a battery system to be integrated at home. The original optimization problem is nonlinear with nonconvex constraints. Nevertheless, by applying a series of transformations, it is possible to find an equivalent Linear Programming (LP) problem which is computationally tractable. The attainment of ZEH can be tackled by introducing a single constraint in the optimization problem. Additionally, we propose a sharing economy approach to the investment problem, a strategy that carries the potential to reduce the cost of the investment and facilitate the attainment of ZEH in a more efficient manner. Finally, we apply the proposed frameworks to a neighborhood in Japan as a case study, demonstrating the potential for long-term ZEH attainment. The results show the importance of choosing an appropriate incentive to motivate residents towards achieving ZEH. ",
        "title": "ZEH-oriented Linear Programming for the Sizing Problem of Photovoltaic  Panels and Batteries",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07426",
        "abstract_url": "http://arxiv.org/abs/2401.07426",
        "authors": [
            {
                "last_name": "Lei",
                "first_name": "Chao"
            },
            {
                "last_name": "Lipovetzky",
                "first_name": "Nir"
            },
            {
                "last_name": "Ehinger",
                "first_name": "Krista A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Abstraction and Reasoning Corpus (ARC) is a general artificial intelligence benchmark that poses difficulties for pure machine learning methods due to its requirement for fluid intelligence with a focus on reasoning and abstraction. In this work, we introduce an ARC solver, Generalized Planning for Abstract Reasoning (GPAR). It casts an ARC problem as a generalized planning (GP) problem, where a solution is formalized as a planning program with pointers. We express each ARC problem using the standard Planning Domain Definition Language (PDDL) coupled with external functions representing object-centric abstractions. We show how to scale up GP solvers via domain knowledge specific to ARC in the form of restrictions over the actions model, predicates, arguments and valid structure of planning programs. Our experiments demonstrate that GPAR outperforms the state-of-the-art solvers on the object-centric tasks of the ARC, showing the effectiveness of GP and the expressiveness of PDDL to model ARC problems. The challenges provided by the ARC benchmark motivate research to advance existing GP solvers and understand new relations with other planning computational models. Code is available at github.com/you68681/GPAR. ",
        "title": "Generalized Planning for the Abstraction and Reasoning Corpus",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07427",
        "abstract_url": "http://arxiv.org/abs/2401.07427",
        "authors": [
            {
                "last_name": "Emre",
                "first_name": "Sariyildiz"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper extends the result of my previous papers on the analysis and synthesis of disturbance observer based robust control systems in state space. ",
        "title": "AMC'24 \"Analysis and Synthesis of the Disturbance Observer-based Robust  Force Control Systems in State Space\"",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07429",
        "abstract_url": "http://arxiv.org/abs/2401.07429",
        "authors": [
            {
                "last_name": "Govindasamy",
                "first_name": "Hariprasadh"
            },
            {
                "last_name": "Esfandiari",
                "first_name": "Babak"
            },
            {
                "last_name": "Garcia",
                "first_name": "Paulo"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  We present a hardware-accelerated SAT solver targeting processor/Field Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint Propagation (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art solutions, our solver eliminates costly clause look-up operations by assigning clauses directly to clause processors on the FPGA and dividing large formulas into smaller partitions manageable by FPGA. Partitions are hot-swapped during runtime as required and the supported formula size is limited only by available external memory, not on-chip FPGA memory. We evaluate our solver on a Xilinx Zynq platform with results showing quicker execution time across various formula sizes, subject to formula partitioning strategy. Compared to prior state-of-the-art, we achieve 1.7x and 1.1x speed up on BCP for 2 representative benchmarks and up to 6x total speedup over software-only implementation. ",
        "title": "Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on  FPGAs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07430",
        "abstract_url": "http://arxiv.org/abs/2401.07430",
        "authors": [
            {
                "last_name": "Emre",
                "first_name": "Sariyildiz"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper presents a new stiffness modulation mechanism that enables infinite-range stiffness modulation in a fast manner. The proposed stiffness modulation mechanism can help improve many robot environment interaction applications such as human-robot collaboration and robotic rehabilitation. ",
        "title": "AMC'24 \"A Novel Stiffness Modulation Mechanism for Energy Efficient  Variable Stiffness Actuators\"",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07433",
        "abstract_url": "http://arxiv.org/abs/2401.07433",
        "authors": [
            {
                "last_name": "Farsiabi",
                "first_name": "Ali"
            },
            {
                "last_name": "Ebrahimzad",
                "first_name": "Hamid"
            },
            {
                "last_name": "Ardakani",
                "first_name": "Masoud"
            },
            {
                "last_name": "Li",
                "first_name": "Chuandong"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Non-binary polar codes (NBPCs) decoded by successive cancellation (SC) algorithm have remarkable bit-error-rate performance compared to the binary polar codes (BPCs). Due to the serial nature, SC decoding suffers from large latency. The latency issue in BPCs has been the topic of extensive research and it has been notably resolved by the introduction of fast SC-based decoders. However, the vast majority of research on NBPCs is devoted to issues concerning design and efficient implementation. In this paper, we propose fast SC decoding for NBPCs constructed based on 2 x 2 kernels. In particular, we identify various non-binary special nodes in the SC decoding tree of NBPCs and propose their fast decoding. This way, we avoid traversing the full decoding tree and significantly reduce the decoding delay compared to symbol-by-symbol SC decoding. We also propose a simplified NBPC structure that facilitates the procedure of non-binary fast SC decoding. Using our proposed fast non-binary decoder, we observed an improvement of up to 95% in latency concerning the original SC decoding. This is while our proposed fast SC decoder for NBPCs incurs no error-rate loss. ",
        "title": "Fast Successive-Cancellation Decoding of 2 x 2 Kernel Non-Binary Polar  Codes: Identification, Decoding and Simplification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07435",
        "abstract_url": "http://arxiv.org/abs/2401.07435",
        "authors": [
            {
                "last_name": "Knill",
                "first_name": "Oliver"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  If f maps a discrete d-manifold G onto a (k+1)-partite complex P then H(G,f,P),the set of simplices x in G such that f(x) contains at least one facet in P defines a (d-k)-manifold. ",
        "title": "Manifolds from Partitions",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07437",
        "abstract_url": "http://arxiv.org/abs/2401.07437",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Kwang-Ting"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Nuclei segmentation is a fundamental prerequisite in the digital pathology workflow. The development of automated methods for nuclei segmentation enables quantitative analysis of the wide existence and large variances in nuclei morphometry in histopathology images. However, manual annotation of tens of thousands of nuclei is tedious and time-consuming, which requires significant amount of human effort and domain-specific expertise. To alleviate this problem, in this paper, we propose a weakly-supervised nuclei segmentation method that only requires partial point labels of nuclei. Specifically, we propose a novel boundary mining framework for nuclei segmentation, named BoNuS, which simultaneously learns nuclei interior and boundary information from the point labels. To achieve this goal, we propose a novel boundary mining loss, which guides the model to learn the boundary information by exploring the pairwise pixel affinity in a multiple-instance learning manner. Then, we consider a more challenging problem, i.e., partial point label, where we propose a nuclei detection module with curriculum learning to detect the missing nuclei with prior morphological knowledge. The proposed method is validated on three public datasets, MoNuSeg, CPM, and CoNIC datasets. Experimental results demonstrate the superior performance of our method to the state-of-the-art weakly-supervised nuclei segmentation methods. Code: https://github.com/hust-linyi/bonus. ",
        "title": "BoNuS: Boundary Mining for Nuclei Segmentation with Partial Point Labels",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07439",
        "abstract_url": "http://arxiv.org/abs/2401.07439",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Tingxuan"
            },
            {
                "last_name": "Miao",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Deng",
                "first_name": "Shizhuo"
            },
            {
                "last_name": "Tong",
                "first_name": ""
            },
            {
                "last_name": "Chen",
                "first_name": "Dongyue"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Depth completion is a critical task for handling depth images with missing pixels, which can negatively impact further applications. Recent approaches have utilized Convolutional Neural Networks (CNNs) to reconstruct depth images with the assistance of color images. However, vanilla convolution has non-negligible drawbacks in handling missing pixels. To solve this problem, we propose a new model for depth completion based on an encoder-decoder structure. Our model introduces two key components: the Mask-adaptive Gated Convolution (MagaConv) architecture and the Bi-directional Progressive Fusion (BP-Fusion) module. The MagaConv architecture is designed to acquire precise depth features by modulating convolution operations with iteratively updated masks, while the BP-Fusion module progressively integrates depth and color features, utilizing consecutive bi-directional fusion structures in a global perspective. Extensive experiments on popular benchmarks, including NYU-Depth V2, DIML, and SUN RGB-D, demonstrate the superiority of our model over state-of-the-art methods. We achieved remarkable performance in completing depth maps and outperformed existing approaches in terms of accuracy and reliability. ",
        "title": "Mask-adaptive Gated Convolution and Bi-directional Progressive Fusion  Network for Depth Completion",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07440",
        "abstract_url": "http://arxiv.org/abs/2401.07440",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Jia-Wei"
            },
            {
                "last_name": "Amenta",
                "first_name": "Nina"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We explore the fairness of a redistricting game introduced by Mixon and Villar, which provides a two-party protocol for dividing a state into electoral districts, without the participation of an impartial independent authority. We analyze the game in an abstract setting that ignores the geographic distribution of voters and assumes that voter preferences are fixed and known. We first show that the minority player can always win at least $p-1$ districts, where $p$ is proportional to the percentage of minority voters, and that when the minority is large they can win more than $p$ districts. We also show that a \"cracking\" strategy by the majority party limits the number of districts the minority player can win as a function of the size of the minority. ",
        "title": "The Fairness of Redistricting Ghost",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07441",
        "abstract_url": "http://arxiv.org/abs/2401.07441",
        "authors": [
            {
                "last_name": "Ouyang",
                "first_name": "Tinghui"
            },
            {
                "last_name": "MaungMaung",
                "first_name": "AprilPyone"
            },
            {
                "last_name": "Konishi",
                "first_name": "Koichi"
            },
            {
                "last_name": "Seo",
                "first_name": "Yoshiki"
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the era of large AI models, the complex architecture and vast parameters present substantial challenges for effective AI quality management (AIQM), e.g. large language model (LLM). This paper focuses on investigating the quality assurance of a specific LLM-based AI product--a ChatGPT-based sentiment analysis system. The study delves into stability issues related to both the operation and robustness of the expansive AI model on which ChatGPT is based. Experimental analysis is conducted using benchmark datasets for sentiment analysis. The results reveal that the constructed ChatGPT-based sentiment analysis system exhibits uncertainty, which is attributed to various operational factors. It demonstrated that the system also exhibits stability issues in handling conventional small text attacks involving robustness. ",
        "title": "Stability Analysis of ChatGPT-based Sentiment Analysis in AI Quality  Assurance",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07444",
        "abstract_url": "http://arxiv.org/abs/2401.07444",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Vint"
            },
            {
                "last_name": "Roy",
                "first_name": "Sohom"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  For small-scale liquid rockets, pressure-fed systems are commonly favoured due to their simplicity and low weight. In such systems, accurate regulation of both tank and injector pressures over a wide range of upstream pressures is critical $-$ more accurate regulation allows for higher engine efficiency and minimal tank mass, thus improving flight performance. However, existing methods such as dome-loaded pressure regulators are inflexible, or require extensive characterization to function accurately. These methods also suffer from limited orifice size, droop, and slow reaction times, making them unsuitable for throttling by adjusting pressures in flight, which are increasingly important as propulsively landing rockets become more common. To overcome these challenges, we designed an electronic pressure regulator (eReg), a multi-input multi-output system utilising closed loop feedback to accurately control downstream pressures. Our design is simple, low-cost and robust: with a single ball valve actuated by a motor, we regulate both gaseous pressurant and cryogenic liquid propellant at high flow rates (1.14 kg/s of liquid; 0.39 kg/s of gas) and upstream pressures (310 bar). Using 2 eRegs to regulate propellant tank pressures, and 2 eRegs for regulating propellant flow to the engine, we demonstrated our system's ability, in a static fire test, to regulate pressures accurately (within 0.2 bar) while simultaneously throttling our engine. To the best of our knowledge, this is the first time any undergraduate team has successfully throttled a liquid bipropellant engine. ",
        "title": "Low-cost, Lightweight Electronic Flow Regulators for Throttling Liquid  Rocket Engines",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07445",
        "abstract_url": "http://arxiv.org/abs/2401.07445",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haowen"
            },
            {
                "last_name": "Du",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Jin",
                "first_name": "Congyun"
            },
            {
                "last_name": "Li",
                "first_name": "Yujiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yingbo"
            },
            {
                "last_name": "Sun",
                "first_name": "Tao"
            },
            {
                "last_name": "Qin",
                "first_name": "Piqi"
            },
            {
                "last_name": "Fan",
                "first_name": "Cong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG"
        ],
        "abstract": "  Predicting click-through rate (CTR) is the core task of many ads online recommendation systems, which helps improve user experience and increase platform revenue. In this type of recommendation system, we often encounter two main problems: the joint usage of multi-page historical advertising data and the cold start of new ads. In this paper, we proposed GACE, a graph-based cross-page ads embedding generation method. It can warm up and generate the representation embedding of cold-start and existing ads across various pages. Specifically, we carefully build linkages and a weighted undirected graph model considering semantic and page-type attributes to guide the direction of feature fusion and generation. We designed a variational auto-encoding task as pre-training module and generated embedding representations for new and old ads based on this task. The results evaluated in the public dataset AliEC from RecBole and the real-world industry dataset from Alipay show that our GACE method is significantly superior to the SOTA method. In the online A/B test, the click-through rate on three real-world pages from Alipay has increased by 3.6%, 2.13%, and 3.02%, respectively. Especially in the cold-start task, the CTR increased by 9.96%, 7.51%, and 8.97%, respectively. ",
        "title": "GACE: Learning Graph-Based Cross-Page Ads Embedding For Click-Through  Rate Prediction",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07446",
        "abstract_url": "http://arxiv.org/abs/2401.07446",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Ruizhe"
            },
            {
                "last_name": "Ren",
                "first_name": "Hong"
            },
            {
                "last_name": "Pan",
                "first_name": "Cunhua"
            },
            {
                "last_name": "Jin",
                "first_name": "Shi"
            },
            {
                "last_name": "Popovski",
                "first_name": "Petar"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiangzhou"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we investigate a cascaded channel estimation method for a millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) system aided by a reconfigurable intelligent surface (RIS) with the BS equipped with low-resolution analog-to-digital converters (ADCs), where the BS and the RIS are both equipped with a uniform planar array (UPA). Due to the sparse property of mmWave channel, the channel estimation can be solved as a compressed sensing (CS) problem. However, the low-resolution quantization cause severe information loss of signals, and traditional CS algorithms are unable to work well. To recovery the signal and the sparse angular domain channel from quantization, we introduce Bayesian inference and efficient vector approximate message passing (VAMP) algorithm to solve the quantize output CS problem. To further improve the efficiency of the VAMP algorithm, a Fast Fourier Transform (FFT) based fast computation method is derived. Simulation results demonstrate the effectiveness and the accuracy of the proposed cascaded channel estimation method for the RIS-aided mmWave massive MIMO system with few-bit ADCs. Furthermore, the proposed channel estimation method can reach an acceptable performance gap between the low-resolution ADCs and the infinite ADCs for the low signal-to-noise ratio (SNR), which implies the applicability of few-bit ADCs in practice. ",
        "title": "Quantized RIS-aided mmWave Massive MIMO Channel Estimation with Uniform  Planar Arrays",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07447",
        "abstract_url": "http://arxiv.org/abs/2401.07447",
        "authors": [
            {
                "last_name": "N\u00e9dellec",
                "first_name": "Claire"
            },
            {
                "last_name": "Sauvion",
                "first_name": "Clara"
            },
            {
                "last_name": "Bossy",
                "first_name": "Robert"
            },
            {
                "last_name": "Borovikova",
                "first_name": "Mariya"
            },
            {
                "last_name": "Del\u00e9ger",
                "first_name": "Louise"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Wheat varieties show a large diversity of traits and phenotypes. Linking them to genetic variability is essential for shorter and more efficient wheat breeding programs. Newly desirable wheat variety traits include disease resistance to reduce pesticide use, adaptation to climate change, resistance to heat and drought stresses, or low gluten content of grains. Wheat breeding experiments are documented by a large body of scientific literature and observational data obtained in-field and under controlled conditions. The cross-referencing of complementary information from the literature and observational data is essential to the study of the genotype-phenotype relationship and to the improvement of wheat selection. The scientific literature on genetic marker-assisted selection describes much information about the genotype-phenotype relationship. However, the variety of expressions used to refer to traits and phenotype values in scientific articles is a hinder to finding information and cross-referencing it. When trained adequately by annotated examples, recent text mining methods perform highly in named entity recognition and linking in the scientific domain. While several corpora contain annotations of human and animal phenotypes, currently, no corpus is available for training and evaluating named entity recognition and entity-linking methods in plant phenotype literature. The Triticum aestivum trait Corpus is a new gold standard for traits and phenotypes of wheat. It consists of 540 PubMed references fully annotated for trait, phenotype, and species named entities using the Wheat Trait and Phenotype Ontology and the species taxonomy of the National Center for Biotechnology Information. A study of the performance of tools trained on the Triticum aestivum trait Corpus shows that the corpus is suitable for the training and evaluation of named entity recognition and linking. ",
        "title": "Taec: a Manually annotated text dataset for trait and phenotype  extraction and entity linking in wheat breeding literature",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07448",
        "abstract_url": "http://arxiv.org/abs/2401.07448",
        "authors": [
            {
                "last_name": "An",
                "first_name": "Ziyan"
            },
            {
                "last_name": "Johnson",
                "first_name": "Taylor T."
            },
            {
                "last_name": "Ma",
                "first_name": "Meiyi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters and develop a partitioning algorithm to effectively group clients based on the alignment of their temporal reasoning properties. We evaluate the proposed method on two tasks: a real-world traffic volume prediction task consisting of sensory data from fifteen states and a smart city multi-task prediction utilizing synthetic data. The evaluation results exhibit clear improvements, with performance accuracy improved by up to 54% across all sequential prediction models. ",
        "title": "Formal Logic Enabled Personalized Federated Learning Through Property  Inference",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07450",
        "abstract_url": "http://arxiv.org/abs/2401.07450",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhifeng"
            },
            {
                "last_name": "li",
                "first_name": "Hao"
            },
            {
                "last_name": "Ding",
                "first_name": "Huiming"
            },
            {
                "last_name": "Li",
                "first_name": "Mengtian"
            },
            {
                "last_name": "Cao",
                "first_name": "Ying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \\eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed them in different time step to the diffusion model according to the criteria of professional clothing designers.HieraFashDiff allows designers to add low-level attributes after high-level prompts for interactive editing incrementally.In addition, we design a differentiable loss function in the sampling process with a mask to keep non-edit areas.Comprehensive experiments performed on our newly conducted Hierarchical fashion dataset,demonstrate that our proposed method outperforms other state-of-the-art competitors. ",
        "title": "Hierarchical Fashion Design with Multi-stage Diffusion Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07451",
        "abstract_url": "http://arxiv.org/abs/2401.07451",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Alkhateeb",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Massive MIMO basestations, operating with frequency-division duplexing (FDD), require the users to feedback their channel state information (CSI) in order to design the precoding matrices. Given the powerful capabilities of deep neural networks in learning quantization codebooks, utilizing these networks in compressing the channels and reducing the massive MIMO CSI feedback overhead has recently gained increased interest. Learning one model, however, for the full cell or sector may not be optimal as the channel distribution could change significantly from one \\textit{zone} (an area or region) to another. In this letter, we introduce the concept of \\textit{zone-specific} CSI feedback. By partitioning the site space into multiple channel zones, the underlying channel distribution can be efficiently leveraged to reduce the CSI feedback. This concept leverages the implicit or explicit user position information to select the right zone-specific model and its parameters. To facilitate the evaluation of associated overhead, we introduce two novel metrics named \\textit{model parameters transmission rate} (MPTR) and \\textit{model parameters update rate} (MPUR). They jointly provide important insights and guidance for the system design and deployment. Simulation results show that significant gains could be achieved by the proposed framework. For example, using the large-scale Boston downtown scenario of DeepMIMO, the proposed zone-specific CSI feedback approach can on average achieve around 6dB NMSE gain compared to the other solutions, while keeping the same model complexity. ",
        "title": "Zone-Specific CSI Feedback for Massive MIMO: A Situation-Aware Deep  Learning Approach",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07453",
        "abstract_url": "http://arxiv.org/abs/2401.07453",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Akshat"
            },
            {
                "last_name": "Rao",
                "first_name": "Anurag"
            },
            {
                "last_name": "Anumanchipalli",
                "first_name": "Gopala"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgetting phase. Both gradual and catastrophic forgetting limit the usefulness of model editing methods at scale -- the former making model editing less effective as multiple edits are made to the model while the latter caps the scalability of such model editing methods. Our analysis also highlights other key limitations of ROME and MEMIT at scale. With our work, we push for the development and evaluation of model editing methods keeping scalability in mind. ",
        "title": "Model Editing at Scale leads to Gradual and Catastrophic Forgetting",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07454",
        "abstract_url": "http://arxiv.org/abs/2401.07454",
        "authors": [
            {
                "last_name": "Do",
                "first_name": "Anh Viet"
            },
            {
                "last_name": "Guo",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Neumann",
                "first_name": "Aneta"
            },
            {
                "last_name": "Neumann",
                "first_name": "Frank"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Creating diverse sets of high quality solutions has become an important problem in recent years. Previous works on diverse solutions problems consider solutions' objective quality and diversity where one is regarded as the optimization goal and the other as the constraint. In this paper, we treat this problem as a bi-objective optimization problem, which is to obtain a range of quality-diversity trade-offs. To address this problem, we frame the evolutionary process as evolving a population of populations, and present a suitable general implementation scheme that is compatible with existing evolutionary multi-objective search methods. We realize the scheme in NSGA-II and SPEA2, and test the methods on various instances of maximum coverage, maximum cut and minimum vertex cover problems. The resulting non-dominated populations exhibit rich qualitative features, giving insights into the optimization instances and the quality-diversity trade-offs they induce. ",
        "title": "Evolutionary Multi-Objective Diversity Optimization",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07455",
        "abstract_url": "http://arxiv.org/abs/2401.07455",
        "authors": [
            {
                "last_name": "Satsukawa",
                "first_name": "Koki"
            },
            {
                "last_name": "Wada",
                "first_name": "Kentaro"
            },
            {
                "last_name": "Iryo",
                "first_name": "Takamasa"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  In this study, we analyse the global stability of the equilibrium in a departure time choice problem using a game-theoretic approach that deals with atomic users. We first formulate the departure time choice problem as a strategic game in which atomic users select departure times to minimise their trip cost; we call this game the 'departure time choice game'. The concept of the epsilon-Nash equilibrium is introduced to ensure the existence of pure-strategy equilibrium corresponding to the departure time choice equilibrium in conventional fluid models. Then, we prove that the departure time choice game is a weakly acyclic game. By analysing the convergent better responses, we clarify the mechanisms of global convergence to equilibrium. This means that the epsilon-Nash equilibrium is achieved by sequential better responses of users, which are departure time changes to improve their own utility, in an appropriate order. Specifically, the following behavioural rules are important to ensure global convergence: (i) the adjustment of the departure time of the first user departing from the origin to the corresponding equilibrium departure time and (ii) the fixation of users to their equilibrium departure times in order (starting with the earliest). Using convergence mechanisms, we construct evolutionary dynamics under which global stability is guaranteed. We also investigate the stable and unstable dynamics studied in the literature based on convergence mechanisms, and gain insight into the factors influencing the different stability results. Finally, numerical experiments are conducted to demonstrate the theoretical results. ",
        "title": "Stability analysis of a departure time choice problem with atomic  vehicle models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07456",
        "abstract_url": "http://arxiv.org/abs/2401.07456",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Yun-Wei"
            },
            {
                "last_name": "Han",
                "first_name": "Dong-Jun"
            },
            {
                "last_name": "Brinton",
                "first_name": "Christopher G."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Federated learning (FL) is a promising approach for solving multilingual tasks, potentially enabling clients with their own language-specific data to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. In this paper, we propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget. ",
        "title": "Only Send What You Need: Learning to Communicate Efficiently in  Federated Multilingual Machine Translation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07457",
        "abstract_url": "http://arxiv.org/abs/2401.07457",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ce"
            },
            {
                "last_name": "Yu",
                "first_name": "Ke"
            },
            {
                "last_name": "Tang",
                "first_name": "Yushun"
            },
            {
                "last_name": "He",
                "first_name": "Zhihai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Contrastive Language-Image Pretraining (CLIP) model has exhibited remarkable efficacy in establishing cross-modal connections between texts and images, yielding impressive performance across a broad spectrum of downstream applications through fine-tuning. However, for generalization tasks, the current fine-tuning methods for CLIP, such as CoOp and CoCoOp, demonstrate relatively low performance on some fine-grained datasets. We recognize the underlying reason is that these previous methods only projected global features into the prompt, neglecting the various visual concepts, such as colors, shapes, and sizes, which are naturally transferable across domains and play a crucial role in generalization tasks. To address this issue, in this work, we propose Concept-Guided Prompt Learning (CPL) for vision-language models. Specifically, we leverage the well-learned knowledge of CLIP to create a visual concept cache to enable concept-guided prompting. In order to refine the text features, we further develop a projector that transforms multi-level visual features into text features. We observe that this concept-guided prompt learning approach is able to achieve enhanced consistency between visual and linguistic modalities. Extensive experimental results demonstrate that our CPL method significantly improves generalization capabilities compared to the current state-of-the-art methods. ",
        "title": "Concept-Guided Prompt Learning for Generalization in Vision-Language  Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07459",
        "abstract_url": "http://arxiv.org/abs/2401.07459",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xin"
            },
            {
                "last_name": "Yan",
                "first_name": "Wending"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuan"
            },
            {
                "last_name": "Mi",
                "first_name": "Michael Bi"
            },
            {
                "last_name": "Tan",
                "first_name": "Robby T."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semantic segmentation's performance is often compromised when applied to unlabeled adverse weather conditions. Unsupervised domain adaptation is a potential approach to enhancing the model's adaptability and robustness to adverse weather. However, existing methods encounter difficulties when sequentially adapting the model to multiple unlabeled adverse weather conditions. They struggle to acquire new knowledge while also retaining previously learned knowledge.To address these problems, we propose a semantic segmentation method for multiple adverse weather conditions that incorporates adaptive knowledge acquisition, pseudolabel blending, and weather composition replay. Our adaptive knowledge acquisition enables the model to avoid learning from extreme images that could potentially cause the model to forget. In our approach of blending pseudo-labels, we not only utilize the current model but also integrate the previously learned model into the ongoing learning process. This collaboration between the current teacher and the previous model enhances the robustness of the pseudo-labels for the current target. Our weather composition replay mechanism allows the model to continuously refine its previously learned weather information while simultaneously learning from the new target domain. Our method consistently outperforms the stateof-the-art methods, and obtains the best performance with averaged mIoU (%) of 65.7 and the lowest forgetting (%) of 3.6 against 60.1 and 11.3, on the ACDC datasets for a four-target continual multi-target domain adaptation. ",
        "title": "Semantic Segmentation in Multiple Adverse Weather Conditions with Domain  Knowledge Retention",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07463",
        "abstract_url": "http://arxiv.org/abs/2401.07463",
        "authors": [
            {
                "last_name": "Calder",
                "first_name": "Jeff"
            },
            {
                "last_name": "Drenska",
                "first_name": "Nadejda"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper we give a broad overview of the intersection of partial differential equations (PDEs) and graph-based semi-supervised learning. The overview is focused on a large body of recent work on PDE continuum limits of graph-based learning, which have been used to prove well-posedness of semi-supervised learning algorithms in the large data limit. We highlight some interesting research directions revolving around consistency of graph-based semi-supervised learning, and present some new results on the consistency of p-Laplacian semi-supervised learning using the stochastic tug-of-war game interpretation of the p-Laplacian. We also present the results of some numerical experiments that illustrate our results and suggest directions for future work. ",
        "title": "Consistency of semi-supervised learning, stochastic tug-of-war games,  and the p-Laplacian",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07464",
        "abstract_url": "http://arxiv.org/abs/2401.07464",
        "authors": [
            {
                "last_name": "Watkins",
                "first_name": "William"
            },
            {
                "last_name": "Wang",
                "first_name": "Heehwan"
            },
            {
                "last_name": "Bae",
                "first_name": "Sangyoon"
            },
            {
                "last_name": "Tseng",
                "first_name": "Huan-Hsin"
            },
            {
                "last_name": "Cha",
                "first_name": "Jiook"
            },
            {
                "last_name": "Chen",
                "first_name": "Samuel Yen-Chi"
            },
            {
                "last_name": "Yoo",
                "first_name": "Shinjae"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  The utility of machine learning has rapidly expanded in the last two decades and presents an ethical challenge. Papernot et. al. developed a technique, known as Private Aggregation of Teacher Ensembles (PATE) to enable federated learning in which multiple teacher models are trained on disjoint datasets. This study is the first to apply PATE to an ensemble of quantum neural networks (QNN) to pave a new way of ensuring privacy in quantum machine learning (QML) models. ",
        "title": "Quantum Privacy Aggregation of Teacher Ensembles (QPATE) for  Privacy-preserving Quantum Machine Learning",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07465",
        "abstract_url": "http://arxiv.org/abs/2401.07465",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Deepak"
            },
            {
                "last_name": "Zideh",
                "first_name": "Mehdi Jabbari"
            },
            {
                "last_name": "Talreja",
                "first_name": "Veeru"
            },
            {
                "last_name": "Verma",
                "first_name": "Vishal"
            },
            {
                "last_name": "Solanki",
                "first_name": "Sarika K."
            },
            {
                "last_name": "Solanki",
                "first_name": "Jignesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Most power systems' approaches are currently tending towards stochastic and probabilistic methods due to the high variability of renewable sources and the stochastic nature of loads. Conventional power flow (PF) approaches such as forward-backward sweep (FBS) and Newton-Raphson require a high number of iterations to solve non-linear PF equations making them computationally very intensive. PF is the most important study performed by utility, required in all stages of the power system, especially in operations and planning. This paper discusses the applications of deep learning (DL) to predict PF solutions for three-phase unbalanced power distribution grids. Three deep neural networks (DNNs); Radial Basis Function Network (RBFnet), Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN), are proposed in this paper to predict PF solutions. The PF problem is formulated as a multi-output regression model where two or more output values are predicted based on the inputs. The training and testing data are generated through the OpenDSS-MATLAB COM interface. These methods are completely data-driven where the training relies on reducing the mismatch at each node without the need for the knowledge of the system. The novelty of the proposed methodology is that the models can accurately predict the PF solutions for the unbalanced distribution grids with mutual coupling and are robust to different R/X ratios, topology changes as well as generation and load variability introduced by the integration of distributed energy resources (DERs) and electric vehicles (EVs). To test the efficacy of the DNN models, they are applied to IEEE 4-node and 123-node test cases, and the American Electric Power (AEP) feeder model. The PF results for RBFnet, MLP, and CNN models are discussed in this paper demonstrating that all three DNN models provide highly accurate results in predicting PF solutions. ",
        "title": "Power Flow Analysis Using Deep Neural Networks in Three-Phase Unbalanced  Smart Distribution Grids",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07466",
        "abstract_url": "http://arxiv.org/abs/2401.07466",
        "authors": [
            {
                "last_name": "Yusuf",
                "first_name": "Imam Nur Bani"
            },
            {
                "last_name": "Jiang",
                "first_name": "Lingxiao"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software, while beneficial, poses potential cybersecurity risks due to inherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep learning has shown promise as an effective tool for this task due to its ability to perform well without extensive feature engineering. However, a challenge in deploying deep learning for vulnerability detection is the limited availability of training data. Recent research highlights the deep learning efficacy in diverse tasks. This success is attributed to instruction fine-tuning, a technique that remains under-explored in the context of vulnerability detection. This paper investigates the capability of models, specifically a recent language model, to generalize beyond the programming languages used in their training data. It also examines the role of natural language instructions in enhancing this generalization. Our study evaluates the model performance on a real-world dataset to predict vulnerable code. We present key insights and lessons learned, contributing to understanding the deep learning application in software vulnerability detection. ",
        "title": "Your Instructions Are Not Always Helpful: Assessing the Efficacy of  Instruction Fine-tuning for Software Vulnerability Detection",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07467",
        "abstract_url": "http://arxiv.org/abs/2401.07467",
        "authors": [
            {
                "last_name": "Wynn",
                "first_name": "Scott"
            },
            {
                "last_name": "Kyritsis",
                "first_name": "Alec"
            },
            {
                "last_name": "Alberi",
                "first_name": "Stephora"
            },
            {
                "last_name": "Lu",
                "first_name": "Enyue"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Sequential algorithms for the Stable Matching Problem are often too slow in the context of some large scale applications like switch scheduling. Parallel architectures can offer a notable decrease in runtime complexity. We propose a stable matching algorithm using n^2 processors that converges in O(nlog(n)) average runtime. The algorithm is structurally based on the Parallel Iterative Improvement (PII) algorithm, which successfully finds a stable matching in approximately 90% of cases. We suggest alternative selection methods for pairs in the PII algorithm, called Right-Minimum and Dynamic Selection, resulting in full convergence over 3.3 million trials and generally much faster termination. ",
        "title": "Selection Improvements for the Parallel Iterative Algorithm for Stable  Matching",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07468",
        "abstract_url": "http://arxiv.org/abs/2401.07468",
        "authors": [
            {
                "last_name": "Or",
                "first_name": "Barak"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this study, a novel deep neural network (DNN) architecture, CarSpeedNet, is introduced to estimate car speed using three-axis accelerometer data from smartphones. Utilizing 13 hours of data collected from smartphones mounted in vehicles navigating through various regions in Israel, the CarSpeedNet effectively learns the relationship between measured smartphone acceleration and car speed. Ground truth speed data was obtained at 1[Hz] from the GPS receiver in the smartphones. The proposed model enables high-frequency speed estimation, incorporating historical inputs. Our trained model demonstrates exceptional accuracy in car speed estimation, achieving a precision of less than 0.72[m/s] during an extended driving test, solely relying on smartphone accelerometer data without any connectivity to the car. ",
        "title": "CarSpeedNet: A Deep Neural Network-based Car Speed Estimation from  Smartphone Accelerometer",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07469",
        "abstract_url": "http://arxiv.org/abs/2401.07469",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Yihu"
            },
            {
                "last_name": "Liu",
                "first_name": "Shuaishi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Most existing methods tackle the problem of occluded person re-identification (ReID) by utilizing auxiliary models, resulting in a complicated and inefficient ReID framework that is unacceptable for real-time applications. In this work, a speed-up person ReID framework named SUReID is proposed to mitigate occlusion interference while speeding up inference. The SUReID consists of three key components: hierarchical token sparsification (HTS) strategy, non-parametric feature alignment knowledge distillation (NPKD), and noise occlusion data augmentation (NODA). The HTS strategy works by pruning the redundant tokens in the vision transformer to achieve highly effective self-attention computation and eliminate interference from occlusions or background noise. However, the pruned tokens may contain human part features that contaminate the feature representation and degrade the performance. To solve this problem, the NPKD is employed to supervise the HTS strategy, retaining more discriminative tokens and discarding meaningless ones. Furthermore, the NODA is designed to introduce more noisy samples, which further trains the ability of the HTS to disentangle different tokens. Experimental results show that the SUReID achieves superior performance with surprisingly fast inference. ",
        "title": "A Deep Hierarchical Feature Sparse Framework for Occluded Person  Re-Identification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07470",
        "abstract_url": "http://arxiv.org/abs/2401.07470",
        "authors": [
            {
                "last_name": "Ahani",
                "first_name": "Zahra"
            },
            {
                "last_name": "Tash",
                "first_name": "Moein Shahiki"
            },
            {
                "last_name": "Mezquita",
                "first_name": "Yoel Ledo"
            },
            {
                "last_name": "Angel",
                "first_name": "Jason"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper provides an extensive examination of a sizable dataset of English tweets focusing on nine widely recognized cryptocurrencies, specifically Cardano, Binance, Bitcoin, Dogecoin, Ethereum, Fantom, Matic, Shiba, and Ripple. Our primary objective was to conduct a psycholinguistic and emotion analysis of social media content associated with these cryptocurrencies. To enable investigators to make more informed decisions. The study involved comparing linguistic characteristics across the diverse digital coins, shedding light on the distinctive linguistic patterns that emerge within each coin's community. To achieve this, we utilized advanced text analysis techniques. Additionally, our work unveiled an intriguing Understanding of the interplay between these digital assets within the cryptocurrency community. By examining which coin pairs are mentioned together most frequently in the dataset, we established correlations between different cryptocurrencies. To ensure the reliability of our findings, we initially gathered a total of 832,559 tweets from Twitter. These tweets underwent a rigorous preprocessing stage, resulting in a refined dataset of 115,899 tweets that were used for our analysis. Overall, our research offers valuable Perception into the linguistic nuances of various digital coins' online communities and provides a deeper understanding of their interactions in the cryptocurrency space. ",
        "title": "Utilizing deep learning models for the identification of enhancers and  super-enhancers based on genomic and epigenomic features",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07472",
        "abstract_url": "http://arxiv.org/abs/2401.07472",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Donggil"
            },
            {
                "last_name": "Kim",
                "first_name": "Taekyoo"
            },
            {
                "last_name": "Lee",
                "first_name": "Seungjoon"
            },
            {
                "last_name": "Shim",
                "first_name": "Hyungbo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we propose a distributed scheme for estimating the network size, which refers to the total number of agents in a network. By leveraging a synchronization technique for multi-agent systems, we devise an agent dynamics that ensures convergence to an equilibrium point located near the network size regardless of its initial condition. Our approach is based on an assumption that each agent has a unique identifier, and an estimation algorithm for obtaining the largest identifier value. By adopting this approach, we successfully implement the agent dynamics in a fully decentralized manner, ensuring accurate network size estimation even when some agents join or leave the network. ",
        "title": "Fully Decentralized Design of Initialization-free Distributed Network  Size Estimation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07475",
        "abstract_url": "http://arxiv.org/abs/2401.07475",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Chengwei"
            },
            {
                "last_name": "Pang",
                "first_name": "Runqi"
            },
            {
                "last_name": "Kuo",
                "first_name": "C. -C. Jay"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As a fundamental tool for natural language processing (NLP), the part-of-speech (POS) tagger assigns the POS label to each word in a sentence. A novel lightweight POS tagger based on word embeddings is proposed and named GWPT (green word-embedding-based POS tagger) in this work. Following the green learning (GL) methodology, GWPT contains three modules in cascade: 1) representation learning, 2) feature learning, and 3) decision learning modules. The main novelty of GWPT lies in representation learning. It uses non-contextual or contextual word embeddings, partitions embedding dimension indices into low-, medium-, and high-frequency sets, and represents them with different N-grams. It is shown by experimental results that GWPT offers state-of-the-art accuracies with fewer model parameters and significantly lower computational complexity in both training and inference as compared with deep-learning-based methods. ",
        "title": "GWPT: A Green Word-Embedding-based POS Tagger",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07477",
        "abstract_url": "http://arxiv.org/abs/2401.07477",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Yingping"
            },
            {
                "last_name": "Fu",
                "first_name": "Ying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Anchor-free object detectors are highly efficient in performing point-based prediction without the need for extra post-processing of anchors. However, different from the 2D grids, the 3D points used in these detectors are often far from the ground truth center, making it challenging to accurately regress the bounding boxes. To address this issue, we propose a Cascade Voting (CascadeV) strategy that provides high-quality 3D object detection with point-based prediction. Specifically, CascadeV performs cascade detection using a novel Cascade Voting decoder that combines two new components: Instance Aware Voting (IA-Voting) and a Cascade Point Assignment (CPA) module. The IA-Voting module updates the object features of updated proposal points within the bounding box using conditional inverse distance weighting. This approach prevents features from being aggregated outside the instance and helps improve the accuracy of object detection. Additionally, since model training can suffer from a lack of proposal points with high centerness, we have developed the CPA module to narrow down the positive assignment threshold with cascade stages. This approach relaxes the dependence on proposal centerness in the early stages while ensuring an ample quantity of positives with high centerness in the later stages. Experiments show that FCAF3D with our CascadeV achieves state-of-the-art 3D object detection results with 70.4\\% mAP@0.25 and 51.6\\% mAP@0.5 on SUN RGB-D and competitive results on ScanNet. Code will be released at https://github.com/Sharpiless/CascadeV-Det ",
        "title": "CascadeV-Det: Cascade Point Voting for 3D Object Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07479",
        "abstract_url": "http://arxiv.org/abs/2401.07479",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Alkhateeb",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Beam codebooks are integral components of the future millimeter wave (mmWave) multiple input multiple output (MIMO) system to relax the reliance on the instantaneous channel state information (CSI). The design of these codebooks, therefore, becomes one of the fundamental problems for these systems, and the well-designed codebooks play key roles in enabling efficient and reliable communications. Prior work has primarily focused on the codebook learning problem within a single cell/network and under stationary interference. In this work, we generalize the interference-aware codebook learning problem to networks with multiple cells/basestations. One of the key differences compared to the single-cell codebook learning problem is that the underlying environment becomes non-stationary, as the behavior of one base station will influence the learning of the others. Moreover, to encompass some of the challenging scenarios, information exchange between the different learning nodes is not allowed, which leads to a fully decentralized system with significantly increased learning difficulties. To tackle the non-stationarity, the averaging of the measurements is used to estimate the interference nulling performance of a particular beam, based on which a decision rule is provided. Furthermore, we theoretically justify the adoption of such estimator and prove that it is a sufficient statistic for the underlying quantity of interest in an asymptotic sense. Finally, a novel reward function based on averaging is proposed to fully decouple the learning of the multiple agents running at different nodes. Simulation results show that the developed solution is capable of learning well-shaped codebook patterns for different networks that significantly suppress the interference without information exchange, highlighting ... ",
        "title": "Decentralized Interference-Aware Codebook Learning in Millimeter Wave  MIMO Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07482",
        "abstract_url": "http://arxiv.org/abs/2401.07482",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Chunxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Feature selection is an important process in machine learning and knowledge discovery. By selecting the most informative features and eliminating irrelevant ones, the performance of learning algorithms can be improved and the extraction of meaningful patterns and insights from data can be facilitated. However, most existing feature selection methods, when applied to large datasets, encountered the bottleneck of high computation costs. To address this problem, we propose a novel filter feature selection method, ContrastFS, which selects discriminative features based on the discrepancies features shown between different classes. We introduce a dimensionless quantity as a surrogate representation to summarize the distributional individuality of certain classes, based on this quantity we evaluate features and study the correlation among them. We validate effectiveness and efficiency of our approach on several widely studied benchmark datasets, results show that the new method performs favorably with negligible computation in comparison with other state-of-the-art feature selection methods. ",
        "title": "A Contrast Based Feature Selection Algorithm for High-dimensional Data  set in Machine Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07484",
        "abstract_url": "http://arxiv.org/abs/2401.07484",
        "authors": [
            {
                "last_name": "Gurvich",
                "first_name": "Vladimir"
            },
            {
                "last_name": "Krnc",
                "first_name": "Matja\u017e"
            },
            {
                "last_name": "Vyalyi",
                "first_name": "Mikhail"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  An amoeba is a tree with a number assigned to each vertex. We describe a natural process of growing trees from a given amoeba and discuss conditions for such a process to be finite. ",
        "title": "Growing Trees and Amoebas' Replications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07487",
        "abstract_url": "http://arxiv.org/abs/2401.07487",
        "authors": [
            {
                "last_name": "Ju",
                "first_name": "Yuanchen"
            },
            {
                "last_name": "Hu",
                "first_name": "Kaizhe"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guowei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Gu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Mingrun"
            },
            {
                "last_name": "Xu",
                "first_name": "Huazhe"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Enabling robotic manipulation that generalizes to out-of-distribution scenes is a crucial step toward open-world embodied intelligence. For human beings, this ability is rooted in the understanding of semantic correspondence among objects, which naturally transfers the interaction experience of familiar objects to novel ones. Although robots lack such a reservoir of interaction experience, the vast availability of human videos on the Internet may serve as a valuable resource, from which we extract an affordance memory including the contact points. Inspired by the natural way humans think, we propose Robo-ABC: when confronted with unfamiliar objects that require generalization, the robot can acquire affordance by retrieving objects that share visual or semantic similarities from the affordance memory. The next step is to map the contact points of the retrieved objects to the new object. While establishing this correspondence may present formidable challenges at first glance, recent research finds it naturally arises from pre-trained diffusion models, enabling affordance mapping even across disparate object categories. Through the Robo-ABC framework, robots may generalize to manipulate out-of-category objects in a zero-shot manner without any manual annotation, additional training, part segmentation, pre-coded knowledge, or viewpoint restrictions. Quantitatively, Robo-ABC significantly enhances the accuracy of visual affordance retrieval by a large margin of 31.6% compared to state-of-the-art (SOTA) end-to-end affordance models. We also conduct real-world experiments of cross-category object-grasping tasks. Robo-ABC achieved a success rate of 85.7%, proving its capacity for real-world tasks. ",
        "title": "Robo-ABC: Affordance Generalization Beyond Categories via Semantic  Correspondence for Robot Manipulation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07488",
        "abstract_url": "http://arxiv.org/abs/2401.07488",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Chunxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  For many data-intensive tasks, feature selection is an important preprocessing step. However, most existing methods do not directly and intuitively explore the intrinsic discriminative information of features. We propose a novel feature selection framework based on the distance between class conditional distributions, measured by integral probability metrics (IPMs). Our framework directly explores the discriminative information of features in the sense of distributions for supervised classification. We analyze the theoretical and practical aspects of IPMs for feature selection, construct criteria based on IPMs. We propose several variant feature selection methods of our framework based on the 1-Wasserstein distance and implement them on real datasets from different domains. Experimental results show that our framework can outperform state-of-the-art methods in terms of classification accuracy and robustness to perturbations. ",
        "title": "Feature Selection via Maximizing Distances between Class Conditional  Distributions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07489",
        "abstract_url": "http://arxiv.org/abs/2401.07489",
        "authors": [
            {
                "last_name": "Alhussein",
                "first_name": "Hussam"
            },
            {
                "last_name": "Daqaq",
                "first_name": "Mohammed"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Recent advances in the application of physics-informed learning into the field of fluid mechanics have been predominantly grounded in the Newtonian framework, primarly leveraging Navier-Stokes Equation or one of its various derivative to train a neural network. Here, we propose an alternative approach based on variational methods. The proposed approach uses the principle of minimum pressure gradient combined with the continuity constraint to train a neural network and predict the flow field in incompressible fluids. We describe the underlying principles of the proposed approach, then use a demonstrative example to illustrate its implementation and show that it reduces the computational time per training epoch when compared to the conventional approach. ",
        "title": "The Principle of Minimum Pressure Gradient: An Alternative Basis for  Physics-Informed Learning of Incompressible Fluid Mechanics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07490",
        "abstract_url": "http://arxiv.org/abs/2401.07490",
        "authors": [
            {
                "last_name": "Hsu",
                "first_name": "Kevin"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Maximin share (MMS) allocations are a popular relaxation of envy-free allocations that have received wide attention in the context of the fair division of indivisible items. Although MMS allocations can fail to exist [1], previous work has found conditions under which they exist. Specifically, MMS allocations exist whenever $m \\leq n+5$ in the context of goods allocation, and this bound is tight in the sense that MMS allocations can fail to exist when $m = n+6$ [2]. Unfortunately, the technique used to establish this result does not generalize readily to the chores and mixed manna settings. This paper generalizes this result to the chores setting and provides a partial solution for the mixed manna setting. Our results depend on the presence of certain types of agents. Specifically, an agent $i$ is a goods agent (resp. chores agent) if every item is a good (resp. chore) to $i$, and a non-negative mixed agent if $i$ is neither a goods nor a chores agent and the MMS guarantee of $i$ is non-negative. In this paper, we prove that an MMS allocation exists if $m \\leq n+5$ and there exists a goods agent, a non-negative mixed agent, or only chores agents.   [1] David Kurokawa, Ariel D Procaccia, and Junxing Wang. When can the maximin share guarantee be guaranteed? In Thirtieth AAAI Conference on Artificial Intelligence, 2016.   [2] Uriel Feige, Ariel Sapir, and Laliv Tauber. A tight negative example for mms fair allocations. In International Conference on Web and Internet Economics, pages 355-372. Springer, 2021. ",
        "title": "Existence of MMS Allocations with Mixed Manna",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07493",
        "abstract_url": "http://arxiv.org/abs/2401.07493",
        "authors": [
            {
                "last_name": "Karmakar",
                "first_name": "T. K."
            },
            {
                "last_name": "Dalal",
                "first_name": "D. C."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  There is a class of problems that exhibit smooth behavior on macroscopic scales, where only a microscopic evolution law is known. Patch dynamics scheme of `equation-free multiscale modelling' is one of the techniques, which aims to extract the macroscopic information using such known time-dependent microscopic model simulation in patches (which is a fraction of the space-time domain) that reduces the computational complexity. Here, extrapolation time step has an important role to reduce the error at macroscopic level. In this study, a generalized patch dynamics (GPD) scheme is proposed by distributing the gap-tooth timesteppers (GTTs) within each long (macroscopic) time step. This distribution is done in two ways, namely, GPD schemes of type-I and type-II. The proposed GPD scheme is based on three different time scales namely, micro, meso and macro to predict the system level behaviours. The GPD scheme of both types are capable of providing better accuracy with less computation time compared to the usual patch dynamics (UPD) scheme. The physical behaviours of the problems can be more appropriately addressed by the GPD scheme as one may use a non-uniform (variable) distribution of gap-tooth timesteppers (GTTs), as well as the extrapolation times based on the physics of the problem. Where the UPD scheme fails to converge for a long extrapolation time, both types of GPD schemes can be successfully applied. The whole method has been analyzed successfully for the one-dimensional reaction-diffusion problem. ",
        "title": "Generalized Patch Dynamics Scheme in Equation-free Multiscale Modelling",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07494",
        "abstract_url": "http://arxiv.org/abs/2401.07494",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zihao"
            },
            {
                "last_name": "Pravin",
                "first_name": "P S"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Network. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor. ",
        "title": "Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering  Tasks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07496",
        "abstract_url": "http://arxiv.org/abs/2401.07496",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Mingzhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Dongzhu"
            },
            {
                "last_name": "Simeone",
                "first_name": "Osvaldo"
            },
            {
                "last_name": "Wen",
                "first_name": "Dingzhu"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "LG"
        ],
        "abstract": "  This paper presents a novel approach to enhance the communication efficiency of federated learning (FL) in multiple input and multiple output (MIMO) wireless systems. The proposed method centers on a low-rank matrix factorization strategy for local gradient compression based on alternating least squares, along with over-the-air computation and error feedback. The proposed protocol, termed over-the-air low-rank compression (Ota-LC), is demonstrated to have lower computation cost and lower communication overhead as compared to existing benchmarks while guaranteeing the same inference performance. As an example, when targeting a test accuracy of 80% on the Cifar-10 dataset, Ota-LC achieves a reduction in total communication costs of at least 30% when contrasted with benchmark schemes, while also reducing the computational complexity order by a factor equal to the sum of the dimension of the gradients. ",
        "title": "Low-Rank Gradient Compression with Error Feedback for MIMO Wireless  Federated Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07500",
        "abstract_url": "http://arxiv.org/abs/2401.07500",
        "authors": [
            {
                "last_name": "Otal",
                "first_name": "Hakan T."
            },
            {
                "last_name": "Zavar",
                "first_name": "Elyse"
            },
            {
                "last_name": "Binder",
                "first_name": "Sherri B."
            },
            {
                "last_name": "Greer",
                "first_name": "Alex"
            },
            {
                "last_name": "Canbaz",
                "first_name": "M. Abdullah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CY"
        ],
        "abstract": "  Environmental disasters such as floods, hurricanes, and wildfires have increasingly threatened communities worldwide, prompting various mitigation strategies. Among these, property buyouts have emerged as a prominent approach to reducing vulnerability to future disasters. This strategy involves governments purchasing at-risk properties from willing sellers and converting the land into open space, ostensibly reducing future disaster risk and impact. However, the aftermath of these buyouts, particularly concerning land-use patterns and community impacts, remains under-explored. This research aims to fill this gap by employing innovative techniques like satellite imagery analysis and deep learning to study these patterns. To achieve this goal, we employed FEMA's Hazard Mitigation Grant Program (HMGP) buyout dataset, encompassing over 41,004 addresses of these buyout properties from 1989 to 2017. Leveraging Google's Maps Static API, we gathered 40,053 satellite images corresponding to these buyout lands. Subsequently, we implemented five cutting-edge machine learning models to evaluate their performance in classifying land cover types. Notably, this task involved multi-class classification, and our model achieved an outstanding ROC-AUC score of 98.86% ",
        "title": "Harnessing Deep Learning and Satellite Imagery for Post-Buyout Land  Cover Mapping",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07502",
        "abstract_url": "http://arxiv.org/abs/2401.07502",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Wenhui"
            },
            {
                "last_name": "Wong",
                "first_name": "Man Sing"
            },
            {
                "last_name": "Yu",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Shi",
                "first_name": "Guoqiang"
            },
            {
                "last_name": "Kwok",
                "first_name": "Coco Yin Tung"
            },
            {
                "last_name": "Zou",
                "first_name": "Kang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semantic segmentation-based methods have attracted extensive attention in oil spill detection from SAR images. However, the existing approaches require a large number of finely annotated segmentation samples in the training stage. To alleviate this issue, we propose a composite oil spill detection framework, SAM-OIL, comprising an object detector (e.g., YOLOv8), an adapted Segment Anything Model (SAM), and an Ordered Mask Fusion (OMF) module. SAM-OIL is the first application of the powerful SAM in oil spill detection. Specifically, the SAM-OIL strategy uses YOLOv8 to obtain the categories and bounding boxes of oil spill-related objects, then inputs bounding boxes into the adapted SAM to retrieve category-agnostic masks, and finally adopts the Ordered Mask Fusion (OMF) module to fuse the masks and categories. The adapted SAM, combining a frozen SAM with a learnable Adapter module, can enhance SAM's ability to segment ambiguous objects. The OMF module, a parameter-free method, can effectively resolve pixel category conflicts within SAM. Experimental results demonstrate that SAM-OIL surpasses existing semantic segmentation-based oil spill detection methods, achieving mIoU of 69.52%. The results also indicated that both OMF and Adapter modules can effectively improve the accuracy in SAM-OIL. ",
        "title": "Compositional Oil Spill Detection Based on Object Detector and Adapted  Segment Anything Model from SAR Images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07503",
        "abstract_url": "http://arxiv.org/abs/2401.07503",
        "authors": [
            {
                "last_name": "Kato",
                "first_name": "Shunya"
            },
            {
                "last_name": "Saito",
                "first_name": "Masaki"
            },
            {
                "last_name": "Ishiguro",
                "first_name": "Katsuhiko"
            },
            {
                "last_name": "Cummings",
                "first_name": "Sol"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Despeckling is a crucial noise reduction task in improving the quality of synthetic aperture radar (SAR) images. Directly obtaining noise-free SAR images is a challenging task that has hindered the development of accurate despeckling algorithms. The advent of deep learning has facilitated the study of denoising models that learn from only noisy SAR images. However, existing methods deal solely with single-polarization images and cannot handle the multi-polarization images captured by modern satellites. In this work, we present an extension of the existing model for generating single-polarization SAR images to handle multi-polarization SAR images. Specifically, we propose a novel self-supervised despeckling approach called channel masking, which exploits the relationship between polarizations. Additionally, we utilize a spatial masking method that addresses pixel-to-pixel correlations to further enhance the performance of our approach. By effectively incorporating multiple polarization information, our method surpasses current state-of-the-art methods in quantitative evaluation in both synthetic and real-world scenarios. ",
        "title": "PolMERLIN: Self-Supervised Polarimetric Complex SAR Image Despeckling  with Masked Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07506",
        "abstract_url": "http://arxiv.org/abs/2401.07506",
        "authors": [
            {
                "last_name": "Sasindran",
                "first_name": "Zitha"
            },
            {
                "last_name": "Yelchuri",
                "first_name": "Harsha"
            },
            {
                "last_name": "Prabhakar",
                "first_name": "T. V."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD"
        ],
        "abstract": "  In this study, we present SeMaScore, generated using a segment-wise mapping and scoring algorithm that serves as an evaluation metric for automatic speech recognition tasks. SeMaScore leverages both the error rate and a more robust similarity score. We show that our algorithm's score generation improves upon the state-of-the-art BERTscore. Our experimental results show that SeMaScore corresponds well with expert human assessments, signal-to-noise ratio levels, and other natural language metrics. We outperform BERTscore by 41x in metric computation speed. Overall, we demonstrate that SeMaScore serves as a more dependable evaluation metric, particularly in real-world situations involving atypical speech patterns. ",
        "title": "SeMaScore : a new evaluation metric for automatic speech recognition  tasks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07510",
        "abstract_url": "http://arxiv.org/abs/2401.07510",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Li",
                "first_name": "Lei"
            },
            {
                "last_name": "Li",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks such as unimodal-related question answering, reading comprehension, reasoning, diagnosis, relation extraction, probability modeling, and others, as well as multimodal-related tasks like vision question answering, image caption, cross-modal retrieval, report summarization, and generation, are discussed in detail. Each section delves into the intricate specifics of the respective method under consideration. This paper highlights the structures and advancements of medical domain explorations against general domain methods, emphasizing their applications across different tasks and datasets. It also outlines current challenges and opportunities for future medical domain research, paving the way for continued innovation and application in this rapidly evolving field. ",
        "title": "Developing ChatGPT for Biology and Medicine: A Complete Review of  Biomedical Question Answering",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07511",
        "abstract_url": "http://arxiv.org/abs/2401.07511",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xiangtong"
            },
            {
                "last_name": "Han",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Yang",
                "first_name": "Menglong"
            },
            {
                "last_name": "Han",
                "first_name": "Songchen"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  This paper presents SNK, a novel simulation platform designed to evaluate the network performance of constellation systems for global Internet services. SNK offers realtime communication visualization and supports the simulation of routing between edge node of network. The platform enables the evaluation of routing and network performance metrics such as latency, stretch, network capacity, and throughput under different network structures and density. The effectiveness of SNK is demonstrated through various simulation cases, including the routing between fixed edge stations or mobile edge stations and analysis of space network structures. ",
        "title": "Space Networking Kit: A Novel Simulation Platform for Emerging LEO  Mega-constellations",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07515",
        "abstract_url": "http://arxiv.org/abs/2401.07515",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Hao"
            },
            {
                "last_name": "Liang",
                "first_name": "Le"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  To enhance the performance of massive multi-input multi-output (MIMO) detection using deep learning, prior research primarily adopts a model-driven methodology, integrating deep neural networks (DNNs) with traditional iterative detectors. Despite these efforts, achieving a purely data-driven detector has remained elusive, primarily due to the inherent complexities arising from the problem's high dimensionality. This paper introduces ChannelNet, a simple yet effective purely data-driven massive MIMO detector. ChannelNet embeds the channel matrix into the network as linear layers rather than viewing it as input, enabling scalability to massive MIMO scenarios. ChannelNet is computationally efficient and has a computational complexity of $\\mathcal{O}(N_t N_r)$, where $N_t$ and $N_r$ represent the numbers of transmit and receive antennas, respectively. Despite the low computation complexity, ChannelNet demonstrates robust empirical performance, matching or surpassing state-of-the-art detectors in various scenarios. In addition, theoretical insights establish ChannelNet as a universal approximator in probability for any continuous permutation-equivariant functions. ChannelNet demonstrates that designing deep learning based massive MIMO detectors can be purely data-driven and free from the constraints posed by the conventional iterative frameworks as well as the channel and noise distribution models. ",
        "title": "On Purely Data-Driven Massive MIMO Detectors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07516",
        "abstract_url": "http://arxiv.org/abs/2401.07516",
        "authors": [
            {
                "last_name": "Fard",
                "first_name": "Sanaz Hasanzadeh"
            },
            {
                "last_name": "Ghassemi",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Graphs are a powerful representation tool in machine learning applications, with link prediction being a key task in graph learning. Temporal link prediction in dynamic networks is of particular interest due to its potential for solving complex scientific and real-world problems. Traditional approaches to temporal link prediction have focused on finding the aggregation of dynamics of the network as a unified output. In this study, we propose a novel perspective on temporal link prediction by defining nodes as Newtonian objects and incorporating the concept of velocity to predict network dynamics. By computing more specific dynamics of each node, rather than overall dynamics, we improve both accuracy and explainability in predicting future connections. We demonstrate the effectiveness of our approach using two datasets, including 17 years of co-authorship data from PubMed. Experimental results show that our temporal graph embedding dynamics approach improves downstream classification models' ability to predict future collaboration efficacy in co-authorship networks by 17.34% (AUROC improvement relative to the baseline model). Furthermore, our approach offers an interpretable layer over traditional approaches to address the temporal link prediction problem. ",
        "title": "Temporal Link Prediction Using Graph Embedding Dynamics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07517",
        "abstract_url": "http://arxiv.org/abs/2401.07517",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xiangtong"
            },
            {
                "last_name": "Yang",
                "first_name": "Menglong"
            },
            {
                "last_name": "Han",
                "first_name": "Songchen"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The structure and routing architecture design is critical for achieving low latency and high capacity in future LEO space networks (SNs). Existing studies mainly focus on topologies of space networks, but there is a lack of analysis on constellation structures, which can greatly affect network performance. In addition, some routing architectures are designed for networks with a small number of network nodes such as Iridium while they introduce significant network overhead for high-density networks (i.e., mega-constellation networks containing thousands of satellites). In this paper, we conduct the quantitatively study on the design of network structure and routing architecture in space. The high density, high dynamics, and large scale nature of emerging Space Networks (SNs) pose significant challenges, such as unstable routing paths, low network reachability, high latency, and large jitter. To alleviate the above challenges, we design the structure of space network to maximum the connectivity through wisely adjusting the inter-plane inter satellite link. We further propose Multi-Protocol Location Forwarding (MPLF), a distributed routing architecture, targeting at minimizing the propagation latency with a distributed, convergence-free routing paradigm, while keeping routing stable and maximum the path diversity. Comprehensive experiments are conducted on a customized platform \\textit{Space Networking Kits} (SNK) which demonstrate that our solution can outperform existing related schemes by about 14\\% reduction of propagation latency and 66\\% reduction of hops-count on average, while sustaining a high path diversity with only $O(1)$ time complexity. ",
        "title": "Multi-Protocol Location Forwarding (MPLF) for Space Routing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07518",
        "abstract_url": "http://arxiv.org/abs/2401.07518",
        "authors": [
            {
                "last_name": "Lan",
                "first_name": "Yunshi"
            },
            {
                "last_name": "Li",
                "first_name": "Xinyuan"
            },
            {
                "last_name": "Du",
                "first_name": "Hanyue"
            },
            {
                "last_name": "Lu",
                "first_name": "Xuesong"
            },
            {
                "last_name": "Gao",
                "first_name": "Ming"
            },
            {
                "last_name": "Qian",
                "first_name": "Weining"
            },
            {
                "last_name": "Zhou",
                "first_name": "Aoying"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions. ",
        "title": "Survey of Natural Language Processing for Education: Taxonomy,  Systematic Review, and Future Trends",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07519",
        "abstract_url": "http://arxiv.org/abs/2401.07519",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qixun"
            },
            {
                "last_name": "Bai",
                "first_name": "Xu"
            },
            {
                "last_name": "Wang",
                "first_name": "Haofan"
            },
            {
                "last_name": "Qin",
                "first_name": "Zekui"
            },
            {
                "last_name": "Chen",
                "first_name": "Anthony"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at https://github.com/InstantID/InstantID. ",
        "title": "InstantID: Zero-shot Identity-Preserving Generation in Seconds",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07521",
        "abstract_url": "http://arxiv.org/abs/2401.07521",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jie"
            },
            {
                "last_name": "Ding",
                "first_name": "Zhaoying"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoshuang"
            },
            {
                "last_name": "Chen",
                "first_name": "Qi"
            },
            {
                "last_name": "Wang",
                "first_name": "Yincheng"
            },
            {
                "last_name": "Zhan",
                "first_name": "Kaiqiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Ben"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The watch time is a significant indicator of user satisfaction in video recommender systems. However, the prediction of watch time as a target variable is often hindered by its highly imbalanced distribution with a scarcity of observations for larger target values and over-populated samples for small values. State-of-the-art watch time prediction models discretize the continuous watch time into a set of buckets in order to consider the distribution of watch time. However, it is highly uninvestigated how these discrete buckets should be created from the continuous watch time distribution, and existing discretization approaches suffer from either a large learning error or a large restoration error. To address this challenge, we propose a Classification-Restoration framework with Error-Adaptive-Discretization (CREAD) to accurately predict the watch time. The proposed framework contains a discretization module, a classification module, and a restoration module. It predicts the watch time through multiple classification problems. The discretization process is a key contribution of the CREAD framework. We theoretically analyze the impacts of the discretization on the learning error and the restoration error, and then propose the error-adaptive discretization (EAD) technique to better balance the two errors, which achieves better performance over traditional discretization approaches. We conduct detailed offline evaluations on a public dataset and an industrial dataset, both showing performance gains through the proposed approach. Moreover, We have fully launched our framework to Kwai App, an online video platform, which resulted in a significant increase in users' video watch time by 0.29% through A/B testing. These results highlight the effectiveness of the CREAD framework in watch time prediction in video recommender systems. ",
        "title": "CREAD: A Classification-Restoration Framework with Error Adaptive  Discretization for Watch Time Prediction in Video Recommender Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07525",
        "abstract_url": "http://arxiv.org/abs/2401.07525",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Yihan"
            },
            {
                "last_name": "Chen",
                "first_name": "Xu"
            },
            {
                "last_name": "Du",
                "first_name": "Lun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Fu",
                "first_name": "Qiang"
            },
            {
                "last_name": "Han",
                "first_name": "Shi"
            },
            {
                "last_name": "Du",
                "first_name": "Yushu"
            },
            {
                "last_name": "Kang",
                "first_name": "Yanbin"
            },
            {
                "last_name": "Lu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Li",
                "first_name": "Zi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Person-job fit is an essential part of online recruitment platforms in serving various downstream applications like Job Search and Candidate Recommendation. Recently, pretrained large language models have further enhanced the effectiveness by leveraging richer textual information in user profiles and job descriptions apart from user behavior features and job metadata. However, the general domain-oriented design struggles to capture the unique structural information within user profiles and job descriptions, leading to a loss of latent semantic correlations. We propose TAROT, a hierarchical multitask co-pretraining framework, to better utilize structural and semantic information for informative text embeddings. TAROT targets semi-structured text in profiles and jobs, and it is co-pretained with multi-grained pretraining tasks to constrain the acquired semantic information at each level. Experiments on a real-world LinkedIn dataset show significant performance improvements, proving its effectiveness in person-job fit tasks. ",
        "title": "TAROT: A Hierarchical Framework with Multitask Co-Pretraining on  Semi-Structured Data towards Effective Person-Job Fit",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07526",
        "abstract_url": "http://arxiv.org/abs/2401.07526",
        "authors": [
            {
                "last_name": "Feigenbaum",
                "first_name": "Itai"
            },
            {
                "last_name": "Arpit",
                "first_name": "Devansh"
            },
            {
                "last_name": "Wang",
                "first_name": "Huan"
            },
            {
                "last_name": "Heinecke",
                "first_name": "Shelby"
            },
            {
                "last_name": "Niebles",
                "first_name": "Juan Carlos"
            },
            {
                "last_name": "Yao",
                "first_name": "Weiran"
            },
            {
                "last_name": "Xiong",
                "first_name": "Caiming"
            },
            {
                "last_name": "Savarese",
                "first_name": "Silvio"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large Language Model (LLM) editing modifies factual information in LLMs. Locate-and-Edit (L\\&E) methods accomplish this by finding where relevant information is stored within the neural network, and editing the weights at that location. The goal of editing is to modify the response of an LLM to a proposition independently of its phrasing, while not modifying its response to other related propositions. Existing methods are limited to binary propositions, which represent straightforward binary relations between a subject and an object. Furthermore, existing methods rely on semantic subject labels, which may not be available or even be well-defined in practice. In this paper, we show that both of these issues can be effectively skirted with a simple and fast localization method called Gradient Tracing (GT). This localization method allows editing arbitrary propositions instead of just binary ones, and does so without the need for subject labels. As propositions always have a truth value, our experiments prompt an LLM as a boolean classifier, and edit its T/F response to propositions. Our method applies GT for location tracing, and then edit the model at that location using a mild variant of Rank-One Model Editing (ROME). On datasets of binary propositions derived from the CounterFact dataset, we show that our method -- without access to subject labels -- performs close to state-of-the-art L\\&E methods which has access subject labels. We then introduce a new dataset, Factual Accuracy Classification Test (FACT), which includes non-binary propositions and for which subject labels are not generally applicable, and therefore is beyond the scope of existing L\\&E methods. Nevertheless, we show that with our method editing is possible on FACT. ",
        "title": "Editing Arbitrary Propositions in LLMs without Subject Labels",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07527",
        "abstract_url": "http://arxiv.org/abs/2401.07527",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Zhitong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fahong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiao Xiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Foundation models characterized by extensive parameters and trained on large-scale datasets have demonstrated remarkable efficacy across various downstream tasks for remote sensing data. Current remote sensing foundation models typically specialize in a single modality or a specific spatial resolution range, limiting their versatility for downstream datasets. While there have been attempts to develop multi-modal remote sensing foundation models, they typically employ separate vision encoders for each modality or spatial resolution, necessitating a switch in backbones contingent upon the input data. To address this issue, we introduce a simple yet effective method, termed OFA-Net (One-For-All Network): employing a single, shared Transformer backbone for multiple data modalities with different spatial resolutions. Using the masked image modeling mechanism, we pre-train a single Transformer backbone on a curated multi-modal dataset with this simple design. Then the backbone model can be used in different downstream tasks, thus forging a path towards a unified foundation backbone model in Earth vision. The proposed method is evaluated on 12 distinct downstream tasks and demonstrates promising performance. ",
        "title": "One for All: Toward Unified Foundation Models for Earth Vision",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07528",
        "abstract_url": "http://arxiv.org/abs/2401.07528",
        "authors": [
            {
                "last_name": "Prieur",
                "first_name": "Nils C."
            },
            {
                "last_name": "Amaro",
                "first_name": "Brian"
            },
            {
                "last_name": "Gonzalez",
                "first_name": "Emiliano"
            },
            {
                "last_name": "Kerner",
                "first_name": "Hannah"
            },
            {
                "last_name": "Medvedev",
                "first_name": "Sergei"
            },
            {
                "last_name": "Rubanenko",
                "first_name": "Lior"
            },
            {
                "last_name": "Werner",
                "first_name": "Stephanie C."
            },
            {
                "last_name": "Xiao8",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Zastrozhnov",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Lap\u00f4tre",
                "first_name": "Mathieu G. A."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Boulders form from a variety of geological processes, which their size, shape, and orientation may help us better understand. Furthermore, they represent potential hazards to spacecraft landing that need to be characterized. However, mapping individual boulders across vast areas is extremely labor-intensive, often limiting the extent over which they are characterized and the statistical robustness of obtained boulder morphometrics. To automate boulder characterization, we use an instance segmentation neural network, Mask R-CNN, to detect and outline boulders in high-resolution satellite images. Our neural network, BoulderNet, was trained from a dataset of > 33,000 boulders in > 750 image tiles from Earth, the Moon, and Mars. BoulderNet not only correctly detects the majority of boulders in images, but it identifies the outline of boulders with high fidelity, achieving average precision and recall values of 72% and 64% relative to manually digitized boulders from the test dataset, when only detections with intersection-over-union ratios > 50% are considered valid. These values are similar to those obtained by human mappers. On Earth, equivalent boulder diameters, aspect ratios, and orientations extracted from predictions were benchmarked against ground measurements and yield values within 15%, 0.20, and 20 degrees of their ground-truth values, respectively. BoulderNet achieves better boulder detection and characterization performance relative to existing methods, providing a versatile open-source tool to characterize entire boulder fields on planetary surfaces. ",
        "title": "Automatic characterization of boulders on planetary surfaces from  high-resolution satellite images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07529",
        "abstract_url": "http://arxiv.org/abs/2401.07529",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yuhao"
            },
            {
                "last_name": "Liao",
                "first_name": "Yusheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Heyang"
            },
            {
                "last_name": "Liu",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanfeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Multimodal Large Language Models (MLLMs) have shown their remarkable abilities in visual perception and understanding recently. However, how to comprehensively evaluate the capabilities of MLLMs remains a challenge. Most of the existing benchmarks predominantly focus on assessing perception, cognition, and reasoning, neglecting the abilities of self-awareness, referring to the model's recognition of its own capability boundary. In our study, we focus on self-awareness in image perception and introduce the knowledge quadrant for MLLMs, which clearly defines the knowns and unknowns in perception. Based on this, we propose a novel benchmark specifically designed to evaluate the Self-Aware capabilities in Perception for MLLMs(MM-SAP). MM-SAP encompasses three distinct sub-datasets, each focusing on different aspects of self-awareness. We evaluated eight well-known MLLMs using MM-SAP, analyzing their self-awareness and providing detailed insights. Code and data are available at https://github.com/YHWmz/MM-SAP ",
        "title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of  Multimodal Large Language Models in Perception",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07532",
        "abstract_url": "http://arxiv.org/abs/2401.07532",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Jun"
            },
            {
                "last_name": "Tang",
                "first_name": "Boshi"
            },
            {
                "last_name": "Sha",
                "first_name": "Binzhu"
            },
            {
                "last_name": "Yang",
                "first_name": "Jing"
            },
            {
                "last_name": "Ju",
                "first_name": "Yaolong"
            },
            {
                "last_name": "Fan",
                "first_name": "Fan"
            },
            {
                "last_name": "Kang",
                "first_name": "Shiyin"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Meng",
                "first_name": "Helen"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music. ",
        "title": "Multi-view MidiVAE: Fusing Track- and Bar-view Representations for Long  Multi-track Symbolic Music Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07533",
        "abstract_url": "http://arxiv.org/abs/2401.07533",
        "authors": [
            {
                "last_name": "Bornes",
                "first_name": "Laetitia"
            },
            {
                "last_name": "Letondal",
                "first_name": "Catherine"
            },
            {
                "last_name": "Vingerhoeds",
                "first_name": "Rob"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Until recently, research into the sustainable design of interactive systems has primarily focused on the direct material impact of a system, through improving its energy efficiency and optimizing its lifecycle. Yet the way a system is designed and marketed often has wider repercussions, such as rebound effects, and systemic change in practices. These effects are harder to assess (and to anticipate) than the direct physical impact of the construction and use of the system itself. Current tools are unable to account for the complexity of these effects: the underlying causal mechanisms, their multi-level nature, their different temporalities, and the variety of their consequences (environmental and societal). This is why we are seeking to develop a specific methodology and tool, inspired by systemic design and system dynamics. These are intended for decision-makers and designers of interactive systems within systems of systems (for example, in the fields of agricultural robotics or public transportation). In this paper, we present this modeling approach and our prototype tool through the example of a second-hand clothing sales platform. ",
        "title": "Understanding the Indirect Effects of Interactive Systems Within Systems  of Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07534",
        "abstract_url": "http://arxiv.org/abs/2401.07534",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jialong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyue"
            },
            {
                "last_name": "Li",
                "first_name": "Nianyu"
            },
            {
                "last_name": "Weyns",
                "first_name": "Danny"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Tei",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Large Language Models (LLMs), with their abilities in knowledge acquisition and reasoning, can potentially enhance the various aspects of Self-adaptive Systems (SAS). Yet, the potential of LLMs in SAS remains largely unexplored and ambiguous, due to the lack of literature from flagship conferences or journals in the field, such as SEAMS and TAAS. The interdisciplinary nature of SAS suggests that drawing and integrating ideas from related fields, such as software engineering and autonomous agents, could unveil innovative research directions for LLMs within SAS. To this end, this paper reports the results of a literature review of studies in relevant fields, summarizes and classifies the studies relevant to SAS, and outlines their potential to specific aspects of SAS. ",
        "title": "Exploring the Potential of Large Language Models in Self-adaptive  Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07538",
        "abstract_url": "http://arxiv.org/abs/2401.07538",
        "authors": [
            {
                "last_name": "Gosti",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Succi",
                "first_name": "Sauro"
            },
            {
                "last_name": "Ruocco",
                "first_name": "Giancarlo"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  It is shown that a Hopfield recurrent neural network, informed by experimentally derived brain topology, recovers the scaling picture recently introduced by Deco et al., according to which the process of information transfer within the human brain shows spatially correlated patterns qualitatively similar to those displayed by turbulent flows. Although both models employ a coupling strength which decays exponentially with the euclidean distance between the nodes, their mathematical nature is widely different, Hopf oscillators versus Hopfield neural network. Hence, their convergence suggests a remarkable robustness of the aforementioned scaling picture. Furthermore, the present analysis shows that the Hopfield model brain remains functional by removing links above about five decay lengths, corresponding to about one sixth of the size of the global brain. This suggests that, in terms of connectivity decay length, the Hopfield brain functions in a sort of intermediate \"turbulent liquid\"-like state, whose essential connections are the intermediate ones between the connectivity decay length and the global brain size. This \"turbulent-like liquid\" appears to be more spiky than actual turbulent fluids, with a scaling exponent around $2/5$ instead of $2/3$. ",
        "title": "Evidence of Scaling Regimes in the Hopfield Dynamics of Whole Brain  Model",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07539",
        "abstract_url": "http://arxiv.org/abs/2401.07539",
        "authors": [
            {
                "last_name": "Aqasizade",
                "first_name": "Hossein"
            },
            {
                "last_name": "Ataie",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Bastam",
                "first_name": "Mostafa"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "AR",
            "PF"
        ],
        "abstract": "  Over the past two decades, the cloud computing paradigm has gradually attracted more popularity due to its efficient resource usage and simple service access model. Virtualization technology is the fundamental element of cloud computing that brings several benefits to cloud users and providers, such as workload isolation, energy efficiency, server consolidation, and cost reduction. This paper examines the combination of operating system-level virtualization (containers) and hardware-level virtualization (virtual machines). To this end, the performance of containers running on top of virtual machines is experimentally compared with standalone virtual machines and containers based on different hardware resources, including the processor, main memory, disk, and network in a real testbed by running the most commonly used benchmarks. Paravirtualization and full virtualization as well as type 1 and type 2 hypervisors are covered in this study. In addition, three prevalent containerization platforms are examined. ",
        "title": "Experimental Assessment of Containers Running on Top of Virtual Machines",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07540",
        "abstract_url": "http://arxiv.org/abs/2401.07540",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Chunxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we present a novel framework for data redundancy measurement based on probabilistic modeling of datasets, and a new criterion for redundancy detection that is resilient to noise. We also develop new methods for data redundancy reduction using both deterministic and stochastic optimization techniques. Our framework is flexible and can handle different types of features, and our experiments on benchmark datasets demonstrate the effectiveness of our methods. We provide a new perspective on feature selection, and propose effective and robust approaches for both supervised and unsupervised learning problems. ",
        "title": "Study Features via Exploring Distribution Structure",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07541",
        "abstract_url": "http://arxiv.org/abs/2401.07541",
        "authors": [
            {
                "last_name": "Habibiroudkenar",
                "first_name": "Pejman"
            },
            {
                "last_name": "Ojala",
                "first_name": "Risto"
            },
            {
                "last_name": "Tammi",
                "first_name": "Kari"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In the field of indoor robotics, accurately navigating and mapping in dynamic environments using point clouds can be a challenging task due to the presence of dynamic points. These dynamic points are often represented by people in indoor environments, but in industrial settings with moving machinery, there can be various types of dynamic points. This study introduces DynaHull, a novel technique designed to enhance indoor mapping accuracy by effectively removing dynamic points from point clouds. DynaHull works by leveraging the observation that, over multiple scans, stationary points have a higher density compared to dynamic ones. Furthermore, DynaHull addresses mapping challenges related to unevenly distributed points by clustering the map into smaller sections. In each section, the density factor of each point is determined by dividing the number of neighbors by the volume these neighboring points occupy using a convex hull method. The algorithm removes the dynamic points using an adaptive threshold based on the point count of each cluster, thus reducing the false positives. The performance of DynaHull was compared to state-of-the-art techniques, such as ERASOR, Removert, OctoMap, and a baseline statistical outlier removal from Open3D, by comparing each method to the ground truth map created during a low activity period in which only a few dynamic points were present. The results indicated that DynaHull outperformed these techniques in various metrics, noticeably in the Earth Mover's Distance. This research contributes to indoor robotics by providing efficient methods for dynamic point removal, essential for accurate mapping and localization in dynamic environments. ",
        "title": "DynaHull: Density-centric Dynamic Point Filtering in Point Clouds",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07542",
        "abstract_url": "http://arxiv.org/abs/2401.07542",
        "authors": [
            {
                "last_name": "Keuth",
                "first_name": "Ron"
            },
            {
                "last_name": "Heinrich",
                "first_name": "Mattias"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  When solving a segmentation task, shaped-base methods can be beneficial compared to pixelwise classification due to geometric understanding of the target object as shape, preventing the generation of anatomical implausible predictions in particular for corrupted data. In this work, we propose a novel hybrid method that combines a lightweight CNN backbone with a geometric neural network (Point Transformer) for shape regression. Using the same CNN encoder, the Point Transformer reaches segmentation quality on per with current state-of-the-art convolutional decoders ($4\\pm1.9$ vs $3.9\\pm2.9$ error in mm and $85\\pm13$ vs $88\\pm10$ Dice), but crucially, is more stable w.r.t image distortion, starting to outperform them at a corruption level of 30%. Furthermore, we include the nnU-Net as an upper baseline, which has $3.7\\times$ more trainable parameters than our proposed method. ",
        "title": "Combining Image- and Geometric-based Deep Learning for Shape Regression:  A Comparison to Pixel-level Methods for Segmentation in Chest X-Ray",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07543",
        "abstract_url": "http://arxiv.org/abs/2401.07543",
        "authors": [
            {
                "last_name": "Zang",
                "first_name": "Zelin"
            },
            {
                "last_name": "Li",
                "first_name": "Liangyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Yongjie"
            },
            {
                "last_name": "Duan",
                "first_name": "Chenrui"
            },
            {
                "last_name": "Wang",
                "first_name": "Kai"
            },
            {
                "last_name": "You",
                "first_name": "Yang"
            },
            {
                "last_name": "Sun",
                "first_name": "Yi"
            },
            {
                "last_name": "Li",
                "first_name": "Stan Z."
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Spatial transcriptomics (ST) technologies have revolutionized the study of gene expression patterns in tissues by providing multimodality data in transcriptomic, spatial, and morphological, offering opportunities for understanding tissue biology beyond transcriptomics. However, we identify the modality bias phenomenon in ST data species, i.e., the inconsistent contribution of different modalities to the labels leads to a tendency for the analysis methods to retain the information of the dominant modality. How to mitigate the adverse effects of modality bias to satisfy various downstream tasks remains a fundamental challenge. This paper introduces Multiple-modality Structure Transformation, named MuST, a novel methodology to tackle the challenge. MuST integrates the multi-modality information contained in the ST data effectively into a uniform latent space to provide a foundation for all the downstream tasks. It learns intrinsic local structures by topology discovery strategy and topology fusion loss function to solve the inconsistencies among different modalities. Thus, these topology-based and deep learning techniques provide a solid foundation for a variety of analytical tasks while coordinating different modalities. The effectiveness of MuST is assessed by performance metrics and biological significance. The results show that it outperforms existing state-of-the-art methods with clear advantages in the precision of identifying and preserving structures of tissues and biomarkers. MuST offers a versatile toolkit for the intricate analysis of complex biological systems. ",
        "title": "Must: Maximizing Latent Capacity of Spatial Transcriptomics Data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07548",
        "abstract_url": "http://arxiv.org/abs/2401.07548",
        "authors": [
            {
                "last_name": "Majumdar",
                "first_name": "Rupak"
            },
            {
                "last_name": "Saglam",
                "first_name": "Irmak"
            },
            {
                "last_name": "Thejaswini",
                "first_name": "K. S."
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "DS",
            "FL"
        ],
        "abstract": "  We provide an algorithm to solve Rabin and Streett games over graphs with $n$ vertices, $m$ edges, and $k$ colours that runs in $\\tilde{O}\\left(mn(k!)^{1+o(1)} \\right)$ time and $O(nk\\log k \\log n)$ space, where $\\tilde{O}$ hides poly-logarithmic factors. Our algorithm is an improvement by a super quadratic dependence on $k!$ from the currently best known run time of $O\\left(mn^2(k!)^{2+o(1)}\\right)$, obtained by converting a Rabin game into a parity game, while simultaneously improving its exponential space requirement.   Our main technical ingredient is a characterisation of progress measures for Rabin games using \\emph{colourful trees} and a combinatorial construction of succinctly-represented, universal colourful trees. Colourful universal trees are generalisations of universal trees used by Jurdzi\\'{n}ski and Lazi\\'{c} (2017) to solve parity games, as well as of Rabin progress measures of Klarlund and Kozen (1991). Our algorithm for Rabin games is a progress measure lifting algorithm where the lifting is performed on succinct, colourful, universal trees. ",
        "title": "Rabin Games and Colourful Universal Trees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07549",
        "abstract_url": "http://arxiv.org/abs/2401.07549",
        "authors": [
            {
                "last_name": "Callard",
                "first_name": "Antonin"
            },
            {
                "last_name": "Salomon",
                "first_name": "L\u00e9o Paviet"
            },
            {
                "last_name": "Vanier",
                "first_name": "Pascal"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "CC"
        ],
        "abstract": "  Subshifts are colorings of $\\mathbb{Z}^d$ defined by families of forbidden patterns. Given a subshift and a finite pattern, its extender set is the set of admissible completions of this pattern. It has been conjectured that the behavior of extender sets, and in particular their growth called extender entropy (arXiv:1711.07515), could provide a way to separate the classes of sofic and effective subshifts. We prove here that both classes have the same possible extender entropies: exactly the $\\Pi_3$ real numbers of $[0,+\\infty)$. We also consider computational properties of extender entropies for subshifts with some language or dynamical properties: computable language, minimal and some mixing properties. ",
        "title": "Computability of extender sets in multidimensional subshifts",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07551",
        "abstract_url": "http://arxiv.org/abs/2401.07551",
        "authors": [
            {
                "last_name": "Xi",
                "first_name": "Wenjuan"
            },
            {
                "last_name": "Song",
                "first_name": "Xin"
            },
            {
                "last_name": "Guo",
                "first_name": "Weili"
            },
            {
                "last_name": "Yang",
                "first_name": "Yang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Existing semi-supervised learning (SSL) methods assume that labeled and unlabeled data share the same class space. However, in real-world applications, unlabeled data always contain classes not present in the labeled set, which may cause classification performance degradation of known classes. Therefore, open-world SSL approaches are researched to handle the presence of multiple unknown classes in the unlabeled data, which aims to accurately classify known classes while fine-grained distinguishing different unknown classes. To address this challenge, in this paper, we propose an open-world SSL method for Self-learning Open-world Classes (SSOC), which can explicitly self-learn multiple unknown classes. Specifically, SSOC first defines class center tokens for both known and unknown classes and autonomously learns token representations according to all samples with the cross-attention mechanism. To effectively discover novel classes, SSOC further designs a pairwise similarity loss in addition to the entropy loss, which can wisely exploit the information available in unlabeled data from instances' predictions and relationships. Extensive experiments demonstrate that SSOC outperforms the state-of-the-art baselines on multiple popular classification benchmarks. Specifically, on the ImageNet-100 dataset with a novel ratio of 90%, SSOC achieves a remarkable 22% improvement. ",
        "title": "Robust Semi-Supervised Learning for Self-learning Open-World Classes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07553",
        "abstract_url": "http://arxiv.org/abs/2401.07553",
        "authors": [
            {
                "last_name": "Lou",
                "first_name": "Xingzhou"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junge"
            },
            {
                "last_name": "Wang",
                "first_name": "Ziyan"
            },
            {
                "last_name": "Huang",
                "first_name": "Kaiqi"
            },
            {
                "last_name": "Du",
                "first_name": "Yali"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Safe reinforcement learning (RL) agents accomplish given tasks while adhering to specific constraints. Employing constraints expressed via easily-understandable human language offers considerable potential for real-world applications due to its accessibility and non-reliance on domain expertise. Previous safe RL methods with natural language constraints typically adopt a recurrent neural network, which leads to limited capabilities when dealing with various forms of human language input. Furthermore, these methods often require a ground-truth cost function, necessitating domain expertise for the conversion of language constraints into a well-defined cost function that determines constraint violation. To address these issues, we proposes to use pre-trained language models (LM) to facilitate RL agents' comprehension of natural language constraints and allow them to infer costs for safe policy learning. Through the use of pre-trained LMs and the elimination of the need for a ground-truth cost, our method enhances safe policy learning under a diverse set of human-derived free-form natural language constraints. Experiments on grid-world navigation and robot control show that the proposed method can achieve strong performance while adhering to given constraints. The usage of pre-trained LMs allows our method to comprehend complicated constraints and learn safe policies without the need for ground-truth cost at any stage of training or evaluation. Extensive ablation studies are conducted to demonstrate the efficacy of each part of our method. ",
        "title": "Safe Reinforcement Learning with Free-form Natural Language Constraints  and Pre-Trained Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07558",
        "abstract_url": "http://arxiv.org/abs/2401.07558",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Biwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongliang"
            },
            {
                "last_name": "Xu",
                "first_name": "Minghui"
            },
            {
                "last_name": "Yu",
                "first_name": "Dongxiao"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xiuzhen"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  Federated learning is a powerful technique that enables collaborative learning among different clients. Prototype-based federated learning is a specific approach that improves the performance of local models under non-IID (non-Independently and Identically Distributed) settings by integrating class prototypes. However, prototype-based federated learning faces several challenges, such as prototype redundancy and prototype failure, which limit its accuracy. It is also susceptible to poisoning attacks and server malfunctions, which can degrade the prototype quality. To address these issues, we propose FedRFQ, a prototype-based federated learning approach that aims to reduce redundancy, minimize failures, and improve \\underline{q}uality. FedRFQ leverages a SoftPool mechanism, which effectively mitigates prototype redundancy and prototype failure on non-IID data. Furthermore, we introduce the BFT-detect, a BFT (Byzantine Fault Tolerance) detectable aggregation algorithm, to ensure the security of FedRFQ against poisoning attacks and server malfunctions. Finally, we conduct experiments on three different datasets, namely MNIST, FEMNIST, and CIFAR-10, and the results demonstrate that FedRFQ outperforms existing baselines in terms of accuracy when handling non-IID data. ",
        "title": "FedRFQ: Prototype-Based Federated Learning with Reduced Redundancy,  Minimal Failure, and Enhanced Quality",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07559",
        "abstract_url": "http://arxiv.org/abs/2401.07559",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng Grace"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The transportation industry remains a significant contributor to greenhouse gas emissions, highlighting the requirement for intelligent systems to enhance vehicle energy efficiency. The intellectual property rights of developed systems should be protected by patents. However, there is no patent overview of eco-driving intelligent systems. Unlike a scientific article, a patent documentation indicates both novelty and commercialization potential of an inventor. To address this research gap, this paper provides a patent overview of eco-driving intelligent systems and algorithms. 424 patents in the Google Patent database are analyzed. The patent analysis results show that the top three Cooperative Patent Classifications are: Y02T - climate change mitigation technologies related to transportation (50.7%), B60W - Conjoint control of vehicle subunits of different types or different functions (34.4%) and B60L - Propulsion of electrically-propelled vehicles (20.2%). 219 patents were filed after 2016 when deep learning became popular and can be categorized into five groups: vehicle energy management, smart driving, ecological and sustainable driving, fuel consumption reduction, and driving behavior optimization. Furthermore, all 219 patents involve the physical components of the intelligent system and/or novel machine learning/deep learning algorithms. Moreover, over 70% of them are granted by the China National Intellectual Property Administration. ",
        "title": "Eco-driving Intelligent Systems and Algorithms: A Patent Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07562",
        "abstract_url": "http://arxiv.org/abs/2401.07562",
        "authors": [
            {
                "last_name": "Oates",
                "first_name": "Chris. J."
            },
            {
                "last_name": "Karvonen",
                "first_name": "Toni"
            },
            {
                "last_name": "Teckentrup",
                "first_name": "Aretha L."
            },
            {
                "last_name": "Strocchi",
                "first_name": "Marina"
            },
            {
                "last_name": "Niederer",
                "first_name": "Steven A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  For over a century, extrapolation methods have provided a powerful tool to improve the convergence order of a numerical method. However, these tools are not well-suited to modern computer codes, where multiple continua are discretised and convergence orders are not easily analysed. To address this challenge we present a probabilistic perspective on Richardson extrapolation, a point of view that unifies classical extrapolation methods with modern multi-fidelity modelling, and handles uncertain convergence orders by allowing these to be statistically estimated. The approach is developed using Gaussian processes, leading to Gauss-Richardson Extrapolation (GRE). Conditions are established under which extrapolation using the conditional mean achieves a polynomial (or even an exponential) speed-up compared to the original numerical method. Further, the probabilistic formulation unlocks the possibility of experimental design, casting the selection of fidelities as a continuous optimisation problem which can then be (approximately) solved. A case-study involving a computational cardiac model demonstrates that practical gains in accuracy can be achieved using the GRE method. ",
        "title": "Probabilistic Richardson Extrapolation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07565",
        "abstract_url": "http://arxiv.org/abs/2401.07565",
        "authors": [
            {
                "last_name": "Pettersen",
                "first_name": "H\u00e5vard"
            },
            {
                "last_name": "Morrison",
                "first_name": "Donn"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  This study addresses the challenge of reverse engineering binaries from unknown instruction set architectures, a complex task with potential implications for software maintenance and cyber-security. We focus on the tasks of detecting candidate call and return opcodes for automatic extraction of call graphs in order to simplify the reverse engineering process. Empirical testing on a small dataset of binary files from different architectures demonstrates that the approach can accurately detect specific opcodes under conditions of noisy data. The method lays the groundwork for a valuable tool for reverse engineering where the reverse engineer has minimal a priori knowledge of the underlying instruction set architecture. ",
        "title": "Call graph discovery in binary programs from unknown instruction set  architectures",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07567",
        "abstract_url": "http://arxiv.org/abs/2401.07567",
        "authors": [
            {
                "last_name": "Qi",
                "first_name": "Zhaobo"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yibo"
            },
            {
                "last_name": "Ruan",
                "first_name": "Xiaowen"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuhui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weigang"
            },
            {
                "last_name": "Huang",
                "first_name": "Qingming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias strategy (BSSARD), which dynamically generates bias-conflict samples by explicitly leveraging potentially spurious correlations between single-modality features and the temporal position of the target moments. Through adversarial training, its bias generators continuously introduce biases and generate bias-conflict samples to deceive its grounding model. Meanwhile, the grounding model continuously eliminates the introduced biases, which requires it to model multi-modality alignment information. BSSARD will cover most kinds of coupling relationships and disrupt language and visual biases simultaneously. Extensive experiments on Charades-CD and ActivityNet-CD demonstrate the promising debiasing capability of BSSARD. Source codes are available at https://github.com/qzhb/BSSARD. ",
        "title": "Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy  for Temporal Sentence Grounding in Video",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07571",
        "abstract_url": "http://arxiv.org/abs/2401.07571",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Shi",
                "first_name": "Sheng"
            },
            {
                "last_name": "An",
                "first_name": "Shan"
            },
            {
                "last_name": "Fan",
                "first_name": "Fengmei"
            },
            {
                "last_name": "Ge",
                "first_name": "Wenshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Qi"
            },
            {
                "last_name": "Yu",
                "first_name": "Feng"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhiren"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous research on the diagnosis of Bipolar disorder has mainly focused on resting-state functional magnetic resonance imaging. However, their accuracy can not meet the requirements of clinical diagnosis. Efficient multimodal fusion strategies have great potential for applications in multimodal data and can further improve the performance of medical diagnosis models. In this work, we utilize both sMRI and fMRI data and propose a novel multimodal diagnosis model for bipolar disorder. The proposed Patch Pyramid Feature Extraction Module extracts sMRI features, and the spatio-temporal pyramid structure extracts the fMRI features. Finally, they are fused by a fusion module to output diagnosis results with a classifier. Extensive experiments show that our proposed method outperforms others in balanced accuracy from 0.657 to 0.732 on the OpenfMRI dataset, and achieves the state of the art. ",
        "title": "A Bi-Pyramid Multimodal Fusion Method for the Diagnosis of Bipolar  Disorders",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07572",
        "abstract_url": "http://arxiv.org/abs/2401.07572",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Qi"
            },
            {
                "last_name": "Cui",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Wengang"
            },
            {
                "last_name": "Li",
                "first_name": "Houqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  In this study, we tackle the challenge of classifying the object category in point clouds, which previous works like PointCLIP struggle to address due to the inherent limitations of the CLIP architecture. Our approach leverages GPT-4 Vision (GPT-4V) to overcome these challenges by employing its advanced generative abilities, enabling a more adaptive and robust classification process. We adapt the application of GPT-4V to process complex 3D data, enabling it to achieve zero-shot recognition capabilities without altering the underlying model architecture. Our methodology also includes a systematic strategy for point cloud image visualization, mitigating domain gap and enhancing GPT-4V's efficiency. Experimental validation demonstrates our approach's superiority in diverse scenarios, setting a new benchmark in zero-shot point cloud classification. ",
        "title": "Exploiting GPT-4 Vision for Zero-shot Point Cloud Understanding",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07575",
        "abstract_url": "http://arxiv.org/abs/2401.07575",
        "authors": [
            {
                "last_name": "Ristea",
                "first_name": "Nicolae-Catalin"
            },
            {
                "last_name": "Anghel",
                "first_name": "Andrei"
            },
            {
                "last_name": "Ionescu",
                "first_name": "Radu Tudor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  Speech classification tasks often require powerful language understanding models to grasp useful features, which becomes problematic when limited training data is available. To attain superior classification performance, we propose to harness the inherent value of multimodal representations by transcribing speech using automatic speech recognition (ASR) models and translating the transcripts into different languages via pretrained translation models. We thus obtain an audio-textual (multimodal) representation for each data sample. Subsequently, we combine language-specific Bidirectional Encoder Representations from Transformers (BERT) with Wav2Vec2.0 audio features via a novel cascaded cross-modal transformer (CCMT). Our model is based on two cascaded transformer blocks. The first one combines text-specific features from distinct languages, while the second one combines acoustic features with multilingual features previously learned by the first transformer block. We employed our system in the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge. CCMT was declared the winning solution, obtaining an unweighted average recall (UAR) of 65.41% and 85.87% for complaint and request detection, respectively. Moreover, we applied our framework on the Speech Commands v2 and HarperValleyBank dialog data sets, surpassing previous studies reporting results on these benchmarks. Our code is freely available for download at: https://github.com/ristea/ccmt. ",
        "title": "Cascaded Cross-Modal Transformer for Audio-Textual Classification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07576",
        "abstract_url": "http://arxiv.org/abs/2401.07576",
        "authors": [
            {
                "last_name": "Takerngsaksiri",
                "first_name": "Wannita"
            },
            {
                "last_name": "Charakorn",
                "first_name": "Rujikorn"
            },
            {
                "last_name": "Tantithamthavorn",
                "first_name": "Chakkrit"
            },
            {
                "last_name": "Li",
                "first_name": "Yuan-Fang"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Test-driven development (TDD) is a widely-employed software development practice that mandates writing test cases based on requirements before writing the actual code. While writing test cases is the centerpiece of TDD, it is time-consuming, expensive, and often shunned by developers. To address these issues associated with TDD, automated test case generation approaches have recently been investigated. Such approaches take source code as input, but not the requirements. Therefore, existing work does not fully support true TDD, as actual code is required to generate test cases. In addition, current deep learning-based test case generation approaches are trained with one learning objective, i.e., to generate test cases that are exactly matched with the ground-truth test cases. However, such approaches may limit the model's ability to generate different yet correct test cases. In this paper, we introduce PyTester, a Text-to-Testcase generation approach that can automatically generate syntactically correct, executable, complete, and effective test cases while being aligned with a given natural language requirement. We evaluate PyTester on the public APPS benchmark dataset, and the results show that our Deep RL approach enables PyTester, a small language model, to outperform much larger language models like GPT3.5, StarCoder, and InCoder. Our findings suggest that future research could consider improving small over large LMs for better resource efficiency by integrating the SE domain knowledge into the design of reinforcement learning architecture. ",
        "title": "TDD Without Tears: Towards Test Case Generation from Requirements  through Deep Reinforcement Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07577",
        "abstract_url": "http://arxiv.org/abs/2401.07577",
        "authors": [
            {
                "last_name": "Garc\u00eda-D\u00edaz",
                "first_name": "Jes\u00fas"
            },
            {
                "last_name": "Cornejo-Acosta",
                "first_name": "Jos\u00e9 Alejandro"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  Given a graph $G$, the optimization version of the graph burning problem seeks for a sequence of vertices, $(u_1,u_2,...,u_k) \\in V(G)^k$, with minimum $k$ and such that every $v \\in V(G)$ has distance at most $k-i$ to some vertex $u_i$. The length $k$ of the optimal solution is known as the burning number and is denoted by $b(G)$, an invariant that helps quantify the graph's vulnerability to contagion. This paper explores the advantages and limitations of an $\\mathcal{O}(mn + kn^2)$ deterministic greedy heuristic for this problem, where $n$ is the graph's order, $m$ is the graph's size, and $k$ is a guess on $b(G)$. This heuristic is based on the relationship between the graph burning problem and the clustered maximum coverage problem, and despite having limitations on paths and cycles, it found most of the optimal and best-known solutions of benchmark and synthetic graphs with up to 102400 vertices. ",
        "title": "A greedy heuristic for graph burning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07578",
        "abstract_url": "http://arxiv.org/abs/2401.07578",
        "authors": [
            {
                "last_name": "Jamshidi",
                "first_name": "Fateme"
            },
            {
                "last_name": "Etesami",
                "first_name": "Jalal"
            },
            {
                "last_name": "Kiyavash",
                "first_name": "Negar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study the problem of learning 'good' interventions in a stochastic environment modeled by its underlying causal graph. Good interventions refer to interventions that maximize rewards. Specifically, we consider the setting of a pre-specified budget constraint, where interventions can have non-uniform costs. We show that this problem can be formulated as maximizing the expected reward for a stochastic multi-armed bandit with side information. We propose an algorithm to minimize the cumulative regret in general causal graphs. This algorithm trades off observations and interventions based on their costs to achieve the optimal reward. This algorithm generalizes the state-of-the-art methods by allowing non-uniform costs and hidden confounders in the causal graph. Furthermore, we develop an algorithm to minimize the simple regret in the budgeted setting with non-uniform costs and also general causal graphs. We provide theoretical guarantees, including both upper and lower bounds, as well as empirical evaluations of our algorithms. Our empirical results showcase that our algorithms outperform the state of the art. ",
        "title": "Confounded Budgeted Causal Bandits",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07579",
        "abstract_url": "http://arxiv.org/abs/2401.07579",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Jiahui"
            },
            {
                "last_name": "Tian",
                "first_name": "Wenhong"
            },
            {
                "last_name": "Xie",
                "first_name": "Yuanlun"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhijia"
            },
            {
                "last_name": "Ou",
                "first_name": "Jie"
            },
            {
                "last_name": "Tian",
                "first_name": "Taoran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Current state-of-the-art medical image segmentation methods prioritize accuracy but often at the expense of increased computational demands and larger model sizes. Applying these large-scale models to the relatively limited scale of medical image datasets tends to induce redundant computation, complicating the process without the necessary benefits. This approach not only adds complexity but also presents challenges for the integration and deployment of lightweight models on edge devices. For instance, recent transformer-based models have excelled in 2D and 3D medical image segmentation due to their extensive receptive fields and high parameter count. However, their effectiveness comes with a risk of overfitting when applied to small datasets and often neglects the vital inductive biases of Convolutional Neural Networks (CNNs), essential for local feature representation. In this work, we propose PMFSNet, a novel medical imaging segmentation model that effectively balances global and local feature processing while avoiding the computational redundancy typical in larger models. PMFSNet streamlines the UNet-based hierarchical structure and simplifies the self-attention mechanism's computational complexity, making it suitable for lightweight applications. It incorporates a plug-and-play PMFS block, a multi-scale feature enhancement module based on attention mechanisms, to capture long-term dependencies. Extensive comprehensive results demonstrate that even with a model (less than 1 million parameters), our method achieves superior performance in various segmentation tasks across different data scales. It achieves (IoU) metrics of 84.68%, 82.02%, and 78.82% on public datasets of teeth CT (CBCT), ovarian tumors ultrasound(MMOTU), and skin lesions dermoscopy images (ISIC 2018), respectively. The source code is available at https://github.com/yykzjh/PMFSNet. ",
        "title": "PMFSNet: Polarized Multi-scale Feature Self-attention Network For  Lightweight Medical Image Segmentation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07582",
        "abstract_url": "http://arxiv.org/abs/2401.07582",
        "authors": [
            {
                "last_name": "Shami",
                "first_name": "Mamoona Birkhez"
            },
            {
                "last_name": "Kiss",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Haakonsen",
                "first_name": "Trond Arve"
            },
            {
                "last_name": "Lindseth",
                "first_name": "Frank"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Geolocation is integral to the seamless functioning of autonomous vehicles and advanced traffic monitoring infrastructures. This paper introduces a methodology to geolocate road objects using a monocular camera, leveraging the NVIDIA DriveWorks platform. We use the Centimeter Positioning Service (CPOS) and the inverse Haversine formula to geo-locate road objects accurately. The real-time algorithm processing capability of the NVIDIA DriveWorks platform enables instantaneous object recognition and spatial localization for Advanced Driver Assistance Systems (ADAS) and autonomous driving platforms. We present a measurement pipeline suitable for autonomous driving (AD) platforms and provide detailed guidelines for calibrating cameras using NVIDIA DriveWorks. Experiments were carried out to validate the accuracy of the proposed method for geolocating targets in both controlled and dynamic settings. We show that our approach can locate targets with less than 1m error when the AD platform is stationary and less than 4m error at higher speeds (i.e. up to 60km/h) within a 15m radius. ",
        "title": "Geo-locating Road Objects using Inverse Haversine Formula with NVIDIA  Driveworks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07583",
        "abstract_url": "http://arxiv.org/abs/2401.07583",
        "authors": [
            {
                "last_name": "Koukoulekidis",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "\u0160imkovic IV",
                "first_name": "Fedor"
            },
            {
                "last_name": "Leib",
                "first_name": "Martin"
            },
            {
                "last_name": "Pereira",
                "first_name": "Francisco Revson Fernandes"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Quantum error correction is rapidly seeing first experimental implementations, but there is a significant gap between asymptotically optimal error-correcting codes and codes that are experimentally feasible. Quantum LDPC codes range from the surface code, which has a vanishing encoding rate, to very promising codes with constant encoding rate and linear distance. In this work, motivated by current small-scale experimental quantum processing units, we devise small quantum codes that are inspired by a subset of quantum LDPC codes, known as generalized bicycle (GB) codes. We introduce a code construction based on algebraic manipulation of the parity-check matrix of GB codes, rather than manipulation of Tanner graphs. Our construction leads to families of quantum LDPC codes of small size, and we demonstrate numerically that their performance scales comparably to the performance of surface codes for similar sizes under a phenomenological noise model. The advantage of our code family is that they encode many logical qubits in one code, at the expense of non-local connectivity. We then explore three variants of the code construction focusing on reducing the long-range connectivity by bringing it closer to the current experimental capabilities of short-range connectivity devices. ",
        "title": "Small Quantum Codes from Algebraic Extensions of Generalized Bicycle  Codes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07584",
        "abstract_url": "http://arxiv.org/abs/2401.07584",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Wan",
                "first_name": "Zhifan"
            },
            {
                "last_name": "Hu",
                "first_name": "Lanqing"
            },
            {
                "last_name": "Lin",
                "first_name": "Stephen"
            },
            {
                "last_name": "Wu",
                "first_name": "Shuzhe"
            },
            {
                "last_name": "Shan",
                "first_name": "Shiguang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Considering the close connection between action recognition and human pose estimation, we design a Collaboratively Self-supervised Video Representation (CSVR) learning framework specific to action recognition by jointly considering generative pose prediction and discriminative context matching as pretext tasks. Specifically, our CSVR consists of three branches: a generative pose prediction branch, a discriminative context matching branch, and a video generating branch. Among them, the first one encodes dynamic motion feature by utilizing Conditional-GAN to predict the human poses of future frames, and the second branch extracts static context features by pulling the representations of clips and compressed key frames from the same video together while pushing apart the pairs from different videos. The third branch is designed to recover the current video frames and predict the future ones, for the purpose of collaboratively improving dynamic motion features and static context features. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the UCF101 and HMDB51 datasets. ",
        "title": "Collaboratively Self-supervised Video Representation Learning for Action  Recognition",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07586",
        "abstract_url": "http://arxiv.org/abs/2401.07586",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Muhammad Asif"
            },
            {
                "last_name": "Menouar",
                "first_name": "Hamid"
            },
            {
                "last_name": "Hamila",
                "first_name": "Ridha"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advances in deep learning techniques have achieved remarkable performance in several computer vision problems. A notably intuitive technique called Curriculum Learning (CL) has been introduced recently for training deep learning models. Surprisingly, curriculum learning achieves significantly improved results in some tasks but marginal or no improvement in others. Hence, there is still a debate about its adoption as a standard method to train supervised learning models. In this work, we investigate the impact of curriculum learning in crowd counting using the density estimation method. We performed detailed investigations by conducting 112 experiments using six different CL settings using eight different crowd models. Our experiments show that curriculum learning improves the model learning performance and shortens the convergence time. ",
        "title": "Curriculum for Crowd Counting -- Is it Worthy?",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07589",
        "abstract_url": "http://arxiv.org/abs/2401.07589",
        "authors": [
            {
                "last_name": "Hurtado",
                "first_name": "Juana Valeria"
            },
            {
                "last_name": "Valada",
                "first_name": "Abhinav"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Comprehensive scene understanding is a critical enabler of robot autonomy. Semantic segmentation is one of the key scene understanding tasks which is pivotal for several robotics applications including autonomous driving, domestic service robotics, last mile delivery, amongst many others. Semantic segmentation is a dense prediction task that aims to provide a scene representation in which each pixel of an image is assigned a semantic class label. Therefore, semantic segmentation considers the full scene context, incorporating the object category, location, and shape of all the scene elements, including the background. Numerous algorithms have been proposed for semantic segmentation over the years. However, the recent advances in deep learning combined with the boost in the computational capacity and the availability of large-scale labeled datasets have led to significant advances in semantic segmentation. In this chapter, we introduce the task of semantic segmentation and present the deep learning techniques that have been proposed to address this task over the years. We first define the task of semantic segmentation and contrast it with other closely related scene understanding problems. We detail different algorithms and architectures for semantic segmentation and the commonly employed loss functions. Furthermore, we present an overview of datasets, benchmarks, and metrics that are used in semantic segmentation. We conclude the chapter with a discussion of challenges and opportunities for further research in this area. ",
        "title": "Semantic Scene Segmentation for Robotics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07591",
        "abstract_url": "http://arxiv.org/abs/2401.07591",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Muhammad Asif"
            },
            {
                "last_name": "Menouar",
                "first_name": "Hamid"
            },
            {
                "last_name": "Hamila",
                "first_name": "Ridha"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Most state-of-the-art crowd counting methods use color (RGB) images to learn the density map of the crowd. However, these methods often struggle to achieve higher accuracy in densely crowded scenes with poor illumination. Recently, some studies have reported improvement in the accuracy of crowd counting models using a combination of RGB and thermal images. Although multimodal data can lead to better predictions, multimodal data might not be always available beforehand. In this paper, we propose the use of generative adversarial networks (GANs) to automatically generate thermal infrared (TIR) images from color (RGB) images and use both to train crowd counting models to achieve higher accuracy. We use a Pix2Pix GAN network first to translate RGB images to TIR images. Our experiments on several state-of-the-art crowd counting models and benchmark crowd datasets report significant improvement in accuracy. ",
        "title": "Multimodal Crowd Counting with Pix2Pix GANs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07598",
        "abstract_url": "http://arxiv.org/abs/2401.07598",
        "authors": [
            {
                "last_name": "Aggarwal",
                "first_name": "Divyanshu"
            },
            {
                "last_name": "Sathe",
                "first_name": "Ashutosh"
            },
            {
                "last_name": "Sitaram",
                "first_name": "Sunayana"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Parameter efficient finetuning has emerged as a viable solution for improving the performance of Large Language Models without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the LLaMA-7B and Mistral-7B models on synthetic multilingual instruction tuning data to determine its effect on model performance on five downstream tasks covering twenty three languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and higher quantisation values benefit low-resource languages. We find that parameter efficient finetuning of smaller open source models sometimes bridges the gap between the performance of these models and the larger ones, however, English performance can take a hit. We also find that finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages. ",
        "title": "MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of  Large Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07599",
        "abstract_url": "http://arxiv.org/abs/2401.07599",
        "authors": [
            {
                "last_name": "Mekacher",
                "first_name": "Amin"
            },
            {
                "last_name": "Falkenberg",
                "first_name": "Max"
            },
            {
                "last_name": "Baronchelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Increasingly, alternative platforms are playing a key role in the social media ecosystem. Koo, a microblogging platform based in India, has emerged as a major new social network hosting high profile politicians from several countries (India, Brazil, Nigeria) and many internationally renowned celebrities. This paper presents the largest publicly available Koo dataset, spanning from the platform's founding in early 2020 to September 2023, providing detailed metadata for 72M posts, 75M comments, 40M shares, 284M likes and 1.4M user profiles. Along with the release of the dataset, we provide an overview of the platform including a discussion of the news ecosystem on the platform, hashtag usage and user engagement. Our results highlight the pivotal role that new platforms play in shaping online communities in emerging economies and the Global South, connecting local politicians and public figures with their followers. With Koo's ambition to become the town hall for diverse non-English speaking communities, our dataset offers new opportunities for studying social media beyond a Western context. ",
        "title": "The Koo Dataset: An Indian Microblogging Platform With Global Ambitions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07602",
        "abstract_url": "http://arxiv.org/abs/2401.07602",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Jing"
            },
            {
                "last_name": "Du",
                "first_name": "Lei"
            },
            {
                "last_name": "Sogabe",
                "first_name": "Tomohiro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shao-Liang"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  It is well-known that a multilinear system with a nonsingular M-tensor and a positive right-hand side has a unique positive solution. Tensor splitting methods generalizing the classical iterative methods for linear systems have been proposed for finding the unique positive solution. The Alternating Anderson-Richardson (AAR) method is an effective method to accelerate the classical iterative methods. In this study, we apply the idea of AAR for finding the unique positive solution quickly. We first present a tensor Richardson method based on tensor regular splittings, then apply Anderson acceleration to the tensor Richardson method and derive a tensor Anderson-Richardson method, finally, we periodically employ the tensor Anderson-Richardson method within the tensor Richardson method and propose a tensor AAR method. Numerical experiments show that the proposed method is effective in accelerating tensor splitting methods. ",
        "title": "A tensor Alternating Anderson-Richardson method for solving multilinear  systems with M-tensors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07603",
        "abstract_url": "http://arxiv.org/abs/2401.07603",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Heecheol"
            },
            {
                "last_name": "Ohmura",
                "first_name": "Yoshiyuki"
            },
            {
                "last_name": "Kuniyoshi",
                "first_name": "Yasuo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In the field of robotic manipulation, deep imitation learning is recognized as a promising approach for acquiring manipulation skills. Additionally, learning from diverse robot datasets is considered a viable method to achieve versatility and adaptability. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise, not addressing the fine-grained object manipulation that robots are expected to perform in the real world. This paper introduces a dataset of diverse object manipulations that includes dual-arm tasks and/or tasks requiring fine manipulation. To this end, we have generated dataset with 224k episodes (150 hours, 1,104 language instructions) which includes dual-arm fine tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data is publicly available. Additionally, this dataset includes visual attention signals as well as dual-action labels, a signal that separates actions into a robust reaching trajectory and precise interaction with objects, and language instructions to achieve robust and precise object manipulation. We applied the dataset to our Dual-Action and Attention (DAA), a model designed for fine-grained dual arm manipulation tasks and robust against covariate shifts. The model was tested with over 7k total trials in real robot manipulation tasks, demonstrating its capability in fine manipulation. ",
        "title": "Multi-task robot data for dual-arm fine manipulation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07604",
        "abstract_url": "http://arxiv.org/abs/2401.07604",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wenqi"
            },
            {
                "last_name": "Bieker",
                "first_name": "Jacob"
            },
            {
                "last_name": "Arcucci",
                "first_name": "Rossella"
            },
            {
                "last_name": "Quilodr\u00e1n-Casas",
                "first_name": "C\u00e9sar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, the convergence of data-driven machine learning models with Data Assimilation (DA) offers a promising avenue for enhancing weather forecasting. This study delves into this emerging trend, presenting our methodologies and outcomes. We harnessed the UK's local ERA5 850 hPa temperature data and refined the U-STN12 global weather forecasting model, tailoring its predictions to the UK's climate nuances. From the ASOS network, we sourced T2m data, representing ground observations across the UK. We employed the advanced kriging method with a polynomial drift term for consistent spatial resolution. Furthermore, Gaussian noise was superimposed on the ERA5 T850 data, setting the stage for ensuing multi-time step synthetic observations. Probing into the assimilation impacts, the ASOS T2m data was integrated with the ERA5 T850 dataset. Our insights reveal that while global forecast models can adapt to specific regions, incorporating atmospheric data in DA significantly bolsters model accuracy. Conversely, the direct assimilation of surface temperature data tends to mitigate this enhancement, tempering the model's predictive prowess. ",
        "title": "Data Assimilation using ERA5, ASOS, and the U-STN model for Weather  Forecasting over the UK",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07606",
        "abstract_url": "http://arxiv.org/abs/2401.07606",
        "authors": [
            {
                "last_name": "Daniely",
                "first_name": "Amit"
            },
            {
                "last_name": "Schain",
                "first_name": "Mariano"
            },
            {
                "last_name": "Yehudai",
                "first_name": "Gilad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Optimizing Neural networks is a difficult task which is still not well understood. On the other hand, fixed representation methods such as kernels and random features have provable optimization guarantees but inferior performance due to their inherent inability to learn the representations. In this paper, we aim at bridging this gap by presenting a novel architecture called RedEx (Reduced Expander Extractor) that is as expressive as neural networks and can also be trained in a layer-wise fashion via a convex program with semi-definite constraints and optimization guarantees. We also show that RedEx provably surpasses fixed representation methods, in the sense that it can efficiently learn a family of target functions which fixed representation methods cannot. ",
        "title": "RedEx: Beyond Fixed Representation Methods via Convex Optimization",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07611",
        "abstract_url": "http://arxiv.org/abs/2401.07611",
        "authors": [
            {
                "last_name": "Behravesh",
                "first_name": "Rasoul"
            },
            {
                "last_name": "Breitgand",
                "first_name": "David"
            },
            {
                "last_name": "Lorenz",
                "first_name": "Dean H."
            },
            {
                "last_name": "Raz",
                "first_name": "Danny"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Mobile edge computing offers a myriad of opportunities to innovate and introduce novel applications, thereby enhancing user experiences considerably. A critical issue extensively investigated in this domain is efficient deployment of Service Function Chains (SFCs) across the physical network, spanning from the edge to the cloud. This problem is known to be NP-hard. As a result of its practical importance, there is significant interest in the development of high-quality sub-optimal solutions.   In this paper, we consider this problem and propose a novel near-optimal heuristic that is extremely efficient and scalable. We compare our solution to the state-of-the-art heuristic and to the theoretical optimum. In our large-scale evaluations, we use realistic topologies which were previously reported in the literature. We demonstrate that the execution time offered by our solution grows slowly as the number of Virtual Network Function (VNF) forwarding graph embedding requests grows, and it handles one million requests in slightly more than 20 seconds for 100 nodes and 150 edges physical topology. ",
        "title": "A Practical Near Optimal Deployment of Service Function Chains in  Edge-to-Cloud Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07612",
        "abstract_url": "http://arxiv.org/abs/2401.07612",
        "authors": [
            {
                "last_name": "Suo",
                "first_name": "Xuchen"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate. This paper introduces the 'Signed-Prompt' method as a novel solution. The study involves signing sensitive instructions within command segments by authorized users, enabling the LLM to discern trusted instruction sources. The paper presents a comprehensive analysis of prompt injection attack patterns, followed by a detailed explanation of the Signed-Prompt concept, including its basic architecture and implementation through both prompt engineering and fine-tuning of LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method, showing substantial resistance to various types of prompt injection attacks, thus validating its potential as a robust defense strategy in AI security. ",
        "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks  Against LLM-Integrated Applications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07616",
        "abstract_url": "http://arxiv.org/abs/2401.07616",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Rewriting logic and its implementation Maude are an expressive framework for the formal specification and verification of software and other kinds of systems. Concurrency is naturally represented by nondeterministic local transformations produced by the application of rewriting rules over algebraic terms in an equational theory. Some aspects of the global behavior of the systems or additional constraints sometimes require restricting this nondeterminism. Rewriting strategies are used as a higher-level and modular resource to cleanly capture these requirements, which can be easily expressed in Maude with an integrated strategy language. However, strategy-aware specifications cannot be verified with the builtin LTL model checker, making strategies less useful and attractive.   In this paper, we discuss model checking for strategy-controlled systems, and present a strategy-aware extension of the Maude LTL model checker. The expressivity of the strategy language is discussed in relation to model checking, the model checker is illustrated with multiple application examples, and its performance is compared. ",
        "title": "Model checking strategy-controlled systems in rewriting logic",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07627",
        "abstract_url": "http://arxiv.org/abs/2401.07627",
        "authors": [
            {
                "last_name": "Ben\u00edtez-Pe\u00f1a",
                "first_name": "Sandra"
            },
            {
                "last_name": "Blanquero",
                "first_name": "Rafael"
            },
            {
                "last_name": "Carrizosa",
                "first_name": "Emilio"
            },
            {
                "last_name": "Ram\u00edrez-Cobo",
                "first_name": "Pepa"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Feature Selection is a crucial procedure in Data Science tasks such as Classification, since it identifies the relevant variables, making thus the classification procedures more interpretable, cheaper in terms of measurement and more effective by reducing noise and data overfit. The relevance of features in a classification procedure is linked to the fact that misclassifications costs are frequently asymmetric, since false positive and false negative cases may have very different consequences. However, off-the-shelf Feature Selection procedures seldom take into account such cost-sensitivity of errors.   In this paper we propose a mathematical-optimization-based Feature Selection procedure embedded in one of the most popular classification procedures, namely, Support Vector Machines, accommodating asymmetric misclassification costs. The key idea is to replace the traditional margin maximization by minimizing the number of features selected, but imposing upper bounds on the false positive and negative rates. The problem is written as an integer linear problem plus a quadratic convex problem for Support Vector Machines with both linear and radial kernels.   The reported numerical experience demonstrates the usefulness of the proposed Feature Selection procedure. Indeed, our results on benchmark data sets show that a substantial decrease of the number of features is obtained, whilst the desired trade-off between false positive and false negative rates is achieved. ",
        "title": "Cost-sensitive Feature Selection for Support Vector Machines",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07629",
        "abstract_url": "http://arxiv.org/abs/2401.07629",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zichen"
            },
            {
                "last_name": "Yang",
                "first_name": "Bo"
            },
            {
                "last_name": "Yue",
                "first_name": "Haonan"
            },
            {
                "last_name": "Ma",
                "first_name": "Zhenghao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Few-shot object detection (FSOD) aims at extending a generic detector for novel object detection with only a few training examples. It attracts great concerns recently due to the practical meanings. Meta-learning has been demonstrated to be an effective paradigm for this task. In general, methods based on meta-learning employ an additional support branch to encode novel examples (a.k.a. support images) into class prototypes, which are then fused with query branch to facilitate the model prediction. However, the class-level prototypes are difficult to precisely generate, and they also lack detailed information, leading to instability in performance.New methods are required to capture the distinctive local context for more robust novel object detection. To this end, we propose to distill the most representative support features into fine-grained prototypes. These prototypes are then assigned into query feature maps based on the matching results, modeling the detailed feature relations between two branches. This process is realized by our Fine-Grained Feature Aggregation (FFA) module. Moreover, in terms of high-level feature fusion, we propose Balanced Class-Agnostic Sampling (B-CAS) strategy and Non-Linear Fusion (NLF) module from differenct perspectives. They are complementary to each other and depict the high-level feature relations more effectively. Extensive experiments on PASCAL VOC and MS COCO benchmarks show that our method sets a new state-of-the-art performance in most settings. Our code is available at https://github.com/wangchen1801/FPD. ",
        "title": "Fine-Grained Prototypes Distillation for Few-Shot Object Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07631",
        "abstract_url": "http://arxiv.org/abs/2401.07631",
        "authors": [
            {
                "last_name": "Dutta",
                "first_name": "Pranjal"
            },
            {
                "last_name": "Gesmundo",
                "first_name": "Fulvio"
            },
            {
                "last_name": "Ikenmeyer",
                "first_name": "Christian"
            },
            {
                "last_name": "Jindal",
                "first_name": "Gorav"
            },
            {
                "last_name": "Lysikov",
                "first_name": "Vladimir"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  Border complexity measures are defined via limits (or topological closures), so that any function which can approximated arbitrarily closely by low complexity functions itself has low border complexity. Debordering is the task of proving an upper bound on some non-border complexity measure in terms of a border complexity measure, thus getting rid of limits.   Debordering is at the heart of understanding the difference between Valiant's determinant vs permanent conjecture, and Mulmuley and Sohoni's variation which uses border determinantal complexity. The debordering of matrix multiplication tensors by Bini played a pivotal role in the development of efficient matrix multiplication algorithms. Consequently, debordering finds applications in both establishing computational complexity lower bounds and facilitating algorithm design. Currently, very few debordering results are known.   In this work, we study the question of debordering the border Waring rank of polynomials. Waring and border Waring rank are very well studied measures in the context of invariant theory, algebraic geometry, and matrix multiplication algorithms. For the first time, we obtain a Waring rank upper bound that is exponential in the border Waring rank and only linear in the degree. All previous known results were exponential in the degree. For polynomials with constant border Waring rank, our results imply an upper bound on the Waring rank linear in degree, which previously was only known for polynomials with border Waring rank at most 5. ",
        "title": "Fixed-parameter debordering of Waring rank",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07636",
        "abstract_url": "http://arxiv.org/abs/2401.07636",
        "authors": [
            {
                "last_name": "Lindeberg",
                "first_name": "Anna"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  The AHU-algorithm solves the computationally difficult graph isomorphism problem for rooted trees, and does so with a linear time complexity. Although the AHU-algorithm has remained state of the art for almost 50 years, it has been criticized for being unclearly presented, and no complete proof of correctness has been given. In this text, that gap is filled: we formalize the algorithm's main point of assigning and compressing labels to provide a characterization of isomorphic rooted trees, and then proceed with proving the correctness and optimal runtime of the AHU-algorithm. ",
        "title": "Isomorphism Testing of Rooted Trees in Linear Time",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07639",
        "abstract_url": "http://arxiv.org/abs/2401.07639",
        "authors": [
            {
                "last_name": "N\u00e9meth",
                "first_name": "G\u00e1bor"
            },
            {
                "last_name": "Matuszka",
                "first_name": "Tam\u00e1s"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Active learning, a powerful paradigm in machine learning, aims at reducing labeling costs by selecting the most informative samples from an unlabeled dataset. However, the traditional active learning process often demands extensive computational resources, hindering scalability and efficiency. In this paper, we address this critical issue by presenting a novel method designed to alleviate the computational burden associated with active learning on massive datasets. To achieve this goal, we introduce a simple, yet effective method-agnostic framework that outlines how to strategically choose and annotate data points, optimizing the process for efficiency while maintaining model performance. Through case studies, we demonstrate the effectiveness of our proposed method in reducing computational costs while maintaining or, in some cases, even surpassing baseline model outcomes. Code is available at https://github.com/aimotive/Compute-Efficient-Active-Learning. ",
        "title": "Compute-Efficient Active Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07640",
        "abstract_url": "http://arxiv.org/abs/2401.07640",
        "authors": [
            {
                "last_name": "Milani",
                "first_name": "Marcelo Garlet"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We define and study a new structural parameter for directed graphs, which we call \\emph{ear anonymity}. Our parameter aims to generalize the useful properties of \\emph{funnels} to larger digraph classes. In particular, funnels are exactly the acyclic digraphs with ear anonymity one. We prove that computing the ear anonymity of a digraph is \\NP/-hard and that it can be solved in $O(m(n + m))$-time on acyclic digraphs (where \\(n\\) is the number of vertices and \\(m\\) is the number of arcs in the input digraph). It remains open where exactly in the polynomial hierarchy the problem of computing ear anonymity lies, however for a related problem we manage to show $\\Sigma_2^p$-completeness. ",
        "title": "Directed Ear Anonymity",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07641",
        "abstract_url": "http://arxiv.org/abs/2401.07641",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Mingxin"
            },
            {
                "last_name": "Peng",
                "first_name": "Dezhi"
            },
            {
                "last_name": "Li",
                "first_name": "Hongliang"
            },
            {
                "last_name": "Peng",
                "first_name": "Zhenghao"
            },
            {
                "last_name": "Liu",
                "first_name": "Chongyu"
            },
            {
                "last_name": "Lin",
                "first_name": "Dahua"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Bai",
                "first_name": "Xiang"
            },
            {
                "last_name": "Jin",
                "first_name": "Lianwen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  End-to-end scene text spotting, which aims to read the text in natural images, has garnered significant attention in recent years. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter v2, which seeks to find a better synergy between text detection and recognition. Specifically, we enhance the relationship between two tasks using novel Recognition Conversion and Recognition Alignment modules. Recognition Conversion explicitly guides text localization through recognition loss, while Recognition Alignment dynamically extracts text features for recognition through the detection predictions. This simple yet effective design results in a concise framework that requires neither an additional rectification module nor character-level annotations for the arbitrarily-shaped text. Furthermore, the parameters of the detector are greatly reduced without performance degradation by introducing a Box Selection Schedule. Qualitative and quantitative experiments demonstrate that SwinTextSpotter v2 achieved state-of-the-art performance on various multilingual (English, Chinese, and Vietnamese) benchmarks. The code will be available at \\href{https://github.com/mxin262/SwinTextSpotterv2}{SwinTextSpotter v2}. ",
        "title": "SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07643",
        "abstract_url": "http://arxiv.org/abs/2401.07643",
        "authors": [
            {
                "last_name": "Qazzaz",
                "first_name": "Mohammed M. H."
            },
            {
                "last_name": "Ku\u0142acz",
                "first_name": "\u0141ukasz"
            },
            {
                "last_name": "Kliks",
                "first_name": "Adrian"
            },
            {
                "last_name": "Zaidi",
                "first_name": "Syed A."
            },
            {
                "last_name": "Dryjanski",
                "first_name": "Marcin"
            },
            {
                "last_name": "McLernon",
                "first_name": "Des"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The disaggregated, distributed and virtualised implementation of radio access networks allows for dynamic resource allocation. These attributes can be realised by virtue of the Open Radio Access Networks (O-RAN) architecture. In this article, we tackle the issue of dynamic resource allocation using a data-driven approach by employing Machine Learning (ML). We present an xApp-based implementation for the proposed ML algorithm. The core aim of this work is to optimise resource allocation and fulfil Service Level Specifications (SLS). This is accomplished by dynamically adjusting the allocation of Physical Resource Blocks (PRBs) based on traffic demand and Quality of Service (QoS) requirements. The proposed ML model effectively selects the best allocation policy for each base station and enhances the performance of scheduler functionality in O-RAN - Distributed Unit (O-DU). We show that an xApp implementing the Random Forest Classifier can yield high (85\\%) performance accuracy for optimal policy selection. This can be attained using the O-RAN instance state input parameters over a short training duration. ",
        "title": "Machine Learning-based xApp for Dynamic Resource Allocation in O-RAN  Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07644",
        "abstract_url": "http://arxiv.org/abs/2401.07644",
        "authors": [
            {
                "last_name": "Amiri",
                "first_name": "Mojtaba"
            },
            {
                "last_name": "Vaezpour",
                "first_name": "Elaheh"
            },
            {
                "last_name": "Javadi",
                "first_name": "Sepideh"
            },
            {
                "last_name": "Mili",
                "first_name": "Mohammad Robat"
            },
            {
                "last_name": "Yanikomeroglu",
                "first_name": "Halim"
            },
            {
                "last_name": "Bennis",
                "first_name": "Mehdi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) is a cutting-edge concept for the sixth-generation (6G) wireless networks. In this letter, we propose a novel system that incorporates STAR-RIS with simultaneous wireless information and power transfer (SWIPT) using rate splitting multiple access (RSMA). The proposed system facilitates communication from a multi-antenna base station (BS) to single-antenna users in a downlink transmission. The BS concurrently sends energy and information signals to multiple energy harvesting receivers (EHRs) and information data receivers (IDRs) with the support of a deployed STAR-RIS. Furthermore, a multi-objective optimization is introduced to strike a balance between users' sum rate and the total harvested energy. To achieve this, an optimization problem is formulated to optimize the energy/information beamforming vectors at the BS, the phase shifts at the STAR-RIS, and the common message rate. Subsequently, we employ a meta deep deterministic policy gradient (Meta-DDPG) approach to solve the complex problem. Simulation results validate that the proposed algorithm significantly enhances both data rate and harvested energy in comparison to conventional DDPG. ",
        "title": "Multi-Objective Optimization in STAR-RIS-Aided SWIPT with RSMA via  Meta-Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07646",
        "abstract_url": "http://arxiv.org/abs/2401.07646",
        "authors": [
            {
                "last_name": "Seckler",
                "first_name": "Henrik"
            },
            {
                "last_name": "Metzler",
                "first_name": "Ralf"
            },
            {
                "last_name": "Kelty-Stephen",
                "first_name": "Damian G."
            },
            {
                "last_name": "Mangalam",
                "first_name": "Madhur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Anomalous diffusion processes pose a unique challenge in classification and characterization. Previously (Mangalam et al., 2023, Physical Review Research 5, 023144), we established a framework for understanding anomalous diffusion using multifractal formalism. The present study delves into the potential of multifractal spectral features for effectively distinguishing anomalous diffusion trajectories from five widely used models: fractional Brownian motion, scaled Brownian motion, continuous time random walk, annealed transient time motion, and L\\'evy walk. To accomplish this, we generate extensive datasets comprising $10^6$ trajectories from these five anomalous diffusion models and extract multiple multifractal spectra from each trajectory. Our investigation entails a thorough analysis of neural network performance, encompassing features derived from varying numbers of spectra. Furthermore, we explore the integration of multifractal spectra into traditional feature datasets, enabling us to assess their impact comprehensively. To ensure a statistically meaningful comparison, we categorize features into concept groups and train neural networks using features from each designated group. Notably, several feature groups demonstrate similar levels of accuracy, with the highest performance observed in groups utilizing moving-window characteristics and $p$-variation features. Multifractal spectral features, particularly those derived from three spectra involving different timescales and cutoffs, closely follow, highlighting their robust discriminatory potential. Remarkably, a neural network exclusively trained on features from a single multifractal spectrum exhibits commendable performance, surpassing other feature groups. Our findings underscore the diverse and potent efficacy of multifractal spectral features in enhancing classification of anomalous diffusion. ",
        "title": "Multifractal-spectral features enhance classification of anomalous  diffusion",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07654",
        "abstract_url": "http://arxiv.org/abs/2401.07654",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Ho Hin"
            },
            {
                "last_name": "Gu",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Theodore"
            },
            {
                "last_name": "Xu",
                "first_name": "Yanbo"
            },
            {
                "last_name": "Yang",
                "first_name": "Jianwei"
            },
            {
                "last_name": "Usuyama",
                "first_name": "Naoto"
            },
            {
                "last_name": "Wong",
                "first_name": "Cliff"
            },
            {
                "last_name": "Wei",
                "first_name": "Mu"
            },
            {
                "last_name": "Landman",
                "first_name": "Bennett A."
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            },
            {
                "last_name": "Santamaria-Pang",
                "first_name": "Alberto"
            },
            {
                "last_name": "Poon",
                "first_name": "Hoifung"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios. ",
        "title": "Foundation Models for Biomedical Image Segmentation: A Survey",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07655",
        "abstract_url": "http://arxiv.org/abs/2401.07655",
        "authors": [
            {
                "last_name": "Zang",
                "first_name": "Runqiang"
            },
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieqiao"
            },
            {
                "last_name": "Shi",
                "first_name": "Xu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Liangfan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  In spite of the rapid advancements in unsupervised log anomaly detection techniques, the current mainstream models still necessitate specific training for individual system datasets, resulting in costly procedures and limited scalability due to dataset size, thereby leading to performance bottlenecks. Furthermore, numerous models lack cognitive reasoning capabilities, posing challenges in direct transferability to similar systems for effective anomaly detection. Additionally, akin to reconstruction networks, these models often encounter the \"identical shortcut\" predicament, wherein the majority of system logs are classified as normal, erroneously predicting normal classes when confronted with rare anomaly logs due to reconstruction errors.   To address the aforementioned issues, we propose MLAD, a novel anomaly detection model that incorporates semantic relational reasoning across multiple systems. Specifically, we employ Sentence-bert to capture the similarities between log sequences and convert them into highly-dimensional learnable semantic vectors. Subsequently, we revamp the formulas of the Attention layer to discern the significance of each keyword in the sequence and model the overall distribution of the multi-system dataset through appropriate vector space diffusion. Lastly, we employ a Gaussian mixture model to highlight the uncertainty of rare words pertaining to the \"identical shortcut\" problem, optimizing the vector space of the samples using the maximum expectation model. Experiments on three real-world datasets demonstrate the superiority of MLAD. ",
        "title": "MLAD: A Unified Model for Multi-system Log Anomaly Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07656",
        "abstract_url": "http://arxiv.org/abs/2401.07656",
        "authors": [
            {
                "last_name": "Bork",
                "first_name": "Alexander"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Debraj"
            },
            {
                "last_name": "Grover",
                "first_name": "Kush"
            },
            {
                "last_name": "Kretinsky",
                "first_name": "Jan"
            },
            {
                "last_name": "Mohr",
                "first_name": "Stefanie"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "LO"
        ],
        "abstract": "  Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable. ",
        "title": "Learning Explainable and Better Performing Representations of POMDP  Strategies",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07657",
        "abstract_url": "http://arxiv.org/abs/2401.07657",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiuyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Guoqing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model. ",
        "title": "Empirical Evidence for the Fragment level Understanding on Drug  Molecular Structure of LLMs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07658",
        "abstract_url": "http://arxiv.org/abs/2401.07658",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Tian Yi"
            },
            {
                "last_name": "Ghignone",
                "first_name": "Edoardo"
            },
            {
                "last_name": "Baumann",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This work introduces SynPF, an MCL-based algorithm tailored for high-speed racing environments. Benchmarked against Cartographer, a state-of-the-art pose-graph SLAM algorithm, SynPF leverages synergies from previous particle-filtering methods and synthesizes them for the high-performance racing domain. Our extensive in-field evaluations reveal that while Cartographer excels under nominal conditions, it struggles when subjected to wheel-slip, a common phenomenon in a racing scenario due to varying grip levels and aggressive driving behaviour. Conversely, SynPF demonstrates robustness in these challenging conditions and a low-latency computation time of 1.25 ms on on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled autonomous racing vehicle, this work not only highlights the vulnerabilities of existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also emphasizes the potential of SynPF as a viable alternative, especially in deteriorating odometry conditions. ",
        "title": "Robustness Evaluation of Localization Techniques for Autonomous Racing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07663",
        "abstract_url": "http://arxiv.org/abs/2401.07663",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lichen"
            },
            {
                "last_name": "Lu",
                "first_name": "Shuai"
            },
            {
                "last_name": "Duan",
                "first_name": "Nan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level project of the seL4 operating system microkernel. Selene provides a comprehensive framework for end-to-end evaluation and a lightweight verification environment. Our experimental results with advanced LLMs, such as GPT-3.5-turbo and GPT-4, highlight the capabilities of large language models (LLMs) in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors. ",
        "title": "Selene: Pioneering Automated Proof in Software Verification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07669",
        "abstract_url": "http://arxiv.org/abs/2401.07669",
        "authors": [
            {
                "last_name": "S",
                "first_name": "Darshan Singh"
            },
            {
                "last_name": "Khan",
                "first_name": "Zeeshan"
            },
            {
                "last_name": "Tapaswi",
                "first_name": "Makarand"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While contrastive language image pretraining (CLIP) have exhibited impressive performance by learning highly semantic and generalized representations, recent works have exposed a fundamental drawback in its syntactic properties, that includes interpreting fine-grained attributes, actions, spatial relations, states, and details that require compositional reasoning. One reason for this is that natural captions often do not capture all the visual details of a scene. This leads to unaddressed visual concepts being misattributed to the wrong words. And the pooled image and text features, ends up acting as a bag of words, hence losing the syntactic information. In this work, we ask: Is it possible to enhance CLIP's fine-grained and syntactic abilities without compromising its semantic properties? We show that this is possible by adapting CLIP efficiently on a high-quality, comprehensive, and relatively small dataset. We demonstrate our adaptation strategy on VidSitu, a video situation recognition dataset annotated with verbs and rich semantic role labels (SRL). We use the SRL and verb information to create rule-based detailed captions, making sure they capture most of the visual concepts. Combined with hard negatives and hierarchical losses, these annotations allow us to learn a powerful visual representation, dubbed Fine-Grained CLIP (FiGCLIP), that preserves semantic understanding while being detail-oriented. We evaluate on five diverse vision-language tasks in both fine-tuning and zero-shot settings, achieving consistent improvements over the base CLIP model. ",
        "title": "FiGCLIP: Fine-Grained CLIP Adaptation via Densely Annotated Videos",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07674",
        "abstract_url": "http://arxiv.org/abs/2401.07674",
        "authors": [
            {
                "last_name": "Koukis",
                "first_name": "Georgios"
            },
            {
                "last_name": "Skaperas",
                "first_name": "Sotiris"
            },
            {
                "last_name": "Kapetanidou",
                "first_name": "Ioanna Angeliki"
            },
            {
                "last_name": "Mamatas",
                "first_name": "Lefteris"
            },
            {
                "last_name": "Tsaoussidis",
                "first_name": "Vassilis"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Kubernetes (K8s) serves as a mature orchestration system for the seamless deployment and management of containerized applications spanning across cloud and edge environments. Since high-performance connectivity and minimal resource utilization become critical factors as we approach the edge, evaluating the performance of K8s networking in this context is essential. This paper contributes to this effort, by conducting a qualitative and quantitative performance evaluation of diverse Container Network Interface (CNI) plugins within different K8s environments, incorporating lightweight implementations designed for the Edge. Our experimental assessment was conducted in two distinct (intra- and inter-host) scenarios, revealing interesting insights for both researchers and practitioners. For example, the deployment of plugins across lightweight distributions does not necessarily lead to resource utilization improvements, e.g., in terms of CPU/memory or throughput. ",
        "title": "Performance Evaluation of Kubernetes Networking Approaches across  Constraint Edge Environments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07677",
        "abstract_url": "http://arxiv.org/abs/2401.07677",
        "authors": [
            {
                "last_name": "R\u00f8nneberg",
                "first_name": "Rasmus Carl"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Secure software architecture is increasingly important in a data-driven world. When security is neglected sensitive information might leak through unauthorized access. To mitigate this software architects needs tools and methods to quantify security risks in complex systems. This paper presents doctoral research in its early stages concerned with creating constructive methods for building secure component-based systems from a quantitative information flow specification. This research aim at developing a method that allows software architects to develop secure systems from a repository of secure components. Planned contributions are refinement rules for secure development of components from a specification and well-formedness rules for secure composition of said components. ",
        "title": "Quantitative Information Flow Control by Construction for  Component-Based Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07680",
        "abstract_url": "http://arxiv.org/abs/2401.07680",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Rewriting logic and its implementation Maude are a natural and expressive framework for the specification of concurrent systems and logics. Its nondeterministic local transformations are described by rewriting rules, which can be controlled at a higher level using a builtin strategy language added to Maude~3. This specification resource would not be of much interest without tools to analyze their models, so in a previous work, we extended the Maude LTL model checker to verify strategy-controlled systems. In this paper, CTL* and $\\mu$-calculus are added to the repertoire of supported logics, after discussing which adaptations are needed for branching-time properties. The new extension relies on some external model checkers that are exposed the Maude models through general and efficient connections, profitable for future extensions and further applications. The performance of these model checkers is compared. ",
        "title": "Strategies, model checking and branching-time properties in Maude",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07681",
        "abstract_url": "http://arxiv.org/abs/2401.07681",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Tong"
            },
            {
                "last_name": "Doclo",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Spatially selective active noise control (ANC) hearables are designed to reduce unwanted noise from certain directions while preserving desired sounds from other directions. In previous studies, the target signal has been defined either as the delayed desired component in one of the reference microphone signals or as the desired component in the error microphone signal without any delay. In this paper, we systematically investigate the influence of delays in different target signals on the ANC performance and provide an intuitive explanation for how the system obtains the desired signal. Simulations were conducted on a pair of open-fitting hearables for localized speech and noise sources in an anechoic environment. The performance was assessed in terms of noise reduction, signal quality and control effort. Results indicate that optimal performance is achieved without delays when the target signal is defined at the error microphone, whereas causality necessitates delays when the target signal is defined at the reference microphone. The optimal delay is found to be the acoustic delay between this reference microphone and the error microphone from the desired source. ",
        "title": "Effect of target signals and delays on spatially selective active noise  control for open-fitting hearables",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07682",
        "abstract_url": "http://arxiv.org/abs/2401.07682",
        "authors": [
            {
                "last_name": "Vandak",
                "first_name": "Samuel"
            },
            {
                "last_name": "Goodell",
                "first_name": "Geoffrey"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CE"
        ],
        "abstract": "  The landscape of payment methods in retail is a complex and evolving area. Vendors are motivated to conduct an appropriate analysis to decide what payment methods to accept out of a vast range of options. Many factors are included in this decision process, some qualitative and some quantitative. The following research project investigates vendors' acceptance of cards and cash from various viewpoints, all chosen to represent a novel perspective, including the barriers and preferences for each and correlations with external demographic factors. We observe that lower interchange fees, limited in this instance by the regulatory framework, play a crucial role in facilitating merchants' acceptance of card payments. The regulatory constraints on interchange fees create a favorable cost structure for merchants, making card payment adoption financially feasible. However, additional factors like technological readiness and consumer preferences might also play a significant role in their decision-making process. We also note that aggregate Merchant Service Providers (MSPs) have positively impacted the payment landscape by offering more competitive fee rates, particularly beneficial for small merchants and entrepreneurs. However, associated risks, such as account freezes or abrupt terminations, pose challenges and often lack transparency. Last, the quantitative analysis of the relationship between demographic variables and acceptance of payment types is presented. This analysis combines the current landscape of payment acceptance in the UK with data from the most recent census from 2021. We show that the unemployment rates shape card and cash acceptance, age affects contactless preference, and work-from-home impacts credit card preference. ",
        "title": "Cash and Card Acceptance in Retail Payments: Motivations and Factors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07683",
        "abstract_url": "http://arxiv.org/abs/2401.07683",
        "authors": [
            {
                "last_name": "Gohsen",
                "first_name": "Marcel"
            },
            {
                "last_name": "Stein",
                "first_name": "Benno"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository of millions of knowledge statements. However, domain-specific knowledge from fields such as history, physics, or medicine is significantly underrepresented in those graphs. Although few domain-specific knowledge graphs exist (e.g., Pubmed for medicine), developing specialized retrieval applications for many domains still requires constructing knowledge graphs from scratch. To facilitate knowledge graph construction, we introduce WAKA: a Web application that allows domain experts to create knowledge graphs through the medium with which they are most familiar: natural language. ",
        "title": "Assisted Knowledge Graph Authoring: Human-Supervised Knowledge Graph  Construction from Natural Language",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07685",
        "abstract_url": "http://arxiv.org/abs/2401.07685",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Binh Vinh Duc"
            },
            {
                "last_name": "Moere",
                "first_name": "Andrew Vande"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  The WeWatt bike serves as an energy station that enables passers-by to charge their mobile devices through physical activity. However, despite multiple people using it simultaneously, the bike is typically used individually. To address this limitation, we developed the WeWattTree, an installation utilising human-powered energy to filter environmental air. Through the orchestration of subtle motion gestures, our goal is to entice passers-by to participate and encourage them to socially interact, synchronising their pace. In this work-in-progress, we provide insights into the prototyping process, combining physical experimentation and computational simulation, and delve into the underlying concepts of our grammar of motion gestures. We highlight how a single design effectively merged multiple functionalities, how the role of material characteristics shaped the interaction design, and discuss the potential for social performances as captivating public displays. ",
        "title": "A Human-Powered Public Display that Nudges Social Biking via Motion  Gesturing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07691",
        "abstract_url": "http://arxiv.org/abs/2401.07691",
        "authors": [
            {
                "last_name": "Buchanan",
                "first_name": "William J"
            },
            {
                "last_name": "Grierson",
                "first_name": "Sam"
            },
            {
                "last_name": "Uribe",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Biometric data is often highly sensitive, and a leak of this data can lead to serious privacy breaches. Some of the most sensitive of this type of data relates to the usage of DNA data on individuals. A leak of this type of data without consent could lead to privacy breaches of data protection laws. Along with this, there have been several recent data breaches related to the leak of DNA information, including from 23andMe and Ancestry. It is thus fundamental that a citizen should have the right to know if their DNA data is contained within a DNA database and ask for it to be removed if they are concerned about its usage. This paper outlines a method of hashing the core information contained within the data stores - known as Single-Nucleotide Polymorphisms (SNPs) - into a bilinear group accumulator in batch mode, which can then be searched by a trusted entity for matches. The time to create the witness proof and to verify were measured at 0.86 ms and 10.90 ms, respectively. ",
        "title": "Privacy-Aware Single-Nucleotide Polymorphisms (SNPs) using Bilinear  Group Accumulators in Batch Mode",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07694",
        "abstract_url": "http://arxiv.org/abs/2401.07694",
        "authors": [
            {
                "last_name": "Powell",
                "first_name": "William G."
            },
            {
                "last_name": "Lyu",
                "first_name": "Hanbaek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  For obtaining optimal first-order convergence guarantee for stochastic optimization, it is necessary to use a recurrent data sampling algorithm that samples every data point with sufficient frequency. Most commonly used data sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed recurrent under mild assumptions. In this work, we show that for a particular class of stochastic optimization algorithms, we do not need any other property (e.g., independence, exponential mixing, and reshuffling) than recurrence in data sampling algorithms to guarantee the optimal rate of first-order convergence. Namely, using regularized versions of Minimization by Incremental Surrogate Optimization (MISO), we show that for non-convex and possibly non-smooth objective functions, the expected optimality gap converges at an optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes. Furthermore, the implied constant depends explicitly on the `speed of recurrence', measured by the expected amount of time to visit a given data point either averaged (`target time') or supremized (`hitting time') over the current location. We demonstrate theoretically and empirically that convergence can be accelerated by selecting sampling algorithms that cover the data set most effectively. We discuss applications of our general framework to decentralized optimization and distributed non-negative matrix factorization. ",
        "title": "Stochastic optimization with arbitrary recurrent data sampling",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07696",
        "abstract_url": "http://arxiv.org/abs/2401.07696",
        "authors": [
            {
                "last_name": "Shome",
                "first_name": "Arumoy"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "van Deursen",
                "first_name": "Arie"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployment. In a prior study, we mined $54,070$ Jupyter notebooks from Github and created a catalogue of $269$ semantically related visualisation-assertion (VA) pairs. Building on this catalogue, we propose to build a taxonomy that organises the VA pairs based on ML verification tasks. The input feature space comprises of a rich source of information mined from the Jupyter notebooks -- visualisations, Python source code, and associated markdown text. The effectiveness of various AI models, including traditional NLP4Code models and modern Large Language Models, will be compared using established machine translation metrics and evaluated through a qualitative study with human participants. The paper also plans to address the challenge of extending the existing VA pair dataset with additional pairs from Kaggle and to compare the tool's effectiveness with commercial generative AI models like ChatGPT. This research not only contributes to the field of ML system validation but also explores novel ways to leverage AI for automating and enhancing software engineering practices in ML. ",
        "title": "Towards Automatic Translation of Machine Learning Visual Insights to  Analytical Assertions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07697",
        "abstract_url": "http://arxiv.org/abs/2401.07697",
        "authors": [
            {
                "last_name": "Shome",
                "first_name": "Arumoy"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "van Deursen",
                "first_name": "Arie"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY",
            "SE"
        ],
        "abstract": "  Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a ``cheap'' and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs. ",
        "title": "Data vs. Model Machine Learning Fairness Testing: An Empirical Study",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07698",
        "abstract_url": "http://arxiv.org/abs/2401.07698",
        "authors": [
            {
                "last_name": "Mari\u0107",
                "first_name": "Ante"
            },
            {
                "last_name": "Li",
                "first_name": "Yiming"
            },
            {
                "last_name": "Calinon",
                "first_name": "Sylvain"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Reasoning about distance is indispensable for establishing or avoiding contact in manipulation tasks. To this end, we present an online method for learning implicit representations of signed distance using piecewise polynomial basis functions. Starting from an arbitrary prior shape, our approach incrementally constructs a continuous representation from incoming point cloud data. It offers fast access to distance and analytical gradients without the need to store training data. We assess the accuracy of our model on a diverse set of household objects and compare it to neural network and Gaussian process counterparts. Distance reconstruction and real-time updates are further evaluated in a physical experiment by simultaneously collecting sparse point cloud data and using the evolving model to control a manipulator. ",
        "title": "Online Learning of Piecewise Polynomial Signed Distance Fields for  Manipulation Tasks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07702",
        "abstract_url": "http://arxiv.org/abs/2401.07702",
        "authors": [
            {
                "last_name": "Davis",
                "first_name": "Christopher"
            },
            {
                "last_name": "Caines",
                "first_name": "Andrew"
            },
            {
                "last_name": "Andersen",
                "first_name": "\u00d8istein"
            },
            {
                "last_name": "Taslimipoor",
                "first_name": "Shiva"
            },
            {
                "last_name": "Yannakoudakis",
                "first_name": "Helen"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Bryant",
                "first_name": "Christopher"
            },
            {
                "last_name": "Rei",
                "first_name": "Marek"
            },
            {
                "last_name": "Buttery",
                "first_name": "Paula"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Thanks to recent advances in generative AI, we are able to prompt large language models (LLMs) to produce texts which are fluent and grammatical. In addition, it has been shown that we can elicit attempts at grammatical error correction (GEC) from LLMs when prompted with ungrammatical input sentences. We evaluate how well LLMs can perform at GEC by measuring their performance on established benchmark datasets. We go beyond previous studies, which only examined GPT* models on a selection of English GEC datasets, by evaluating seven open-source and three commercial LLMs on four established GEC benchmarks. We investigate model performance and report results against individual error types. Our results indicate that LLMs do not always outperform supervised English GEC models except in specific contexts -- namely commercial LLMs on benchmarks annotated with fluency corrections as opposed to minimal edits. We find that several open-source models outperform commercial ones on minimal edit benchmarks, and that in some settings zero-shot prompting is just as competitive as few-shot prompting. ",
        "title": "Prompting open-source and commercial language models for grammatical  error correction of English learner text",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07704",
        "abstract_url": "http://arxiv.org/abs/2401.07704",
        "authors": [
            {
                "last_name": "Oxenhorn",
                "first_name": "Arthur"
            },
            {
                "last_name": "Mor",
                "first_name": "Almog"
            },
            {
                "last_name": "Stern",
                "first_name": "Uri"
            },
            {
                "last_name": "Feitelson",
                "first_name": "Dror G."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Perhaps the most widely used form of code documentation is function header comments. We performed a large-scale survey of 367 developers to catalog their expectations from such documentation and to chronicle actual practice. Paradoxically, we found that developers appreciate the value of header comments and estimate that they are worth the investment in time, but nevertheless they tend not to write such documentation in their own code. Reasons for not writing header comments vary from the belief that code should be self-documenting to concern that documentation will not be kept up-to-date. A possible outcome of this situation is that developers may evade requirements to write documentation by using templates to generate worthless comments that do not provide any real information. We define a simple metric for information-less documentation based on its similarity to the function signature. Applying this to 21,140 files in GitHub Python projects shows that most functions are undocumented, but when header comments are written they typically do contain additional information beyond the function signature. ",
        "title": "The Paradox of Function Header Comments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07706",
        "abstract_url": "http://arxiv.org/abs/2401.07706",
        "authors": [
            {
                "last_name": "Della Rossa",
                "first_name": "Matteo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The topic of this manuscript is the stability analysis of continuous-time switched nonlinear systems with constraints on the admissible switching signals. Our particular focus lies in considering signals characterized by upper and lower bounds on the length of the switching intervals. We adapt and extend the existing theory of multiple Lyapunov functions, providing converse results and thus a complete characterization of uniform stability for this class of systems. We specify our results in the context of switched linear systems, providing the equivalence of exponential stability and the existence of multiple Lyapunov norms. By restricting the class of candidate Lyapunov functions to the set of quadratic functions, we are able to provide semidefinite-optimization-based numerical schemes to check the proposed conditions. We provide numerical examples to illustrate our approach and highlight its advantages over existing methods. ",
        "title": "Converse Lyapunov Results for Switched Systems with Lower and Upper  Bounds on Switching Intervals",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07709",
        "abstract_url": "http://arxiv.org/abs/2401.07709",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Siyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiji"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiyi"
            },
            {
                "last_name": "He",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chaoyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Rongsheng"
            },
            {
                "last_name": "Hu",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiaoshuai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which often applies a semantic mask to control the target area for diffusion-based editing. However, most existing solutions obtain these masks via manual operations or off-line processing, greatly reducing their efficiency. In this paper, we propose a novel and efficient image editing method for Text-to-Image (T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In particular, InstDiffEdit aims to employ the cross-modal attention ability of existing diffusion models to achieve instant mask guidance during the diffusion steps. To reduce the noise of attention maps and realize the full automatics, we equip InstDiffEdit with a training-free refinement scheme to adaptively aggregate the attention distributions for the automatic yet accurate mask generation. Meanwhile, to supplement the existing evaluations of DIE, we propose a new benchmark called Editing-Mask to examine the mask accuracy and local editing ability of existing methods. To validate InstDiffEdit, we also conduct extensive experiments on ImageNet and Imagen, and compare it with a bunch of the SOTA methods. The experimental results show that InstDiffEdit not only outperforms the SOTA methods in both image quality and editing results, but also has a much faster inference speed, i.e., +5 to +6 times. Our code available at https://anonymous.4open.science/r/InstDiffEdit-C306/ ",
        "title": "Towards Efficient Diffusion-Based Image Editing with Instant Attention  Masks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07710",
        "abstract_url": "http://arxiv.org/abs/2401.07710",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Junlin"
            },
            {
                "last_name": "Mannion",
                "first_name": "Patrick"
            },
            {
                "last_name": "Mason",
                "first_name": "Karl"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Reinforcement learning is commonly applied in residential energy management, particularly for optimizing energy costs. However, RL agents often face challenges when dealing with deceptive and sparse rewards in the energy control domain, especially with stochastic rewards. In such situations, thorough exploration becomes crucial for learning an optimal policy. Unfortunately, the exploration mechanism can be misled by deceptive reward signals, making thorough exploration difficult. Go-Explore is a family of algorithms which combines planning methods and reinforcement learning methods to achieve efficient exploration. We use the Go-Explore algorithm to solve the cost-saving task in residential energy management problems and achieve an improvement of up to 19.84\\% compared to the well-known reinforcement learning algorithms. ",
        "title": "Go-Explore for Residential Energy Management",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07711",
        "abstract_url": "http://arxiv.org/abs/2401.07711",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Zerui"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Toshihisa"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qibin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In numerous applications, binary reactions or event counts are observed and stored within high-order tensors. Tensor decompositions (TDs) serve as a powerful tool to handle such high-dimensional and sparse data. However, many traditional TDs are explicitly or implicitly designed based on the Gaussian distribution, which is unsuitable for discrete data. Moreover, most TDs rely on predefined multi-linear structures, such as CP and Tucker formats. Therefore, they may not be effective enough to handle complex real-world datasets. To address these issues, we propose ENTED, an \\underline{E}fficient \\underline{N}onparametric \\underline{TE}nsor \\underline{D}ecomposition for binary and count tensors. Specifically, we first employ a nonparametric Gaussian process (GP) to replace traditional multi-linear structures. Next, we utilize the \\pg augmentation which provides a unified framework to establish conjugate models for binary and count distributions. Finally, to address the computational issue of GPs, we enhance the model by incorporating sparse orthogonal variational inference of inducing points, which offers a more effective covariance approximation within GPs and stochastic natural gradient updates for nonparametric models. We evaluate our model on several real-world tensor completion tasks, considering binary and count datasets. The results manifest both better performance and computational advantages of the proposed model. ",
        "title": "Efficient Nonparametric Tensor Decomposition for Binary and Count Data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07713",
        "abstract_url": "http://arxiv.org/abs/2401.07713",
        "authors": [
            {
                "last_name": "Gast",
                "first_name": "Nicolas"
            },
            {
                "last_name": "van Houdt",
                "first_name": "Benny"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF"
        ],
        "abstract": "  As job redundancy has been recognized as an effective means to improve performance of large-scale computer systems, queueing systems with redundancy have been studied by various authors. Existing results include methods to compute the queue length distribution and response time but only when the service discipline is First-Come-First-Served (FCFS). For other service disciplines, such as Processor Sharing (PS), or Last-Come-First-Served (LCFS), only the stability conditions are known. In this paper we develop the first methods to approximate the queue length distribution in a queueing system with redundancy under various service disciplines. We focus on a system with exponential job sizes, i.i.d. copies, and a large number of servers. We first derive a mean field approximation that is independent of the scheduling policy. In order to study the impact of service discipline, we then derive refinements of this approximation to specific scheduling policies. In the case of Processor Sharing, we provide a pair and a triplet approximation. The pair approximation can be regarded as a refinement of the classic mean field approximation and takes the service discipline into account, while the triplet approximation further refines the pair approximation. We also develop a pair approximation for three other service disciplines: First-Come-First-Served, Limited Processor Sharing and Last-Come-First-Served. We present numerical evidence that shows that all the approximations presented in the paper are highly accurate, but that none of them are asymptotically exact (as the number of servers goes to infinity). This makes these approximations suitable to study the impact of the service discipline on the queue length distribution. Our results show that FCFS yields the shortest queue length, and that the differences are more substantial at higher loads. ",
        "title": "Approximations to Study the Impact of the Service Discipline in Systems  with Redundancy",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07717",
        "abstract_url": "http://arxiv.org/abs/2401.07717",
        "authors": [
            {
                "last_name": "Skaperas",
                "first_name": "Sotiris"
            },
            {
                "last_name": "Koukis",
                "first_name": "George"
            },
            {
                "last_name": "Kapetanidou",
                "first_name": "Ioanna Angeliki"
            },
            {
                "last_name": "Tsaoussidis",
                "first_name": "Vassilis"
            },
            {
                "last_name": "Mamatas",
                "first_name": "Lefteris"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Anomaly detection (AD) has been recently employed in the context of edge cloud computing, e.g., for intrusion detection and identification of performance issues. However, state-of-the-art anomaly detection procedures do not systematically consider restrictions and performance requirements inherent to the edge, such as system responsiveness and resource consumption. In this paper, we attempt to investigate the performance of change-point based detectors, i.e., a class of lightweight and accurate AD methods, in relation to the requirements of edge cloud systems. Firstly, we review the theoretical properties of two major categories of change point approaches, i.e., Bayesian and cumulative sum (CUSUM), also discussing their suitability for edge systems. Secondly, we introduce a novel experimental methodology and apply it over two distinct edge cloud test-beds to evaluate the performance of such mechanisms in real-world edge environments. Our experimental results reveal important insights and trade-offs for the applicability and the online performance of the selected change point detectors. ",
        "title": "A Pragmatical Approach to Anomaly Detection Evaluation in Edge Cloud  Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07718",
        "abstract_url": "http://arxiv.org/abs/2401.07718",
        "authors": [
            {
                "last_name": "Peshkovskaya",
                "first_name": "Anastasia"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yu-Tao"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC",
            "SI"
        ],
        "abstract": "  In the light of increasing clues on social media impact on self-harm and suicide risks, there is still no evidence on who are and how factually engaged in suicide-related online behaviors. This study reports new findings of high-performance supercomputing investigation of publicly accessible big data sourced from one of the world-largest social networking site. Three-month supercomputer searching resulted in 570,156 young adult users who consumed suicide-related information on social media. Most of them were 21-24 year olds with higher share of females (58%) of predominantly younger age. Every eight user was alarmingly engrossed with up to 15 suicide-related online groups. Evidently, suicide groups on social media are highly underrated public health issue that might weaken the prevention efforts. Suicide prevention strategies that target social media users must be implemented extensively. While major gap in functional understanding of technologies relevance for use in public mental health still exists, current findings act for better understanding digital technologies utility for translational advance and offer relevant evidence-based framework for improving suicide prevention in general population. ",
        "title": "How Social Media Big Data Can Improve Suicide Prevention",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07721",
        "abstract_url": "http://arxiv.org/abs/2401.07721",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Hao"
            },
            {
                "last_name": "Shao",
                "first_name": "Ling"
            },
            {
                "last_name": "Sebe",
                "first_name": "Nicu"
            },
            {
                "last_name": "Van Gool",
                "first_name": "Luc"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a novel graph Transformer generative adversarial network (GTGAN) to learn effective graph node relations in an end-to-end fashion for challenging graph-constrained architectural layout generation tasks. The proposed graph-Transformer-based generator includes a novel graph Transformer encoder that combines graph convolutions and self-attentions in a Transformer to model both local and global interactions across connected and non-connected graph nodes. Specifically, the proposed connected node attention (CNA) and non-connected node attention (NNA) aim to capture the global relations across connected nodes and non-connected nodes in the input graph, respectively. The proposed graph modeling block (GMB) aims to exploit local vertex interactions based on a house layout topology. Moreover, we propose a new node classification-based discriminator to preserve the high-level semantic and discriminative node features for different house components. To maintain the relative spatial relationships between ground truth and predicted graphs, we also propose a novel graph-based cycle-consistency loss. Finally, we propose a novel self-guided pre-training method for graph representation learning. This approach involves simultaneous masking of nodes and edges at an elevated mask ratio (i.e., 40%) and their subsequent reconstruction using an asymmetric graph-centric autoencoder architecture. This method markedly improves the model's learning proficiency and expediency. Experiments on three challenging graph-constrained architectural layout generation tasks (i.e., house layout generation, house roof generation, and building layout generation) with three public datasets demonstrate the effectiveness of the proposed method in terms of objective quantitative scores and subjective visual realism. New state-of-the-art results are established by large margins on these three tasks. ",
        "title": "Graph Transformer GANs with Graph Masked Modeling for Architectural  Layout Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07722",
        "abstract_url": "http://arxiv.org/abs/2401.07722",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Junlin"
            },
            {
                "last_name": "Mannion",
                "first_name": "Patrick"
            },
            {
                "last_name": "Mason",
                "first_name": "Karl"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  It is often challenging for a user to articulate their preferences accurately in multi-objective decision-making problems. Demonstration-based preference inference (DemoPI) is a promising approach to mitigate this problem. Understanding the behaviours and values of energy customers is an example of a scenario where preference inference can be used to gain insights into the values of energy customers with multiple objectives, e.g. cost and comfort. In this work, we applied the state-of-art DemoPI method, i.e., the dynamic weight-based preference inference (DWPI) algorithm in a multi-objective residential energy consumption setting to infer preferences from energy consumption demonstrations by simulated users following a rule-based approach. According to our experimental results, the DWPI model achieves accurate demonstration-based preference inferring in three scenarios. These advancements enhance the usability and effectiveness of multi-objective reinforcement learning (MORL) in energy management, enabling more intuitive and user-friendly preference specifications, and opening the door for DWPI to be applied in real-world settings. ",
        "title": "Inferring Preferences from Demonstrations in Multi-Objective Residential  Energy Management",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07725",
        "abstract_url": "http://arxiv.org/abs/2401.07725",
        "authors": [
            {
                "last_name": "Siddik",
                "first_name": "Md. Abubakar"
            },
            {
                "last_name": "Hasi",
                "first_name": "Most. Anju Ara"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Rajiul"
            },
            {
                "last_name": "Nitu",
                "first_name": "Jakia Akter"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The recently released IEEE 802.15.6 standard specifies several physical (PHY) layer and medium access control (MAC) layer protocols for variety of medical and non-medical applications of Wireless Body Area Networks (WBAN). The most suitable way for enhancing network performance is to be the choice of different MAC and PHY parameters based on quality of service (QoS) requirements of different applications. The impact of different MAC and PHY parameters on the network performance and the trade-off relationship between the parameters are essential to overcome the limitations of exiting carrier sense multiple access with collision avoidance (CSMA/CA) scheme of IEEE 802.15.6 standard. To address this issue, we develop a Markov chain-based analytical model of IEEE 802.15.6 CSMA/CA for all user priorities (UPs) and apply this general model to different network scenarios to investigate the effects of the packet arrival rate, channel condition, payload size, access phase length, access mechanism and number of nodes on the performance parameters viz. reliability, normalized throughput, energy consumption and average access delay. Moreover, we conclude the effectiveness of different access phases, access mechanisms and user priorities of intra-WBAN. ",
        "title": "Effects Investigation of MAC and PHY Layer Parameters on the Performance  of IEEE 802.15.6 CSMA/CA",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07726",
        "abstract_url": "http://arxiv.org/abs/2401.07726",
        "authors": [
            {
                "last_name": "Garcia",
                "first_name": "Paulo"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "RO"
        ],
        "abstract": "  We evaluate the use of software interpretation to push High Level Synthesis of application-specific accelerators toward a higher level of abstraction. Our methodology is supported by a formal power consumption model that computes the power consumption of accelerator components, accurately predicting the power consumption on new designs from prior optimization estimations. We demonstrate how our approach simplifies the re-use of power optimizations across distinct designs, by leveraging the higher level of design abstraction, using two accelerators representative of the robotics domain, implemented through the Bambu High Level Synthesis tool. Results support the research hypothesis, achieving predictions accurate within +/- 1%. ",
        "title": "Preserving Power Optimizations Across the High Level Synthesis of  Distinct Application-Specific Circuits",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07727",
        "abstract_url": "http://arxiv.org/abs/2401.07727",
        "authors": [
            {
                "last_name": "Mercier",
                "first_name": "Antoine"
            },
            {
                "last_name": "Nakhli",
                "first_name": "Ramin"
            },
            {
                "last_name": "Reddy",
                "first_name": "Mahesh"
            },
            {
                "last_name": "Yasarla",
                "first_name": "Rajeev"
            },
            {
                "last_name": "Cai",
                "first_name": "Hong"
            },
            {
                "last_name": "Porikli",
                "first_name": "Fatih"
            },
            {
                "last_name": "Berger",
                "first_name": "Guillaume"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite the latest remarkable advances in generative modeling, efficient generation of high-quality 3D assets from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of assets, while their 2D counterparts contain billions of text-image pairs. To address this, we propose a novel approach which harnesses the power of large, pretrained 2D diffusion models. More specifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding latent triplane. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization, and can infer high-quality and diverse objects from textual prompts in 7 seconds, offering significantly better quality-to-latency trade-offs when comparing to existing approaches. Furthermore, HexaGen3D demonstrates strong generalization to new objects or compositions. ",
        "title": "HexaGen3D: StableDiffusion is just one step away from Fast and Diverse  Text-to-3D Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07729",
        "abstract_url": "http://arxiv.org/abs/2401.07729",
        "authors": [
            {
                "last_name": "Bhattacharyya",
                "first_name": "Prarthana"
            },
            {
                "last_name": "Huang",
                "first_name": "Chengjie"
            },
            {
                "last_name": "Czarnecki",
                "first_name": "Krzysztof"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  This paper addresses motion forecasting in multi-agent environments, pivotal for ensuring safety of autonomous vehicles. Traditional as well as recent data-driven marginal trajectory prediction methods struggle to properly learn non-linear agent-to-agent interactions. We present SSL-Interactions that proposes pretext tasks to enhance interaction modeling for trajectory prediction. We introduce four interaction-aware pretext tasks to encapsulate various aspects of agent interactions: range gap prediction, closest distance prediction, direction of movement prediction, and type of interaction prediction. We further propose an approach to curate interaction-heavy scenarios from datasets. This curated data has two advantages: it provides a stronger learning signal to the interaction model, and facilitates generation of pseudo-labels for interaction-centric pretext tasks. We also propose three new metrics specifically designed to evaluate predictions in interactive scenes. Our empirical evaluations indicate SSL-Interactions outperforms state-of-the-art motion forecasting methods quantitatively with up to 8% improvement, and qualitatively, for interaction-heavy scenarios. ",
        "title": "SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07732",
        "abstract_url": "http://arxiv.org/abs/2401.07732",
        "authors": [
            {
                "last_name": "Fournier",
                "first_name": "Ga\u00ebtan"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Marc"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We study a variant of the Hotelling-Downs model of spatial competition between firms where consumer choices are influenced by their individual preferences as well as the popularity of the firms. In general, a multiplicity of market equilibria might exist due to the popularity effect. To elucidate firm decision-making, we explore three distinct behavioral attitudes towards this multiplicity of equilibria: optimistic, neutral, and pessimistic. For each behavior, we characterize the set of Nash equilibria and measure the impact of the selfish behavior on the social welfare by means of the price of anarchy and price of stability. ",
        "title": "Popularity in location games",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07733",
        "abstract_url": "http://arxiv.org/abs/2401.07733",
        "authors": [
            {
                "last_name": "Jaber",
                "first_name": "Edgar"
            },
            {
                "last_name": "Blot",
                "first_name": "Vincent"
            },
            {
                "last_name": "Brunel",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Chabridon",
                "first_name": "Vincent"
            },
            {
                "last_name": "Remy",
                "first_name": "Emmanuel"
            },
            {
                "last_name": "Iooss",
                "first_name": "Bertrand"
            },
            {
                "last_name": "Lucor",
                "first_name": "Didier"
            },
            {
                "last_name": "Mougeot",
                "first_name": "Mathilde"
            },
            {
                "last_name": "Leite",
                "first_name": "Alessandro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Gaussian processes (GPs) are a Bayesian machine learning approach widely used to construct surrogate models for the uncertainty quantification of computer simulation codes in industrial applications. It provides both a mean predictor and an estimate of the posterior prediction variance, the latter being used to produce Bayesian credibility intervals. Interpreting these intervals relies on the Gaussianity of the simulation model as well as the well-specification of the priors which are not always appropriate. We propose to address this issue with the help of conformal prediction. In the present work, a method for building adaptive cross-conformal prediction intervals is proposed by weighting the non-conformity score with the posterior standard deviation of the GP. The resulting conformal prediction intervals exhibit a level of adaptivity akin to Bayesian credibility sets and display a significant correlation with the surrogate model local approximation error, while being free from the underlying model assumptions and having frequentist coverage guarantees. These estimators can thus be used for evaluating the quality of a GP surrogate model and can assist a decision-maker in the choice of the best prior for the specific application of the GP. The performance of the method is illustrated through a panel of numerical examples based on various reference databases. Moreover, the potential applicability of the method is demonstrated in the context of surrogate modeling of an expensive-to-evaluate simulator of the clogging phenomenon in steam generators of nuclear reactors. ",
        "title": "Conformal Approach To Gaussian Process Surrogate Evaluation With  Coverage Guarantees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07743",
        "abstract_url": "http://arxiv.org/abs/2401.07743",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Membrane systems are a biologically-inspired computational model based on the structure of biological cells and the way chemicals interact and traverse their membranes. Although their dynamics are described by rules, encoding membrane systems into rewriting logic is not straightforward due to its complex control mechanisms. Multiple alternatives have been proposed in the literature and implemented in the Maude specification language. The recent release of the Maude strategy language and its associated strategy-aware model checker allow specifying these systems more easily, so that they become executable and verifiable for free. An easily-extensible interactive environment transforms membrane specifications into rewrite theories controlled by appropriate strategies, and allows simulating and verifying membrane computations by means of them. ",
        "title": "Simulating and model checking membrane systems using strategies in Maude",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07744",
        "abstract_url": "http://arxiv.org/abs/2401.07744",
        "authors": [
            {
                "last_name": "Ghidalia",
                "first_name": "Sarah"
            },
            {
                "last_name": "Narsis",
                "first_name": "Ouassila Labbani"
            },
            {
                "last_name": "Bertaux",
                "first_name": "Aur\u00e9lie"
            },
            {
                "last_name": "Nicolle",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Motivated by the desire to explore the process of combining inductive and deductive reasoning, we conducted a systematic literature review of articles that investigate the integration of machine learning and ontologies. The objective was to identify diverse techniques that incorporate both inductive reasoning (performed by machine learning) and deductive reasoning (performed by ontologies) into artificial intelligence systems. Our review, which included the analysis of 128 studies, allowed us to identify three main categories of hybridization between machine learning and ontologies: learning-enhanced ontologies, semantic data mining, and learning and reasoning systems. We provide a comprehensive examination of all these categories, emphasizing the various machine learning algorithms utilized in the studies. Furthermore, we compared our classification with similar recent work in the field of hybrid AI and neuro-symbolic approaches. ",
        "title": "Combining Machine Learning and Ontology: A Systematic Literature Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07745",
        "abstract_url": "http://arxiv.org/abs/2401.07745",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Mi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiazhao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yan"
            },
            {
                "last_name": "Wang",
                "first_name": "He"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Open-vocabulary 3D instance segmentation has emerged as a frontier topic due to its capability to segment 3D instances beyond a predefined set of categories. However, compared to significant progress in the 2D domain, methods for 3D open-vocabulary instance segmentation are hindered by the limited scale of high-quality annotated 3D data. To harness the capabilities of 2D models, recent efforts have focused on merging 2D masks based on metrics such as geometric and semantic similarity to form 3D instances. In contrast to these local metrics, we propose a novel metric called view consensus to better exploit multi-view observation. The key insight is that two 2D masks should be considered as belonging to the same instance if a considerable number of other 2D masks from other views contain both these two masks. Based on this metric, we build a global mask graph and iteratively cluster masks, prioritizing mask pairs with solid view consensus. The corresponding 3D points cluster of these 2D mask clusters can be regarded as 3D instances, along with the fused open-vocabulary features from clustered 2D masks. Through this multi-view verification and fusion mechanism, our method effectively leverages the prior instance knowledge from massive 2D masks predicted by visual foundation models, eliminating the need for training on 3D data. Experiments on publicly available datasets, including ScanNet200 and MatterPort3D, demonstrate that our method achieves state-of-the-art performance in both open-vocabulary instance segmentation and class-agnostic mask generation. Our project page is at https://pku-epic.github.io/MaskClustering. ",
        "title": "MaskClustering: View Consensus based Mask Graph Clustering for  Open-Vocabulary 3D Instance Segmentation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07746",
        "abstract_url": "http://arxiv.org/abs/2401.07746",
        "authors": [
            {
                "last_name": "Valera",
                "first_name": "Patris"
            },
            {
                "last_name": "Vizca\u00edno",
                "first_name": "Josu\u00e9 Page"
            },
            {
                "last_name": "Lasser",
                "first_name": "Tobias"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single-molecule localization microscopy techniques, like stochastic optical reconstruction microscopy (STORM), visualize biological specimens by stochastically exciting sparse blinking emitters. The raw images suffer from unwanted background fluorescence, which must be removed to achieve super-resolution. We introduce a sparsity-based background removal method by adapting a neural network (SLNet) from a different microscopy domain. The SLNet computes a low-rank representation of the images, and then, by subtracting it from the raw images, the sparse component is computed, representing the frames without the background. We compared our approach with widely used background removal methods, such as the median background removal or the rolling ball algorithm, on two commonly used STORM datasets, one glial cell, and one microtubule dataset. The SLNet delivers STORM frames with less background, leading to higher emitters' localization precision and higher-resolution reconstructed images than commonly used methods. Notably, the SLNet is lightweight and easily trainable (<5 min). Since it is trained in an unsupervised manner, no prior information is required and can be applied to any STORM dataset. We uploaded a pre-trained SLNet to the Bioimage model zoo, easily accessible through ImageJ. Our results show that our sparse decomposition method could be an essential and efficient STORM pre-processing tool. ",
        "title": "Sparsity-based background removal for STORM super-resolution images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07749",
        "abstract_url": "http://arxiv.org/abs/2401.07749",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  In the reflective Maude specification language, based on rewriting logic, a strategy language has been introduced to control rule rewriting while avoiding complex and verbose metalevel programs. However, just as multiple levels of reflection are required for some metaprogramming tasks, reflective manipulation and generation of strategies are convenient in multiple situations. Some examples of reflective strategy transformations are presented, which implement special forms of evaluation or extend the strategy language while preserving its advantages. ",
        "title": "Metalevel transformation of strategies",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07751",
        "abstract_url": "http://arxiv.org/abs/2401.07751",
        "authors": [
            {
                "last_name": "Ruiz-Perez",
                "first_name": "Marina"
            },
            {
                "last_name": "Morell-Ortega",
                "first_name": "Sergio"
            },
            {
                "last_name": "Gadea",
                "first_name": "Marien"
            },
            {
                "last_name": "Vivo-Hernando",
                "first_name": "Roberto"
            },
            {
                "last_name": "Rubio",
                "first_name": "Gregorio"
            },
            {
                "last_name": "Aparici",
                "first_name": "Fernando"
            },
            {
                "last_name": "de la Iglesia-Vaya",
                "first_name": "Mariam"
            },
            {
                "last_name": "Tourdias",
                "first_name": "Thomas"
            },
            {
                "last_name": "Coup\u00e9",
                "first_name": "Pierrick"
            },
            {
                "last_name": "Manj\u00f3n",
                "first_name": "Jos\u00e9 V."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The implication of the thalamus in multiple neurological pathologies makes it a structure of interest for volumetric analysis. In the present work, we have designed and implemented a multimodal volumetric deep neural network for the segmentation of thalamic nuclei at ultra-high resolution (0.125 mm3). Current tools either operate at standard resolution (1 mm3) or use monomodal data. To achieve the proposed objective, first, a database of semiautomatically segmented thalamic nuclei was created using ultra-high resolution T1, T2 and White Matter nulled (WMn) images. Then, a novel Deep learning based strategy was designed to obtain the automatic segmentations and trained to improve its robustness and accuaracy using a semisupervised approach. The proposed method was compared with a related state-of-the-art method showing competitive results both in terms of segmentation quality and efficiency. To make the proposed method fully available to the scientific community, a full pipeline able to work with monomodal standard resolution T1 images is also proposed. ",
        "title": "DeepThalamus: A novel deep learning method for automatic segmentation of  brain thalamic nuclei from multimodal ultra-high resolution MRI",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07753",
        "abstract_url": "http://arxiv.org/abs/2401.07753",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Minghua"
            },
            {
                "last_name": "Qin",
                "first_name": "Xiangdong"
            },
            {
                "last_name": "Du",
                "first_name": "Shuangli"
            },
            {
                "last_name": "Bai",
                "first_name": "Xuefei"
            },
            {
                "last_name": "Lyu",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yiguang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unlike single image task, stereo image enhancement can use another view information, and its key stage is how to perform cross-view feature interaction to extract useful information from another view. However, complex noise in low-light image and its impact on subsequent feature encoding and interaction are ignored by the existing methods. In this paper, a method is proposed to perform enhancement and de-noising simultaneously. First, to reduce unwanted noise interference, a low-frequency information enhanced module (IEM) is proposed to suppress noise and produce a new image space. Additionally, a cross-channel and spatial context information mining module (CSM) is proposed to encode long-range spatial dependencies and to enhance inter-channel feature interaction. Relying on CSM, an encoder-decoder structure is constructed, incorporating cross-view and cross-scale feature interactions to perform enhancement in the new image space. Finally, the network is trained with the constraints of both spatial and frequency domain losses. Extensive experiments on both synthesized and real datasets show that our method obtains better detail recovery and noise removal compared with state-of-the-art methods. In addition, a real stereo image enhancement dataset is captured with stereo camera ZED2. The code and dataset are publicly available at: https://www.github.com/noportraits/LFENet. ",
        "title": "Low-light Stereo Image Enhancement and De-noising in the Low-frequency  Information Enhanced Image Space",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07756",
        "abstract_url": "http://arxiv.org/abs/2401.07756",
        "authors": [
            {
                "last_name": "Marnissi",
                "first_name": "Ouiame"
            },
            {
                "last_name": "Hammouti",
                "first_name": "Hajar EL"
            },
            {
                "last_name": "Bergou",
                "first_name": "El Houcine"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we study the performance of federated learning over wireless networks, where devices with a limited energy budget train a machine learning model. The federated learning performance depends on the selection of the clients participating in the learning at each round. Most existing studies suggest deterministic approaches for the client selection, resulting in challenging optimization problems that are usually solved using heuristics, and therefore without guarantees on the quality of the final solution. We formulate a new probabilistic approach to jointly select clients and allocate power optimally so that the expected number of participating clients is maximized. To solve the problem, a new alternating algorithm is proposed, where at each step, the closed-form solutions for user selection probabilities and power allocations are obtained. Our numerical results show that the proposed approach achieves a significant performance in terms of energy consumption, completion time and accuracy as compared to the studied benchmarks. ",
        "title": "Joint Probability Selection and Power Allocation for Federated Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07760",
        "abstract_url": "http://arxiv.org/abs/2401.07760",
        "authors": [
            {
                "last_name": "Ghaddar",
                "first_name": "Abbas"
            },
            {
                "last_name": "Langlais",
                "first_name": "Philippe"
            },
            {
                "last_name": "Rezagholizadeh",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Chen",
                "first_name": "Boxing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pretraining monolingual language models have been proven to be vital for performance in Arabic Natural Language Processing (NLP) tasks. In this paper, we conduct a comprehensive study on the role of data in Arabic Pretrained Language Models (PLMs). More precisely, we reassess the performance of a suite of state-of-the-art Arabic PLMs by retraining them on massive-scale, high-quality Arabic corpora. We have significantly improved the performance of the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in their respective model categories. In addition, our analysis strongly suggests that pretraining data by far is the primary contributor to performance, surpassing other factors. Our models and source code are publicly available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch. ",
        "title": "On the importance of Data Scale in Pretraining Arabic Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07761",
        "abstract_url": "http://arxiv.org/abs/2401.07761",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Zhaohong"
            },
            {
                "last_name": "Yamada",
                "first_name": "Naoyuki"
            },
            {
                "last_name": "Takenami",
                "first_name": "Yoshihiro"
            },
            {
                "last_name": "Moriwaki",
                "first_name": "Daisuke"
            },
            {
                "last_name": "Yokoo",
                "first_name": "Makoto"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We study a practical two-sided matching problem of allocating children to daycare centers, which has significant social implications. We are cooperating with several municipalities in Japan and our goal is to devise a reliable and trustworthy clearing algorithm to deal with the problem. In this paper, we describe the design of our new algorithm that minimizes the number of unmatched children while ensuring stability. We evaluate our algorithm using real-life data sets, and experimental results demonstrate that our algorithm surpasses the commercial software that currently dominates the market in terms of both the number of matched children and the number of blocking coalitions (measuring stability). Our findings have been reported to local governments, and some are considering adopting our proposed algorithm in the near future, instead of the existing solution. Moreover, our model and algorithm have broader applicability to other important matching markets, such as hospital-doctor matching with couples and school choice with siblings. ",
        "title": "Stable Matchings in Practice: A Constraint Programming Approach",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07762",
        "abstract_url": "http://arxiv.org/abs/2401.07762",
        "authors": [
            {
                "last_name": "Ying",
                "first_name": "Jun"
            },
            {
                "last_name": "Dong",
                "first_name": "Xin"
            },
            {
                "last_name": "Li",
                "first_name": "Bowei"
            },
            {
                "last_name": "Tian",
                "first_name": "Zihan"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Traffic flow prediction is widely used in travel decision making, traffic control, roadway system planning, business sectors, and government agencies. ARX models have proved to be highly effective and versatile. In this research, we investigated the applications of ARX models in prediction for real traffic flow in New York City. The ARX models were constructed by linear/polynomial or neural networks. Comparative studies were carried out based on the results by the efficiency, accuracy, and training computational demand of the algorithms. ",
        "title": "Auto-Regressive Model with Exogenous Input--ARX--based traffic-flow  prediction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07764",
        "abstract_url": "http://arxiv.org/abs/2401.07764",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Minrui"
            },
            {
                "last_name": "Dusit",
                "first_name": "Niyato"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zehui"
            },
            {
                "last_name": "Mao",
                "first_name": "Shiwen"
            },
            {
                "last_name": "Han",
                "first_name": "Zhu"
            },
            {
                "last_name": "Kim",
                "first_name": "Dong In"
            },
            {
                "last_name": "Letaief",
                "first_name": "Khaled B."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents. ",
        "title": "When Large Language Model Agents Meet 6G Networks: Perception,  Grounding, and Alignment",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07768",
        "abstract_url": "http://arxiv.org/abs/2401.07768",
        "authors": [
            {
                "last_name": "Kudo",
                "first_name": "Momonari"
            },
            {
                "last_name": "Yokoyama",
                "first_name": "Kazuhiro"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC"
        ],
        "abstract": "  Gr\\\"{o}bner bases are nowadays central tools for solving various problems in commutative algebra and algebraic geometry. A typical use of Gr\\\"{o}bner bases is the multivariate polynomial system solving, which enables us to construct algebraic attacks against post-quantum cryptographic protocols. Therefore, the determination of the complexity of computing Gr\\\"{o}bner bases is very important both in theory and in practice: One of the most important cases is the case where input polynomials compose an (overdetermined) affine semi-regular sequence. The first part of this paper aims to present a survey on the Gr\\\"{o}bner basis computation and its complexity. In the second part, we shall give an explicit formula on the (truncated) Hilbert-Poincar\\'{e} series associated to the homogenization of an affine semi-regular sequence. Based on the formula, we also study (reduced) Gr\\\"{o}bner bases of the ideals generated by an affine semi-regular sequence and its homogenization. Some of our results are considered to give mathematically rigorous proofs of the correctness of methods for computing Gr\\\"{o}bner bases of the ideal generated by an affine semi-regular sequence. ",
        "title": "On Hilbert-Poincar\\'{e} series of affine semi-regular polynomial  sequences and related Gr\\\"{o}bner bases",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07770",
        "abstract_url": "http://arxiv.org/abs/2401.07770",
        "authors": [
            {
                "last_name": "Ramrakhya",
                "first_name": "Ram"
            },
            {
                "last_name": "Kembhavi",
                "first_name": "Aniruddha"
            },
            {
                "last_name": "Batra",
                "first_name": "Dhruv"
            },
            {
                "last_name": "Kira",
                "first_name": "Zsolt"
            },
            {
                "last_name": "Zeng",
                "first_name": "Kuo-Hao"
            },
            {
                "last_name": "Weihs",
                "first_name": "Luca"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Computer vision tasks typically involve describing what is present in an image (e.g. classification, detection, segmentation, and captioning). We study a visual common sense task that requires understanding what is not present. Specifically, given an image (e.g. of a living room) and name of an object (\"cushion\"), a vision system is asked to predict semantically-meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely be placed by humans (e.g. on the sofa). We call this task: Semantic Placement (SP) and believe that such common-sense visual understanding is critical for assitive robots (tidying a house), and AR devices (automatically rendering an object in the user's space). Studying the invisible is hard. Datasets for image description are typically constructed by curating relevant images and asking humans to annotate the contents of the image; neither of those two steps are straightforward for objects not present in the image. We overcome this challenge by operating in the opposite direction: we start with an image of an object in context from web, and then remove that object from the image via inpainting. This automated pipeline converts unstructured web data into a dataset comprising pairs of images with/without the object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images across $9$ object categories, and train a SP prediction model called CLIP-UNet. CLIP-UNet outperforms existing VLMs and baselines that combine semantic priors with object detectors on real-world and simulated images. In our user studies, we find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and $31.3\\%$ times when comparing against the $4$ SP baselines on real and simulated images. In addition, we demonstrate leveraging SP mask predictions from CLIP-UNet enables downstream applications like building tidying robots in indoor environments. ",
        "title": "Seeing the Unseen: Visual Common Sense for Semantic Placement",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07774",
        "abstract_url": "http://arxiv.org/abs/2401.07774",
        "authors": [
            {
                "last_name": "Franco",
                "first_name": "Nicola"
            },
            {
                "last_name": "Sakhnenko",
                "first_name": "Alona"
            },
            {
                "last_name": "Stolpmann",
                "first_name": "Leon"
            },
            {
                "last_name": "Thuerck",
                "first_name": "Daniel"
            },
            {
                "last_name": "Petsch",
                "first_name": "Fabian"
            },
            {
                "last_name": "R\u00fcll",
                "first_name": "Annika"
            },
            {
                "last_name": "Lorenz",
                "first_name": "Jeanette Miriam"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Quantum Machine Learning (QML) has emerged as a promising intersection of quantum computing and classical machine learning, anticipated to drive breakthroughs in computational tasks. This paper discusses the question which security concerns and strengths are connected to QML by means of a systematic literature review. We categorize and review the security of QML models, their vulnerabilities inherent to quantum architectures, and the mitigation strategies proposed. The survey reveals that while QML possesses unique strengths, it also introduces novel attack vectors not seen in classical systems. Techniques like adversarial training, quantum noise exploitation, and quantum differential privacy have shown potential in enhancing QML robustness. Our review discuss the need for continued and rigorous research to ensure the secure deployment of QML in real-world applications. This work serves as a foundational reference for researchers and practitioners aiming to navigate the security aspects of QML. ",
        "title": "Predominant Aspects on Security for Quantum Machine Learning: Literature  Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07776",
        "abstract_url": "http://arxiv.org/abs/2401.07776",
        "authors": [
            {
                "last_name": "Aubian",
                "first_name": "Guillaume"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  The clique number of a tournament is the maximum clique number of a graph formed by keeping backwards arcs in an ordering of its vertices. We study the time complexity of computing the clique number of a tournament and prove that, for any integer $k \\geq 3$, deciding whether a tournament has clique number at most $k$ is NP-complete. This answers an interrogation of Nguyen, Scott and Seymour. To do so, we make use of a construction which we then modify to provide a counterexample to a conjecture of Aboulker, Aubian, Charbit and Lopes. ",
        "title": "Computing the clique number of tournaments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07777",
        "abstract_url": "http://arxiv.org/abs/2401.07777",
        "authors": [
            {
                "last_name": "Buonaiuto",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Guarasci",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Minutolo",
                "first_name": "Aniello"
            },
            {
                "last_name": "De Pietro",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Esposito",
                "first_name": "Massimo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Hybrid quantum-classical classifiers promise to positively impact critical aspects of natural language processing tasks, particularly classification-related ones. Among the possibilities currently investigated, quantum transfer learning, i.e., using a quantum circuit for fine-tuning pre-trained classical models for a specific task, is attracting significant attention as a potential platform for proving quantum advantage.   This work shows potential advantages, both in terms of performance and expressiveness, of quantum transfer learning algorithms trained on embedding vectors extracted from a large language model to perform classification on a classical Linguistics task: acceptability judgments. Acceptability judgment is the ability to determine whether a sentence is considered natural and well-formed by a native speaker. The approach has been tested on sentences extracted from ItaCoLa, a corpus that collects Italian sentences labeled with their acceptability judgment. The evaluation phase shows results for the quantum transfer learning pipeline comparable to state-of-the-art classical transfer learning algorithms, proving current quantum computers' capabilities to tackle NLP tasks for ready-to-use applications. Furthermore, a qualitative linguistic analysis, aided by explainable AI methods, reveals the capabilities of quantum transfer learning algorithms to correctly classify complex and more structured sentences, compared to their classical counterpart. This finding sets the ground for a quantifiable quantum advantage in NLP in the near future. ",
        "title": "Quantum Transfer Learning for Acceptability Judgements",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07780",
        "abstract_url": "http://arxiv.org/abs/2401.07780",
        "authors": [
            {
                "last_name": "Chatzikiriakos",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Wabersich",
                "first_name": "Kim P."
            },
            {
                "last_name": "Berkel",
                "first_name": "Felix"
            },
            {
                "last_name": "Pauli",
                "first_name": "Patricia"
            },
            {
                "last_name": "Iannelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Model Predictive Control (MPC) can be applied to safety-critical control problems, providing closed-loop safety and performance guarantees. Implementation of MPC controllers requires solving an optimization problem at every sampling instant, which is challenging to execute on embedded hardware. To address this challenge, we propose a framework that combines a tightened soft constrained MPC formulation with supervised learning to approximate the MPC value function. This combination enables us to obtain a corresponding optimal control law, which can be implemented efficiently on embedded platforms. The framework ensures stability and constraint satisfaction for various nonlinear systems. While the design effort is similar to that of nominal MPC, the proposed formulation provides input-to-state stability (ISS) with respect to the approximation error of the value function. Furthermore, we prove that the value function corresponding to the soft constrained MPC problem is Lipschitz continuous for Lipschitz continuous systems, even if the optimal control law may be discontinuous. This serves two purposes: First, it allows to relate approximation errors to a sufficiently large constraint tightening to obtain constraint satisfaction guarantees. Second, it paves the way for an efficient supervised learning procedure to obtain a continuous value function approximation. We demonstrate the effectiveness of the method using a nonlinear numerical example. ",
        "title": "Learning Soft Constrained MPC Value Functions: Efficient MPC Design and  Implementation providing Stability and Safety Guarantees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07781",
        "abstract_url": "http://arxiv.org/abs/2401.07781",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jay Zhangjie"
            },
            {
                "last_name": "Fang",
                "first_name": "Guian"
            },
            {
                "last_name": "Wu",
                "first_name": "Haoning"
            },
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "Ge",
                "first_name": "Yixiao"
            },
            {
                "last_name": "Cun",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Zhang",
                "first_name": "David Junhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Jia-Wei"
            },
            {
                "last_name": "Gu",
                "first_name": "Yuchao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Rui"
            },
            {
                "last_name": "Lin",
                "first_name": "Weisi"
            },
            {
                "last_name": "Hsu",
                "first_name": "Wynne"
            },
            {
                "last_name": "Shan",
                "first_name": "Ying"
            },
            {
                "last_name": "Shou",
                "first_name": "Mike Zheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Generative models have demonstrated remarkable capability in synthesizing high-quality text, images, and videos. For video generation, contemporary text-to-video models exhibit impressive capabilities, crafting visually stunning videos. Nonetheless, evaluating such videos poses significant challenges. Current research predominantly employs automated metrics such as FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis, particularly in the temporal assessment of video content, thus rendering them unreliable indicators of true video quality. Furthermore, while user studies have the potential to reflect human perception accurately, they are hampered by their time-intensive and laborious nature, with outcomes that are often tainted by subjective bias. In this paper, we investigate the limitations inherent in existing metrics and introduce a novel evaluation pipeline, the Text-to-Video Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video Alignment, which scrutinizes the fidelity of the video in representing the given text description, and (2) Video Quality, which evaluates the video's overall production caliber with a mixture of experts. Moreover, to evaluate the proposed metrics and facilitate future improvements on them, we present the TVGE dataset, collecting human judgements of 2,543 text-to-video generated videos on the two criteria. Experiments on the TVGE dataset demonstrate the superiority of the proposed T2VScore on offering a better metric for text-to-video generation. ",
        "title": "Towards A Better Metric for Text-to-Video Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07782",
        "abstract_url": "http://arxiv.org/abs/2401.07782",
        "authors": [
            {
                "last_name": "Hackstein",
                "first_name": "Jakob"
            },
            {
                "last_name": "Sumbul",
                "first_name": "Gencer"
            },
            {
                "last_name": "Clasen",
                "first_name": "Kai Norman"
            },
            {
                "last_name": "Demir",
                "first_name": "Beg\u00fcm"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Self-supervised learning through masked autoencoders (MAEs) has recently attracted great attention for remote sensing (RS) image representation learning, and thus embodies a significant potential for content-based image retrieval (CBIR) from ever-growing RS image archives. However, the existing studies on MAEs in RS assume that the considered RS images are acquired by a single image sensor, and thus are only suitable for uni-modal CBIR problems. The effectiveness of MAEs for cross-sensor CBIR, which aims to search semantically similar images across different image modalities, has not been explored yet. In this paper, we take the first step to explore the effectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a systematic overview on the possible adaptations of the vanilla MAE to exploit masked image modeling on multi-sensor RS image archives (denoted as cross-sensor masked autoencoders [CSMAEs]). Based on different adjustments applied to the vanilla MAE, we introduce different CSMAE models. We also provide an extensive experimental analysis of these CSMAE models. We finally derive a guideline to exploit masked image modeling for uni-modal and cross-modal CBIR problems in RS. The code of this work is publicly available at https://github.com/jakhac/CSMAE. ",
        "title": "Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in  Remote Sensing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07783",
        "abstract_url": "http://arxiv.org/abs/2401.07783",
        "authors": [
            {
                "last_name": "Giaretta",
                "first_name": "Alberto"
            },
            {
                "last_name": "Loutfi",
                "first_name": "Amy"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "RO"
        ],
        "abstract": "  Modern robots are stepping away from monolithic entities built using ad-hoc sensors and actuators, due to new technologies and communication paradigms, such as the Internet of Things (IoT) and the Robotic Operating System (ROS). Using such paradigms, robots can be built by acquiring heterogeneous standard devices and putting them in communication with each other. This approach brings high degrees of modularity, but it also yields uncertainty of providing cybersecurity assurances, and guarantees on the integrity of the embodiment. In this paper, we first illustrate how cyberattacks on different devices can have radically different consequences on the robot's ability to complete its tasks and preserve its embodiment. We also claim that modern robots should have self-awareness for what it concerns such aspects, and formulate the different characteristics that robots should integrate for doing so. Then, we show that achieving these propositions requires that robots possess at least three properties that conceptually link devices and tasks. Last, we reflect on how these three properties could be achieved in a larger conceptual framework. ",
        "title": "Cybersecurity and Embodiment Integrity for Modern Robots: A Conceptual  Framework",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07784",
        "abstract_url": "http://arxiv.org/abs/2401.07784",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yingjian"
            },
            {
                "last_name": "Wen",
                "first_name": "Xiangyong"
            },
            {
                "last_name": "Gao",
                "first_name": "Fei"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Bearing measurements,as the most common modality in nature, have recently gained traction in multi-robot systems to enhance mutual localization and swarm collaboration. Despite their advantages, challenges such as sensory noise, obstacle occlusion, and uncoordinated swarm motion persist in real-world scenarios, potentially leading to erroneous state estimation and undermining the system's flexibility, practicality, and robustness.In response to these challenges, in this paper we address theoretical and practical problem related to both mutual localization and swarm planning.Firstly, we propose a certifiable mutual localization algorithm.It features a concise problem formulation coupled with lossless convex relaxation, enabling independence from initial values and globally optimal relative pose recovery.Then, to explore how detection noise and swarm motion influence estimation optimality, we conduct a comprehensive analysis on the interplay between robots' mutual spatial relationship and mutual localization. We develop a differentiable metric correlated with swarm trajectories to explicitly evaluate the noise resistance of optimal estimation.By establishing a finite and pre-computable threshold for this metric and accordingly generating swarm trajectories, the estimation optimality can be strictly guaranteed under arbitrary noise. Based on these findings, an optimization-based swarm planner is proposed to generate safe and smooth trajectories, with consideration of both inter-robot visibility and estimation optimality.Through numerical simulations, we evaluate the optimality and certifiablity of our estimator, and underscore the significance of our planner in enhancing estimation performance.The results exhibit considerable potential of our methods to pave the way for advanced closed-loop intelligence in swarm systems. ",
        "title": "Certifiable Mutual Localization and Trajectory Planning for  Bearing-Based Robot Swarm",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07787",
        "abstract_url": "http://arxiv.org/abs/2401.07787",
        "authors": [
            {
                "last_name": "Fleischhacker",
                "first_name": "David"
            },
            {
                "last_name": "Goederle",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Kern",
                "first_name": "Roman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This paper addresses a major challenge to historical research on the 19th century. Large quantities of sources have become digitally available for the first time, while extraction techniques are lagging behind. Therefore, we researched machine learning (ML) models to recognise and extract complex data structures in a high-value historical primary source, the Schematismus. It records every single person in the Habsburg civil service above a certain hierarchical level between 1702 and 1918 and documents the genesis of the central administration over two centuries. Its complex and intricate structure as well as its enormous size have so far made any more comprehensive analysis of the administrative and social structure of the later Habsburg Empire on the basis of this source impossible. We pursued two central objectives: Primarily, the improvement of the OCR quality, for which we considered an improved structure recognition to be essential; in the further course, it turned out that this also made the extraction of the data structure possible. We chose Faster R-CNN as base for the ML architecture for structure recognition. In order to obtain the required amount of training data quickly and economically, we synthesised Hof- und Staatsschematismus-style data, which we used to train our model. The model was then fine-tuned with a smaller set of manually annotated historical source data. We then used Tesseract-OCR, which was further optimised for the style of our documents, to complete the combined structure extraction and OCR process. Results show a significant decrease in the two standard parameters of OCR-performance, WER and CER (where lower values are better). Combined structure detection and fine-tuned OCR improved CER and WER values by remarkable 71.98 percent (CER) respectively 52.49 percent (WER). ",
        "title": "Improving OCR Quality in 19th Century Historical Documents Using a  Combined Machine Learning Based Approach",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07788",
        "abstract_url": "http://arxiv.org/abs/2401.07788",
        "authors": [
            {
                "last_name": "Rudakov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Beznosikov",
                "first_name": "Aleksandr"
            },
            {
                "last_name": "Kholodov",
                "first_name": "Yaroslav"
            },
            {
                "last_name": "Gasnikov",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\\%$ is the lowest TopK compression level, which does not harm model convergence severely. Experiments also show that models trained with TopK perform well only when compression is also applied during inference. We find that error feedback techniques do not improve model-parallel training compared to plain compression, but allow model inference without compression with almost no quality drop. Finally, when applied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens model performance significantly. ",
        "title": "Activations and Gradients Compression for Model-Parallel Training",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07793",
        "abstract_url": "http://arxiv.org/abs/2401.07793",
        "authors": [
            {
                "last_name": "Shao",
                "first_name": "Ninglu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Shitao"
            },
            {
                "last_name": "Liu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peitian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) are in need of sufficient contexts to handle many critical applications, such as retrieval augmented generation and few-shot learning. However, due to the constrained window size, the LLMs can only access to the information within a limited context. Although the size of context window can be extended by fine-tuning, it will result in a substantial cost in both training and inference stage. In this paper, we present Extensible Tokenization as an alternative method which realizes the flexible scaling of LLMs' context. Extensible Tokenization stands as a midware in between of the tokenized context and the LLM, which transforms the raw token embeddings into the extensible embeddings. Such embeddings provide a more compact representation for the long context, on top of which the LLM is able to perceive more information with the same context window. Extensible Tokenization is also featured by its flexibility: the scaling factor can be flexibly determined within a feasible scope, leading to the extension of an arbitrary context length at the inference time. Besides, Extensible Tokenization is introduced as a drop-in component, which can be seamlessly plugged into not only the LLM itself and but also its fine-tuned derivatives, bringing in the extended contextual information while fully preserving the LLM's existing capabilities. We perform comprehensive experiments on long-context language modeling and understanding tasks, which verify Extensible Tokenization as an effective, efficient, flexible, and compatible method to extend LLM's context. Our model and source code will be made publicly available. ",
        "title": "Flexibly Scaling Large Language Models Contexts Through Extensible  Tokenization",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07796",
        "abstract_url": "http://arxiv.org/abs/2401.07796",
        "authors": [
            {
                "last_name": "Painchaud",
                "first_name": "Nathan"
            },
            {
                "last_name": "Courand",
                "first_name": "Pierre-Yves"
            },
            {
                "last_name": "Jodoin",
                "first_name": "Pierre-Marc"
            },
            {
                "last_name": "Duchateau",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Bernard",
                "first_name": "Olivier"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Deep learning now enables automatic and robust extraction of cardiac function descriptors from echocardiographic sequences, such as ejection fraction or strain. These descriptors provide fine-grained information that physicians consider, in conjunction with more global variables from the clinical record, to assess patients' condition. Drawing on novel transformer models applied to tabular data (e.g., variables from electronic health records), we propose a method that considers all descriptors extracted from medical records and echocardiograms to learn the representation of a difficult-to-characterize cardiovascular pathology, namely hypertension. Our method first projects each variable into its own representation space using modality-specific approaches. These standardized representations of multimodal data are then fed to a transformer encoder, which learns to merge them into a comprehensive representation of the patient through a pretext task of predicting a clinical rating. This pretext task is formulated as an ordinal classification to enforce a pathological continuum in the representation space. We observe the major trends along this continuum for a cohort of 239 hypertensive patients to describe, with unprecedented gradation, the effect of hypertension on a number of cardiac function descriptors. Our analysis shows that i) pretrained weights from a foundation model allow to reach good performance (83% accuracy) even with limited data (less than 200 training samples), ii) trends across the population are reproducible between trainings, and iii) for descriptors whose interactions with hypertension are well documented, patterns are consistent with prior physiological knowledge. ",
        "title": "Fusing Echocardiography Images and Medical Records for Continuous  Patient Stratification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07801",
        "abstract_url": "http://arxiv.org/abs/2401.07801",
        "authors": [
            {
                "last_name": "Ghari",
                "first_name": "Bahareh"
            },
            {
                "last_name": "Tourani",
                "first_name": "Ali"
            },
            {
                "last_name": "Shahbahrami",
                "first_name": "Asadollah"
            },
            {
                "last_name": "Gaydadjiev",
                "first_name": "Georgi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Pedestrian detection remains a critical problem in various domains, such as computer vision, surveillance, and autonomous driving. In particular, accurate and instant detection of pedestrians in low-light conditions and reduced visibility is of utmost importance for autonomous vehicles to prevent accidents and save lives. This paper aims to comprehensively survey various pedestrian detection approaches, baselines, and datasets that specifically target low-light conditions. The survey discusses the challenges faced in detecting pedestrians at night and explores state-of-the-art methodologies proposed in recent years to address this issue. These methodologies encompass a diverse range, including deep learning-based, feature-based, and hybrid approaches, which have shown promising results in enhancing pedestrian detection performance under challenging lighting conditions. Furthermore, the paper highlights current research directions in the field and identifies potential solutions that merit further investigation by researchers. By thoroughly examining pedestrian detection techniques in low-light conditions, this survey seeks to contribute to the advancement of safer and more reliable autonomous driving systems and other applications related to pedestrian safety. Accordingly, most of the current approaches in the field use deep learning-based image fusion methodologies (i.e., early, halfway, and late fusion) for accurate and reliable pedestrian detection. Moreover, the majority of the works in the field (approximately 48%) have been evaluated on the KAIST dataset, while the real-world video feeds recorded by authors have been used in less than six percent of the works. ",
        "title": "Pedestrian Detection in Low-Light Conditions: A Comprehensive Survey",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07803",
        "abstract_url": "http://arxiv.org/abs/2401.07803",
        "authors": [
            {
                "last_name": "Reich",
                "first_name": "Daniel"
            },
            {
                "last_name": "Schultz",
                "first_name": "Tanja"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model's reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits.   In this work, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that the potential benefits of these methods are severely underestimated as a result. ",
        "title": "Uncovering the Full Potential of Visual Grounding Methods in VQA",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07805",
        "abstract_url": "http://arxiv.org/abs/2401.07805",
        "authors": [
            {
                "last_name": "Schreter-Fleischhacker",
                "first_name": "Magdalena"
            },
            {
                "last_name": "Munch",
                "first_name": "Peter"
            },
            {
                "last_name": "Much",
                "first_name": "Nils"
            },
            {
                "last_name": "Kronbichler",
                "first_name": "Martin"
            },
            {
                "last_name": "Wall",
                "first_name": "Wolfgang A."
            },
            {
                "last_name": "Meier",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  We present accurate and mathematically consistent formulations of a diffuse-interface model for two-phase flow problems involving rapid evaporation. The model addresses challenges including discontinuities in the density field by several orders of magnitude, leading to high velocity and pressure jumps across the liquid-vapor interface, along with dynamically changing interface topologies. To this end, we integrate an incompressible Navier--Stokes solver combined with a conservative level-set formulation and a regularized, i.e., diffuse, representation of discontinuities into a matrix-free adaptive finite element framework. The achievements are three-fold: First, this work proposes mathematically consistent definitions for the level-set transport velocity in the diffuse interface region by extrapolating the velocity from the liquid or gas phase, which exhibit superior prediction accuracy for the evaporated mass and the resulting interface dynamics compared to a local velocity evaluation, especially for highly curved interfaces. Second, we show that accurate prediction of the evaporation-induced pressure jump requires a consistent, namely a reciprocal, density interpolation across the interface, which satisfies local mass conservation. Third, the combination of diffuse interface models for evaporation with standard Stokes-type constitutive relations for viscous flows leads to significant pressure artifacts in the diffuse interface region. To mitigate these, we propose a modification for such constitutive model types. Through selected analytical and numerical examples, the aforementioned properties are validated. The presented model promises new insights in simulation-based prediction of melt-vapor interactions in thermal multiphase flows such as in laser-based powder bed fusion of metals. ",
        "title": "A consistent diffuse-interface model for two-phase flow problems with  rapid evaporation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07806",
        "abstract_url": "http://arxiv.org/abs/2401.07806",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Ruhui"
            },
            {
                "last_name": "Guerra",
                "first_name": "Martin"
            },
            {
                "last_name": "Li",
                "first_name": "Qin"
            },
            {
                "last_name": "Wright",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Optimal experimental design (OED) has far-reaching impacts in many scientific domains. We study OED over a continuous-valued design space, a setting that occurs often in practice. Optimization of a distributional function over an infinite-dimensional probability measure space is conceptually distinct from the discrete OED tasks that are conventionally tackled. We propose techniques based on optimal transport and Wasserstein gradient flow. A practical computational approach is derived from the Monte Carlo simulation, which transforms the infinite-dimensional optimization problem to a finite-dimensional problem over Euclidean space, to which gradient descent can be applied. We discuss first-order criticality and study the convexity properties of the OED objective. We apply our algorithm to the tomography inverse problem, where the solution reveals optimal sensor placements for imaging. ",
        "title": "Optimal experimental design via gradient flow",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07807",
        "abstract_url": "http://arxiv.org/abs/2401.07807",
        "authors": [
            {
                "last_name": "Heimann",
                "first_name": "Fabian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a higher order space-time unfitted finite element method for convection-diffusion problems on coupled (surface and bulk) domains. In that way, we combine a method suggested by Heimann, Lehrenfeld, Preu{\\ss} (SIAM J. Sci. Comput. 45(2), 2023, B139 - B165) for the bulk case with a method suggested by Sass, Reusken (Comput. Math. Appl. 146(15), 2023, 253-270) for the surface case. The geometry is allowed to change with time, and the higher order discrete approximation of this geometry is ensured by a time-dependent isoparametric mapping. The space-time discretisation approach allows for straightforward handling of arbitrary high orders. In that way, we also generalise results of Hansbo, Larson, Zahedi (Comput. Methods Appl. Mech. Engrg. 307, 2016, 96-116) to higher orders. The convergence of the proposed higher order discretisations is confirmed numerically. ",
        "title": "A Higher Order Unfitted Space-Time Finite Element Method for Coupled  Surface-Bulk problems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07809",
        "abstract_url": "http://arxiv.org/abs/2401.07809",
        "authors": [
            {
                "last_name": "Medyakov",
                "first_name": "Daniil"
            },
            {
                "last_name": "Molodtsov",
                "first_name": "Gleb"
            },
            {
                "last_name": "Beznosikov",
                "first_name": "Aleksandr"
            },
            {
                "last_name": "Gasnikov",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The distributed optimization problem has become increasingly relevant recently. It has a lot of advantages such as processing a large amount of data in less time compared to non-distributed methods. However, most distributed approaches suffer from a significant bottleneck - the cost of communications. Therefore, a large amount of research has recently been directed at solving this problem. One such approach uses local data similarity. In particular, there exists an algorithm provably optimally exploiting the similarity property. But this result, as well as results from other works solve the communication bottleneck by focusing only on the fact that communication is significantly more expensive than local computing and does not take into account the various capacities of network devices and the different relationship between communication time and local computing expenses. We consider this setup and the objective of this study is to achieve an optimal ratio of distributed data between the server and local machines for any costs of communications and local computations. The running times of the network are compared between uniform and optimal distributions. The superior theoretical performance of our solutions is experimentally validated. ",
        "title": "Optimal Data Splitting in Distributed Optimization for Machine Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07810",
        "abstract_url": "http://arxiv.org/abs/2401.07810",
        "authors": [
            {
                "last_name": "Saha",
                "first_name": "Sougata"
            },
            {
                "last_name": "Srihari",
                "first_name": "Rohini"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counter-arguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristics-based qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora. ",
        "title": "Consolidating Strategies for Countering Hate Speech Using Persuasive  Dialogues",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07812",
        "abstract_url": "http://arxiv.org/abs/2401.07812",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Diefenbach",
                "first_name": "Dennis"
            },
            {
                "last_name": "Gourru",
                "first_name": "Antoine"
            },
            {
                "last_name": "Gravier",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Wikidata has grown to a knowledge graph with an impressive size. To date, it contains more than 17 billion triples collecting information about people, places, films, stars, publications, proteins, and many more. On the other side, most of the information on the Web is not published in highly structured data repositories like Wikidata, but rather as unstructured and semi-structured content, more concretely in HTML pages containing text and tables. Finding, monitoring, and organizing this data in a knowledge graph is requiring considerable work from human editors. The volume and complexity of the data make this task difficult and time-consuming. In this work, we present a framework that is able to identify and extract new facts that are published under multiple Web domains so that they can be proposed for validation by Wikidata editors. The framework is relying on question-answering technologies. We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages. For achieving this, we demonstrate that language models can be adapted to extract facts not only from textual collections but also from Web pages. By exploiting the information already contained in Wikidata the proposed framework can be trained without the need for any additional learning signals and can extract new facts for a wide range of properties and domains. Following this path, Wikidata can be used as a seed to extract facts on the Web. Our experiments show that we can achieve a mean performance of 84.07 at F1-score. Moreover, our estimations show that we can potentially extract millions of facts that can be proposed for human validation. The goal is to help editors in their daily tasks and contribute to the completion of the Wikidata knowledge graph. ",
        "title": "Wikidata as a seed for Web Extraction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07815",
        "abstract_url": "http://arxiv.org/abs/2401.07815",
        "authors": [
            {
                "last_name": "Card\u00f3",
                "first_name": "Carles"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  Context-free languages can be characterized in several ways. This article studies projective linearisations of languages of simple dependency trees, i.e., dependency trees in which a node can govern at most one node with a given syntactic function. We prove that the projective linearisations of local languages of simple dependency trees coincide with the context-free languages.   Simple dependency trees suggest alternative dual notions of locality and projectivity, which permits defining a dual language for each context-free language. We call this new class of languages anti-context-free. These languages are related to some linguistic constructions exhibiting the so-called cross-serial dependencies that were historically important for the development of computational linguistics. We propose that this duality could be a relevant linguistic phenomenon. ",
        "title": "Anti-Context-Free languages",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07817",
        "abstract_url": "http://arxiv.org/abs/2401.07817",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            },
            {
                "last_name": "Yuan",
                "first_name": "Fei"
            },
            {
                "last_name": "She",
                "first_name": "Shuaijie"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Birch",
                "first_name": "Alexandra"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of chain-of-thought and mathematical reasoning instructions. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English question data. In this way we perform targetted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3\\% and 16.1\\% accuracy across ten languages on the MGSM and MSVAMP maths reasoning benchmarks (The project will be available at: https://github.com/NJUNLP/QAlign). ",
        "title": "Question Translation Training for Better Multilingual Reasoning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07819",
        "abstract_url": "http://arxiv.org/abs/2401.07819",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Zhongjie"
            },
            {
                "last_name": "De Persis",
                "first_name": "Claudio"
            },
            {
                "last_name": "Tesi",
                "first_name": "Pietro"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present data-based conditions for enforcing contractivity via feedback control and obtain desired asymptotic properties of the closed-loop system. We focus on unknown nonlinear control systems whose vector fields are expressible via a dictionary of functions and derive data-dependent semidefinite programs whose solution returns the controller that guarantees contractivity. When data are perturbed by disturbances that are linear combination of sinusoids of known frequencies (but unknown amplitude and phase) and constants, we remarkably obtain conditions for contractivity that do not depend on the magnitude of the disturbances, with imaginable positive consequences for the synthesis of the controller. Finally, we show how to design from data an integral controller for nonlinear systems that achieves constant reference tracking and constant disturbance rejection. ",
        "title": "Enforcing contraction via data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07823",
        "abstract_url": "http://arxiv.org/abs/2401.07823",
        "authors": [
            {
                "last_name": "Febrianto",
                "first_name": "Eky"
            },
            {
                "last_name": "Sistek",
                "first_name": "Jakub"
            },
            {
                "last_name": "Kus",
                "first_name": "Pavel"
            },
            {
                "last_name": "Kecman",
                "first_name": "Matija"
            },
            {
                "last_name": "Cirak",
                "first_name": "Fehmi"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The automated finite element analysis of complex CAD models using boundary-fitted meshes is rife with difficulties. Immersed finite element methods are intrinsically more robust but usually less accurate. In this work, we introduce an efficient, robust, high-order immersed finite element method for complex CAD models. Our approach relies on three adaptive structured grids: a geometry grid for representing the implicit geometry, a finite element grid for discretising physical fields and a quadrature grid for evaluating the finite element integrals. The geometry grid is a sparse VDB (Volumetric Dynamic B+ tree) grid that is highly refined close to physical domain boundaries. The finite element grid consists of a forest of octree grids distributed over several processors, and the quadrature grid in each finite element cell is an octree grid constructed in a bottom-up fashion. We discretise physical fields on the finite element grid using high-order Lagrange basis functions. The resolution of the quadrature grid ensures that finite element integrals are evaluated with sufficient accuracy and that any sub-grid geometric features, like small holes or corners, are resolved up to a desired resolution. The conceptual simplicity and modularity of our approach make it possible to reuse open-source libraries, i.e. openVDB and p4est for implementing the geometry and finite element grids, respectively, and BDDCML for iteratively solving the discrete systems of equations in parallel using domain decomposition. We demonstrate the efficiency and robustness of the proposed approach by solving the Poisson equation on domains given by complex CAD models and discretised with tens of millions of degrees of freedom. ",
        "title": "A three-grid high-order immersed finite element method for the analysis  of CAD models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07835",
        "abstract_url": "http://arxiv.org/abs/2401.07835",
        "authors": [
            {
                "last_name": "Baghban",
                "first_name": "Akram"
            },
            {
                "last_name": "Newman",
                "first_name": "Marc"
            },
            {
                "last_name": "Horlemann",
                "first_name": "Anna-Lena"
            },
            {
                "last_name": "Ghiyasvand",
                "first_name": "Mehdi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work focuses on sequential locally recoverable codes (SLRCs), a special family of locally repairable codes, capable of correcting multiple code symbol erasures, which are commonly used for distributed storage systems. First, we construct an extended $q$-ary family of non-binary SLRCs using code products with a novel maximum number of recoverable erasures $t$ and a minimal repair alternativity $A$. Second, we study how MDS and BCH codes can be used to construct $q$-ary SLRCs. Finally, we compare our codes to other LRCs. ",
        "title": "$q$-ary Sequential Locally Recoverable Codes from the Product  Construction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07836",
        "abstract_url": "http://arxiv.org/abs/2401.07836",
        "authors": [
            {
                "last_name": "Kasirzadeh",
                "first_name": "Atoosa"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional \"decisive AI x-risk hypothesis\" with an \"accumulative AI x-risk hypothesis.\" While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic erosion of econopolitical structures. The accumulative hypothesis suggests a boiling frog scenario where incremental AI risks slowly converge, undermining resilience until a triggering event results in irreversible collapse. Through systems analysis, this paper examines the distinct assumptions differentiating these two hypotheses. It is then argued that the accumulative view reconciles seemingly incompatible perspectives on AI risks. The implications of differentiating between these causal pathways -- the decisive and the accumulative -- for the governance of AI risks as well as long-term AI safety are discussed. ",
        "title": "Two Types of AI Existential Risk: Decisive and Accumulative",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07841",
        "abstract_url": "http://arxiv.org/abs/2401.07841",
        "authors": [
            {
                "last_name": "Deubert",
                "first_name": "Darius"
            },
            {
                "last_name": "Klingel",
                "first_name": "Lars"
            },
            {
                "last_name": "Selig",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The importance of simulation at machine level in industrial environments is steadily increasing especially in the design and commissioning phase. Using models during the operation phase together with the real machine or plant is referred to as online simulation. Online simulation is used for system monitoring, predictive analyses, decision support or online optimization and therefore has various advantages and a wide field of applications. This paper has the aim to characterize online simulation at machine level in industrial automation focusing on key technologies and common applications. Therefore, a set of 65 relevant publications, which are focusing on this subject, is found by database search, expert consultation, and snowballing. As key technological aspects, the used model types, interfaces and platforms, and the aspects of initialization and synchronization are further investigated. The results are interpreted and limitations, knowledge gaps and future prospects are discussed. The potential of online simulation at machine level especially arises due to the increasing availability of component and machine models from the design and commissioning phase, which can be reused for online simulation. Remaining challenges are identified concerning implementation, simulation platforms, model maintenance and especially in the field of synchronization. ",
        "title": "Online Simulation at Machine Level: A Systematic Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07842",
        "abstract_url": "http://arxiv.org/abs/2401.07842",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Peng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Recent advancements in low-Earth orbit (LEO) satellites represented by large constellations and advanced payloads provide great promises for enabling beyond 5G and 6G telecommunications and high-quality and ubiquitous Internet connectivity to everyone anywhere on Earth. LEO satellite networks are envisioned to bridge the urban-rural connectivity gap for the digital divide. However, the digital divide can hardly be closed by only providing connectivity to rural and remote areas. Various unprecedented challenges brought by the emerging satellite Internet still need to be resolved, such as inconsistent end-to-end performance guarantees and a lack of efficient management and operations in these areas, which are referred to as \"performance gap\" and \"management gap\", respectively. This position paper will briefly discuss these gaps, approaches to addressing the gaps, and some research directions based on our recent works. ",
        "title": "Closing the Performance and Management Gaps with Satellite Internet:  Challenges, Approaches, and Future Directions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07844",
        "abstract_url": "http://arxiv.org/abs/2401.07844",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Shuze"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuhang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shangtong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible. ",
        "title": "The ODE Method for Stochastic Approximation and Reinforcement Learning  with Markovian Noise",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07847",
        "abstract_url": "http://arxiv.org/abs/2401.07847",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Saptarshi"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Shreya"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            },
            {
                "last_name": "Tamiti",
                "first_name": "Tarikul Islam"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Sentiment Analysis (SA) refers to the task of associating a view polarity (usually, positive, negative, or neutral; or even fine-grained such as slightly angry, sad, etc.) to a given text, essentially breaking it down to a supervised (since we have the view labels apriori) classification task. Although heavily studied in resource-rich languages such as English thus pushing the SOTA by leaps and bounds, owing to the arrival of the Transformer architecture, the same cannot be said for resource-poor languages such as Bengali (BN). For a language spoken by roughly 300 million people, the technology enabling them to run trials on their favored tongue is severely lacking. In this paper, we analyze the SOTA for SA in Bengali, particularly, Transformer-based models. We discuss available datasets, their drawbacks, the nuances associated with Bengali i.e. what makes this a challenging language to apply SA on, and finally provide insights for future direction to mitigate the limitations in the field. ",
        "title": "Milestones in Bengali Sentiment Analysis leveraging Transformer-models:  Fundamentals, Challenges and Future Directions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07849",
        "abstract_url": "http://arxiv.org/abs/2401.07849",
        "authors": [
            {
                "last_name": "Fejgin",
                "first_name": "Daniel"
            },
            {
                "last_name": "Hadad",
                "first_name": "Elior"
            },
            {
                "last_name": "Gannot",
                "first_name": "Sharon"
            },
            {
                "last_name": "Koldovsk\u00fd",
                "first_name": "Zbyn\u011bk"
            },
            {
                "last_name": "Doclo",
                "first_name": "Simon"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  To estimate the direction of arrival (DOA) of multiple speakers with methods that use prototype transfer functions, frequency-dependent spatial spectra (SPS) are usually constructed. To make the DOA estimation robust, SPS from different frequencies can be combined. According to how the SPS are combined, frequency fusion mechanisms are categorized into narrowband, broadband, or speaker-grouped, where the latter mechanism requires a speaker-wise grouping of frequencies. For a binaural hearing aid setup, in this paper we propose an interaural time difference (ITD)-based speaker-grouped frequency fusion mechanism. By exploiting the DOA dependence of ITDs, frequencies can be grouped according to a common ITD and be used for DOA estimation of the respective speaker. We apply the proposed ITD-based speaker-grouped frequency fusion mechanism for different DOA estimation methods, namely the multiple signal classification, steered response power and a recently published method based on relative transfer function (RTF) vectors. In our experiments, we compare DOA estimation with different fusion mechanisms. For all considered DOA estimation methods, the proposed ITD-based speaker-grouped frequency fusion mechanism results in a higher DOA estimation accuracy compared with the narrowband and broadband fusion mechanisms. ",
        "title": "Comparison of Frequency-Fusion Mechanisms for Binaural  Direction-of-Arrival Estimation for Multiple Speakers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07851",
        "abstract_url": "http://arxiv.org/abs/2401.07851",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Heming"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Dong",
                "first_name": "Qingxiu"
            },
            {
                "last_name": "Wang",
                "first_name": "Peiyi"
            },
            {
                "last_name": "Li",
                "first_name": "Yongqi"
            },
            {
                "last_name": "Ge",
                "first_name": "Tao"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Sui",
                "first_name": "Zhifang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first efficiently drafts several future tokens and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, including current leading techniques, the challenges faced, and potential future directions in this field. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference. ",
        "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive  Survey of Speculative Decoding",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07853",
        "abstract_url": "http://arxiv.org/abs/2401.07853",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Rongyu"
            },
            {
                "last_name": "Cai",
                "first_name": "Zefan"
            },
            {
                "last_name": "Yang",
                "first_name": "Huanrui"
            },
            {
                "last_name": "Liu",
                "first_name": "Zidong"
            },
            {
                "last_name": "Gudovskiy",
                "first_name": "Denis"
            },
            {
                "last_name": "Okuno",
                "first_name": "Tomoyuki"
            },
            {
                "last_name": "Nakata",
                "first_name": "Yohei"
            },
            {
                "last_name": "Keutzer",
                "first_name": "Kurt"
            },
            {
                "last_name": "Chang",
                "first_name": "Baobao"
            },
            {
                "last_name": "Du",
                "first_name": "Yuan"
            },
            {
                "last_name": "Du",
                "first_name": "Li"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shanghang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Finetuning a pretrained vision model (PVM) is a common technique for learning downstream vision tasks. The conventional finetuning process with the randomly sampled data points results in diminished training efficiency. To address this drawback, we propose a novel approach, VLM-empowered Collaborative Active Finetuning (VeCAF). VeCAF optimizes a parametric data selection model by incorporating the training objective of the model being tuned. Effectively, this guides the PVM towards the performance goal with improved data and computational efficiency. As vision-language models (VLMs) have achieved significant advancements by establishing a robust connection between image and language domains, we exploit the inherent semantic richness of the text embedding space and utilize text embedding of pretrained VLM models to augment PVM image features for better data selection and finetuning. Furthermore, the flexibility of text-domain augmentation gives VeCAF a unique ability to handle out-of-distribution scenarios without external augmented data. Extensive experiments show the leading performance and high efficiency of VeCAF that is superior to baselines in both in-distribution and out-of-distribution image classification tasks. On ImageNet, VeCAF needs up to 3.3x less training batches to reach the target performance compared to full finetuning and achieves 2.8% accuracy improvement over SOTA methods with the same number of batches. ",
        "title": "VeCAF: VLM-empowered Collaborative Active Finetuning with Training  Objective Awareness",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07854",
        "abstract_url": "http://arxiv.org/abs/2401.07854",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Quan"
            },
            {
                "last_name": "Yao",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Yao",
                "first_name": "Lisha"
            },
            {
                "last_name": "Chen",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jingren"
            },
            {
                "last_name": "Lu",
                "first_name": "Le"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ling"
            },
            {
                "last_name": "Liu",
                "first_name": "Zaiyi"
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Colorectal cancer (CRC) micro-satellite instability (MSI) prediction on histopathology images is a challenging weakly supervised learning task that involves multi-instance learning on gigapixel images. To date, radiology images have proven to have CRC MSI information and efficient patient imaging techniques. Different data modalities integration offers the opportunity to increase the accuracy and robustness of MSI prediction. Despite the progress in representation learning from the whole slide images (WSI) and exploring the potential of making use of radiology data, CRC MSI prediction remains a challenge to fuse the information from multiple data modalities (e.g., pathology WSI and radiology CT image). In this paper, we propose $M^{2}$Fusion: a Bayesian-based multimodal multi-level fusion pipeline for CRC MSI. The proposed fusion model $M^{2}$Fusion is capable of discovering more novel patterns within and across modalities that are beneficial for predicting MSI than using a single modality alone, as well as other fusion methods. The contribution of the paper is three-fold: (1) $M^{2}$Fusion is the first pipeline of multi-level fusion on pathology WSI and 3D radiology CT image for MSI prediction; (2) CT images are the first time integrated into multimodal fusion for CRC MSI prediction; (3) feature-level fusion strategy is evaluated on both Transformer-based and CNN-based method. Our approach is validated on cross-validation of 352 cases and outperforms either feature-level (0.8177 vs. 0.7908) or decision-level fusion strategy (0.8177 vs. 0.7289) on AUC score. ",
        "title": "$M^{2}$Fusion: Bayesian-based Multimodal Multi-level Fusion on  Colorectal Cancer Microsatellite Instability Prediction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07855",
        "abstract_url": "http://arxiv.org/abs/2401.07855",
        "authors": [
            {
                "last_name": "Suulker",
                "first_name": "Cem"
            },
            {
                "last_name": "Skach",
                "first_name": "Sophie"
            },
            {
                "last_name": "Kaleel",
                "first_name": "Danyaal"
            },
            {
                "last_name": "Abrar",
                "first_name": "Taqi"
            },
            {
                "last_name": "Murtaza",
                "first_name": "Zain"
            },
            {
                "last_name": "Suulker",
                "first_name": "Dilara"
            },
            {
                "last_name": "Althoefer",
                "first_name": "Kaspar"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Here we present a flexible tip mount for eversion (vine) robots. This soft cap allows attaching a payload to an eversion robot while allowing moving through narrow openings, as well as the eversion of protruding objects, and expanded surfaces. ",
        "title": "Deformable Tip Mount for Soft Growing Eversion Robots",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07856",
        "abstract_url": "http://arxiv.org/abs/2401.07856",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Bijie"
            },
            {
                "last_name": "Lee",
                "first_name": "Ryan"
            },
            {
                "last_name": "Li",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Gan",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuntian"
            },
            {
                "last_name": "Jarrahi",
                "first_name": "Mona"
            },
            {
                "last_name": "Ozcan",
                "first_name": "Aydogan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Data protection methods like cryptography, despite being effective, inadvertently signal the presence of secret communication, thereby drawing undue attention. Here, we introduce an optical information hiding camera integrated with an electronic decoder, optimized jointly through deep learning. This information hiding-decoding system employs a diffractive optical processor as its front-end, which transforms and hides input images in the form of ordinary-looking patterns that deceive/mislead human observers. This information hiding transformation is valid for infinitely many combinations of secret messages, all of which are transformed into ordinary-looking output patterns, achieved all-optically through passive light-matter interactions within the optical processor. By processing these ordinary-looking output images, a jointly-trained electronic decoder neural network accurately reconstructs the original information hidden within the deceptive output pattern. We numerically demonstrated our approach by designing an information hiding diffractive camera along with a jointly-optimized convolutional decoder neural network. The efficacy of this system was demonstrated under various lighting conditions and noise levels, showing its robustness. We further extended this information hiding camera to multi-spectral operation, allowing the concealment and decoding of multiple images at different wavelengths, all performed simultaneously in a single feed-forward operation. The feasibility of our framework was also demonstrated experimentally using THz radiation. This optical encoder-electronic decoder-based co-design provides a novel information hiding camera interface that is both high-speed and energy-efficient, offering an intriguing solution for visual information security. ",
        "title": "Information hiding cameras: optical concealment of object information  into ordinary images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07861",
        "abstract_url": "http://arxiv.org/abs/2401.07861",
        "authors": [
            {
                "last_name": "Fernandes",
                "first_name": "Joao B."
            },
            {
                "last_name": "da Silva",
                "first_name": "Felipe H. S."
            },
            {
                "last_name": "Xavier-de-Souza",
                "first_name": "Samuel"
            },
            {
                "last_name": "Assis",
                "first_name": "Italo A. S."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Programs with high levels of complexity often face challenges in adjusting execution parameters, particularly when these parameters vary based on the execution context. These dynamic parameters significantly impact the program's performance, such as loop granularity, which can vary depending on factors like the execution environment, program input, or the choice of compiler. Given the expensive nature of testing each case individually, one viable solution is to automate parameter adjustments using optimization methods. This article introduces PATSMA, a parameter auto-tuning tool that leverages Coupled Simulated Annealing (CSA) and Nelder-Mead (NM) optimization methods to fine-tune existing parameters in an application. We demonstrate how auto-tuning can contribute to the real-time optimization of parallel algorithms designed for shared memory systems. PATSMA is a C++ library readily available under the MIT license. ",
        "title": "PATSMA: Parameter Auto-tuning for Shared Memory Algorithms",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07862",
        "abstract_url": "http://arxiv.org/abs/2401.07862",
        "authors": [
            {
                "last_name": "Lamarque",
                "first_name": "Maxence"
            },
            {
                "last_name": "Bhan",
                "first_name": "Luke"
            },
            {
                "last_name": "Shi",
                "first_name": "Yuanyuan"
            },
            {
                "last_name": "Krstic",
                "first_name": "Miroslav"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  To stabilize PDEs, feedback controllers require gain kernel functions, which are themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on the PDE plants' functional coefficients. The functional coefficients in PDE plants are often unknown. This requires an adaptive approach to PDE control, i.e., an estimation of the plant coefficients conducted concurrently with control, where a separate PDE for the gain kernel must be solved at each timestep upon the update in the plant coefficient function estimate. Solving a PDE at each timestep is computationally expensive and a barrier to the implementation of real-time adaptive control of PDEs. Recently, results in neural operator (NO) approximations of functional mappings have been introduced into PDE control, for replacing the computation of the gain kernel with a neural network that is trained, once offline, and reused in real-time for rapid solution of the PDEs. In this paper, we present the first result on applying NOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with recirculation. We establish global stabilization via Lyapunov analysis, in the plant and parameter error states, and also present an alternative approach, via passive identifiers, which avoids the strong assumptions on kernel differentiability. We then present numerical simulations demonstrating stability and observe speedups up to three orders of magnitude, highlighting the real-time efficacy of neural operators in adaptive control. Our code (Github) is made publicly available for future researchers. ",
        "title": "Adaptive Neural-Operator Backstepping Control of a Benchmark Hyperbolic  PDE",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07867",
        "abstract_url": "http://arxiv.org/abs/2401.07867",
        "authors": [
            {
                "last_name": "Macko",
                "first_name": "Dominik"
            },
            {
                "last_name": "Moro",
                "first_name": "Robert"
            },
            {
                "last_name": "Uchendu",
                "first_name": "Adaku"
            },
            {
                "last_name": "Srba",
                "first_name": "Ivan"
            },
            {
                "last_name": "Lucas",
                "first_name": "Jason Samuel"
            },
            {
                "last_name": "Yamashita",
                "first_name": "Michiharu"
            },
            {
                "last_name": "Tripto",
                "first_name": "Nafis Irtiza"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongwon"
            },
            {
                "last_name": "Simko",
                "first_name": "Jakub"
            },
            {
                "last_name": "Bielikova",
                "first_name": "Maria"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  High-quality text generation capability of latest Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause detection evasion in all tested languages, where homoglyph attacks are especially successful. ",
        "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07868",
        "abstract_url": "http://arxiv.org/abs/2401.07868",
        "authors": [
            {
                "last_name": "Sakib",
                "first_name": "Md Sadman"
            },
            {
                "last_name": "Sun",
                "first_name": "Yu"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CL"
        ],
        "abstract": "  The inherent probabilistic nature of Large Language Models (LLMs) introduces an element of unpredictability, raising concerns about potential discrepancies in their output. This paper introduces an innovative approach aims to generate correct and optimal robotic task plans for diverse real-world demands and scenarios. LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps. The proposed approach uses LLM to generate a number of task plans as trees and amalgamates them into a graph by removing questionable paths. Then an optimal task tree can be retrieved to circumvent questionable and high-cost nodes, thereby improving planning accuracy and execution efficiency. The approach is further improved by incorporating a large knowledge network. Leveraging GPT-4 further, the high-level task plan is converted into a low-level Planning Domain Definition Language (PDDL) plan executable by a robot. Evaluation results highlight the superior accuracy and efficiency of our approach compared to previous methodologies in the field of task planning. ",
        "title": "Consolidating Trees of Robotic Plans Generated Using Large Language  Models to Improve Reliability",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07870",
        "abstract_url": "http://arxiv.org/abs/2401.07870",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Mouxiang"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhongxin"
            },
            {
                "last_name": "Ren",
                "first_name": "Xiaoxue"
            },
            {
                "last_name": "Sun",
                "first_name": "Jianling"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel modelagnostic framework that enables online modification and non-sequential generation to augment the code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an infill-first, judge-later strategy, which experiments with filling at the $k$ most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple benchmarks consistently indicate significant improvements over all baselines. Notably, JumpCoder assists code LLMs in achieving up to a 3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the multilingual HumanEval benchmarks. Our code is public at https://github.com/Keytoyze/JumpCoder. ",
        "title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07871",
        "abstract_url": "http://arxiv.org/abs/2401.07871",
        "authors": [
            {
                "last_name": "Cummins",
                "first_name": "Logan"
            },
            {
                "last_name": "Sommers",
                "first_name": "Alex"
            },
            {
                "last_name": "Ramezani",
                "first_name": "Somayeh Bakhtiari"
            },
            {
                "last_name": "Mittal",
                "first_name": "Sudip"
            },
            {
                "last_name": "Jabour",
                "first_name": "Joseph"
            },
            {
                "last_name": "Seale",
                "first_name": "Maria"
            },
            {
                "last_name": "Rahimi",
                "first_name": "Shahram"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  Predictive maintenance is a well studied collection of techniques that aims to prolong the life of a mechanical system by using artificial intelligence and machine learning to predict the optimal time to perform maintenance. The methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep. As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system. This attracts the field of Explainable AI (XAI) to introduce explainability and interpretability into the predictive system. XAI brings methods to the field of predictive maintenance that can amplify trust in the users while maintaining well-performing systems. This survey on explainable predictive maintenance (XPM) discusses and presents the current methods of XAI as applied to predictive maintenance while following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We categorize the different XPM methods into groups that follow the XAI literature. Additionally, we include current challenges and a discussion on future research directions in XPM. ",
        "title": "Explainable Predictive Maintenance: A Survey of Current Methods,  Challenges and Opportunities",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07872",
        "abstract_url": "http://arxiv.org/abs/2401.07872",
        "authors": [
            {
                "last_name": "Pawar",
                "first_name": "Saurav"
            },
            {
                "last_name": "Tonmoy",
                "first_name": "S. M Towhidul Islam"
            },
            {
                "last_name": "Zaman",
                "first_name": "S M Mehedi"
            },
            {
                "last_name": "Jain",
                "first_name": "Vinija"
            },
            {
                "last_name": "Chadha",
                "first_name": "Aman"
            },
            {
                "last_name": "Das",
                "first_name": "Amitava"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The advent of Large Language Models (LLMs) represents a notable breakthrough in Natural Language Processing (NLP), contributing to substantial progress in both text comprehension and generation. However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation. Understanding and extending the context length for LLMs is crucial in enhancing their performance across various NLP applications. In this survey paper, we delve into the multifaceted aspects of exploring why it is essential, and the potential transformations that superior techniques could bring to NLP applications. We study the inherent challenges associated with extending context length and present an organized overview of the existing strategies employed by researchers. Additionally, we discuss the intricacies of evaluating context extension techniques and highlight the open challenges that researchers face in this domain. Furthermore, we explore whether there is a consensus within the research community regarding evaluation standards and identify areas where further agreement is needed. This comprehensive survey aims to serve as a valuable resource for researchers, guiding them through the nuances of context length extension techniques and fostering discussions on future advancements in this evolving field. ",
        "title": "The What, Why, and How of Context Length Extension Techniques in Large  Language Models -- A Detailed Survey",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07874",
        "abstract_url": "http://arxiv.org/abs/2401.07874",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Z. N. D."
            },
            {
                "last_name": "Hansen",
                "first_name": "A. C."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In deep learning (DL) the instability phenomenon is widespread and well documented, most commonly using the classical measure of stability, the Lipschitz constant. While a small Lipchitz constant is traditionally viewed as guarantying stability, it does not capture the instability phenomenon in DL for classification well. The reason is that a classification function -- which is the target function to be approximated -- is necessarily discontinuous, thus having an 'infinite' Lipchitz constant. As a result, the classical approach will deem every classification function unstable, yet basic classification functions a la 'is there a cat in the image?' will typically be locally very 'flat' -- and thus locally stable -- except at the decision boundary. The lack of an appropriate measure of stability hinders a rigorous theory for stability in DL, and consequently, there are no proper approximation theoretic results that can guarantee the existence of stable networks for classification functions. In this paper we introduce a novel stability measure $\\mathscr{S}(f)$, for any classification function $f$, appropriate to study the stability of discontinuous functions and their approximations. We further prove two approximation theorems: First, for any $\\epsilon > 0$ and any classification function $f$ on a \\emph{compact set}, there is a neural network (NN) $\\psi$, such that $\\psi - f \\neq 0$ only on a set of measure $< \\epsilon$, moreover, $\\mathscr{S}(\\psi) \\geq \\mathscr{S}(f) - \\epsilon$ (as accurate and stable as $f$ up to $\\epsilon$). Second, for any classification function $f$ and $\\epsilon > 0$, there exists a NN $\\psi$ such that $\\psi = f$ on the set of points that are at least $\\epsilon$ away from the decision boundary. ",
        "title": "Do stable neural networks exist for classification problems? -- A new  view on stability in AI",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07875",
        "abstract_url": "http://arxiv.org/abs/2401.07875",
        "authors": [
            {
                "last_name": "Wright",
                "first_name": "Ryan"
            },
            {
                "last_name": "Parekh",
                "first_name": "Sagar"
            },
            {
                "last_name": "White",
                "first_name": "Robin"
            },
            {
                "last_name": "Losey",
                "first_name": "Dylan P."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Labor shortages in the United States are impacting a number of industries including the meat processing sector. Collaborative technologies that work alongside humans while increasing production abilities may support the industry by enhancing automation and improving job quality. However, existing automation technologies used in the meat industry have limited collaboration potential, low flexibility, and high cost. The objective of this work was to explore the use of a robot arm to collaboratively work alongside a human and complete tasks performed in a meat processing facility. Toward this objective, we demonstrated proof-of-concept approaches to ensure human safety while exploring the capacity of the robot arm to perform example meat processing tasks. In support of human safety, we developed a knife instrumentation system to detect when the cutting implement comes into contact with meat within the collaborative space. To demonstrate the capability of the system to flexibly conduct a variety of basic meat processing tasks, we developed vision and control protocols to execute slicing, trimming, and cubing of pork loins. We also collected a subjective evaluation of the actions from experts within the U.S. meat processing industry. On average the experts rated the robot's performance as adequate. Moreover, the experts generally preferred the cuts performed in collaboration with a human worker to cuts completed autonomously, highlighting the benefits of robotic technologies that assist human workers rather than replace them. Video demonstrations of our proposed framework can be found here: https://youtu.be/56mdHjjYMVc ",
        "title": "Safely and Autonomously Cutting Meat with a Collaborative Robot Arm",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07877",
        "abstract_url": "http://arxiv.org/abs/2401.07877",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mingjie"
            },
            {
                "last_name": "Verspoor",
                "first_name": "Karin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Information extraction techniques, including named entity recognition (NER) and relation extraction (RE), are crucial in many domains to support making sense of vast amounts of unstructured text data by identifying and connecting relevant information. Such techniques can assist researchers in extracting valuable insights. In this paper, we introduce the Entity-aware Masking for Biomedical Relation Extraction (EMBRE) method for biomedical relation extraction, as applied in the context of the BioRED challenge Task 1, in which human-annotated entities are provided as input. Specifically, we integrate entity knowledge into a deep neural network by pretraining the backbone model with an entity masking objective. We randomly mask named entities for each instance and let the model identify the masked entity along with its type. In this way, the model is capable of learning more specific knowledge and more robust representations. Then, we utilize the pre-trained model as our backbone to encode language representations and feed these representations into two multilayer perceptron (MLPs) to predict the logits for relation and novelty, respectively. The experimental results demonstrate that our proposed method can improve the performances of entity pair, relation and novelty extraction over our baseline. ",
        "title": "EMBRE: Entity-aware Masking for Biomedical Relation Extraction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07879",
        "abstract_url": "http://arxiv.org/abs/2401.07879",
        "authors": [
            {
                "last_name": "Pandey",
                "first_name": "Ashutosh"
            },
            {
                "last_name": "Xu",
                "first_name": "Buye"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  We present a novel model designed for resource-efficient multichannel speech enhancement in the time domain, with a focus on low latency, lightweight, and low computational requirements. The proposed model incorporates explicit spatial and temporal processing within deep neural network (DNN) layers. Inspired by frequency-dependent multichannel filtering, our spatial filtering process applies multiple trainable filters to each hidden unit across the spatial dimension, resulting in a multichannel output. The temporal processing is applied over a single-channel output stream from the spatial processing using a Long Short-Term Memory (LSTM) network. The output from the temporal processing stage is then further integrated into the spatial dimension through elementwise multiplication. This explicit separation of spatial and temporal processing results in a resource-efficient network design. Empirical findings from our experiments show that our proposed model significantly outperforms robust baseline models while demanding far fewer parameters and computations, while achieving an ultra-low algorithmic latency of just 2 milliseconds. ",
        "title": "Decoupled Spatial and Temporal Processing for Resource Efficient  Multichannel Speech Enhancement",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07882",
        "abstract_url": "http://arxiv.org/abs/2401.07882",
        "authors": [
            {
                "last_name": "Hsieh",
                "first_name": "Tsun-An"
            },
            {
                "last_name": "Donley",
                "first_name": "Jacob"
            },
            {
                "last_name": "Wong",
                "first_name": "Daniel"
            },
            {
                "last_name": "Xu",
                "first_name": "Buye"
            },
            {
                "last_name": "Pandey",
                "first_name": "Ashutosh"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  We introduce a time-domain framework for efficient multichannel speech enhancement, emphasizing low latency and computational efficiency. This framework incorporates two compact deep neural networks (DNNs) surrounding a multichannel neural Wiener filter (NWF). The first DNN enhances the speech signal to estimate NWF coefficients, while the second DNN refines the output from the NWF. The NWF, while conceptually similar to the traditional frequency-domain Wiener filter, undergoes a training process optimized for low-latency speech enhancement, involving fine-tuning of both analysis and synthesis transforms. Our research results illustrate that the NWF output, having minimal nonlinear distortions, attains performance levels akin to those of the first DNN, deviating from conventional Wiener filter paradigms. Training all components jointly outperforms sequential training, despite its simplicity. Consequently, this framework achieves superior performance with fewer parameters and reduced computational demands, making it a compelling solution for resource-efficient multichannel speech enhancement. ",
        "title": "On the Importance of Neural Wiener Filter for Resource Efficient  Multichannel Speech Enhancement",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07883",
        "abstract_url": "http://arxiv.org/abs/2401.07883",
        "authors": [
            {
                "last_name": "Finardi",
                "first_name": "Paulo"
            },
            {
                "last_name": "Avila",
                "first_name": "Leonardo"
            },
            {
                "last_name": "Castaldoni",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Gengo",
                "first_name": "Pedro"
            },
            {
                "last_name": "Larcher",
                "first_name": "Celio"
            },
            {
                "last_name": "Piau",
                "first_name": "Marcos"
            },
            {
                "last_name": "Costa",
                "first_name": "Pablo"
            },
            {
                "last_name": "Carid\u00e1",
                "first_name": "Vinicius"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "IR"
        ],
        "abstract": "  Retrieval Augmented Generation (RAG) has become one of the most popular paradigms for enabling LLMs to access external data, and also as a mechanism for grounding to mitigate against hallucinations. When implementing RAG you can face several challenges like effective integration of retrieval models, efficient representation learning, data diversity, computational efficiency optimization, evaluation, and quality of text generation. Given all these challenges, every day a new technique to improve RAG appears, making it unfeasible to experiment with all combinations for your problem. In this context, this paper presents good practices to implement, optimize, and evaluate RAG for the Brazilian Portuguese language, focusing on the establishment of a simple pipeline for inference and experiments. We explored a diverse set of methods to answer questions about the first Harry Potter book. To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview, gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to the baseline. When optimizing the input size in the application, we observed that it is possible to further enhance it by 2.4%. Finally, we present the complete architecture of the RAG with our recommendations. As result, we moved from a baseline of 57.88% to a maximum relative score of 98.61%. ",
        "title": "The Chronicles of RAG: The Retriever, the Chunk and the Generator",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07886",
        "abstract_url": "http://arxiv.org/abs/2401.07886",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Hooper",
                "first_name": "Coleman"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoxuan"
            },
            {
                "last_name": "Kim",
                "first_name": "Sehoon"
            },
            {
                "last_name": "Keutzer",
                "first_name": "Kurt"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "DC"
        ],
        "abstract": "  Many applications must provide low-latency LLM service to users or risk unacceptable user experience. However, over-provisioning resources to serve fluctuating request patterns is often prohibitively expensive. In this work, we present a best-effort serving system that employs deep reinforcement learning to adjust service quality based on the task distribution and system load. Our best-effort system can maintain availability with over 10x higher client request rates, serves above 96% of peak performance 4.1x more often, and serves above 98% of peak performance 2.3x more often than static serving on unpredictable workloads. Our learned router is robust to shifts in both the arrival and task distribution. Compared to static serving, learned best-effort serving allows for cost-efficient serving through increased hardware utility. Additionally, we argue that learned best-effort LLM serving is applicable in wide variety of settings and provides application developers great flexibility to meet their specific needs. ",
        "title": "Learned Best-Effort LLM Serving",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07888",
        "abstract_url": "http://arxiv.org/abs/2401.07888",
        "authors": [
            {
                "last_name": "Heinlein",
                "first_name": "Alexander"
            },
            {
                "last_name": "Howard",
                "first_name": "Amanda A."
            },
            {
                "last_name": "Beecroft",
                "first_name": "Damien"
            },
            {
                "last_name": "Stinis",
                "first_name": "Panos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multiscale problems are challenging for neural network-based discretizations of differential equations, such as physics-informed neural networks (PINNs). This can be (partly) attributed to the so-called spectral bias of neural networks. To improve the performance of PINNs for time-dependent problems, a combination of multifidelity stacking PINNs and domain decomposition-based finite basis PINNs are employed. In particular, to learn the high-fidelity part of the multifidelity model, a domain decomposition in time is employed. The performance is investigated for a pendulum and a two-frequency problem as well as the Allen-Cahn equation. It can be observed that the domain decomposition approach clearly improves the PINN and stacking PINN approaches. ",
        "title": "Multifidelity domain decomposition-based physics-informed neural  networks for time-dependent problems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07889",
        "abstract_url": "http://arxiv.org/abs/2401.07889",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Ryan"
            },
            {
                "last_name": "Patel",
                "first_name": "Sunil"
            },
            {
                "last_name": "Cho",
                "first_name": "Kyu Taek"
            },
            {
                "last_name": "Hwang",
                "first_name": "Jaejin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study investigated the use of forearm EMG data for distinguishing eight hand gestures, employing the Neural Network and Random Forest algorithms on data from ten participants. The Neural Network achieved 97 percent accuracy with 1000-millisecond windows, while the Random Forest achieved 85 percent accuracy with 200-millisecond windows. Larger window sizes improved gesture classification due to increased temporal resolution. The Random Forest exhibited faster processing at 92 milliseconds, compared to the Neural Network's 124 milliseconds. In conclusion, the study identified a Neural Network with a 1000-millisecond stream as the most accurate (97 percent), and a Random Forest with a 200-millisecond stream as the most efficient (85 percent). Future research should focus on increasing sample size, incorporating more hand gestures, and exploring different feature extraction methods and modeling algorithms to enhance system accuracy and efficiency. ",
        "title": "Machine Learning Techniques to Identify Hand Gestures amidst Forearm  Muscle Signals",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07890",
        "abstract_url": "http://arxiv.org/abs/2401.07890",
        "authors": [
            {
                "last_name": "Shahbazi",
                "first_name": "Alireza"
            },
            {
                "last_name": "Mirsanei",
                "first_name": "Seyyed Ahmad"
            },
            {
                "last_name": "Sarraf",
                "first_name": "Malikeh Haj Khan Mirzaye"
            },
            {
                "last_name": "Bidgoli",
                "first_name": "Behrouz Minaei"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Planning and reasoning about actions and processes, in addition to reasoning about propositions, are important issues in recent logical and computer science studies. The widespread use of actions in everyday life such as IoT, semantic web services, etc., and the limitations and issues in the action formalisms are two factors that lead us to study about how actions are represented.   Since 2007, there was some ideas to integrate Description Logic (DL) and action formalisms for representing both static and dynamic knowledge. In meanwhile, time is an important factor in dynamic situations, and actions change states over time. In this study, on the one hand, we examined related logical structures such as extensions of description logics (DLs), temporal formalisms, and action formalisms. On the other hand, we analyzed possible tools for designing and developing the Knowledge and Action Base (KAB).   For representation and reasoning about actions, we embedded actions into DLs (such as Dynamic-ALC and its extensions). We propose a terminable algorithm for action projection, planning, checking the satisfiability, consistency, realizability, and executability, and also querying from KAB. Actions in this framework were modeled with SPIN and added to state space. This framework has also been implemented as a plugin for the Prot\\'eg\\'e ontology editor.   During the last two decades, various algorithms have been presented, but due to the high computational complexity, we face many problems in implementing dynamic ontologies. In addition, an algorithm to detect the inconsistency of actions' effects was not explicitly stated. In the proposed strategy, the interactions of actions with other parts of modeled knowledge, and a method to check consistency between the effects of actions are presented. With this framework, the ramification problem can be well handled in future works. ",
        "title": "A Strategy for Implementing description Temporal Dynamic Algorithms in  Dynamic Knowledge Graphs by SPIN",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07892",
        "abstract_url": "http://arxiv.org/abs/2401.07892",
        "authors": [
            {
                "last_name": "Asif",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Ali",
                "first_name": "Noman"
            },
            {
                "last_name": "Mishra",
                "first_name": "Sudhakar"
            },
            {
                "last_name": "Dandawate",
                "first_name": "Anushka"
            },
            {
                "last_name": "Tiwary",
                "first_name": "Uma Shanker"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Recently, the representation of emotions in the Valence, Arousal and Dominance (VAD) space has drawn enough attention. However, the complex nature of emotions and the subjective biases in self-reported values of VAD make the emotion model too specific to a particular experiment. This study aims to develop a generic model representing emotions using a fuzzy VAD space and improve emotion recognition by utilizing this representation. We partitioned the crisp VAD space into a fuzzy VAD space using low, medium and high type-2 fuzzy dimensions to represent emotions. A framework that integrates fuzzy VAD space with EEG data has been developed to recognize emotions. The EEG features were extracted using spatial and temporal feature vectors from time-frequency spectrograms, while the subject-reported values of VAD were also considered. The study was conducted on the DENS dataset, which includes a wide range of twenty-four emotions, along with EEG data and subjective ratings. The study was validated using various deep fuzzy framework models based on type-2 fuzzy representation, cuboid probabilistic lattice representation and unsupervised fuzzy emotion clusters. These models resulted in emotion recognition accuracy of 96.09\\%, 95.75\\% and 95.31\\%, respectively, for the classes of 24 emotions. The study also included an ablation study, one with crisp VAD space and the other without VAD space. The result with crisp VAD space performed better, while the deep fuzzy framework outperformed both models. The model was extended to predict cross-subject cases of emotions, and the results with 78.37\\% accuracy are promising, proving the generality of our model. The generic nature of the developed model, along with its successful cross-subject predictions, gives direction for real-world applications in the areas such as affective computing, human-computer interaction, and mental health monitoring. ",
        "title": "Deep Fuzzy Framework for Emotion Recognition using EEG Signals and  Emotion Representation in Type-2 Fuzzy VAD Space",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07897",
        "abstract_url": "http://arxiv.org/abs/2401.07897",
        "authors": [
            {
                "last_name": "van Deemter",
                "first_name": "Kees"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in Data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs. ",
        "title": "The Pitfalls of Defining Hallucination",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07898",
        "abstract_url": "http://arxiv.org/abs/2401.07898",
        "authors": [
            {
                "last_name": "Yavuz",
                "first_name": "Tuba"
            },
            {
                "last_name": "Khor",
                "first_name": "Chin"
            },
            {
                "last_name": "Ken",
                "first_name": ""
            },
            {
                "last_name": "Bai",
                "first_name": ""
            },
            {
                "last_name": "Lutz",
                "first_name": "Robyn"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Testing configurable systems continues to be challenging and costly. Generation of configurations for testing tends to use either techniques based on semantic sampling (e.g., logical formulas over configuration variables, often called presence conditions) or structural code metrics (e.g., code coverage). In this paper we describe our hybrid approaches that combine these two kinds of techniques to good effect. We present new configuration-generation algorithms that leverage constraint solving (SAT and MaxSAT) and configuration fuzzing, and implement our approach in a configuration-generation framework, CONFIZZ. CONFIZZ both enables the generation of maximal configurations (maximal sets of presence conditions that can be satisfied together) and performs code-metric guided configuration fuzzing. Results from evaluation on BusyBox, a highly configurable benchmark, show that our MaxSAT-based configuration generation achieves better coverage for several code metrics. Results also show that, when high coverage of multiple configurations is needed, CONFIZZ's presence-condition fuzzing outperforms alternatives. ",
        "title": "Generating Maximal Configurations and Their Variants Using Code Metrics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07912",
        "abstract_url": "http://arxiv.org/abs/2401.07912",
        "authors": [
            {
                "last_name": "Weggemans",
                "first_name": "Jordi"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  In unitary property testing a quantum algorithm, also known as a tester, is given query access to a black-box unitary and has to decide whether it satisfies some property. We propose a new technique for proving lower bounds on the quantum query complexity of unitary property testing and related problems, which utilises the connection between unitary property testing and unitary channel discrimination. The main advantage of this technique is that all obtained lower bounds hold for any $\\mathsf{C}$-tester with $\\mathsf{C} \\subseteq \\mathsf{QMA}(\\text{poly(n)} / \\mathsf{qpoly}$, showing that even having access to both (unentangled) quantum proofs and advice does not help for many unitary problems. We apply our technique to prove lower bounds for problems like quantum phase estimation, the entanglement entropy problem, quantum Gibbs sampling and more, removing all logarithmic factors in the lower bounds obtained by the sample-to-query lifting theorem of Wang and Zhang (2023). As a direct corollary, we show that there exists a quantum oracle relative to which $\\mathsf{QMA}(\\text{poly(n)} / \\mathsf{qpoly} \\not\\supset \\mathsf{SBQP}$. ",
        "title": "Lower Bounds for Unitary Property Testing with Proofs and Advice",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07914",
        "abstract_url": "http://arxiv.org/abs/2401.07914",
        "authors": [
            {
                "last_name": "Booth",
                "first_name": "Robert I."
            },
            {
                "last_name": "Carette",
                "first_name": "Titouan"
            },
            {
                "last_name": "Comfort",
                "first_name": "Cole"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We give complete presentations for the dagger-compact props of affine Lagrangian and coisotropic relations over an arbitrary field. This provides a unified family of graphical languages for both affinely constrained classical mechanical systems, as well as odd-prime-dimensional stabiliser quantum circuits. To this end, we present affine Lagrangian relations by a particular class of undirected coloured graphs. In order to reason about composite systems, we introduce a powerful scalable notation where the vertices of these graphs are themselves coloured by graphs. In the setting of stabiliser quantum mechanics, this scalable notation gives an extremely concise description of graph states, which can be composed via ``phased spider fusion.'' Likewise, in the classical mechanical setting of electrical circuits, we show that impedance matrices for reciprocal networks are presented in essentially the same way. ",
        "title": "Graphical Symplectic Algebra",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07915",
        "abstract_url": "http://arxiv.org/abs/2401.07915",
        "authors": [
            {
                "last_name": "Weinberg",
                "first_name": "Abraham Itzhak"
            },
            {
                "last_name": "Shirizly",
                "first_name": "Alon"
            },
            {
                "last_name": "Azulay",
                "first_name": "Osher"
            },
            {
                "last_name": "Sintov",
                "first_name": "Avishai"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Human dexterity is an invaluable capability for precise manipulation of objects in complex tasks. The capability of robots to similarly grasp and perform in-hand manipulation of objects is critical for their use in the ever changing human environment, and for their ability to replace manpower. In recent decades, significant effort has been put in order to enable in-hand manipulation capabilities to robotic systems. Initial robotic manipulators followed carefully programmed paths, while later attempts provided a solution based on analytical modeling of motion and contact. However, these have failed to provide practical solutions due to inability to cope with complex environments and uncertainties. Therefore, the effort has shifted to learning-based approaches where data is collected from the real world or through a simulation, during repeated attempts to complete various tasks. The vast majority of learning approaches focused on learning data-based models that describe the system to some extent or Reinforcement Learning (RL). RL, in particular, has seen growing interest due to the remarkable ability to generate solutions to problems with minimal human guidance. In this survey paper, we track the developments of learning approaches for in-hand manipulations and, explore the challenges and opportunities. This survey is designed both as an introduction for novices in the field with a glossary of terms as well as a guide of novel advances for advanced practitioners. ",
        "title": "Survey of Learning Approaches for Robotic In-Hand Manipulation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07923",
        "abstract_url": "http://arxiv.org/abs/2401.07923",
        "authors": [
            {
                "last_name": "Gow-Smith",
                "first_name": "Edward"
            },
            {
                "last_name": "Phelps",
                "first_name": "Dylan"
            },
            {
                "last_name": "Madabushi",
                "first_name": "Harish Tayyar"
            },
            {
                "last_name": "Scarton",
                "first_name": "Carolina"
            },
            {
                "last_name": "Villavicencio",
                "first_name": "Aline"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  All existing transformer-based approaches to NLP using subword tokenisation algorithms encode whitespace (word boundary information) through the use of special space symbols (such as \\#\\# or \\_) forming part of tokens. These symbols have been shown to a) lead to reduced morphological validity of tokenisations, and b) give substantial vocabulary redundancy. As such, removing these symbols has been shown to have a beneficial effect on the processing of morphologically complex words for transformer encoders in the pretrain-finetune paradigm. In this work, we explore whether word boundary information is at all useful to such models. In particular, we train transformer encoders across four different training scales, and investigate several alternative approaches to including word boundary information, evaluating on a range of tasks across different domains and problem set-ups: GLUE (for sentence-level classification), NER (for token-level classification), and two classification datasets involving complex words (Superbizarre and FLOTA). Overall, through an extensive experimental setup that includes the pre-training of 29 models, we find no substantial improvements from our alternative approaches, suggesting that modifying tokenisers to remove word boundary information isn't leading to a loss of useful information. ",
        "title": "Word Boundary Information Isn't Useful for Encoder Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07927",
        "abstract_url": "http://arxiv.org/abs/2401.07927",
        "authors": [
            {
                "last_name": "Madsen",
                "first_name": "Andreas"
            },
            {
                "last_name": "Chandar",
                "first_name": "Sarath"
            },
            {
                "last_name": "Reddy",
                "first_name": "Siva"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to three types of self-explanations: counterfactuals, importance measures, and redactions. Our work demonstrate that faithfulness is both task and model dependent, e.g., for sentiment classification, counterfactual explanations are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B. Finally, our findings are robust to prompt-variations. ",
        "title": "Can Large Language Models Explain Themselves?",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07928",
        "abstract_url": "http://arxiv.org/abs/2401.07928",
        "authors": [
            {
                "last_name": "Klein",
                "first_name": "Emily"
            },
            {
                "last_name": "Golbeck",
                "first_name": "Jennifer"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Incels are an extremist online community of men who believe in an ideology rooted in misogyny, racism, the glorification of violence, and dehumanization. In their online forums, they use an extensive, evolving cryptolect - a set of ingroup terms that have meaning within the group, reflect the ideology, demonstrate membership in the community, and are difficult for outsiders to understand. This paper presents a lexicon with terms and definitions for common incel root words, prefixes, and affixes. The lexicon is text-based for use in automated analysis and is derived via a Qualitative Content Analysis of the most frequent incel words, their structure, and their meaning on five of the most active incel communities from 2016 to 2023. This lexicon will support future work examining radicalization and deradicalization/disengagement within the community. ",
        "title": "A Lexicon for Studying Radicalization in Incel Communities",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07929",
        "abstract_url": "http://arxiv.org/abs/2401.07929",
        "authors": [
            {
                "last_name": "Akanda",
                "first_name": "Md Rakibul Karim"
            },
            {
                "last_name": "Reynolds",
                "first_name": "Joshua"
            },
            {
                "last_name": "Jackson",
                "first_name": "Treylin"
            },
            {
                "last_name": "Gray",
                "first_name": "Milijah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Machine learning based object detection as well as tracking that object have been performed in this paper. The authors were able to set a range of interest (ROI) around an object using Open Computer Vision, better known as OpenCV. Next a tracking algorithm has been used to maintain tracking on an object while simultaneously operating two servo motors to keep the object centered in the frame. Detailed procedure and code are included in this paper. ",
        "title": "Machine Learning Based Object Tracking",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07930",
        "abstract_url": "http://arxiv.org/abs/2401.07930",
        "authors": [
            {
                "last_name": "L\u00f3pez",
                "first_name": "Jos\u00e9 Antonio Hern\u00e1ndez"
            },
            {
                "last_name": "Chen",
                "first_name": "Boqi"
            },
            {
                "last_name": "Sharma",
                "first_name": "Tushar"
            },
            {
                "last_name": "Varr\u00f3",
                "first_name": "D\u00e1niel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Motivation. Large language models (LLMs) have exhibited remarkable proficiency in diverse software engineering (SE) tasks. Handling such tasks typically involves acquiring foundational coding knowledge on large, general-purpose datasets during a pre-training phase, and subsequently refining on smaller, task-specific datasets as part of a fine-tuning phase.   Problem statement. Data leakage is a well-known issue in training of machine learning models. A manifestation of this issue is the intersection of the training and testing splits. While intra-dataset code duplication examines this intersection within a given dataset and has been addressed in prior research, inter-dataset code duplication, which gauges the overlap between different datasets, remains largely unexplored. If this phenomenon exists, it could compromise the integrity of LLM evaluations because of the inclusion of fine-tuning test samples that were already encountered during pre-training, resulting in inflated performance metrics.   Contribution. This paper explores the phenomenon of inter-dataset code duplication and its impact on evaluating LLMs across diverse SE tasks.   Study design. We conduct an empirical study using the CSN dataset, a widely adopted pre-training dataset, and five fine-tuning datasets used for various SE tasks. We first identify the intersection between the pre-training and fine-tuning datasets using a deduplication process. Then, we fine-tune four models pre-trained on CSN to evaluate their performance on samples encountered during pre-training and those unseen during that phase.   Results. Our findings reveal a potential threat to the evaluation of various LLMs across multiple SE tasks, stemming from the inter-dataset code duplication phenomenon. Moreover, we demonstrate that this threat is accentuated by factors like the LLM's size and the chosen fine-tuning technique. ",
        "title": "On Inter-dataset Code Duplication and Data Leakage in Large Language  Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07931",
        "abstract_url": "http://arxiv.org/abs/2401.07931",
        "authors": [
            {
                "last_name": "Mandal",
                "first_name": "Paul K."
            },
            {
                "last_name": "Leo",
                "first_name": "Cole"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "DC",
            "LG"
        ],
        "abstract": "  With the popularization of AI solutions for image based problems, there has been a growing concern for both data privacy and acquisition. In a large number of cases, information is located on separate data silos and it can be difficult for a developer to consolidate all of it in a fashion that is appropriate for machine learning model development. Alongside this, a portion of these localized data regions may not have access to a labelled ground truth. This indicates that they have the capacity to reach conclusions numerically, but are not able to assign classifications amid a lack of pertinent information. Such a determination is often negligible, especially when attempting to develop image based solutions that often necessitate this capability. With this being the case, we propose an innovative vertical federated learning (VFL) model architecture that can operate under this common set of conditions. This is the first (and currently the only) implementation of a system that can work under the constraints of a VFL environment and perform image segmentation while maintaining nominal accuracies. We achieved this by utilizing an FCN that boasts the ability to operate on federates that lack labelled data and privately share the respective weights with a central server, that of which hosts the necessary features for classification. Tests were conducted on the CamVid dataset in order to determine the impact of heavy feature compression required for the transfer of information between federates, as well as to reach nominal conclusions about the overall performance metrics when working under such constraints. ",
        "title": "Vertical Federated Image Segmentation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07935",
        "abstract_url": "http://arxiv.org/abs/2401.07935",
        "authors": [
            {
                "last_name": "S\u00f3ti",
                "first_name": "Gergely"
            },
            {
                "last_name": "Huang",
                "first_name": "Xi"
            },
            {
                "last_name": "Wurll",
                "first_name": "Christian"
            },
            {
                "last_name": "Hein",
                "first_name": "Bj\u00f6rn"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We address the problem of robotic grasping of known and unknown objects using implicit behavior cloning. We train a grasp evaluation model from a small number of demonstrations that outputs higher values for grasp candidates that are more likely to succeed in grasping. This evaluation model serves as an objective function, that we maximize to identify successful grasps. Key to our approach is the utilization of learned implicit representations of visual and geometric features derived from a pre-trained NeRF. Though trained exclusively in a simulated environment with simplified objects and 4-DoF top-down grasps, our evaluation model and optimization procedure demonstrate generalization to 6-DoF grasps and novel objects both in simulation and in real-world settings, without the need for additional data. Supplementary material is available at: https://gergely-soti.github.io/grasp ",
        "title": "6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from  NeRFs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07936",
        "abstract_url": "http://arxiv.org/abs/2401.07936",
        "authors": [
            {
                "last_name": "Tschernutter",
                "first_name": "Daniel"
            },
            {
                "last_name": "Kraus",
                "first_name": "Mathias"
            },
            {
                "last_name": "Feuerriegel",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  We propose an algorithm for optimizing the parameters of single hidden layer neural networks. Specifically, we derive a blockwise difference-of-convex (DC) functions representation of the objective function. Based on the latter, we propose a block coordinate descent (BCD) approach that we combine with a tailored difference-of-convex functions algorithm (DCA). We prove global convergence of the proposed algorithm. Furthermore, we mathematically analyze the convergence rate of parameters and the convergence rate in value (i.e., the training loss). We give conditions under which our algorithm converges linearly or even faster depending on the local shape of the loss function. We confirm our theoretical derivations numerically and compare our algorithm against state-of-the-art gradient-based solvers in terms of both training loss and test loss. ",
        "title": "A Globally Convergent Algorithm for Neural Network Parameter  Optimization Based on Difference-of-Convex Functions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07937",
        "abstract_url": "http://arxiv.org/abs/2401.07937",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Shihao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Andy G. X."
            },
            {
                "last_name": "Haibe-Kains",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Goldenberg",
                "first_name": "Anna"
            },
            {
                "last_name": "Dick",
                "first_name": "John E"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  High-throughput omics profiling advancements have greatly enhanced cancer patient stratification. However, incomplete data in multi-omics integration presents a significant challenge, as traditional methods like sample exclusion or imputation often compromise biological diversity and dependencies. Furthermore, the critical task of accurately classifying new patients with partial omics data into existing subtypes is commonly overlooked. To address these issues, we introduce IntegrAO (Integrate Any Omics), an unsupervised framework for integrating incomplete multi-omics data and classifying new samples. IntegrAO first combines partially overlapping patient graphs from diverse omics sources and utilizes graph neural networks to produce unified patient embeddings. Our systematic evaluation across five cancer cohorts involving six omics modalities demonstrates IntegrAO's robustness to missing data and its accuracy in classifying new samples with partial profiles. An acute myeloid leukemia case study further validates its capability to uncover biological and clinical heterogeneity in incomplete datasets. IntegrAO's ability to handle heterogeneous and incomplete data makes it an essential tool for precision oncology, offering a holistic approach to patient characterization. ",
        "title": "Integrate Any Omics: Towards genome-wide data integration for patient  stratification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07942",
        "abstract_url": "http://arxiv.org/abs/2401.07942",
        "authors": [
            {
                "last_name": "Moradi",
                "first_name": "Morteza"
            },
            {
                "last_name": "Palazzo",
                "first_name": "Simone"
            },
            {
                "last_name": "Spampinato",
                "first_name": "Concetto"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  In recent years, finding an effective and efficient strategy for exploiting spatial and temporal information has been a hot research topic in video saliency prediction (VSP). With the emergence of spatio-temporal transformers, the weakness of the prior strategies, e.g., 3D convolutional networks and LSTM-based networks, for capturing long-range dependencies has been effectively compensated. While VSP has drawn benefits from spatio-temporal transformers, finding the most effective way for aggregating temporal features is still challenging. To address this concern, we propose a transformer-based video saliency prediction approach with high temporal dimension decoding network (THTD-Net). This strategy accounts for the lack of complex hierarchical interactions between features that are extracted from the transformer-based spatio-temporal encoder: in particular, it does not require multiple decoders and aims at gradually reducing temporal features' dimensions in the decoder. This decoder-based architecture yields comparable performance to multi-branch and over-complicated models on common benchmarks such as DHF1K, UCF-sports and Hollywood-2. ",
        "title": "Transformer-based Video Saliency Prediction with High Temporal Dimension  Decoding",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07944",
        "abstract_url": "http://arxiv.org/abs/2401.07944",
        "authors": [
            {
                "last_name": "Das",
                "first_name": "Rupak Kumar"
            },
            {
                "last_name": "Pedersen",
                "first_name": "Dr. Ted"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper uses the BERT model, which is a transformer-based architecture, to solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017. BERT is a very powerful large language model for classification tasks when the amount of training data is small. For this experiment, we have used the BERT{\\textsubscript{\\tiny BASE}} model, which has 12 hidden layers. This model provides better accuracy, precision, recall, and f1 score than the Naive Bayes baseline model. It performs better in binary classification subtasks than the multi-class classification subtasks. We also considered all kinds of ethical issues during this experiment, as Twitter data contains personal and sensible information. The dataset and code used in our experiment can be found in this GitHub repository. ",
        "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07947",
        "abstract_url": "http://arxiv.org/abs/2401.07947",
        "authors": [
            {
                "last_name": "Akanda",
                "first_name": "Md Rakibul Karim"
            },
            {
                "last_name": "Lazo",
                "first_name": "Jason"
            },
            {
                "last_name": "Carter",
                "first_name": "Quintwon"
            },
            {
                "last_name": "Roberts",
                "first_name": "Haineef"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The project we embarked on is making an electronic robot that can deliver a package along a set route through infrared sensors. It uses the infrared sensors to determine if the path it is following is correct or if it is off course. This is determined by sending off a photon to reflect off the path and determines if it is on a light surface by the amount of light emitted back or if it is a dark surface by the amount of light that is not present. In addition to following a line, the user can stop and start the robot at any interval through the infrared remote control. The project is a combination of the practical parts of machinery with the software part of coding in Arduino which is a coding subsect of C++. This can lead to endless possibilities that could help a wide variety of people from all ranges of life, especially with those that live with disabilities ",
        "title": "Delivery Line Tracking Robot",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07950",
        "abstract_url": "http://arxiv.org/abs/2401.07950",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dan"
            },
            {
                "last_name": "Hu",
                "first_name": "Ziniu"
            },
            {
                "last_name": "Zhoubian",
                "first_name": "Sining"
            },
            {
                "last_name": "Du",
                "first_name": "Zhengxiao"
            },
            {
                "last_name": "Yang",
                "first_name": "Kaiyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Yue",
                "first_name": "Yisong"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuxiao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  \\label{sec:abstract} Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing mathematics, physics, chemistry, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing their capabilities in scientific and mathematical reasoning. Remarkably, SciGLM consistently improves both the base model (ChatGLM3-6B-Base) and larger-scale models (12B and 32B), without sacrificing the language understanding capabilities of the base model. This makes SciGLM a suitable foundational model to facilitate diverse scientific discovery tasks. For the benefit of the wider research community, we release SciInstruct, SciGLM, alongside a self-reflective framework and fine-tuning code at \\url{https://github.com/THUDM/SciGLM}. ",
        "title": "SciGLM: Training Scientific Language Models with Self-Reflective  Instruction Annotation and Tuning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07951",
        "abstract_url": "http://arxiv.org/abs/2401.07951",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Zukang"
            },
            {
                "last_name": "Chen",
                "first_name": "Min"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image similarity has been extensively studied in computer vision. In recently years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling similarity, assigning a numerical score to a pair of images is less intuitive than determining if an image A is closer to a reference image R than another image B. In this work, we present a novel approach for building an image similarity model based on labelled data in the form of A:R vs B:R. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. In particular, we employed two ML techniques to construct such an ensemble model, namely dimensionality reduction and MLP regressors. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the model trained with mixed imagery data as well as existing similarity models, e.g., CLIP and DINO. This work demonstrate that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling. ",
        "title": "Image Similarity using An Ensemble of Context-Sensitive Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07955",
        "abstract_url": "http://arxiv.org/abs/2401.07955",
        "authors": [
            {
                "last_name": "Khatun",
                "first_name": "Aisha"
            },
            {
                "last_name": "Brown",
                "first_name": "Daniel G."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The widespread adoption of Large Language Models (LLMs) has become commonplace, particularly with the emergence of open-source models. More importantly, smaller models are well-suited for integration into consumer devices and are frequently employed either as standalone solutions or as subroutines in various AI tasks. Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations. In this study, we tackle one of the most widely used tasks - answering Multiple Choice Question (MCQ). We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent. These results are rather alarming given the extensive use of MCQ tests with these models. We recommend exercising caution and testing task understanding before using MCQ to evaluate LLMs in any field whatsoever. ",
        "title": "A Study on Large Language Models' Limitations in Multiple-Choice  Question Answering",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07957",
        "abstract_url": "http://arxiv.org/abs/2401.07957",
        "authors": [
            {
                "last_name": "Jacobellis",
                "first_name": "Dan"
            },
            {
                "last_name": "Cummings",
                "first_name": "Daniel"
            },
            {
                "last_name": "Yadwadkar",
                "first_name": "Neeraja J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "SD"
        ],
        "abstract": "  In the field of neural data compression, the prevailing focus has been on optimizing algorithms for either classical distortion metrics, such as PSNR or SSIM, or human perceptual quality. With increasing amounts of data consumed by machines rather than humans, a new paradigm of machine-oriented compression$\\unicode{x2013}$which prioritizes the retention of features salient for machine perception over traditional human-centric criteria$\\unicode{x2013}$has emerged, creating several new challenges to the development, evaluation, and deployment of systems utilizing lossy compression. In particular, it is unclear how different approaches to lossy compression will affect the performance of downstream machine perception tasks. To address this under-explored area, we evaluate various perception models$\\unicode{x2013}$including image classification, image segmentation, speech recognition, and music source separation$\\unicode{x2013}$under severe lossy compression. We utilize several popular codecs spanning conventional, neural, and generative compression architectures. Our results indicate three key findings: (1) using generative compression, it is feasible to leverage highly compressed data while incurring a negligible impact on machine perceptual quality; (2) machine perceptual quality correlates strongly with deep similarity metrics, indicating a crucial role of these metrics in the development of machine-oriented codecs; and (3) using lossy compressed datasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive scenarios where lossy compression increases machine perceptual quality rather than degrading it. To encourage engagement on this growing area of research, our code and experiments are available at: https://github.com/danjacobellis/MPQ. ",
        "title": "Machine Perceptual Quality: Evaluating the Impact of Severe Lossy  Compression on Audio and Image Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07958",
        "abstract_url": "http://arxiv.org/abs/2401.07958",
        "authors": [
            {
                "last_name": "Vatamany",
                "first_name": "Lorand"
            },
            {
                "last_name": "Mehrkanoon",
                "first_name": "Siamak"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Accurate precipitation nowcasting is essential for various purposes, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolutional operations. This enhancement enables the model to directly process the high-dimensional spatiotemporal graph of precipitation maps and exploits higher-order correlations between the data dimensions. We evaluate our model on seven years of precipitation maps across Europe and its neighboring areas collected from the ERA5 dataset, provided by Copernicus. The model receives a fully connected graph in which each node represents historical observations from a specific region on the map. Consequently, each node contains a 3D tensor with time, height, and width dimensions. Experimental results demonstrate that the proposed GD-CAF model outperforms the other examined models. Furthermore, the averaged seasonal spatial and temporal attention scores over the test set are visualized to provide additional insights about the strongest connections between different regions or time steps. These visualizations shed light on the decision-making process of our model. ",
        "title": "GD-CAF: Graph Dual-stream Convolutional Attention Fusion for  Precipitation Nowcasting",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07960",
        "abstract_url": "http://arxiv.org/abs/2401.07960",
        "authors": [
            {
                "last_name": "Kumar",
                "first_name": "Vimal"
            },
            {
                "last_name": "Mayo",
                "first_name": "Juliette"
            },
            {
                "last_name": "Bahiss",
                "first_name": "Khadija"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Machine learning (ML) and artificial intelligence (AI) techniques have now become commonplace in software products and services. When threat modelling a system, it is therefore important that we consider threats unique to ML and AI techniques, in addition to threats to our software. In this paper, we present a threat model that can be used to systematically uncover threats to AI based software. The threat model consists of two main parts, a model of the software development process for AI based software and an attack taxonomy that has been developed using attacks found in adversarial AI research. We apply the threat model to two real life AI based software and discuss the process and the threats found. ",
        "title": "ADMIn: Attacks on Dataset, Model and Input. A Threat Model for AI Based  Software",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07961",
        "abstract_url": "http://arxiv.org/abs/2401.07961",
        "authors": [
            {
                "last_name": "Teter",
                "first_name": "Alexis M. H."
            },
            {
                "last_name": "Nodozi",
                "first_name": "Iman"
            },
            {
                "last_name": "Halder",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection of the OMT with the Schr\\\"odinger bridge problem (SBP). This also shows that the probabilistic Lambert problem with additive dynamic process noise is in fact a generalized SBP, and can be solved numerically using the so-called Schr\\\"odinger factors, as we do in this work. We explain how the resulting analysis leads to solving a boundary-coupled system of reaction-diffusion PDEs where the nonlinear gravitational potential appears as the reaction rate. We propose novel algorithms for the same, and present illustrative numerical results. Our analysis and the algorithmic framework are nonparametric, i.e., we make neither statistical (e.g., Gaussian, first few moments, mixture or exponential family, finite dimensionality of the sufficient statistic) nor dynamical (e.g., Taylor series) approximations. ",
        "title": "Solution of the Probabilistic Lambert Problem: Connections with Optimal  Mass Transport, Schr\\\"odinger Bridge and Reaction-Diffusion PDEs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07962",
        "abstract_url": "http://arxiv.org/abs/2401.07962",
        "authors": [
            {
                "last_name": "Beam",
                "first_name": "Chris"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jincheng"
            },
            {
                "last_name": "Kakavitsas",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Hague",
                "first_name": "Collin"
            },
            {
                "last_name": "Wolek",
                "first_name": "Artur"
            },
            {
                "last_name": "Willis",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  This article discusses the use of a simulated environment to predict algorithm results in the real world. Simulators are crucial in allowing researchers to test algorithms, sensor integration, and navigation systems without deploying expensive hardware. This article examines how the AirSim simulator, Unreal Engine, and Cesium plugin can be used to generate simulated digital twin models of real-world locations. Several technical challenges in completing the analysis are discussed and the technical solutions are detailed in this article. Work investigates how to assess mapping results for a real-life experiment using Cesium Tiles provided by digital twins of the experimental location. This is accompanied by a description of a process for duplicating real-world flights in simulation. The performance of these methods is evaluated by analyzing real-life and experimental image telemetry with the Direct Sparse Odometry (DSO) mapping algorithm. Results indicate that Cesium Tiles environments can provide highly accurate models of ground truth geometry after careful alignment. Further, results from real-life and simulated telemetry analysis indicate that the virtual simulation results accurately predict real-life results. Findings indicate that the algorithm results in real life and in the simulated duplicate exhibited a high degree of similarity. This indicates that the use of Cesium Tiles environments as a virtual digital twin for real-life experiments will provide representative results for such algorithms. The impact of this can be significant, potentially allowing expansive virtual testing of robotic systems at specific deployment locations to develop solutions that are tailored to the environment and potentially outperforming solutions meant to work in completely generic environments. ",
        "title": "Cesium Tiles for High-realism Simulation and Comparing SLAM Results in  Corresponding Virtual and Real-world Environments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07964",
        "abstract_url": "http://arxiv.org/abs/2401.07964",
        "authors": [
            {
                "last_name": "Mollo",
                "first_name": "Dimitri Coelho"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no less relevant to intelligence research, to those hypothesised for humans. ",
        "title": "AI-as-exploration: Navigating intelligence space",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07967",
        "abstract_url": "http://arxiv.org/abs/2401.07967",
        "authors": [
            {
                "last_name": "Kimelman",
                "first_name": "Robert G."
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "HC"
        ],
        "abstract": "  A novel freestyle rap software, MCMChaos 0.0.1, based on rap music transcriptions created in previous research is presented. The software has three different versions, each making use of different mathematical simulation methods: collapsed gibbs sampler and lorenz attractor simulation. As far as we know, these simulation methods have never been used in rap music generation before. The software implements Python Text-to-Speech processing (pyttxs) to convert text wrangled from the MCFlow corpus into English speech. In each version, values simulated from each respective mathematical model alter the rate of speech, volume, and (in the multiple voice case) the voice of the text-to-speech engine on a line-by-line basis. The user of the software is presented with a real-time graphical user interface (GUI) which instantaneously changes the initial values read into the mathematical simulation methods. Future research might attempt to allow for more user control and autonomy. ",
        "title": "MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07969",
        "abstract_url": "http://arxiv.org/abs/2401.07969",
        "authors": [
            {
                "last_name": "Battle",
                "first_name": "Steve"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a novel form of Liquid Automata, using this to simulate autopoiesis, whereby living machines self-organise in the physical realm. This simulation is based on an earlier Cellular Automaton described by Francisco Varela. The basis of Liquid Automata is a particle simulation with additional rules about how particles are transformed on collision with other particles. Unlike cellular automata, there is no fixed grid or time-step, only particles moving about and colliding with each other in a continuous space/time. ",
        "title": "Simulated Autopoiesis in Liquid Automata",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07971",
        "abstract_url": "http://arxiv.org/abs/2401.07971",
        "authors": [
            {
                "last_name": "Koval",
                "first_name": "Karina"
            },
            {
                "last_name": "Herzog",
                "first_name": "Roland"
            },
            {
                "last_name": "Scheichl",
                "first_name": "Robert"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a flexible method for computing Bayesian optimal experimental designs (BOEDs) for inverse problems with intractable posteriors. The approach is applicable to a wide range of BOED problems and can accommodate various optimality criteria, prior distributions and noise models. The key to our approach is the construction of a transport-map-based surrogate to the joint probability law of the design, observational and inference random variables. This order-preserving transport map is constructed using tensor trains and can be used to efficiently sample from (and evaluate approximate densities of) conditional distributions that are used to define many commonly-used optimality criteria. The algorithm is also extended to sequential data acquisition problems, where experiments can be performed in sequence and used to update the state of knowledge about the unknown parameters. The sequential BOED problem is made computationally feasible by preconditioning the approximation of the joint density at the current stage using transport maps constructed at previous stages. The flexibility of our approach in finding optimal designs is illustrated with some numerical examples inspired by disease modeling and the reconstruction of subsurface structures in aquifers. ",
        "title": "Tractable Optimal Experimental Design using Transport Maps",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07974",
        "abstract_url": "http://arxiv.org/abs/2401.07974",
        "authors": [
            {
                "last_name": "Zhandry",
                "first_name": "Mark"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  General quantum computation consists of unitary operations and also measurements. It is well known that intermediate quantum measurements can be deferred to the end of the computation, resulting in an equivalent purely unitary computation. While time efficient, this transformation blows up the space to linear in the running time, which could be super-polynomial for low-space algorithms. Fefferman and Remscrim (STOC'21) and Girish, Raz and Zhan (ICALP'21) show different transformations which are space efficient, but blow up the running time by a factor that is exponential in the space. This leaves the case of algorithms with small-but-super-logarithmic space as incurring a large blowup in either time or space complexity. We show that such a blowup is likely inherent, demonstrating that any \"black-box\" transformation which removes intermediate measurements must significantly blow up either space or time. ",
        "title": "The Space-Time Cost of Purifying Quantum Computations",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07977",
        "abstract_url": "http://arxiv.org/abs/2401.07977",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Saptarshi"
            },
            {
                "last_name": "Heaton",
                "first_name": "Connor"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Soumalya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Machine Reading Comprehension (MRC) has been a long-standing problem in NLP and, with the recent introduction of the BERT family of transformer based language models, it has come a long way to getting solved. Unfortunately, however, when BERT variants trained on general text corpora are applied to domain-specific text, their performance inevitably degrades on account of the domain shift i.e. genre/subject matter discrepancy between the training and downstream application data. Knowledge graphs act as reservoirs for either open or closed domain information and prior studies have shown that they can be used to improve the performance of general-purpose transformers in domain-specific applications. Building on existing work, we introduce a method using Multi-Layer Perceptrons (MLPs) for aligning and integrating embeddings extracted from knowledge graphs with the embeddings spaces of pre-trained language models (LMs). We fuse the aligned embeddings with open-domain LMs BERT and RoBERTa, and fine-tune them for two MRC tasks namely span detection (COVID-QA) and multiple-choice questions (PubMedQA). On the COVID-QA dataset, we see that our approach allows these models to perform similar to their domain-specific counterparts, Bio/Sci-BERT, as evidenced by the Exact Match (EM) metric. With regards to PubMedQA, we observe an overall improvement in accuracy while the F1 stays relatively the same over the domain-specific models. ",
        "title": "Leveraging External Knowledge Resources to Enable Domain-Specific  Comprehension",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07985",
        "abstract_url": "http://arxiv.org/abs/2401.07985",
        "authors": [
            {
                "last_name": "Barbie",
                "first_name": "Alexander"
            },
            {
                "last_name": "Hasselbring",
                "first_name": "Wilhelm"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The transformation to Industry 4.0 also transforms the processes of how we develop intelligent manufacturing production systems. To advance the software development of these new (embedded) software systems, digital twins may be employed. However, there is no consensual definition of what a digital twin is. In this paper, we give an overview of the current state of the digital twin concept and formalize the digital twin concept using the Object-Z notation. This formalization includes the concepts of physical twins, digital models, digital templates, digital threads, digital shadows, digital twins, and digital twin prototypes. The relationships between all these concepts are visualized as UML class diagrams.   Our digital twin prototype (DTP) approach supports engineers during the development and automated testing of complex embedded software systems. This approach enable engineers to test embedded software systems in a virtual context, without the need of a connection to a physical object. In continuous integration / continuous deployment pipelines such digital twin prototypes can be used for automated integration testing and, thus, allow for an agile verification and validation process.   In this paper, we demonstrate and report on how to apply and implement a digital twin by the example of two real-world field studies (ocean observation systems and smart farming). For independent replication and extension of our approach by other researchers, we provide a lab study published open source on GitHub. ",
        "title": "From Digital Twins to Digital Twin Prototypes: Concepts, Formalization,  and Applications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07986",
        "abstract_url": "http://arxiv.org/abs/2401.07986",
        "authors": [
            {
                "last_name": "Cherubini",
                "first_name": "Giacomo"
            },
            {
                "last_name": "Micheli",
                "first_name": "Giacomo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Let $n$ be a prime power, $r$ be a prime with $r\\mid n-1$, and $\\varepsilon\\in (0,1/2)$. Using the theory of multiplicative character sums and superelliptic curves, we construct new codes over $\\mathbb F_r$ having length $n$, relative distance $(r-1)/r+O(n^{-\\varepsilon})$ and rate $n^{-1/2-\\varepsilon}$. When $r=2$, our binary codes have exponential size when compared to all previously known families of linear and non-linear codes with relative distance asymptotic to $1/2$, such as Delsarte--Goethals codes. Moreover, our codes are linear. ",
        "title": "A New Class of Linear Codes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07988",
        "abstract_url": "http://arxiv.org/abs/2401.07988",
        "authors": [
            {
                "last_name": "Gavriilidis",
                "first_name": "Panagiotis"
            },
            {
                "last_name": "Atzeni",
                "first_name": "Italo"
            },
            {
                "last_name": "Alexandropoulos",
                "first_name": "George C."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "IT"
        ],
        "abstract": "  The massive Multiple-Input Multiple-Output (mMIMO) concept has been recently moving forward to extreme scales to address the envisioned requirements of next generation networks. However, the extension of conventional architectures will result in significant cost and power consumption. To this end, metasurface-based transceivers, consisting of microstrips of metamaterials, have recently emerged as an efficient enabler of extreme mMIMO systems. In this paper, we consider metasurface-based receivers with a $1$-bit Analog-to-Digital Converter (ADC) per microstrip and develop an analytical framework for the optimization of the analog and digital combining matrices. Our numerical results, including comparisons with fully digital, infinite-resolution MIMO, provide useful insights into the role of various system parameters. ",
        "title": "Metasurface-Based Receivers with $1$-bit ADCs for Multi-User Uplink  Communications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07990",
        "abstract_url": "http://arxiv.org/abs/2401.07990",
        "authors": [
            {
                "last_name": "Khanal",
                "first_name": "Bidur"
            },
            {
                "last_name": "Bhattarai",
                "first_name": "Binod"
            },
            {
                "last_name": "Khanal",
                "first_name": "Bishesh"
            },
            {
                "last_name": "Linte",
                "first_name": "Cristian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Noisy labels can significantly impact medical image classification, particularly in deep learning, by corrupting learned features. Self-supervised pretraining, which doesn't rely on labeled data, can enhance robustness against noisy labels. However, this robustness varies based on factors like the number of classes, dataset complexity, and training size. In medical images, subtle inter-class differences and modality-specific characteristics add complexity. Previous research hasn't comprehensively explored the interplay between self-supervised learning and robustness against noisy labels in medical image classification, considering all these factors. In this study, we address three key questions: i) How does label noise impact various medical image classification datasets? ii) Which types of medical image datasets are more challenging to learn and more affected by label noise? iii) How do different self-supervised pretraining methods enhance robustness across various medical image datasets? Our results show that DermNet, among five datasets (Fetal plane, DermNet, COVID-DU-Ex, MURA, NCT-CRC-HE-100K), is the most challenging but exhibits greater robustness against noisy labels. Additionally, contrastive learning stands out among the eight self-supervised methods as the most effective approach to enhance robustness against noisy labels. ",
        "title": "How does self-supervised pretraining improve robustness against noisy  labels across various medical image classification datasets?",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07991",
        "abstract_url": "http://arxiv.org/abs/2401.07991",
        "authors": [
            {
                "last_name": "Hamidi",
                "first_name": "Shayan Mohajer"
            },
            {
                "last_name": "Ye",
                "first_name": "Linfeng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Deep neural networks (DNNs) could be deceived by generating human-imperceptible perturbations of clean samples. Therefore, enhancing the robustness of DNNs against adversarial attacks is a crucial task. In this paper, we aim to train robust DNNs by limiting the set of outputs reachable via a norm-bounded perturbation added to a clean sample. We refer to this set as adversarial polytope, and each clean sample has a respective adversarial polytope. Indeed, if the respective polytopes for all the samples are compact such that they do not intersect the decision boundaries of the DNN, then the DNN is robust against adversarial samples. Hence, the inner-working of our algorithm is based on learning \\textbf{c}onfined \\textbf{a}dversarial \\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we demonstrate the effectiveness of CAP over existing adversarial robustness methods in improving the robustness of models against state-of-the-art attacks including AutoAttack. ",
        "title": "Robustness Against Adversarial Attacks via Learning Confined Adversarial  Polytopes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07992",
        "abstract_url": "http://arxiv.org/abs/2401.07992",
        "authors": [
            {
                "last_name": "\u00d6z",
                "first_name": "Burak"
            },
            {
                "last_name": "Gebele",
                "first_name": "Jonas"
            },
            {
                "last_name": "Singh",
                "first_name": "Parshant"
            },
            {
                "last_name": "Rezabek",
                "first_name": "Filip"
            },
            {
                "last_name": "Matthes",
                "first_name": "Florian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Maximal Extractable Value (MEV) searching has gained prominence on the Ethereum blockchain since the surge in Decentralized Finance activities. In Ethereum, MEV extraction primarily hinges on fee payments to block proposers. However, in First-Come-First-Served (FCFS) blockchain networks, the focus shifts to latency optimizations, akin to High-Frequency Trading in Traditional Finance. This paper illustrates the dynamics of the MEV extraction game in an FCFS network, specifically Algorand. We introduce an arbitrage detection algorithm tailored to the unique time constraints of FCFS networks and assess its effectiveness. Additionally, our experiments investigate potential optimizations in Algorand's network layer to secure optimal execution positions.   Our analysis reveals that while the states of relevant trading pools are updated approximately every six blocks on median, pursuing MEV at the block state level is not viable on Algorand, as arbitrage opportunities are typically executed within the blocks they appear. Our algorithm's performance under varying time constraints underscores the importance of timing in arbitrage discovery. Furthermore, our network-level experiments identify critical transaction prioritization strategies for Algorand's FCFS network. Key among these is reducing latency in connections with relays that are well-connected to high-staked proposers. ",
        "title": "Playing the MEV Game on a First-Come-First-Served Blockchain",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07994",
        "abstract_url": "http://arxiv.org/abs/2401.07994",
        "authors": [
            {
                "last_name": "Ruiz",
                "first_name": "Fernando Vallecillos"
            },
            {
                "last_name": "Grishina",
                "first_name": "Anastasiia"
            },
            {
                "last_name": "Hort",
                "first_name": "Max"
            },
            {
                "last_name": "Moonen",
                "first_name": "Leon"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "LG"
        ],
        "abstract": "  Research shows that grammatical mistakes in a sentence can be corrected by translating it to another language and back using neural machine translation with language models. We investigate whether this correction capability of Large Language Models (LLMs) extends to Automatic Program Repair (APR). Current generative models for APR are pre-trained on source code and fine-tuned for repair. This paper proposes bypassing the fine-tuning step and using Round-Trip Translation (RTT): translation of code from one programming language to another programming or natural language, and back. We hypothesize that RTT with LLMs restores the most commonly seen patterns in code during pre-training, i.e., performs a regression toward the mean, which removes bugs as they are a form of noise w.r.t. the more frequent, natural, bug-free code in the training data. To test this hypothesis, we employ eight recent LLMs pre-trained on code, including the latest GPT versions, and four common program repair benchmarks in Java. We find that RTT with English as an intermediate language repaired 101 of 164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are unique bugs that are not repaired by other LLMs fine-tuned for APR. Our findings highlight the viability of round-trip translation with LLMs as a technique for automated program repair and its potential for research in software engineering.   Keywords: automated program repair, large language model, machine translation ",
        "title": "A Novel Approach for Automatic Program Repair using Round-Trip  Translation with Large Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07995",
        "abstract_url": "http://arxiv.org/abs/2401.07995",
        "authors": [
            {
                "last_name": "Varlioglu",
                "first_name": "Said"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Varlioglu",
                "first_name": "Eva Ruhsar"
            },
            {
                "last_name": "Ozer",
                "first_name": "Murat"
            },
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Fileless malware predominantly relies on PowerShell scripts, leveraging the native capabilities of Windows systems to execute stealthy attacks that leave no traces on the victim's system. The effectiveness of the fileless method lies in its ability to remain operational on victim endpoints through memory execution, even if the attacks are detected, and the original malicious scripts are removed. Threat actors have increasingly utilized this technique, particularly since 2017, to conduct cryptojacking attacks. With the emergence of new Remote Code Execution (RCE) vulnerabilities in ubiquitous libraries, widespread cryptocurrency mining attacks have become prevalent, often employing fileless techniques. This paper provides a comprehensive analysis of PowerShell scripts of fileless cryptojacking, dissecting the common malicious patterns based on the MITRE ATT&CK framework. ",
        "title": "The Pulse of Fileless Cryptojacking Attacks: Malicious PowerShell  Scripts",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07996",
        "abstract_url": "http://arxiv.org/abs/2401.07996",
        "authors": [
            {
                "last_name": "Aiswarya",
                "first_name": "C"
            },
            {
                "last_name": "Mal",
                "first_name": "Soumodev"
            },
            {
                "last_name": "Saivasan",
                "first_name": "Prakash"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "LO"
        ],
        "abstract": "  We study the satisfiability of string constraints where context-free membership constraints may be imposed on variables. Additionally a variable may be constrained to be a subword of a word obtained by shuffling variables and their transductions. The satisfiability problem is known to be undecidable even without rational transductions. It is known to be NExptime-complete without transductions, if the subword relations between variables do not have a cyclic dependency between them. We show that the satisfiability problem stays decidable in this fragment even when rational transductions are added. It is 2NExptime-complete with context-free membership, and NExptime-complete with only regular membership. For the lower bound we prove a technical lemma that is of independent interest: The length of the shortest word in the intersection of a pushdown automaton (of size $O(n)$) and $n$ finite-state automata (each of size $O(n)$) can be double exponential in $n$. ",
        "title": "Satisfiability of Context-free String Constraints with Subword-ordering  and Transducers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08001",
        "abstract_url": "http://arxiv.org/abs/2401.08001",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Donghyun"
            },
            {
                "last_name": "Yin",
                "first_name": "Ruokai"
            },
            {
                "last_name": "Kim",
                "first_name": "Youngeun"
            },
            {
                "last_name": "Moitra",
                "first_name": "Abhishek"
            },
            {
                "last_name": "Li",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Panda",
                "first_name": "Priyadarshini"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Spiking Neural Networks (SNNs) have gained significant attention as a potentially energy-efficient alternative for standard neural networks with their sparse binary activation. However, SNNs suffer from memory and computation overhead due to spatio-temporal dynamics and multiple backpropagation computations across timesteps during training. To address this issue, we introduce Tensor Train Decomposition for Spiking Neural Networks (TT-SNN), a method that reduces model size through trainable weight decomposition, resulting in reduced storage, FLOPs, and latency. In addition, we propose a parallel computation pipeline as an alternative to the typical sequential tensor computation, which can be flexibly integrated into various existing SNN architectures. To the best of our knowledge, this is the first of its kind application of tensor decomposition in SNNs. We validate our method using both static and dynamic datasets, CIFAR10/100 and N-Caltech101, respectively. We also propose a TT-SNN-tailored training accelerator to fully harness the parallelism in TT-SNN. Our results demonstrate substantial reductions in parameter size (7.98X), FLOPs (9.25X), training time (17.7%), and training energy (28.3%) during training for the N-Caltech101 dataset, with negligible accuracy degradation. ",
        "title": "TT-SNN: Tensor Train Decomposition for Efficient Spiking Neural Network  Training",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08002",
        "abstract_url": "http://arxiv.org/abs/2401.08002",
        "authors": [
            {
                "last_name": "Ghaderi",
                "first_name": "Hamid"
            },
            {
                "last_name": "Foreman",
                "first_name": "Brandon"
            },
            {
                "last_name": "Reddy",
                "first_name": "Chandan K."
            },
            {
                "last_name": "Subbian",
                "first_name": "Vignesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traumatic Brain Injury (TBI) presents a broad spectrum of clinical presentations and outcomes due to its inherent heterogeneity, leading to diverse recovery trajectories and varied therapeutic responses. While many studies have delved into TBI phenotyping for distinct patient populations, identifying TBI phenotypes that consistently generalize across various settings and populations remains a critical research gap. Our research addresses this by employing multivariate time-series clustering to unveil TBI's dynamic intricates. Utilizing a self-supervised learning-based approach to clustering multivariate time-Series data with missing values (SLAC-Time), we analyzed both the research-centric TRACK-TBI and the real-world MIMIC-IV datasets. Remarkably, the optimal hyperparameters of SLAC-Time and the ideal number of clusters remained consistent across these datasets, underscoring SLAC-Time's stability across heterogeneous datasets. Our analysis revealed three generalizable TBI phenotypes ({\\alpha}, \\b{eta}, and {\\gamma}), each exhibiting distinct non-temporal features during emergency department visits, and temporal feature profiles throughout ICU stays. Specifically, phenotype {\\alpha} represents mild TBI with a remarkably consistent clinical presentation. In contrast, phenotype \\b{eta} signifies severe TBI with diverse clinical manifestations, and phenotype {\\gamma} represents a moderate TBI profile in terms of severity and clinical diversity. Age is a significant determinant of TBI outcomes, with older cohorts recording higher mortality rates. Importantly, while certain features varied by age, the core characteristics of TBI manifestations tied to each phenotype remain consistent across diverse populations. ",
        "title": "Discovery of Generalizable TBI Phenotypes Using Multivariate Time-Series  Clustering",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08003",
        "abstract_url": "http://arxiv.org/abs/2401.08003",
        "authors": [
            {
                "last_name": "Alcalde-Llergo",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Yeguas-Bol\u00edvar",
                "first_name": "Enrique"
            },
            {
                "last_name": "Zingoni",
                "first_name": "Andrea"
            },
            {
                "last_name": "Fuerte-Jurado",
                "first_name": "Alejandro"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Jewelry recognition is a complex task due to the different styles and designs of accessories. Precise descriptions of the various accessories is something that today can only be achieved by experts in the field of jewelry. In this work, we propose an approach for jewelry recognition using computer vision techniques and image captioning, trying to simulate this expert human behavior of analyzing accessories. The proposed methodology consist on using different image captioning models to detect the jewels from an image and generate a natural language description of the accessory. Then, this description is also utilized to classify the accessories at different levels of detail. The generated caption includes details such as the type of jewel, color, material, and design. To demonstrate the effectiveness of the proposed method in accurately recognizing different types of jewels, a dataset consisting of images of accessories belonging to jewelry stores in C\\'ordoba (Spain) has been created. After testing the different image captioning architectures designed, the final model achieves a captioning accuracy of 95\\%. The proposed methodology has the potential to be used in various applications such as jewelry e-commerce, inventory management or automatic jewels recognition to analyze people's tastes and social status. ",
        "title": "Jewelry Recognition via Encoder-Decoder Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08008",
        "abstract_url": "http://arxiv.org/abs/2401.08008",
        "authors": [
            {
                "last_name": "Alcalde-Llergo",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Garc\u00eda-Mart\u00ednez",
                "first_name": "Carlos"
            },
            {
                "last_name": "Vaquero-Abell\u00e1n",
                "first_name": "Manuel"
            },
            {
                "last_name": "Aparicio-Mart\u00ednez",
                "first_name": "Pilar"
            },
            {
                "last_name": "Yeguas-Bol\u00edvar",
                "first_name": "Enrique"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Homelessness is a social and health problem with great repercussions in Europe. Many non-governmental organisations help homeless people by collecting and analysing large amounts of information about them. However, these tasks are not always easy to perform, and hinder other of the organisations duties. The SINTECH project was created to tackle this issue proposing two different tools: a mobile application to quickly and easily collect data; and a software based on artificial intelligence which obtains interesting information from the collected data. The first one has been distributed to some Spanish organisations which are using it to conduct surveys of homeless people. The second tool implements different feature selection and association rules mining methods. These artificial intelligence techniques have allowed us to identify the most relevant features and some interesting association rules from previously collected homeless data. ",
        "title": "Analysing the Needs of Homeless People Using Feature Selection and  Mining Association Rules",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08013",
        "abstract_url": "http://arxiv.org/abs/2401.08013",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jiayang"
            },
            {
                "last_name": "Wang",
                "first_name": "Qianni"
            },
            {
                "last_name": "Feng",
                "first_name": "Liyang"
            },
            {
                "last_name": "Xie",
                "first_name": "Jun"
            },
            {
                "last_name": "Nie",
                "first_name": "Yu Marco"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "MA"
        ],
        "abstract": "  The lack of a unique user equilibrium (UE) route flow in traffic assignment has posed a significant challenge to many transportation applications. The maximum-entropy principle, which advocates for the consistent selection of the most likely solution as a representative, is often used to address the challenge. Built on a recently proposed day-to-day (DTD) discrete-time dynamical model called cumulative logit (CULO), this study provides a new behavioral underpinning for the maximum-entropy UE (MEUE) route flow. It has been proven that CULO can reach a UE state without presuming travelers are perfectly rational. Here, we further establish that CULO always converges to the MEUE route flow if (i) travelers have zero prior information about routes and thus are forced to give all routes an equal choice probability, or (ii) all travelers gather information from the same source such that the so-called general proportionality condition is satisfied. Thus, CULO may be used as a practical solution algorithm for the MEUE problem. To put this idea into practice, we propose to eliminate the route enumeration requirement of the original CULO model through an iterative route discovery scheme. We also examine the discrete-time versions of four popular continuous-time dynamical models and compare them to CULO. The analysis shows that the replicator dynamic is the only one that has the potential to reach the MEUE solution with some regularity. The analytical results are confirmed through numerical experiments. ",
        "title": "A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium  Problem",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08014",
        "abstract_url": "http://arxiv.org/abs/2401.08014",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Manish"
            },
            {
                "last_name": "Heard",
                "first_name": "Jamison"
            },
            {
                "last_name": "Saber",
                "first_name": "Eli"
            },
            {
                "last_name": "Markopoulos",
                "first_name": "Panos P."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While Convolutional Neural Networks (CNNs) excel at learning complex latent-space representations, their over-parameterization can lead to overfitting and reduced performance, particularly with limited data. This, alongside their high computational and memory demands, limits the applicability of CNNs for edge deployment. Low-rank matrix approximation has emerged as a promising approach to reduce CNN parameters, but its application presents challenges including rank selection and performance loss. To address these issues, we propose an efficient training method for CNN compression via dynamic parameter rank pruning. Our approach integrates efficient matrix factorization and novel regularization techniques, forming a robust framework for dynamic rank reduction and model compression. We use Singular Value Decomposition (SVD) to model low-rank convolutional filters and dense weight matrices and we achieve model compression by training the SVD factors with back-propagation in an end-to-end way. We evaluate our method on an array of modern CNNs, including ResNet-18, ResNet-20, and ResNet-32, and datasets like CIFAR-10, CIFAR-100, and ImageNet (2012), showcasing its applicability in computer vision. Our experiments show that the proposed method can yield substantial storage savings while maintaining or even enhancing classification performance. ",
        "title": "Convolutional Neural Network Compression via Dynamic Parameter Rank  Pruning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08015",
        "abstract_url": "http://arxiv.org/abs/2401.08015",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Quanquan C."
            },
            {
                "last_name": "Shun",
                "first_name": "Julian"
            },
            {
                "last_name": "Zablotchi",
                "first_name": "Igor"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "DS"
        ],
        "abstract": "  Maintaining a dynamic $k$-core decomposition is an important problem that identifies dense subgraphs in dynamically changing graphs. Recent work by Liu et al. [SPAA 2022] presents a parallel batch-dynamic algorithm for maintaining an approximate $k$-core decomposition. In their solution, both reads and updates need to be batched, and therefore each type of operation can incur high latency waiting for the other type to finish. To tackle most real-world workloads, which are dominated by reads, this paper presents a novel hybrid concurrent-parallel dynamic $k$-core data structure where asynchronous reads can proceed concurrently with batches of updates, leading to significantly lower read latencies. Our approach is based on tracking causal dependencies between updates, so that causally related groups of updates appear atomic to concurrent readers. Our data structure guarantees linearizability and liveness for both reads and updates, and maintains the same approximation guarantees as prior work. Our experimental evaluation on a 30-core machine shows that our approach reduces read latency by orders of magnitude compared to the batch-dynamic algorithm, up to a $\\left(4.05 \\cdot 10^{5}\\right)$-factor. Compared to an unsynchronized (non-linearizable) baseline, our read latency overhead is only up to a $3.21$-factor greater, while improving accuracy of coreness estimates by up to a factor of $52.7$. ",
        "title": "Parallel $k$-Core Decomposition with Batched Updates and Asynchronous  Reads",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08016",
        "abstract_url": "http://arxiv.org/abs/2401.08016",
        "authors": [
            {
                "last_name": "Pacchiano",
                "first_name": "Aldo"
            },
            {
                "last_name": "Ghavamzadeh",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Bartlett",
                "first_name": "Peter"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study contextual bandits in the presence of a stage-wise constraint (a constraint at each round), when the constraint must be satisfied both with high probability and in expectation. Obviously the setting where the constraint is in expectation is a relaxation of the one with high probability. We start with the linear case where both the contextual bandit problem (reward function) and the stage-wise constraint (cost function) are linear. In each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $T$-round regret bound for it. Our algorithms balance exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. In the high probability setting, we describe the minimum requirements for the action set in order for our algorithm to be tractable. In the setting that the constraint is in expectation, we further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting with regret analysis. Finally, we extend our results to the case where the reward and cost functions are both non-linear. We propose an algorithm for this case and prove a regret bound for it that characterize the function class complexity by the eluder dimension. ",
        "title": "Contextual Bandits with Stage-wise Constraints",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08017",
        "abstract_url": "http://arxiv.org/abs/2401.08017",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Ji"
            },
            {
                "last_name": "Wang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The main challenge for small object detection algorithms is to ensure accuracy while pursuing real-time performance. The RT-DETR model performs well in real-time object detection, but performs poorly in small object detection accuracy. In order to compensate for the shortcomings of the RT-DETR model in small object detection, two key improvements are proposed in this study. Firstly, The RT-DETR utilises a Transformer that receives input solely from the final layer of Backbone features. This means that the Transformer's input only receives semantic information from the highest level of abstraction in the Deep Network, and ignores detailed information such as edges, texture or color gradients that are critical to the location of small objects at lower levels of abstraction. Including only deep features can introduce additional background noise. This can have a negative impact on the accuracy of small object detection. To address this issue, we propose the fine-grained path augmentation method. This method helps to locate small objects more accurately by providing detailed information to the deep network. So, the input to the transformer contains both semantic and detailed information. Secondly, In RT-DETR, the decoder takes feature maps of different levels as input after concatenating them with equal weight. However, this operation is not effective in dealing with the complex relationship of multi-scale information captured by feature maps of different sizes. Therefore, we propose an adaptive feature fusion algorithm that assigns learnable parameters to each feature map from different levels. This allows the model to adaptively fuse feature maps from different levels and effectively integrate feature information from different scales. This enhances the model's ability to capture object features at different scales, thereby improving the accuracy of detecting small objects. ",
        "title": "Small Object Detection by DETR via Information Augmentation and Adaptive  Feature Fusion",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08018",
        "abstract_url": "http://arxiv.org/abs/2401.08018",
        "authors": [
            {
                "last_name": "Zaman",
                "first_name": "Mobasshira"
            },
            {
                "last_name": "Hwang",
                "first_name": "Jaejin"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This study evaluated the effect of virtual hand representation on the typing performance, upper extremity angle, neck muscle activity, and usability during virtual reality (VR) typing. A total of 15 participants (7 females and 8 males) performed a typing task with and without virtual hand representations. The optical motion capture data was utilized to capture the upper extremity angles, and electromyography device was used to collect the neck muscle activities (left and right splenius capitis). The results showed that the typing performance, upper extremity angle, neck muscle activity, and usability were significantly different with and without virtual hand representations. With the virtual hand representation, net typing speed (WPM) and usability increased significantly by 171.4% and 25% compared to the without hand representation. Without the virtual hand representation, participants showed increased wrist extension, and decreased right shoulder abduction angles. The variability of the neck rotation was increased while typing without the virtual hand representation. The neck muscle activities were increased with the virtual hand representation. The results suggest that typing with the virtual hand representation could positively affect the typing performance and usability and reduce the risk of the musculoskeletal disorders of the upper extremity. Future study could explore the effect of the virtual hand representation for users with varying levels of typing skills. ",
        "title": "Effects of Virtual Hand Representation on the Typing Performance, Upper  Extremity Angle, and Neck Muscle Activity during Virtual Reality Typing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08019",
        "abstract_url": "http://arxiv.org/abs/2401.08019",
        "authors": [
            {
                "last_name": "Phosavanh",
                "first_name": "Johnson"
            },
            {
                "last_name": "Matsypura",
                "first_name": "Dmytro"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  The degree centrality of a node, defined to be the number of nodes adjacent to it, can be used as a measure of importance of a node to the structure of a network. This metric can be extended to paths in a network, where the degree centrality of a path is defined to be the number of nodes adjacent to it. In this paper, we reconsider the problem of finding the most degree-central shortest path in an unweighted network. We propose a polynomial algorithm with the worst-case running time of $O(|V|^3(\\Delta(G))^2)$, where $|V|$ is the number of vertices in the network and $\\Delta(G)$ is the maximum degree of the graph. We conduct a numerical study of our algorithm on synthetic and real-world networks and compare our results to the existing literature. In addition, we show that the same problem is NP-hard when a weighted graph is considered. ",
        "title": "A polynomial algorithm for the most degree-central shortest path problem",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08020",
        "abstract_url": "http://arxiv.org/abs/2401.08020",
        "authors": [
            {
                "last_name": "Salim",
                "first_name": "Shahreen"
            },
            {
                "last_name": "Hoque",
                "first_name": "Md Naimul"
            },
            {
                "last_name": "Mueller",
                "first_name": "Klaus"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Causal belief is a cognitive practice that humans apply everyday to reason about cause and effect relations between factors, phenomena, or events. Like optical illusions, humans are prone to drawing causal relations between events that are only coincidental (i.e., causal illusions). Researchers in domains such as cognitive psychology and healthcare often use logistically expensive experiments to understand causal beliefs and illusions. In this paper, we propose Belief Miner, a crowdsourcing method for evaluating people's causal beliefs and illusions. Our method uses the (dis)similarities between the causal relations collected from the crowds and experts to surface the causal beliefs and illusions. Through an iterative design process, we developed a web-based interface for collecting causal relations from a target population. We then conducted a crowdsourced experiment with 101 workers on Amazon Mechanical Turk and Prolific using this interface and analyzed the collected data with Belief Miner. We discovered a variety of causal beliefs and potential illusions, and we report the design implications for future research. ",
        "title": "Belief Miner: A Methodology for Discovering Causal Beliefs and Causal  Illusions from General Populations",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08021",
        "abstract_url": "http://arxiv.org/abs/2401.08021",
        "authors": [
            {
                "last_name": "Tsai",
                "first_name": "Chia Hsuan"
            },
            {
                "last_name": "Elyasi",
                "first_name": "Fatemeh"
            },
            {
                "last_name": "Ren",
                "first_name": "Peng"
            },
            {
                "last_name": "Manduchi",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  We introduce two iOS apps that have been designed to support wayfinding and backtracking for blind travelers navigating in indoor building environments. Wayfinding involves determining and following a route through the building's corridors to reach a destination, and assumes that the app has access to the floor plan of the building. Backtracking one's route, on the other hand, requires no map knowledge. Our apps only use the inertial and magnetic sensors of the smartphone, and thus require no infrastructure modification (e.g., installation and support of BLE beacons). Unlike systems that use the phone's camera, users of our apps can conveniently keep their phone tucked inside a pocket, while interacting with the apps using a smartwatch. Routing directions are given via speech. Both apps were tested in a user study with seven blind participants, who used them while navigating a campus building. ",
        "title": "All the Way There and Back: Inertial-Based, Phone-in-Pocket Indoor  Wayfinding and Backtracking Apps for Blind Travelers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08022",
        "abstract_url": "http://arxiv.org/abs/2401.08022",
        "authors": [
            {
                "last_name": "Natarajan",
                "first_name": "Ramkumar"
            },
            {
                "last_name": "Yang",
                "first_name": "Hanlan"
            },
            {
                "last_name": "Xie",
                "first_name": "Qintong"
            },
            {
                "last_name": "Oza",
                "first_name": "Yash"
            },
            {
                "last_name": "Das",
                "first_name": "Manash Pratim"
            },
            {
                "last_name": "Islam",
                "first_name": "Fahad"
            },
            {
                "last_name": "Saleem",
                "first_name": "Muhammad Suhail"
            },
            {
                "last_name": "Choset",
                "first_name": "Howie"
            },
            {
                "last_name": "Likhachev",
                "first_name": "Maxim"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We are interested in studying sports with robots and starting with the problem of intercepting a projectile moving toward a robot manipulator equipped with a shield. To successfully perform this task, the robot needs to (i) detect the incoming projectile, (ii) predict the projectile's future motion, (iii) plan a minimum-time rapid trajectory that can evade obstacles and intercept the projectile, and (iv) execute the planned trajectory. These four steps must be performed under the manipulator's dynamic limits and extreme time constraints (<350ms in our setting) to successfully intercept the projectile. In addition, we want these trajectories to be smooth to reduce the robot's joint torques and the impulse on the platform on which it is mounted. To this end, we propose a kinodynamic motion planning framework that preprocesses smooth trajectories offline to allow real-time collision-free executions online. We present an end-to-end pipeline along with our planning framework, including perception, prediction, and execution modules. We evaluate our framework experimentally in simulation and show that it has a higher blocking success rate than the baselines. Further, we deploy our pipeline on a robotic system comprising an industrial arm (ABB IRB-1600) and an onboard stereo camera (ZED 2i), which achieves a 78% success rate in projectile interceptions. ",
        "title": "Preprocessing-based Kinodynamic Motion Planning Framework for  Intercepting Projectiles using a Robot Manipulator",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08023",
        "abstract_url": "http://arxiv.org/abs/2401.08023",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lihao"
            },
            {
                "last_name": "Sun",
                "first_name": "Haijian"
            },
            {
                "last_name": "Zeng",
                "first_name": "Yong"
            },
            {
                "last_name": "Hu",
                "first_name": "Rose Qingyang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "CV"
        ],
        "abstract": "  As 5G technology becomes increasingly established, the anticipation for 6G is growing, which promises to deliver faster and more reliable wireless connections via cutting-edge radio technologies. However, efficient management method of the large-scale antenna arrays deployed by those radio technologies is crucial. Traditional management methods are mainly reactive, usually based on feedback from users to adapt to the dynamic wireless channel. However, a more promising approach lies in the prediction of spatial channel state information (spatial-CSI), which is an all-inclusive channel characterization and consists of all the feasible line-of-sight (LoS) and non-line-of-sight (NLoS) paths between the transmitter (Tx) and receiver (Rx), with the three-dimension (3D) trajectory, attenuation, phase shift, delay, and polarization of each path. Advances in hardware and neural networks make it possible to predict such spatial-CSI using precise environmental information, and further look into the possibility of holographic communication, which implies complete control over every aspect of the radio waves emitted. Based on the integration of holographic communication and digital twin, we proposed a new framework, digital radio twin, which takes advantages from both the digital world and deterministic control over radio waves, supporting a wide range of high-level applications. As a preliminary attempt towards this visionary direction, in this paper, we explore the use of generative artificial intelligence (AI) to pinpoint the valid paths in a given environment, demonstrating promising results, and highlighting the potential of this approach in driving forward the evolution of 6G wireless communication technologies. ",
        "title": "Spatial Channel State Information Prediction with Generative AI: Towards  Holographic Communication and Digital Radio Twin",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08025",
        "abstract_url": "http://arxiv.org/abs/2401.08025",
        "authors": [
            {
                "last_name": "Akter",
                "first_name": "Syeda Nahida"
            },
            {
                "last_name": "Madaan",
                "first_name": "Aman"
            },
            {
                "last_name": "Lee",
                "first_name": "Sangwu"
            },
            {
                "last_name": "Yang",
                "first_name": "Yiming"
            },
            {
                "last_name": "Nyberg",
                "first_name": "Eric"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The potential of Vision-Language Models (\\textsc{vlm}s) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose \\textsc{Self-Imagine}. We leverage a single Vision-Language Model (\\textsc{vlm}) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same \\vlm to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach in three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art \\textsc{vlm}. Our approach boosts the performance of \\textsc{vlm} on all math tasks (\\gsm: +4.62\\%; \\asdiv: +4.49\\%; \\svamp: +9.30\\%) and the majority of the general-purpose reasoning tasks by 0.4\\% to 13.20\\% while achieving comparable performance in other tasks.   Code and data at https://github.com/snat1505027/self-imagine . ",
        "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using  Self-Imagination",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08026",
        "abstract_url": "http://arxiv.org/abs/2401.08026",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Fengzhu"
            },
            {
                "last_name": "Gao",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation is previously oversimplified as summarization of fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim for \\underline{Ex}plainable fact-checking of real-world \\underline{Claim}s, and introduce JustiLM, a novel few-shot \\underline{Justi}fication generation based on retrieval-augmented \\underline{L}anguage \\underline{M}odel by using fact-check articles as auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension. ",
        "title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking  of Real-world Claims",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08032",
        "abstract_url": "http://arxiv.org/abs/2401.08032",
        "authors": [
            {
                "last_name": "Omee",
                "first_name": "Sadman Sadeed"
            },
            {
                "last_name": "Fu",
                "first_name": "Nihang"
            },
            {
                "last_name": "Dong",
                "first_name": "Rongzhi"
            },
            {
                "last_name": "Hu",
                "first_name": "Ming"
            },
            {
                "last_name": "Hu",
                "first_name": "Jianjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In real-world material research, machine learning (ML) models are usually expected to predict and discover novel exceptional materials that deviate from the known materials. It is thus a pressing question to provide an objective evaluation of ML model performances in property prediction of out-of-distribution (OOD) materials that are different from the training set distribution. Traditional performance evaluation of materials property prediction models through random splitting of the dataset frequently results in artificially high performance assessments due to the inherent redundancy of typical material datasets. Here we present a comprehensive benchmark study of structure-based graph neural networks (GNNs) for extrapolative OOD materials property prediction. We formulate five different categories of OOD ML problems for three benchmark datasets from the MatBench study. Our extensive experiments show that current state-of-the-art GNN algorithms significantly underperform for the OOD property prediction tasks on average compared to their baselines in the MatBench study, demonstrating a crucial generalization gap in realistic material prediction tasks. We further examine the latent physical spaces of these GNN models and identify the sources of CGCNN, ALIGNN, and DeeperGATGNN's significantly more robust OOD performance than those of the current best models in the MatBench study (coGN and coNGN), and provide insights to improve their performance. ",
        "title": "Structure-based out-of-distribution (OOD) materials property prediction:  a benchmark study",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08035",
        "abstract_url": "http://arxiv.org/abs/2401.08035",
        "authors": [
            {
                "last_name": "Saha",
                "first_name": "Chandrika"
            },
            {
                "last_name": "Rahman",
                "first_name": "Md. Mostafijur"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Handwritten character recognition is a crucial task because of its abundant applications. The recognition task of Bangla handwritten characters is especially challenging because of the cursive nature of Bangla characters and the presence of compound characters with more than one way of writing. In this paper, a classification model based on the ensembling of several Convolutional Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic characters, compound characters, numerals, and modifiers. Three different models based on the idea of state-of-the-art CNN models like Inception, ResNet, and DenseNet have been trained with both augmented and non-augmented inputs. Finally, all these models are averaged or ensembled to get the finishing model. Rigorous experimentation on three benchmark Bangla handwritten characters datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited significant recognition accuracies compared to some recent CNN-based research. The top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and the top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb, BanglaLekha-Isolated, and Ekush datasets respectively. ",
        "title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of  Convolutional Neural Network",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08036",
        "abstract_url": "http://arxiv.org/abs/2401.08036",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Haibin"
            },
            {
                "last_name": "Chang",
                "first_name": "Jun"
            },
            {
                "last_name": "Lu",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Huabing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  3D lanes offer a more comprehensive understanding of the road surface geometry than 2D lanes, thereby providing crucial references for driving decisions and trajectory planning. While many efforts aim to improve prediction accuracy, we recognize that an efficient network can bring results closer to lane modeling. However, if the modeling data is imprecise, the results might not accurately capture the real-world scenario. Therefore, accurate lane modeling is essential to align prediction results closely with the environment. This study centers on efficient and accurate lane modeling, proposing a joint modeling approach that combines Bezier curves and interpolation methods. Furthermore, based on this lane modeling approach, we developed a Global2Local Lane Matching method with Bezier Control-Point and Key-Point, which serve as a comprehensive solution that leverages hierarchical features with two mathematical models to ensure a precise match. We also introduce a novel 3D Spatial Constructor, representing an exploration of 3D surround-view lane detection research. The framework is suitable for front-view or surround-view 3D lane detection. By directly outputting the key points of lanes in 3D space, it overcomes the limitations of anchor-based methods, enabling accurate prediction of closed-loop or U-shaped lanes and effective adaptation to complex road conditions. This innovative method establishes a new benchmark in front-view 3D lane detection on the Openlane dataset and achieves competitive performance in surround-view 2D lane detection on the Argoverse2 dataset. ",
        "title": "3D Lane Detection from Front or Surround-View using Joint-Modeling &  Matching",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08037",
        "abstract_url": "http://arxiv.org/abs/2401.08037",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Akash Deep"
            },
            {
                "last_name": "Wang",
                "first_name": "Brian"
            },
            {
                "last_name": "Garcia",
                "first_name": "Luis"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiang"
            },
            {
                "last_name": "Srivastava",
                "first_name": "Mani"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  While IoT sensors in physical spaces have provided utility and comfort in our lives, their instrumentation in private and personal spaces has led to growing concerns regarding privacy. The existing notion behind IoT privacy is that the sensors whose data can easily be understood and interpreted by humans (such as cameras) are more privacy-invasive than sensors that are not human-understandable, such as RF (radio-frequency) sensors. However, given recent advancements in machine learning, we can not only make sensitive inferences on RF data but also translate between modalities. Thus, the existing notions of privacy for IoT sensors need to be revisited. In this paper, our goal is to understand what factors affect the privacy notions of a non-expert user (someone who is not well-versed in privacy concepts). To this regard, we conduct an online study of 162 participants from the USA to find out what factors affect the privacy perception of a user regarding an RF-based device or a sensor. Our findings show that a user's perception of privacy not only depends upon the data collected by the sensor but also on the inferences that can be made on that data, familiarity with the device and its form factor as well as the control a user has over the device design and its data policies. When the data collected by the sensor is not human-interpretable, it is the inferences that can be made on the data and not the data itself that users care about when making informed decisions regarding device privacy. ",
        "title": "Understanding factors behind IoT privacy -- A user's perspective on RF  sensors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08038",
        "abstract_url": "http://arxiv.org/abs/2401.08038",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Lie",
                "first_name": "David"
            },
            {
                "last_name": "Austin",
                "first_name": "Lisa"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CR",
            "HC",
            "LG"
        ],
        "abstract": "  A significant challenge to training accurate deep learning models on privacy policies is the cost and difficulty of obtaining a large and comprehensive set of training data. To address these challenges, we present Calpric , which combines automatic text selection and segmentation, active learning and the use of crowdsourced annotators to generate a large, balanced training set for privacy policies at low cost. Automated text selection and segmentation simplifies the labeling task, enabling untrained annotators from crowdsourcing platforms, like Amazon's Mechanical Turk, to be competitive with trained annotators, such as law students, and also reduces inter-annotator agreement, which decreases labeling cost. Having reliable labels for training enables the use of active learning, which uses fewer training samples to efficiently cover the input space, further reducing cost and improving class and data category balance in the data set. The combination of these techniques allows Calpric to produce models that are accurate over a wider range of data categories, and provide more detailed, fine-grain labels than previous work. Our crowdsourcing process enables Calpric to attain reliable labeled data at a cost of roughly $0.92-$1.71 per labeled text segment. Calpric 's training process also generates a labeled data set of 16K privacy policy text segments across 9 Data categories with balanced positive and negative samples. ",
        "title": "Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with  Crowdsourcing and Active Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08041",
        "abstract_url": "http://arxiv.org/abs/2401.08041",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Jiaming"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Duong Thuy Anh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Duong Tung"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Edge computing (EC) promises to deliver low-latency and ubiquitous computation to numerous devices at the network edge. This paper aims to jointly optimize edge node (EN) placement and resource allocation for an EC platform, considering demand uncertainty. Diverging from existing approaches treating uncertainties as exogenous, we propose a novel two-stage decision-dependent distributionally robust optimization (DRO) framework to effectively capture the interdependence between EN placement decisions and uncertain demands. The first stage involves making EN placement decisions, while the second stage optimizes resource allocation after uncertainty revelation. We present an exact mixed-integer linear program reformulation for solving the underlying ``min-max-min\" two-stage model. We further introduce a valid inequality method to enhance computational efficiency, especially for large-scale networks. Extensive numerical experiments demonstrate the benefits of considering endogenous uncertainties and the advantages of the proposed model and approach. ",
        "title": "Two-Stage Distributionally Robust Edge Node Placement Under Endogenous  Demand Uncertainty",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08043",
        "abstract_url": "http://arxiv.org/abs/2401.08043",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Yi-Fan"
            },
            {
                "last_name": "Xu",
                "first_name": "Wanting"
            },
            {
                "last_name": "Wang",
                "first_name": "Xia"
            },
            {
                "last_name": "Wang",
                "first_name": "Yifu"
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras. ",
        "title": "Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging  Conditions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08045",
        "abstract_url": "http://arxiv.org/abs/2401.08045",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Xu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haiming"
            },
            {
                "last_name": "Cai",
                "first_name": "Yingjie"
            },
            {
                "last_name": "Guo",
                "first_name": "Jingming"
            },
            {
                "last_name": "Qiu",
                "first_name": "Weichao"
            },
            {
                "last_name": "Gao",
                "first_name": "Bin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kaiqiang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            },
            {
                "last_name": "Jin",
                "first_name": "Huan"
            },
            {
                "last_name": "Gao",
                "first_name": "Jiantao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhen"
            },
            {
                "last_name": "Jiang",
                "first_name": "Lihui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Dai",
                "first_name": "Dengxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Bingbing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. ",
        "title": "Forging Vision Foundation Models for Autonomous Driving: Challenges,  Methodologies, and Opportunities",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08046",
        "abstract_url": "http://arxiv.org/abs/2401.08046",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Guo",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chang",
                "first_name": "Ching-Chun"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The emergence of large language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and broader community. While these models offer numerous advantages in terms of revolutionizing work and study methods, they have also garnered significant attention due to their potential negative consequences. One example is generating academic reports or papers with little to no human contribution. Consequently, researchers have focused on developing detectors to address the misuse of LLMs. However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount. In this paper, we present a comprehensive analysis of the impact of prompts on the text generated by LLMs and highlight the potential lack of robustness in one of the current state-of-the-art GPT detectors. To mitigate these issues concerning the misuse of LLMs in academic writing, we propose a reference-based Siamese detector named Synthetic-Siamese which takes a pair of texts, one as the inquiry and the other as the reference. Our method effectively addresses the lack of robustness of previous detectors (OpenAI detector and DetectGPT) and significantly improves the baseline performances in realistic academic writing scenarios by approximately 67% to 95%. ",
        "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic  Writing: A Comprehensive Analysis",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08047",
        "abstract_url": "http://arxiv.org/abs/2401.08047",
        "authors": [
            {
                "last_name": "Chowdhury",
                "first_name": "Somnath Basu Roy"
            },
            {
                "last_name": "Monath",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Dubey",
                "first_name": "Avinava"
            },
            {
                "last_name": "Zaheer",
                "first_name": "Manzil"
            },
            {
                "last_name": "McCallum",
                "first_name": "Andrew"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Amr"
            },
            {
                "last_name": "Chaturvedi",
                "first_name": "Snigdha"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Extractive opinion summarization involves automatically producing a summary of text about an entity (e.g., a product's reviews) by extracting representative sentences that capture prevalent opinions in the review set. Typically, in online marketplaces user reviews accrue over time, and opinion summaries need to be updated periodically to provide customers with up-to-date information. In this work, we study the task of extractive opinion summarization in an incremental setting, where the underlying review set evolves over time. Many of the state-of-the-art extractive opinion summarization approaches are centrality-based, such as CentroidRank. CentroidRank performs extractive summarization by selecting a subset of review sentences closest to the centroid in the representation space as the summary. However, these methods are not capable of operating efficiently in an incremental setting, where reviews arrive one at a time. In this paper, we present an efficient algorithm for accurately computing the CentroidRank summaries in an incremental setting. Our approach, CoverSumm, relies on indexing review representations in a cover tree and maintaining a reservoir of candidate summary review sentences. CoverSumm's efficacy is supported by a theoretical and empirical analysis of running time. Empirically, on a diverse collection of data (both real and synthetically created to illustrate scaling considerations), we demonstrate that CoverSumm is up to 25x faster than baseline methods, and capable of adapting to nuanced changes in data distribution. We also conduct human evaluations of the generated summaries and find that CoverSumm is capable of producing informative summaries consistent with the underlying review set. ",
        "title": "Incremental Extractive Opinion Summarization Using Cover Trees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08049",
        "abstract_url": "http://arxiv.org/abs/2401.08049",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Bingyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xulong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ning"
            },
            {
                "last_name": "Yu",
                "first_name": "Jun"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jing"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianzong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "SD"
        ],
        "abstract": "  In recent years, the field of talking faces generation has attracted considerable attention, with certain methods adept at generating virtual faces that convincingly imitate human expressions. However, existing methods face challenges related to limited generalization, particularly when dealing with challenging identities. Furthermore, methods for editing expressions are often confined to a singular emotion, failing to adapt to intricate emotions. To overcome these challenges, this paper proposes EmoTalker, an emotionally editable portraits animation approach based on the diffusion model. EmoTalker modifies the denoising process to ensure preservation of the original portrait's identity during inference. To enhance emotion comprehension from text input, Emotion Intensity Block is introduced to analyze fine-grained emotions and strengths derived from prompts. Additionally, a crafted dataset is harnessed to enhance emotion comprehension within prompts. Experiments show the effectiveness of EmoTalker in generating high-quality, emotionally customizable facial expressions. ",
        "title": "EmoTalker: Emotionally Editable Talking Face Generation via Diffusion  Model",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08053",
        "abstract_url": "http://arxiv.org/abs/2401.08053",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhixuan"
            },
            {
                "last_name": "Schaldenbrand",
                "first_name": "Peter"
            },
            {
                "last_name": "Okogwu",
                "first_name": "Beverley-Claire"
            },
            {
                "last_name": "Peng",
                "first_name": "Wenxuan"
            },
            {
                "last_name": "Yun",
                "first_name": "Youngsik"
            },
            {
                "last_name": "Hundt",
                "first_name": "Andrew"
            },
            {
                "last_name": "Kim",
                "first_name": "Jihie"
            },
            {
                "last_name": "Oh",
                "first_name": "Jean"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate representation in media is known to improve the well-being of the people who consume it. Generative image models trained on large web-crawled datasets such as LAION are known to produce images with harmful stereotypes and misrepresentations of cultures. We improve inclusive representation in generated images by (1) engaging with communities to collect a culturally representative dataset that we call the Cross-Cultural Understanding Benchmark (CCUB) and (2) proposing a novel Self-Contrastive Fine-Tuning (SCoFT) method that leverages the model's known biases to self-improve. SCoFT is designed to prevent overfitting on small datasets, encode only high-level information from the data, and shift the generated distribution away from misrepresentations encoded in a pretrained model. Our user study conducted on 51 participants from 5 different countries based on their self-selected national cultural affiliation shows that fine-tuning on CCUB consistently generates images with higher cultural relevance and fewer stereotypes when compared to the Stable Diffusion baseline, which is further improved with our SCoFT technique. ",
        "title": "SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08054",
        "abstract_url": "http://arxiv.org/abs/2401.08054",
        "authors": [
            {
                "last_name": "Suzuki",
                "first_name": "Taro"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a novel positioning technique suitable for use in mobile robots in urban environments in which large global navigation satellite system (GNSS) positioning errors occur because of multipath signals. During GNSS positioning, the GNSS satellites that are obstructed by buildings emit reflection and diffraction signals, which are called non-line-of-sight (NLOS) multipath signals. These multipath signals cause major positioning errors. The key concept considered in this paper is the estimation of a user's position using the likelihood of the position hypotheses computed from the GNSS pseudoranges, consisting only of LOS signals based on the analysis of the pseudorange residuals. To determine the NLOS GNSS signals from the pseudorange residuals at the user's position, it is necessary to accurately determine the position before the computation of the pseudorange residuals. This problem is solved using a particle filter. We propose a likelihood estimation method using the Mahalanobis distance between the hypotheses of the user's position computed from only the LOS pseudoranges and the particles. To confirm the effectiveness of the proposed technique, a positioning test was performed in a real-world urban environment. The results demonstrated that the proposed method is effective for accurately estimating the user's position in urban canyons. ",
        "title": "Mobile robot localization with GNSS multipath detection using  pseudorange residuals",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08056",
        "abstract_url": "http://arxiv.org/abs/2401.08056",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Haoran"
            },
            {
                "last_name": "Xu",
                "first_name": "Chang"
            },
            {
                "last_name": "Yang",
                "first_name": "Wen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ruixiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Xia",
                "first_name": "Gui-Song"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Precise detection of tiny objects in remote sensing imagery remains a significant challenge due to their limited visual information and frequent occurrence within scenes. This challenge is further exacerbated by the practical burden and inherent errors associated with manual annotation: annotating tiny objects is laborious and prone to errors (i.e., label noise). Training detectors for such objects using noisy labels often leads to suboptimal performance, with networks tending to overfit on noisy labels. In this study, we address the intricate issue of tiny object detection under noisy label supervision. We systematically investigate the impact of various types of noise on network training, revealing the vulnerability of object detectors to class shifts and inaccurate bounding boxes for tiny objects. To mitigate these challenges, we propose a DeNoising Tiny Object Detector (DN-TOD), which incorporates a Class-aware Label Correction (CLC) scheme to address class shifts and a Trend-guided Learning Strategy (TLS) to handle bounding box noise. CLC mitigates inaccurate class supervision by identifying and filtering out class-shifted positive samples, while TLS reduces noisy box-induced erroneous supervision through sample reweighting and bounding box regeneration. Additionally, Our method can be seamlessly integrated into both one-stage and two-stage object detection pipelines. Comprehensive experiments conducted on synthetic (i.e., noisy AI-TOD-v2.0 and DOTA-v2.0) and real-world (i.e., AI-TOD) noisy datasets demonstrate the robustness of DN-TOD under various types of label noise. Notably, when applied to the strong baseline RFLA, DN-TOD exhibits a noteworthy performance improvement of 4.9 points under 40% mixed noise. Datasets, codes, and models will be made publicly available. ",
        "title": "Robust Tiny Object Detection in Aerial Images amidst Label Noise",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08058",
        "abstract_url": "http://arxiv.org/abs/2401.08058",
        "authors": [
            {
                "last_name": "Gamble",
                "first_name": "Cooper"
            },
            {
                "last_name": "Faghani",
                "first_name": "Shahriar"
            },
            {
                "last_name": "Erickson",
                "first_name": "Bradley J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  As deep learning (DL) continues to demonstrate its ability in radiological tasks, it is critical that we optimize clinical DL solutions to include safety. One of the principal concerns in the clinical adoption of DL tools is trust. This study aims to apply conformal prediction as a step toward trustworthiness for DL in radiology. This is a retrospective study of 491 non-contrast head CTs from the CQ500 dataset, in which three senior radiologists annotated slices containing intracranial hemorrhage (ICH). The dataset was split into definite and challenging subsets, where challenging images were defined to those in which there was disagreement among readers. A DL model was trained on 146 patients (10,815 slices) from the definite data (training dataset) to perform ICH localization and classification for five classes of ICH. To develop an uncertainty-aware DL model, 1,546 cases of the definite data (calibration dataset) was used for Mondrian conformal prediction (MCP). The uncertainty-aware DL model was tested on 8,401 definite and challenging cases to assess its ability to identify challenging cases. After the MCP procedure, the model achieved an F1 score of 0.920 for ICH classification on the test dataset. Additionally, it correctly identified 6,837 of the 6,856 total challenging cases as challenging (99.7% accuracy). It did not incorrectly label any definite cases as challenging. The uncertainty-aware ICH detector performs on par with state-of-the-art models. MCP's performance in detecting challenging cases demonstrates that it is useful in automated ICH detection and promising for trustworthiness in radiological DL. ",
        "title": "Toward Clinically Trustworthy Deep Learning: Applying Conformal  Prediction to Intracranial Hemorrhage Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08061",
        "abstract_url": "http://arxiv.org/abs/2401.08061",
        "authors": [
            {
                "last_name": "Duan",
                "first_name": "Lei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Carlson",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Fusing abundant satellite data with sparse ground measurements constitutes a major challenge in climate modeling. To address this, we propose a strategy to augment the training dataset by introducing unlabeled satellite images paired with pseudo-labels generated through a spatial interpolation technique known as ordinary kriging, thereby making full use of the available satellite data resources. We show that the proposed data augmentation strategy helps enhance the performance of the state-of-the-art convolutional neural network-random forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy improvement in spatial correlation and a reduction in prediction error. ",
        "title": "Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label  Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08064",
        "abstract_url": "http://arxiv.org/abs/2401.08064",
        "authors": [
            {
                "last_name": "Allen",
                "first_name": "Scott E."
            },
            {
                "last_name": "Kizilcec",
                "first_name": "Ren\u00e9 F."
            },
            {
                "last_name": "Redish",
                "first_name": "A. David"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  More than 30 years of research has firmly established the vital role of trust in human organizations and relationships, but the underlying mechanisms by which people build, lose, and rebuild trust remains incompletely understood. We propose a mechanistic model of trust that is grounded in the modern neuroscience of decision making. Since trust requires anticipating the future actions of others, any mechanistic model must be built upon up-to-date theories on how the brain learns, represents, and processes information about the future within its decision-making systems. Contemporary neuroscience has revealed that decision making arises from multiple parallel systems that perform distinct, complementary information processing. Each system represents information in different forms, and therefore learns via different mechanisms. When an act of trust is reciprocated or violated, this provides new information that can be used to anticipate future actions. The taxonomy of neural information representations that is the basis for the system boundaries between neural decision-making systems provides a taxonomy for categorizing different forms of trust and generating mechanistic predictions about how these forms of trust are learned and manifested in human behavior. Three key predictions arising from our model are (1) strategic risk-taking can reveal how to best proceed in a relationship, (2) human organizations and environments can be intentionally designed to encourage trust among their members, and (3) violations of trust need not always degrade trust, but can also provide opportunities to build trust. ",
        "title": "A new model of trust based on neural information processing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08066",
        "abstract_url": "http://arxiv.org/abs/2401.08066",
        "authors": [
            {
                "last_name": "Chiu",
                "first_name": "Ching-Hao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yu-Jen"
            },
            {
                "last_name": "Wu",
                "first_name": "Yawen"
            },
            {
                "last_name": "Shi",
                "first_name": "Yiyu"
            },
            {
                "last_name": "Ho",
                "first_name": "Tsung-Yi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In medical image diagnosis, fairness has become increasingly crucial. Without bias mitigation, deploying unfair AI would harm the interests of the underprivileged population and potentially tear society apart. Recent research addresses prediction biases in deep learning models concerning demographic groups (e.g., gender, age, and race) by utilizing demographic (sensitive attribute) information during training. However, many sensitive attributes naturally exist in dermatological disease images. If the trained model only targets fairness for a specific attribute, it remains unfair for other attributes. Moreover, training a model that can accommodate multiple sensitive attributes is impractical due to privacy concerns. To overcome this, we propose a method enabling fair predictions for sensitive attributes during the testing phase without using such information during training. Inspired by prior work highlighting the impact of feature entanglement on fairness, we enhance the model features by capturing the features related to the sensitive and target attributes and regularizing the feature entanglement between corresponding classes. This ensures that the model can only classify based on the features related to the target attribute without relying on features associated with sensitive attributes, thereby improving fairness and accuracy. Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets. ",
        "title": "Achieve Fairness without Demographics for Dermatological Disease  Diagnosis",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08067",
        "abstract_url": "http://arxiv.org/abs/2401.08067",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zuotian"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Tang",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Pengyue"
            },
            {
                "last_name": "Jin",
                "first_name": "Nanxin"
            },
            {
                "last_name": "Eadon",
                "first_name": "Michael"
            },
            {
                "last_name": "Song",
                "first_name": "Qianqian"
            },
            {
                "last_name": "Chen",
                "first_name": "Yingjie"
            },
            {
                "last_name": "Su",
                "first_name": "Jing"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Objective: Our objective is to develop and validate TrajVis, an interactive tool that assists clinicians in using artificial intelligence (AI) models to leverage patients' longitudinal electronic medical records (EMR) for personalized precision management of chronic disease progression. Methods: We first perform requirement analysis with clinicians and data scientists to determine the visual analytics tasks of the TrajVis system as well as its design and functionalities. A graph AI model for chronic kidney disease (CKD) trajectory inference named DEPOT is used for system development and demonstration. TrajVis is implemented as a full-stack web application with synthetic EMR data derived from the Atrium Health Wake Forest Baptist Translational Data Warehouse and the Indiana Network for Patient Care research database. A case study with a nephrologist and a user experience survey of clinicians and data scientists are conducted to evaluate the TrajVis system. Results: The TrajVis clinical information system is composed of four panels: the Patient View for demographic and clinical information, the Trajectory View to visualize the DEPOT-derived CKD trajectories in latent space, the Clinical Indicator View to elucidate longitudinal patterns of clinical features and interpret DEPOT predictions, and the Analysis View to demonstrate personal CKD progression trajectories. System evaluations suggest that TrajVis supports clinicians in summarizing clinical data, identifying individualized risk predictors, and visualizing patient disease progression trajectories, overcoming the barriers of AI implementation in healthcare. Conclusion: TrajVis bridges the gap between the fast-growing AI/ML modeling and the clinical use of such models for personalized and precision management of chronic diseases. ",
        "title": "TrajVis: a visual clinical decision support system to translate  artificial intelligence trajectory models in the precision management of  chronic kidney disease",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08068",
        "abstract_url": "http://arxiv.org/abs/2401.08068",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Beibei"
            },
            {
                "last_name": "Li",
                "first_name": "Weiling"
            },
            {
                "last_name": "Fang",
                "first_name": "Yan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Event cameras are neuromorphic sensors that capture asynchronous and sparse event stream when per-pixel brightness changes. The state-of-the-art processing methods for event signals typically aggregate events into a frame or a grid. However, events are dense in time, these works are limited to local information of events due to the stacking. In this paper, we present a novel spatiotemporal representation learning method which can capture the global correlations of all events in the event stream simultaneously by tensor decomposition. In addition, with the events are sparse in space, we propose an Elastic Net-incorporated tensor network (ENTN) model to obtain more spatial and temporal details about event stream. Empirically, the results indicate that our method can represent the spatiotemporal correlation of events with high quality, and can achieve effective results in applications like filtering noise compared with the state-of-the-art methods. ",
        "title": "Representation Learning on Event Stream via an Elastic Net-incorporated  Tensor Network",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08073",
        "abstract_url": "http://arxiv.org/abs/2401.08073",
        "authors": [
            {
                "last_name": "Ramanathan",
                "first_name": "Alagappan"
            },
            {
                "last_name": "Sankaran",
                "first_name": "Rishika"
            },
            {
                "last_name": "Jyothi",
                "first_name": "Sangeetha Abdu"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  A resilient Internet infrastructure is critical in our highly interconnected society. However, the Internet faces several vulnerabilities, ranging from natural disasters to human activities, that can impact the physical layer and, in turn, the higher network layers, such as IP links. In this paper, we introduce Xaminer, the first Internet cross-layer resilience analysis tool, to evaluate the interplay between physical- and network-layer failures. Using a cross-layer Internet map and a failure event model, Xaminer generates a risk profile encompassing a cross-layer impact report, critical infrastructure identification at each layer, and the discovery of trends and patterns under different failure event settings. Xaminer's key strengths lie in its adaptability to diverse disaster scenarios, the ability to assess risks at various granularities, and the capability to generate joint risk profiles for multiple events. We demonstrate Xaminer's capabilities in cross-layer analysis across a spectrum of disaster event models and regions, showcasing its potential role in facilitating well-informed decision-making for resilience planning and deployments. ",
        "title": "Xaminer: An Internet Cross-Layer Resilience Analysis Tool",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08077",
        "abstract_url": "http://arxiv.org/abs/2401.08077",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Shubham"
            },
            {
                "last_name": "Bhat",
                "first_name": "Mayur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments. ",
        "title": "Transformer-based approach for Ethereum Price Prediction Using  Crosscurrency correlation and Sentiment Analysis",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08079",
        "abstract_url": "http://arxiv.org/abs/2401.08079",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Huafeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Yiquan"
            },
            {
                "last_name": "El-Yacoubi",
                "first_name": "Mounim A."
            },
            {
                "last_name": "Wang",
                "first_name": "Jun"
            },
            {
                "last_name": "Yang",
                "first_name": "Guangxiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vein recognition has received increasing attention due to its high security and privacy. Recently, deep neural networks such as Convolutional neural networks (CNN) and Transformers have been introduced for vein recognition and achieved state-of-the-art performance. Despite the recent advances, however, existing solutions for finger-vein feature extraction are still not optimal due to scarce training image samples. To overcome this problem, in this paper, we propose an adversarial masking contrastive learning (AMCL) approach, that generates challenging samples to train a more robust contrastive learning model for the downstream palm-vein recognition task, by alternatively optimizing the encoder in the contrastive learning model and a set of latent variables. First, a huge number of masks are generated to train a robust generative adversarial network (GAN). The trained generator transforms a latent variable from the latent variable space into a mask space. Then, we combine the trained generator with a contrastive learning model to obtain our AMCL, where the generator produces challenging masking images to increase the contrastive loss and the contrastive learning model is trained based on the harder images to learn a more robust feature representation. After training, the trained encoder in the contrastive learning model is combined with a classification layer to build a classifier, which is further fine-tuned on labeled training data for vein recognition. The experimental results on three databases demonstrate that our approach outperforms existing contrastive learning approaches in terms of improving identification accuracy of vein classifiers and achieves state-of-the-art recognition results. ",
        "title": "Adversarial Masking Contrastive Learning for vein recognition",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08080",
        "abstract_url": "http://arxiv.org/abs/2401.08080",
        "authors": [
            {
                "last_name": "Costa",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "MS",
        "categories": [
            "MS"
        ],
        "abstract": "  Two approximations of the integral of a class of sinusoidal composite functions, for which an explicit form does not exist, are derived. Numerical experiments show that the proposed approximations yield an error that does not depend on the width of the integration interval. Using such approximations, definite integrals can be computed in almost real-time. ",
        "title": "Approximations of the integral of a class of sinusoidal composite  functions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08081",
        "abstract_url": "http://arxiv.org/abs/2401.08081",
        "authors": [
            {
                "last_name": "Nezhadettehad",
                "first_name": "Alireza"
            },
            {
                "last_name": "Zaslavsky",
                "first_name": "Arkady"
            },
            {
                "last_name": "Abdur",
                "first_name": "Rakib"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Siraj Ahmed"
            },
            {
                "last_name": "Loke",
                "first_name": "Seng W."
            },
            {
                "last_name": "Huang",
                "first_name": "Guang-Li"
            },
            {
                "last_name": "Hassani",
                "first_name": "Alireza"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Predicting the future location of mobile objects reinforces location-aware services with proactive intelligence and helps businesses and decision-makers with better planning and near real-time scheduling in different applications such as traffic congestion control, location-aware advertisements, and monitoring public health and well-being. The recent developments in the smartphone and location sensors technology and the prevalence of using location-based social networks alongside the improvements in artificial intelligence and machine learning techniques provide an excellent opportunity to exploit massive amounts of historical and real-time contextual information to recognise mobility patterns and achieve more accurate and intelligent predictions. This survey provides a comprehensive overview of the next useful location prediction problem with context-awareness. First, we explain the concepts of context and context-awareness and define the next location prediction problem. Then we analyse nearly thirty studies in this field concerning the prediction method, the challenges addressed, the datasets and metrics used for training and evaluating the model, and the types of context incorporated. Finally, we discuss the advantages and disadvantages of different approaches, focusing on the usefulness of the predicted location and identifying the open challenges and future work on this subject by introducing two potential use cases of next location prediction in the automotive industry. ",
        "title": "Predicting Next Useful Location With Context-Awareness: The  State-Of-The-Art",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08083",
        "abstract_url": "http://arxiv.org/abs/2401.08083",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yu"
            },
            {
                "last_name": "Lin",
                "first_name": "Yuming"
            },
            {
                "last_name": "Liao",
                "first_name": "Qingming"
            },
            {
                "last_name": "Li",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM. ",
        "title": "UV-SAM: Adapting Segment Anything Model for Urban Village Identification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08086",
        "abstract_url": "http://arxiv.org/abs/2401.08086",
        "authors": [
            {
                "last_name": "Su",
                "first_name": "Yukun"
            },
            {
                "last_name": "Cao",
                "first_name": "Yiwen"
            },
            {
                "last_name": "Deng",
                "first_name": "Jingliang"
            },
            {
                "last_name": "Rao",
                "first_name": "Fengyun"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingyao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A large amount of User Generated Content (UGC) is uploaded to the Internet daily and displayed to people world-widely through the client side (e.g., mobile and PC). This requires the cropping algorithms to produce the aesthetic thumbnail within a specific aspect ratio on different devices. However, existing image cropping works mainly focus on landmark or landscape images, which fail to model the relations among the multi-objects with the complex background in UGC. Besides, previous methods merely consider the aesthetics of the cropped images while ignoring the content integrity, which is crucial for UGC cropping. In this paper, we propose a Spatial-Semantic Collaborative cropping network (S2CNet) for arbitrary user generated content accompanied by a new cropping benchmark. Specifically, we first mine the visual genes of the potential objects. Then, the suggested adaptive attention graph recasts this task as a procedure of information association over visual nodes. The underlying spatial and semantic relations are ultimately centralized to the crop candidate through differentiable message passing, which helps our network efficiently to preserve both the aesthetics and the content integrity. Extensive experiments on the proposed UGCrop5K and other public datasets demonstrate the superiority of our approach over state-of-the-art counterparts. Our project is available at https://github.com/suyukun666/S2CNet. ",
        "title": "Spatial-Semantic Collaborative Cropping for User Generated Content",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08088",
        "abstract_url": "http://arxiv.org/abs/2401.08088",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yachao"
            },
            {
                "last_name": "Li",
                "first_name": "Junhui"
            },
            {
                "last_name": "Jiang",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Existing large language models (LLMs) for machine translation are typically fine-tuned on sentence-level translation instructions and achieve satisfactory performance at the sentence level. However, when applied to document-level translation, these models face a significant challenge, particularly when dealing with documents containing over 512 tokens. This challenge arises from the issue of sentence-level coverage, where subsequent sentences in the document remain untranslated. As a result, the document-level translation capability of LLMs fine-tuned on sentence-level translation instructions is significantly limited. We conjecture that the primary cause of LLMs' weak document-level translation performance is the absence of document-to-document mapping ability. To address the issue, we propose an approach that combines sentence-level and document-level translation instructions of varying lengths to fine-tune LLMs. Our proposed translation mixed-instructions enable LLMs (Llama-2~7B and 13B) to maintain consistent translation performance from the sentence level to documents containing as many as 2048 tokens. Extensive experimental results show that the proposed approach significantly enhances the document-level translation capabilities of LLMs on 10 language pairs, effectively mitigating the sentence-level coverage issue in document-level translation. Experimentation on discourse phenomena has demonstrated that our document-level translation approach significantly improves translation quality, both in terms of BLEU score and discourse coherence. ",
        "title": "Enhancing Document-level Translation of Large Language Model via  Translation Mixed-instructions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08089",
        "abstract_url": "http://arxiv.org/abs/2401.08089",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Fu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xueying"
            },
            {
                "last_name": "Li",
                "first_name": "Bin"
            },
            {
                "last_name": "Wu",
                "first_name": "Yunlong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanzhen"
            },
            {
                "last_name": "Yi",
                "first_name": "Xiaodong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "RO"
        ],
        "abstract": "  This paper presents an innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks. The conventional manual BT generation method is inefficient and heavily reliant on domain expertise. On the other hand, existing automatic BT generation technologies encounter bottlenecks related to task complexity, model adaptability, and reliability. In order to overcome these challenges, we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs. The core contribution of this paper lies in the design of a BT generation framework based on LLM, which encompasses the entire process, from data synthesis and model training to application developing and data verification. Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance. In order to ensure the effectiveness and executability of the generated BTs, we emphasize the importance of data verification and introduce a multilevel verification strategy. Additionally, we explore a range of agent design and development schemes with LLM as the central element. We hope that the work in this paper may provide a reference for the researchers who are interested in BT generation based on LLMs. ",
        "title": "A Study on Training and Developing Large Language Models for Behavior  Tree Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08091",
        "abstract_url": "http://arxiv.org/abs/2401.08091",
        "authors": [
            {
                "last_name": "Shahid",
                "first_name": "Farhana"
            },
            {
                "last_name": "Agarwal",
                "first_name": "Dhruv"
            },
            {
                "last_name": "Vashistha",
                "first_name": "Aditya"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  WhatsApp is the largest social media platform in the Global South and is a virulent force in global misinformation and political propaganda. Due to end-to-end encryption WhatsApp can barely review any content and this often pushes the responsibility of moderation towards group admins. Yet, little is known about how WhatsApp group admins manage their groups, what factors and values influence moderation decisions, and what challenges they face in moderating their groups. To fill this gap, we interviewed admins of 32 diverse groups and reviewed content from 30 public groups in India and Bangladesh. We observed notable differences in the formation, members' behavior, and moderation of public versus private groups, as well as in how WhatsApp admins operate compared to those on other platforms. We used Baumrind's typology of 'parenting styles' as a lens to explore moderation practices in WhatsApp groups and identified four moderation styles based on how responsive and controlling the admins were and discuss design recommendations to help them better manage problematic content in WhatsApp groups. ",
        "title": "'One Style Does Not Regulate All': Moderation Practices in Public and  Private WhatsApp Groups",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08092",
        "abstract_url": "http://arxiv.org/abs/2401.08092",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Mengwei"
            },
            {
                "last_name": "Yin",
                "first_name": "Wangsong"
            },
            {
                "last_name": "Cai",
                "first_name": "Dongqi"
            },
            {
                "last_name": "Yi",
                "first_name": "Rongjie"
            },
            {
                "last_name": "Xu",
                "first_name": "Daliang"
            },
            {
                "last_name": "Wang",
                "first_name": "Qipeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Bingyang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yihao"
            },
            {
                "last_name": "Yang",
                "first_name": "Chen"
            },
            {
                "last_name": "Wang",
                "first_name": "Shihe"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiyang"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhenyan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Li"
            },
            {
                "last_name": "Wang",
                "first_name": "Shangguang"
            },
            {
                "last_name": "Li",
                "first_name": "Yuanchun"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunxin"
            },
            {
                "last_name": "Jin",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xuanzhe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field. ",
        "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08095",
        "abstract_url": "http://arxiv.org/abs/2401.08095",
        "authors": [
            {
                "last_name": "Oh",
                "first_name": "Hyoung-Seok"
            },
            {
                "last_name": "Lee",
                "first_name": "Sang-Hoon"
            },
            {
                "last_name": "Cho",
                "first_name": "Deok-Hyun"
            },
            {
                "last_name": "Lee",
                "first_name": "Seong-Whan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Emotional voice conversion (EVC) seeks to modify the emotional tone of a speaker's voice while preserving the original linguistic content and the speaker's unique vocal characteristics. Recent advancements in EVC have involved the simultaneous modeling of pitch and duration, utilizing the potential of sequence-to-sequence (seq2seq) models. To enhance reliability and efficiency in conversion, this study shifts focus towards parallel speech generation. We introduce Duration-Flexible EVC (DurFlex-EVC), which integrates a style autoencoder and unit aligner. Traditional models, while incorporating self-supervised learning (SSL) representations that contain both linguistic and paralinguistic information, have neglected this dual nature, leading to reduced controllability. Addressing this issue, we implement cross-attention to synchronize these representations with various emotions. Additionally, a style autoencoder is developed for the disentanglement and manipulation of style elements. The efficacy of our approach is validated through both subjective and objective evaluations, establishing its superiority over existing models in the field. ",
        "title": "DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel  Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08096",
        "abstract_url": "http://arxiv.org/abs/2401.08096",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Yimin"
            },
            {
                "last_name": "Tang",
                "first_name": "Huaizhen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xulong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ning"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jing"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianzong"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Voice conversion refers to transferring speaker identity with well-preserved content. Better disentanglement of speech representations leads to better voice conversion. Recent studies have found that phonetic information from input audio has the potential ability to well represent content. Besides, the speaker-style modeling with pre-trained models making the process more complex. To tackle these issues, we introduce a new method named \"CTVC\" which utilizes disentangled speech representations with contrastive learning and time-invariant retrieval. Specifically, a similarity-based compression module is used to facilitate a more intimate connection between the frame-level hidden features and linguistic information at phoneme-level. Additionally, a time-invariant retrieval is proposed for timbre extraction based on multiple segmentations and mutual information. Experimental results demonstrate that \"CTVC\" outperforms previous studies and improves the sound quality and similarity of converted results. ",
        "title": "Learning Disentangled Speech Representations with Contrastive Learning  and Time-Invariant Retrieval",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08097",
        "abstract_url": "http://arxiv.org/abs/2401.08097",
        "authors": [
            {
                "last_name": "Nasab",
                "first_name": "Ali Rezaei"
            },
            {
                "last_name": "Dashti",
                "first_name": "Maedeh"
            },
            {
                "last_name": "Shahin",
                "first_name": "Mojtaba"
            },
            {
                "last_name": "Zahedi",
                "first_name": "Mansooreh"
            },
            {
                "last_name": "Khalajzadeh",
                "first_name": "Hourieh"
            },
            {
                "last_name": "Arora",
                "first_name": "Chetan"
            },
            {
                "last_name": "Liang",
                "first_name": "Peng"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CY"
        ],
        "abstract": "  With the growing application of AI-based systems in our lives and society, there is a rising need to ensure that AI-based systems are developed and used in a responsible way. Fairness is one of the socio-technical concerns that must be addressed in AI-based systems for this purpose. Unfair AI-based systems, particularly, unfair AI-based mobile apps, can pose difficulties for a significant proportion of the global populace. This paper aims to deeply analyze fairness concerns in AI-based app reviews. We first manually constructed a ground-truth dataset including a statistical sample of fairness and non-fairness reviews. Leveraging the ground-truth dataset, we then developed and evaluated a set of machine learning and deep learning classifiers that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing classifier can detect fairness reviews with a precision of 94%. We then applied the best-performing classifier on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. While the fairness reviews appear in 23 app categories, we found that the 'communication' and 'social' app categories have the highest percentage of fairness reviews. Next, applying the K-means clustering technique to the 92K fairness reviews, followed by manual analysis, led to the identification of six distinct types of fairness concerns (e.g., 'receiving different quality of features and services in different platforms and devices' and 'lack of transparency and fairness in dealing with user-generated content'). Finally, the manual analysis of 2,248 app owners' responses to the fairness reviews identified six root causes (e.g., 'copyright issues', 'external factors', 'development cost') that app owners report to justify fairness concerns. ",
        "title": "A Study of Fairness Concerns in AI-based Mobile App Reviews",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08099",
        "abstract_url": "http://arxiv.org/abs/2401.08099",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Hancheng"
            },
            {
                "last_name": "Tiddeman",
                "first_name": "Bernard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  This study introduces a novel method for inpainting normal maps using a generative adversarial network (GAN). Normal maps, often derived from a lightstage, are crucial in performance capture but can have obscured areas due to movement (e.g., by arms, hair, or props). Inpainting fills these missing areas with plausible data. Our approach extends previous general image inpainting techniques, employing a bow tie-like generator network and a discriminator network, with alternating training phases. The generator aims to synthesize images aligning with the ground truth and deceive the discriminator, which differentiates between real and processed images. Periodically, the discriminator undergoes retraining to enhance its ability to identify processed images. Importantly, our method adapts to the unique characteristics of normal map data, necessitating modifications to the loss function. We utilize a cosine loss instead of mean squared error loss for generator training. Limited training data availability, even with synthetic datasets, demands significant augmentation, considering the specific nature of the input data. This includes appropriate image flipping and in-plane rotations to accurately alter normal vectors. Throughout training, we monitored key metrics such as average loss, Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR) for the generator, along with average loss and accuracy for the discriminator. Our findings suggest that the proposed model effectively generates high-quality, realistic inpainted normal maps, suitable for performance capture applications. These results establish a foundation for future research, potentially involving more advanced networks and comparisons with inpainting of source images used to create the normal maps. ",
        "title": "Inpainting Normal Maps for Lightstage data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08100",
        "abstract_url": "http://arxiv.org/abs/2401.08100",
        "authors": [
            {
                "last_name": "Pham",
                "first_name": "Anh-Cuong"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Van-Quang"
            },
            {
                "last_name": "Vuong",
                "first_name": "Thi-Hong"
            },
            {
                "last_name": "Ha",
                "first_name": "Quang-Thuy"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image captioning is a crucial task with applications in a wide range of domains, including healthcare and education. Despite extensive research on English image captioning datasets, the availability of such datasets for Vietnamese remains limited, with only two existing datasets. In this study, we introduce KTVIC, a comprehensive Vietnamese Image Captioning dataset focused on the life domain, covering a wide range of daily activities. This dataset comprises 4,327 images and 21,635 Vietnamese captions, serving as a valuable resource for advancing image captioning in the Vietnamese language. We conduct experiments using various deep neural networks as the baselines on our dataset, evaluating them using the standard image captioning metrics, including BLEU, METEOR, CIDEr, and ROUGE. Our findings underscore the effectiveness of the proposed dataset and its potential contributions to the field of image captioning in the Vietnamese context. ",
        "title": "KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08102",
        "abstract_url": "http://arxiv.org/abs/2401.08102",
        "authors": [
            {
                "last_name": "Im",
                "first_name": "Jaekwon"
            },
            {
                "last_name": "Nam",
                "first_name": "Juhan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Properly setting up recording conditions, including microphone type and placement, room acoustics, and ambient noise, is essential to obtaining the desired acoustic characteristics of speech. In this paper, we propose Diff-R-EN-T, a Diffusion model for Recording ENvironment Transfer which transforms the input speech to have the recording conditions of a reference speech while preserving the speech content. Our model comprises the content enhancer, the recording environment encoder, and the diffusion decoder which generates the target mel-spectrogram by utilizing both enhancer and encoder as input conditions. We evaluate DiffRENT in the speech enhancement and acoustic matching scenarios. The results show that DiffRENT generalizes well to unseen environments and new speakers. Also, the proposed model achieves superior performances in objective and subjective evaluation. Sound examples of our proposed model are available online. ",
        "title": "DIFFRENT: A Diffusion Model for Recording Environment Transfer of Speech",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08103",
        "abstract_url": "http://arxiv.org/abs/2401.08103",
        "authors": [
            {
                "last_name": "Sanderson",
                "first_name": "Conrad"
            },
            {
                "last_name": "Schleiger",
                "first_name": "Emma"
            },
            {
                "last_name": "Douglas",
                "first_name": "David"
            },
            {
                "last_name": "Kuhnert",
                "first_name": "Petra"
            },
            {
                "last_name": "Lu",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements. ",
        "title": "Resolving Ethics Trade-offs in Implementing Responsible AI",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08104",
        "abstract_url": "http://arxiv.org/abs/2401.08104",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Koopman",
                "first_name": "Bevan"
            },
            {
                "last_name": "Zuccon",
                "first_name": "Guido"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks. ",
        "title": "A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08105",
        "abstract_url": "http://arxiv.org/abs/2401.08105",
        "authors": [
            {
                "last_name": "Briley",
                "first_name": "Austin"
            },
            {
                "last_name": "Afghah",
                "first_name": "Fatemeh"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Early wildfire detection in remote and forest areas is crucial for minimizing devastation and preserving ecosystems. Autonomous drones offer agile access to remote, challenging terrains, equipped with advanced imaging technology that delivers both high-temporal and detailed spatial resolution, making them valuable assets in the early detection and monitoring of wildfires. However, the limited computation and battery resources of Unmanned Aerial Vehicles (UAVs) pose significant challenges in implementing robust and efficient image classification models. Current works in this domain often operate offline, emphasizing the need for solutions that can perform inference in real time, given the constraints of UAVs. To address these challenges, this paper aims to develop a real-time image classification and fire segmentation model. It presents a comprehensive investigation into hardware acceleration using the Jetson Nano P3450 and the implications of TensorRT, NVIDIA's high-performance deep-learning inference library, on fire classification accuracy and speed. The study includes implementations of Quantization Aware Training (QAT), Automatic Mixed Precision (AMP), and post-training mechanisms, comparing them against the latest baselines for fire segmentation and classification. All experiments utilize the FLAME dataset - an image dataset collected by low-altitude drones during a prescribed forest fire. This work contributes to the ongoing efforts to enable real-time, on-board wildfire detection capabilities for UAVs, addressing speed and the computational and energy constraints of these crucial monitoring systems. The results show a 13% increase in classification speed compared to similar models without hardware optimization. Comparatively, loss and accuracy are within 1.225% of the original values. ",
        "title": "Hardware Acceleration for Real-Time Wildfire Detection Onboard Drone  Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08107",
        "abstract_url": "http://arxiv.org/abs/2401.08107",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yixuan"
            },
            {
                "last_name": "Chen",
                "first_name": "Peilin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hanwei"
            },
            {
                "last_name": "Ding",
                "first_name": "Keyan"
            },
            {
                "last_name": "Li",
                "first_name": "Leida"
            },
            {
                "last_name": "Wang",
                "first_name": "Shiqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  Opinion-Unaware Blind Image Quality Assessment (OU-BIQA) models aim to predict image quality without training on reference images and subjective quality scores. Thereinto, image statistical comparison is a classic paradigm, while the performance is limited by the representation ability of visual descriptors. Deep features as visual descriptors have advanced IQA in recent research, but they are discovered to be highly texture-biased and lack of shape-bias. On this basis, we find out that image shape and texture cues respond differently towards distortions, and the absence of either one results in an incomplete image representation. Therefore, to formulate a well-round statistical description for images, we utilize the shapebiased and texture-biased deep features produced by Deep Neural Networks (DNNs) simultaneously. More specifically, we design a Shape-Texture Adaptive Fusion (STAF) module to merge shape and texture information, based on which we formulate qualityrelevant image statistics. The perceptual quality is quantified by the variant Mahalanobis Distance between the inner and outer Shape-Texture Statistics (DSTS), wherein the inner and outer statistics respectively describe the quality fingerprints of the distorted image and natural images. The proposed DSTS delicately utilizes shape-texture statistical relations between different data scales in the deep domain, and achieves state-of-the-art (SOTA) quality prediction performance on images with artificial and authentic distortions. ",
        "title": "Deep Shape-Texture Statistics for Completely Blind Image Quality  Evaluation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08111",
        "abstract_url": "http://arxiv.org/abs/2401.08111",
        "authors": [
            {
                "last_name": "Grosz",
                "first_name": "Steven A."
            },
            {
                "last_name": "Godbole",
                "first_name": "Akash"
            },
            {
                "last_name": "Jain",
                "first_name": "Anil K."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Contactless palmprints are comprised of both global and local discriminative features. Most prior work focuses on extracting global features or local features alone for palmprint matching, whereas this research introduces a novel framework that combines global and local features for enhanced palmprint matching accuracy. Leveraging recent advancements in deep learning, this study integrates a vision transformer (ViT) and a convolutional neural network (CNN) to extract complementary local and global features. Next, a mobile-based, end-to-end palmprint recognition system is developed, referred to as Palm-ID. On top of the ViT and CNN features, Palm-ID incorporates a palmprint enhancement module and efficient dimensionality reduction (for faster matching). Palm-ID balances the trade-off between accuracy and latency, requiring just 18ms to extract a template of size 516 bytes, which can be efficiently searched against a 10,000 palmprint gallery in 0.33ms on an AMD EPYC 7543 32-Core CPU utilizing 128-threads. Cross-database matching protocols and evaluations on large-scale operational datasets demonstrate the robustness of the proposed method, achieving a TAR of 98.06% at FAR=0.01% on a newly collected, time-separated dataset. To show a practical deployment of the end-to-end system, the entire recognition pipeline is embedded within a mobile device for enhanced user privacy and security. ",
        "title": "Mobile Contactless Palmprint Recognition: Use of Multiscale, Multimodel  Embeddings",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08115",
        "abstract_url": "http://arxiv.org/abs/2401.08115",
        "authors": [
            {
                "last_name": "Khateri",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Ghahremani",
                "first_name": "Morteza"
            },
            {
                "last_name": "Sierra",
                "first_name": "Alejandra"
            },
            {
                "last_name": "Tohka",
                "first_name": "Jussi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The inability to acquire clean high-resolution (HR) electron microscopy (EM) images over a large brain tissue volume hampers many neuroscience studies. To address this challenge, we propose a deep-learning-based image super-resolution (SR) approach to computationally reconstruct clean HR 3D-EM with a large field of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are I) Investigating training with no-clean references for $\\ell_2$ and $\\ell_1$ loss functions; II) Introducing a novel network architecture, named EMSR, for enhancing the resolution of LR EM images while reducing inherent noise; and, III) Comparing different training strategies including using acquired LR and HR image pairs, i.e., real pairs with no-clean references contaminated with real corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR and denoised HR pairs. Experiments with nine brain datasets showed that training with real pairs can produce high-quality super-resolved results, demonstrating the feasibility of training with non-clean references for both loss functions. Additionally, comparable results were observed, both visually and numerically, when employing denoised and noisy references for training. Moreover, utilizing the network trained with synthetically generated LR images from HR counterparts proved effective in yielding satisfactory SR results, even in certain cases, outperforming training with real pairs. The proposed SR network was compared quantitatively and qualitatively with several established SR techniques, showcasing either the superiority or competitiveness of the proposed method in mitigating noise while recovering fine details. ",
        "title": "No-Clean-Reference Image Super-Resolution: Application to Electron  Microscopy",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08117",
        "abstract_url": "http://arxiv.org/abs/2401.08117",
        "authors": [
            {
                "last_name": "Qu",
                "first_name": "Qiang"
            },
            {
                "last_name": "Shen",
                "first_name": "Yiran"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoming"
            },
            {
                "last_name": "Chung",
                "first_name": "Yuk Ying"
            },
            {
                "last_name": "Liu",
                "first_name": "Tongliang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  The bio-inspired event cameras or dynamic vision sensors are capable of asynchronously capturing per-pixel brightness changes (called event-streams) in high temporal resolution and high dynamic range. However, the non-structural spatial-temporal event-streams make it challenging for providing intuitive visualization with rich semantic information for human vision. It calls for events-to-video (E2V) solutions which take event-streams as input and generate high quality video frames for intuitive visualization. However, current solutions are predominantly data-driven without considering the prior knowledge of the underlying statistics relating event-streams and video frames. It highly relies on the non-linearity and generalization capability of the deep neural networks, thus, is struggling on reconstructing detailed textures when the scenes are complex. In this work, we propose \\textbf{E2HQV}, a novel E2V paradigm designed to produce high-quality video frames from events. This approach leverages a model-aided deep learning framework, underpinned by a theory-inspired E2V model, which is meticulously derived from the fundamental imaging principles of event cameras. To deal with the issue of state-reset in the recurrent components of E2HQV, we also design a temporal shift embedding module to further improve the quality of the video frames. Comprehensive evaluations on the real world event camera datasets validate our approach, with E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the second best by over 40\\% for some evaluation metrics. ",
        "title": "E2HQV: High-Quality Video Generation from Event Camera via  Theory-Inspired Model-Aided Deep Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08119",
        "abstract_url": "http://arxiv.org/abs/2401.08119",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Lequan"
            },
            {
                "last_name": "Shi",
                "first_name": "Dai"
            },
            {
                "last_name": "Han",
                "first_name": "Andi"
            },
            {
                "last_name": "Gao",
                "first_name": "Junbin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traffic forecasting, a crucial application of spatio-temporal graph (STG) learning, has traditionally relied on deterministic models for accurate point estimations. Yet, these models fall short of identifying latent risks of unexpected volatility in future observations. To address this gap, probabilistic methods, especially variants of diffusion models, have emerged as uncertainty-aware solutions. However, existing diffusion methods typically focus on generating separate future time series for individual sensors in the traffic network, resulting in insufficient involvement of spatial network characteristics in the probabilistic learning process. To better leverage spatial dependencies and systematic patterns inherent in traffic data, we propose SpecSTG, a novel spectral diffusion framework. Our method generates the Fourier representation of future time series, transforming the learning process into the spectral domain enriched with spatial information. Additionally, our approach incorporates a fast spectral graph convolution designed for Fourier input, alleviating the computational burden associated with existing models. Numerical experiments show that SpecSTG achieves outstanding performance with traffic flow and traffic speed datasets compared to state-of-the-art baselines. The source code for SpecSTG is available at https://anonymous.4open.science/r/SpecSTG. ",
        "title": "SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic  Spatio-Temporal Traffic Forecasting",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08120",
        "abstract_url": "http://arxiv.org/abs/2401.08120",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Yang"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhaohui"
            },
            {
                "last_name": "Wang",
                "first_name": "Chengcheng"
            },
            {
                "last_name": "Guo",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Mei",
                "first_name": "Junyao"
            },
            {
                "last_name": "Qi",
                "first_name": "Yueran"
            },
            {
                "last_name": "Liu",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junyu"
            },
            {
                "last_name": "Wu",
                "first_name": "Jixuan"
            },
            {
                "last_name": "Zhan",
                "first_name": "Xuepeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiezhi"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Flash memory has been widely adopted as stand-alone memory and embedded memory due to its robust reliability. However, the limited endurance obstacles its further applications in storage class memory (SCM) and to proceed endurance-required computing-in-memory (CIM) tasks. In this work, the optimization strategies have been studied to tackle this concern. It is shown that by adopting the channel hot electrons injection (CHEI) and hot hole injection (HHI) to implement program/erase (PE) cycling together with a balanced memory window (MW) at the high-Vth (HV) mode, impressively, the endurance can be greatly extended to 1010 PE cycles, which is a record-high value in flash memory. Moreover, by using the proposed electric-field-assisted relaxation (EAR) scheme, the degradation of flash cells can be well suppressed with better subthreshold swings (SS) and lower leakage currents (sub-10pA after 1010 PE cycles). Our results shed light on the optimization strategy of flash memory to serve as SCM and implementendurance-required CIM tasks. ",
        "title": "Operation Scheme Optimizations to Achieve Ultra-high Endurance (1010) in  Flash Memory with Robust Reliabilities",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08121",
        "abstract_url": "http://arxiv.org/abs/2401.08121",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Gengyue"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Peng",
                "first_name": "Xianyue"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            },
            {
                "last_name": "Han",
                "first_name": "Yu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study introduces CycLight, a novel cycle-level deep reinforcement learning (RL) approach for network-level adaptive traffic signal control (NATSC) systems. Unlike most traditional RL-based traffic controllers that focus on step-by-step decision making, CycLight adopts a cycle-level strategy, optimizing cycle length and splits simultaneously using Parameterized Deep Q-Networks (PDQN) algorithm. This cycle-level approach effectively reduces the computational burden associated with frequent data communication, meanwhile enhancing the practicality and safety of real-world applications. A decentralized framework is formulated for multi-agent cooperation, while attention mechanism is integrated to accurately assess the impact of the surroundings on the current intersection. CycLight is tested in a large synthetic traffic grid using the microscopic traffic simulation tool, SUMO. Experimental results not only demonstrate the superiority of CycLight over other state-of-the-art approaches but also showcase its robustness against information transmission delays. ",
        "title": "CycLight: learning traffic signal cooperation with a cycle-level  strategy",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08123",
        "abstract_url": "http://arxiv.org/abs/2401.08123",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Xinni"
            },
            {
                "last_name": "Kuang",
                "first_name": "Zengsheng"
            },
            {
                "last_name": "Guo",
                "first_name": "Chunle"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ruixun"
            },
            {
                "last_name": "Cai",
                "first_name": "Lei"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiao"
            },
            {
                "last_name": "Li",
                "first_name": "Chongyi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Guided depth super-resolution (GDSR) involves restoring missing depth details using the high-resolution RGB image of the same scene. Previous approaches have struggled with the heterogeneity and complementarity of the multi-modal inputs, and neglected the issues of modal misalignment, geometrical misalignment, and feature selection. In this study, we rethink some essential components in GDSR networks and propose a simple yet effective Dynamic Dual Alignment and Aggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment module that adapts to alleviate the modal misalignment via a learnable domain alignment block and geometrically align cross-modal features by learning the offset; and 2) a mask-to-pixel feature aggregate module that uses the gated mechanism and pixel attention to filter out irrelevant texture noise from RGB features and combine the useful features with depth features. By combining the strengths of RGB and depth features while minimizing disturbance introduced by the RGB image, our method with simple reuse and redesign of basic components achieves state-of-the-art performance on multiple benchmark datasets. The code is available at https://github.com/JiangXinni/D2A2. ",
        "title": "The Devil is in the Details: Boosting Guided Depth Super-Resolution via  Rethinking Cross-Modal Alignment and Aggregation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08124",
        "abstract_url": "http://arxiv.org/abs/2401.08124",
        "authors": [
            {
                "last_name": "Kitson",
                "first_name": "Joy"
            },
            {
                "last_name": "Costello",
                "first_name": "Ian"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiangzhuo"
            },
            {
                "last_name": "Jim\u00e9nez",
                "first_name": "Diego"
            },
            {
                "last_name": "Hoops",
                "first_name": "Stefan"
            },
            {
                "last_name": "Mortveit",
                "first_name": "Henning"
            },
            {
                "last_name": "Meneses",
                "first_name": "Esteban"
            },
            {
                "last_name": "Yeom",
                "first_name": "Jae-Seung"
            },
            {
                "last_name": "Marathe",
                "first_name": "Madhav V."
            },
            {
                "last_name": "Bhatele",
                "first_name": "Abhinav"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Global pandemics can wreak havoc and lead to significant social, economic, and personal losses. Preventing the spread of infectious diseases requires implementing interventions at different levels of government, and evaluating the potential impact and efficacy of those preemptive measures. Agent-based modeling can be used for detailed studies of epidemic diffusion and possible interventions. We present Loimos, a highly parallel simulation of epidemic diffusion written on top of Charm++, an asynchronous task-based parallel runtime. Loimos uses a hybrid of time-stepping and discrete-event simulation to model disease spread. We demonstrate that our implementation of Loimos is able to scale to large core counts on an HPC system. In particular, Loimos is able to simulate a US-scale synthetic interaction network in an average of 1.497 seconds per simulation day when executed on 16 nodes on Rivanna at the University of Virginia, processing around 428 billion interactions (person-person edges) in under five minutes for an average of 1.4 billion traversed edges per second (TEPS). ",
        "title": "A Large-Scale Epidemic Simulation Framework for Realistic Social Contact  Networks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08126",
        "abstract_url": "http://arxiv.org/abs/2401.08126",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Junzhi"
            },
            {
                "last_name": "Li",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Ray",
                "first_name": "Devdeep"
            },
            {
                "last_name": "Yap",
                "first_name": "KK"
            },
            {
                "last_name": "Dukkipati",
                "first_name": "Nandita"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The packet delivery fairness is critical in many applications in the cloud, such as exchange systems, consensus protocols, and online gaming applications. However, due to nonidentical and dynamic packet forwarding paths, as well as many in-network queuing delays, supporting packet delivery fairness is challenging in a shared compute environment. In this paper, we present Octopus, the first general fair packet delivery service to achieve packet arrival time variations smaller than tens of nanoseconds, with the existence of latency variations in the network. The key ideas of Octopus to support such good fairness come from repurposing hardware traffic shaping capabilities in modern NICs, and deploying agents at local SmartNICs to minimize latency variations from packet forwarding. Evaluation results show that Octopus has less than 40 ns unfairness for up to 99.97\\% multicast packets. ",
        "title": "Octopus: A Fair Packet Delivery Service",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08127",
        "abstract_url": "http://arxiv.org/abs/2401.08127",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Vincent"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Quantum communications are based on the law of physics for information security and the implications for this form of future information security enabled by quantum science has to be studied. Physics-based vulnerabilities may exist due to the inherent physics properties and behavior of quantum technologies such as Quantum Key Distribution (QKD), thus resulting in new threats that may emerge with attackers exploiting the physics-based vulnerabilities. There were many studies and experiments done to demonstrate the threat of physics-based attacks on quantum links. However, there is a lack of a framework that provides a common language to communicate about the threats and type of adversaries being dealt with for physics-based attacks. This paper is a review of physics-based attacks that were being investigated and attempt to initialize a framework based on the attack objectives and methodologies, referencing the concept from the well-established MITRE ATT&CK, therefore pioneering the classification of Indicator of Compromises (IoCs) for physics-based attacks. This paper will then pave the way for future work in the development of a forensic tool for the different classification of IoCs, with the methods of evidence collections and possible points of extractions for analysis being further investigated. ",
        "title": "Framework and Classification of Indicator of Compromise for  physics-based attacks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08131",
        "abstract_url": "http://arxiv.org/abs/2401.08131",
        "authors": [
            {
                "last_name": "Wen",
                "first_name": "Xin-Cheng"
            },
            {
                "last_name": "Gao",
                "first_name": "Cuiyun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinchen"
            },
            {
                "last_name": "Wang",
                "first_name": "Ruiqi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tao"
            },
            {
                "last_name": "Liao",
                "first_name": "Qing"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Recent years have witnessed a growing focus on automated software vulnerability detection. Notably, deep learning (DL)-based methods, which employ source code for the implicit acquisition of vulnerability patterns, have demonstrated superior performance compared to other approaches. However, the DL-based approaches are still hard to capture the vulnerability-related information from the whole code snippet, since the vulnerable parts usually account for only a small proportion. As evidenced by our experiments, the approaches tend to excessively emphasize semantic information, potentially leading to limited vulnerability detection performance in practical scenarios. First, they cannot well distinguish between the code snippets before (i.e., vulnerable code) and after (i.e., non-vulnerable code) developers' fixes due to the minimal code changes. Besides, substituting user-defined identifiers with placeholders (e.g., \"VAR1\" and \"FUN1\") in obvious performance degradation at up to 14.53% with respect to the F1 score. To mitigate these issues, we propose to leverage the vulnerable and corresponding fixed code snippets, in which the minimal changes can provide hints about semantic-agnostic features for vulnerability detection. In this paper, we propose a software vulneRability dEteCtion framework with zerO-sum game and prototype learNing, named RECON. In RECON, we propose a zero-sum game construction module. Distinguishing the vulnerable code from the corresponding fixed code is regarded as one player (i.e. Calibrator), while the conventional vulnerability detection is another player (i.e. Detector) in the zero-sum game. The goal is to capture the semantic-agnostic features of the first player for enhancing the second player's performance for vulnerability detection. Experiments on the public benchmark dataset show that RECON outperforms the state-of-the-art baseline by 6.29% in F1 score. ",
        "title": "Game Rewards Vulnerabilities: Software Vulnerability Detection with  Zero-Sum Game and Prototype Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08132",
        "abstract_url": "http://arxiv.org/abs/2401.08132",
        "authors": [
            {
                "last_name": "Canh",
                "first_name": "Thanh Nguyen"
            },
            {
                "last_name": "Elibol",
                "first_name": "Armagan"
            },
            {
                "last_name": "Chong",
                "first_name": "Nak Young"
            },
            {
                "last_name": "HoangVan",
                "first_name": "Xiem"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  To autonomously navigate in real-world environments, special in search and rescue operations, Unmanned Aerial Vehicles (UAVs) necessitate comprehensive maps to ensure safety. However, the prevalent metric map often lacks semantic information crucial for holistic scene comprehension. In this paper, we proposed a system to construct a probabilistic metric map enriched with object information extracted from the environment from RGB-D images. Our approach combines a state-of-the-art YOLOv8-based object detection framework at the front end and a 2D SLAM method - CartoGrapher at the back end. To effectively track and position semantic object classes extracted from the front-end interface, we employ the innovative BoT-SORT methodology. A novel association method is introduced to extract the position of objects and then project it with the metric map. Unlike previous research, our approach takes into reliable navigating in the environment with various hollow bottom objects. The output of our system is a probabilistic map, which significantly enhances the map's representation by incorporating object-specific attributes, encompassing class distinctions, accurate positioning, and object heights. A number of experiments have been conducted to evaluate our proposed approach. The results show that the robot can effectively produce augmented semantic maps containing several objects (notably chairs and desks). Furthermore, our system is evaluated within an embedded computer - Jetson Xavier AGX unit to demonstrate the use case in real-world applications. ",
        "title": "Object-Oriented Semantic Mapping for Reliable UAVs Navigation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08134",
        "abstract_url": "http://arxiv.org/abs/2401.08134",
        "authors": [
            {
                "last_name": "Canh",
                "first_name": "Thanh Nguyen"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Van-Truong"
            },
            {
                "last_name": "HoangVan",
                "first_name": "Xiem"
            },
            {
                "last_name": "Elibol",
                "first_name": "Armagan"
            },
            {
                "last_name": "Chong",
                "first_name": "Nak Young"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Unmanned Aerial Vehicles (UAVs) hold immense potential for critical applications, such as search and rescue operations, where accurate perception of indoor environments is paramount. However, the concurrent amalgamation of localization, 3D reconstruction, and semantic segmentation presents a notable hurdle, especially in the context of UAVs equipped with constrained power and computational resources. This paper presents a novel approach to address challenges in semantic information extraction and utilization within UAV operations. Our system integrates state-of-the-art visual SLAM to estimate a comprehensive 6-DoF pose and advanced object segmentation methods at the back end. To improve the computational and storage efficiency of the framework, we adopt a streamlined voxel-based 3D map representation - OctoMap to build a working system. Furthermore, the fusion algorithm is incorporated to obtain the semantic information of each frame from the front-end SLAM task, and the corresponding point. By leveraging semantic information, our framework enhances the UAV's ability to perceive and navigate through indoor spaces, addressing challenges in pose estimation accuracy and uncertainty reduction. Through Gazebo simulations, we validate the efficacy of our proposed system and successfully embed our approach into a Jetson Xavier AGX unit for real-world applications. ",
        "title": "S3M: Semantic Segmentation Sparse Mapping for UAVs with RGB-D Camera",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08135",
        "abstract_url": "http://arxiv.org/abs/2401.08135",
        "authors": [
            {
                "last_name": "Canh",
                "first_name": "Thanh Nguyen"
            },
            {
                "last_name": "HoangVan",
                "first_name": "Xiem"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "NI"
        ],
        "abstract": "  With the rapid growth of Vehicle Ad-hoc Network (VANET) as a promising technology for efficient and reliable communication among vehicles and infrastructure, the security and integrity of VANET communications has become a critical concern. One of the significant threats to VANET is the presence of blackhole attacks, where malicious nodes disrupt the network's functionality and compromise data confidentiality, integrity, and availability. In this paper, we propose a machine learning-based approach for blackhole detection in VANET. To achieve this task, we first create a comprehensive dataset comprising normal and malicious traffic flows. Afterward, we study and define a promising set of features to discriminate the blackhole attacks. Finally, we evaluate various machine learning algorithms, including Gradient Boosting, Random Forest, Support Vector Machines, k-Nearest Neighbors, Gaussian Naive Bayes, and Logistic Regression. Experimental results demonstrate the effectiveness of these algorithms in distinguishing between normal and malicious nodes. Our findings also highlight the potential of machine learning based approach in enhancing the security of VANET by detecting and mitigating blackhole attacks. ",
        "title": "Machine Learning-Based Malicious Vehicle Detection for Security Threats  and Attacks in Vehicle Ad-hoc Network (VANET) Communications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08136",
        "abstract_url": "http://arxiv.org/abs/2401.08136",
        "authors": [
            {
                "last_name": "Yi",
                "first_name": "Baozhao"
            },
            {
                "last_name": "Du",
                "first_name": "Xinhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Wu",
                "first_name": "Xiaogang"
            },
            {
                "last_name": "Hu",
                "first_name": "Qiuhao"
            },
            {
                "last_name": "Jiang",
                "first_name": "Weiran"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaosong"
            },
            {
                "last_name": "Song",
                "first_name": "Ziyou"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Accurate estimation of the state of charge (SOC) and state of health (SOH) is crucial for the safe and reliable operation of batteries. However, the measurement bias of voltage can highly deteriorate the estimation accuracy. One such example is the lithium iron phosphate (LFP) battery, which is highly prone to suffer from this issue owing to its flat open-circuit voltage curve. This work proposes a bias-compensated framework that reliably estimates the SOC and SOH of LFP batteries under the influence of voltage measurement bias. To validate the proposed approach, four LFP batteries are tested at various ambient temperatures and SOH conditions, with two different values of voltage measurement bias added. The results show that the bias-compensated algorithm achieves test errors that are less than 1.5% and 2% for SOC and SOH estimation, respectively. Additionally, the proposed approach outperforms the traditional estimation method that ignores the effects of voltage measurement bias. ",
        "title": "Bias-Compensated State of Charge and State of Health Joint Estimation  for Lithium Iron Phosphate Batteries",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08138",
        "abstract_url": "http://arxiv.org/abs/2401.08138",
        "authors": [
            {
                "last_name": "Rasool",
                "first_name": "Zafaryab"
            },
            {
                "last_name": "Barnett",
                "first_name": "Scott"
            },
            {
                "last_name": "Willie",
                "first_name": "David"
            },
            {
                "last_name": "Kurniawan",
                "first_name": "Stefanus"
            },
            {
                "last_name": "Balugo",
                "first_name": "Sherwin"
            },
            {
                "last_name": "Thudumu",
                "first_name": "Srikanth"
            },
            {
                "last_name": "Abdelrazek",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Large language models (LLMs) enable state-of-the-art semantic capabilities to be added to software systems such as semantic search of unstructured documents and text generation. However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience. To address this problem, semantic caches are used to check for answers to similar queries (that may have been phrased differently) without hitting the LLM service. Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system. Adopting semantic cache techniques usually requires testing the effectiveness of a semantic cache (accurate cache hits and misses) which requires a labelled test set of similar queries and responses which is often unavailable. In this paper, we present VaryGen, an approach for using LLMs for test input generation that produces similar questions from unstructured text documents. Our novel approach uses the reasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise subtle variations to queries, and 3) evaluate the synthesised test dataset. We evaluated our approach in the domain of a student question and answer system by qualitatively analysing 100 generated queries and result pairs, and conducting an empirical case study with an open source semantic cache. Our results show that query pairs satisfy human expectations of similarity and our generated data demonstrates failure cases of a semantic cache. Additionally, we also evaluate our approach on Qasper dataset. This work is an important first step into test input generation for semantic applications and presents considerations for practitioners when calibrating a semantic cache. ",
        "title": "LLMs for Test Input Generation for Semantic Caches",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08139",
        "abstract_url": "http://arxiv.org/abs/2401.08139",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Fu"
            },
            {
                "last_name": "Wang",
                "first_name": "Jing"
            },
            {
                "last_name": "Geng",
                "first_name": "Xin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  The pre-training paradigm fine-tunes the models trained on large-scale datasets to downstream tasks with enhanced performance. It transfers all knowledge to downstream tasks without discriminating which part is necessary or unnecessary, which may lead to negative transfer. In comparison, knowledge transfer in nature is much more efficient. When passing genetic information to descendants, ancestors encode only the essential knowledge into genes, which act as the medium. Inspired by that, we adopt a recent concept called ``learngene'' and refine its structures by mimicking the structures of natural genes. We propose the Genetic Transfer Learning (GTL) -- a framework to copy the evolutionary process of organisms into neural networks. GTL trains a population of networks, selects superior learngenes by tournaments, performs learngene mutations, and passes the learngenes to next generations. Finally, we successfully extract the learngenes of VGG11 and ResNet12. We show that the learngenes bring the descendant networks instincts and strong learning ability: with 20% parameters, the learngenes bring 12% and 16% improvements of accuracy on CIFAR-FS and miniImageNet. Besides, the learngenes have the scalability and adaptability on the downstream structure of networks and datasets. Overall, we offer a novel insight that transferring core knowledge via learngenes may be sufficient and efficient for neural networks. ",
        "title": "Transferring Core Knowledge via Learngenes",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08140",
        "abstract_url": "http://arxiv.org/abs/2401.08140",
        "authors": [
            {
                "last_name": "Nakayama",
                "first_name": "Kiyohiro"
            },
            {
                "last_name": "Uy",
                "first_name": "Mikaela Angelina"
            },
            {
                "last_name": "You",
                "first_name": "Yang"
            },
            {
                "last_name": "Li",
                "first_name": "Ke"
            },
            {
                "last_name": "Guibas",
                "first_name": "Leonidas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: \"from where has each point been seen?\" -- which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. ",
        "title": "ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08141",
        "abstract_url": "http://arxiv.org/abs/2401.08141",
        "authors": [
            {
                "last_name": "Alam",
                "first_name": "Md Morshed"
            },
            {
                "last_name": "Jahan",
                "first_name": "Israt"
            },
            {
                "last_name": "Wang",
                "first_name": "Weichao"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  In trigger-action IoT platforms, IoT devices report event conditions to IoT hubs notifying their cyber states and let the hubs invoke actions in other IoT devices based on functional dependencies defined as rules in a rule engine. These functional dependencies create a chain of interactions that help automate network tasks. Adversaries exploit this chain to report fake event conditions to IoT hubs and perform remote injection attacks upon a smart environment to indirectly control targeted IoT devices. Existing defense efforts usually depend on static analysis over IoT apps to develop rule-based anomaly detection mechanisms. We also see ML-based defense mechanisms in the literature that harness physical event fingerprints to determine anomalies in an IoT network. However, these methods often demonstrate long response time and lack of adaptability when facing complicated attacks. In this paper, we propose to build a deep reinforcement learning based real-time defense system for injection attacks. We define the reward functions for defenders and implement a deep Q-network based approach to identify the optimal defense policy. Our experiments show that the proposed mechanism can effectively and accurately identify and defend against injection attacks with reasonable computation overhead. ",
        "title": "IoTWarden: A Deep Reinforcement Learning Based Real-time Defense System  to Mitigate Trigger-action IoT Attacks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08144",
        "abstract_url": "http://arxiv.org/abs/2401.08144",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yue"
            },
            {
                "last_name": "Yi",
                "first_name": "Peng"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Stackelberg game depicts a leader-follower relationship wherein decisions are made sequentially, and the Stackelberg equilibrium represents an expected optimal solution when the leader can anticipate the rational response of the follower. Motivated by control of network systems with two levels of decision-making hierarchy, such as the management of energy networks and power coordination at cellular networks, a networked multi-leaders and multi-followers Stackelberg game is proposed. Due to the constraint of limited information interaction among players, a clustered information structure is assumed that each leader can only communicate with a portion of overall followers, namely its subordinated followers, and also only with its local neighboring leaders. In this case, the leaders cannot fully anticipate the collective rational response of all followers with its local information. To address Stackelberg equilibrium seeking under this partial information structure, we propose a distributed seeking algorithm based on implicit gradient estimation and network consensus mechanisms. We rigorously prove the convergence of the algorithm for both diminishing and constant step sizes under strict and strong monotonicity conditions, respectively. Furthermore, the model and the algorithm can also incorporate linear equality and inequality constraints into the followers' optimization problems, with the approach of the interior point barrier function. Finally, we present numerical simulations in applications to corroborate our claims on the proposed framework. ",
        "title": "Distributed Stackelberg Equilibrium Seeking for Networked Multi-Leader  Multi-Follower Games with A Clustered Information Structure",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08147",
        "abstract_url": "http://arxiv.org/abs/2401.08147",
        "authors": [
            {
                "last_name": "Fard",
                "first_name": "Sanaz Hasanzadeh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Dynamic graph learning has gained significant attention as it offers a powerful means to model intricate interactions among entities across various real-world and scientific domains. Notably, graphs serve as effective representations for diverse networks such as transportation, brain, social, and internet networks. Furthermore, the rapid advancements in machine learning have expanded the scope of dynamic graph applications beyond the aforementioned domains. In this paper, we present a review of lesser-explored applications of dynamic graph learning. This study revealed the potential of machine learning on dynamic graphs in addressing challenges across diverse domains, including those with limited levels of association with the field. ",
        "title": "Machine Learning on Dynamic Graphs: A Survey on Applications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08149",
        "abstract_url": "http://arxiv.org/abs/2401.08149",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Shaohua"
            },
            {
                "last_name": "Zeng",
                "first_name": "Shuhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Liang"
            },
            {
                "last_name": "Di",
                "first_name": "Boya"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  To realize holographic communications, a potential technology for spectrum efficiency improvement in the future sixth-generation (6G) network, antenna arrays inlaid with numerous antenna elements will be deployed. However, the increase in antenna aperture size makes some users lie in the Fresnel region, leading to the hybrid near-field and far-field communication mode, where the conventional far-field channel estimation methods no longer work well. To tackle the above challenge, this paper considers channel estimation in a hybrid-field multipath environment, where each user and each scatterer can be in either the far-field or the near-field region. First, a joint angular-polar domain channel transform is designed to capture the hybrid-field channel's near-field and far-field features. We then analyze the power diffusion effect in the hybrid-field channel, which indicates that the power corresponding to one near-field (far-field) path component of the multipath channel may spread to far-field (near-field) paths and causes estimation error. We design a novel power-diffusion-based orthogonal matching pursuit channel estimation algorithm (PD-OMP). It can eliminate the prior knowledge requirement of path numbers in the far field and near field, which is a must in other OMP-based channel estimation algorithms. Simulation results show that PD-OMP outperforms current hybrid-field channel estimation methods. ",
        "title": "Channel Estimation for Holographic Communications in Hybrid Near-Far  Field",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08150",
        "abstract_url": "http://arxiv.org/abs/2401.08150",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Xintao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Linjun"
            },
            {
                "last_name": "Cai",
                "first_name": "Zhanrui"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. Proposed by Li (1991), sliced inverse regression has emerged as a widely utilized statistical technique for reducing covariate dimensionality while maintaining sufficient statistical information. In this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. We proceed to establish lower bounds for differentially private sliced inverse regression in both the low and high-dimensional settings. Moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. Through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. As a natural extension, we can readily offer analogous lower and upper bounds for differentially private sparse principal component analysis, a topic that may also be of potential interest to the statistical and machine learning community. ",
        "title": "Differentially Private Sliced Inverse Regression: Minimax Optimality and  Algorithm",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08151",
        "abstract_url": "http://arxiv.org/abs/2401.08151",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Arif Ali"
            },
            {
                "last_name": "Akbar",
                "first_name": "Muhammad Azeem"
            },
            {
                "last_name": "Lahtinen",
                "first_name": "Valtteri"
            },
            {
                "last_name": "Paavola",
                "first_name": "Marko"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Context: Quantum software systems represent a new realm in software engineering, utilizing quantum bits (Qubits) and quantum gates (Qgates) to solve the complex problems more efficiently than classical counterparts . Agile software development approaches are considered to address many inherent challenges in quantum software development, but their effective integration remains unexplored Objective: This study investigates key causes of challenges that could hinders the adoption of traditional agile approaches in quantum software projects and develop an Agile Quantum Software Project Success Prediction Model (AQSSPM). Methodology: Firstly, w e identified 19 causes of challenging factors discussed in our previous study, which are potentially impacting agile quantum project success. Secondly, a survey was conducted to collect expert opinions on these causes and applied Genetic Algorithm (GA) with Na i ve Bayes Classifier (NBC) and Logistic Regression (LR) to develop the AQSSPM Results: Utilizing GA with NBC, project success probability improved from 53.17% to 99.68%, with cost reductions from 0.463% to 0.403%. Similarly, GA with LR increased success rates from 55.52% to 98.99%, and costs decreased from 0.496% to 0.409% after 100 iterati ons. Both methods result showed a strong positive correlation (rs=0.955) in causes ranking, with no significant difference between them (t=1.195, p=0.240>0.05). Conclusion: The AQSSPM highlights critical focus areas for efficiently and successfully implementing agile quantum projects considering the cost factor of a particular project ",
        "title": "Agile Meets Quantum: A Novel Genetic Algorithm Model for Predicting the  Success of Quantum Software Development Project",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08153",
        "abstract_url": "http://arxiv.org/abs/2401.08153",
        "authors": [
            {
                "last_name": "Fan",
                "first_name": "Fletcher"
            },
            {
                "last_name": "Yi",
                "first_name": "Bowen"
            },
            {
                "last_name": "Rye",
                "first_name": "David"
            },
            {
                "last_name": "Shi",
                "first_name": "Guodong"
            },
            {
                "last_name": "Manchester",
                "first_name": "Ian R."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper introduces new model parameterizations for learning dynamical systems from data via the Koopman operator, and studies their properties. Whereas most existing works on Koopman learning do not take into account the stability or stabilizability of the model -- two fundamental pieces of prior knowledge about a given system to be identified -- in this paper, we propose new classes of Koopman models that have built-in guarantees of these properties. These models are guaranteed to be stable or stabilizable via a novel {\\em direct parameterization approach} that leads to {\\em unconstrained} optimization problems with respect to their parameter sets. To explore the representational flexibility of these model sets, we establish novel theoretical connections between the stability of discrete-time Koopman embedding and contraction-based forms of nonlinear stability and stabilizability. The proposed approach is illustrated in applications to stable nonlinear system identification and imitation learning via stabilizable models. Simulation results empirically show that the learning approaches based on the proposed models outperform prior methods lacking stability guarantees. ",
        "title": "Learning Stable Koopman Embeddings for Identification and Control",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08154",
        "abstract_url": "http://arxiv.org/abs/2401.08154",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhai",
                "first_name": "Yongqi"
            },
            {
                "last_name": "Li",
                "first_name": "Hangyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Ronggang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This one page paper describes our method for the track of image compression. To achieve better perceptual quality, we use the adversarial loss to generate realistic textures, use region of interest (ROI) mask to guide the bit allocation for different regions. Our Team name is TLIC. ",
        "title": "Learned Image Compression with ROI-Weighted Distortion and Bit  Allocation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08156",
        "abstract_url": "http://arxiv.org/abs/2401.08156",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Cong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Rui"
            },
            {
                "last_name": "Xu",
                "first_name": "Jiale"
            },
            {
                "last_name": "Leng",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Liu",
                "first_name": "Zihan"
            },
            {
                "last_name": "Huang",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Guo",
                "first_name": "Minyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Hao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shouren"
            },
            {
                "last_name": "Zhao",
                "first_name": "Junping"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ke"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Large-scale deep neural networks (DNNs), such as large language models (LLMs), have revolutionized the artificial intelligence (AI) field and become increasingly popular. However, training or fine-tuning such models requires substantial computational power and resources, where the memory capacity of a single acceleration device like a GPU is one of the most important bottlenecks. Owing to the prohibitively large overhead (e.g., $10 \\times$) of GPUs' native memory allocator, DNN frameworks like PyTorch and TensorFlow adopt a caching allocator that maintains a memory pool with a splitting mechanism for fast memory (de)allocation. Unfortunately, the caching allocator's efficiency degrades quickly for popular memory reduction techniques such as recomputation, offloading, distributed training, and low-rank adaptation. The primary reason is that those memory reduction techniques introduce frequent and irregular memory (de)allocation requests, leading to severe fragmentation problems for the splitting-based caching allocator. To mitigate this fragmentation problem, we propose a novel memory allocation framework based on low-level GPU virtual memory management called GPU memory lake (GMLake). GMLake employs a novel virtual memory stitching (VMS) mechanism, which can fuse or combine non-contiguous memory blocks with a virtual memory address mapping. GMLake can reduce an average of 9.2 GB (up to 25 GB) GPU memory usage and 15% (up to 33% ) fragmentation among eight LLM models on GPU A100 with 80 GB memory. GMLake is completely transparent to the DNN models and memory reduction techniques and ensures the seamless execution of resource-intensive deep-learning tasks. We have open-sourced GMLake at https://github.com/intelligent-machine-learning/glake/tree/main/GMLake. ",
        "title": "GMLake: Efficient and Transparent GPU Memory Defragmentation for  Large-scale DNN Training with Virtual Memory Stitching",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08161",
        "abstract_url": "http://arxiv.org/abs/2401.08161",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Xiaoxiong"
            },
            {
                "last_name": "Li",
                "first_name": "Chengqing"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  Generating random and pseudorandom numbers with a deterministic system is a long-standing challenge in theoretical research and engineering applications. Several pseudorandom number generators based on the inversive congruential method have been designed as attractive alternatives to those based on the classical linear congruential method. This paper discloses the least period of sequences generated by iterating an inversive pseudorandom number generator over the ring $\\mathbb{Z}_e$ by transforming it into a two-order linear congruential recurrence relation. Depending on whether the sequence is periodic or ultimately periodic, all states in the domain can be attributed to two types of objects: some cycles of different lengths and one unilateral connected digraph whose structure remains unchanged concerning parameter $e$. The graph structure of the generator over the ring $\\mathbb{Z}_e$ is precisely disclosed with rigorous theoretical analysis and verified experimentally. The adopted analysis methodology can be extended to study the graph structure of other nonlinear maps. ",
        "title": "Graph Structure of an Inversive Pseudorandom Number Generator over Ring  $\\mathbb{Z}_{p^{e}}$",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08164",
        "abstract_url": "http://arxiv.org/abs/2401.08164",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Gulshan"
            },
            {
                "last_name": "Madan",
                "first_name": "Surbhi"
            },
            {
                "last_name": "Bilalpur",
                "first_name": "Maneesh"
            },
            {
                "last_name": "Dhall",
                "first_name": "Abhinav"
            },
            {
                "last_name": "Subramanian",
                "first_name": "Ramanathan"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Sonification is a data visualization technique which expresses data attributes via psychoacoustic parameters, which are non-speech audio signals used to convey information. This paper investigates the binary estimation of cognitive load induced by psychoacoustic parameters conveying the focus level of an astronomical image via Electroencephalogram (EEG) embeddings. Employing machine learning and deep learning methodologies, we demonstrate that EEG signals are reliable for (a) binary estimation of cognitive load, (b) isolating easy vs difficult visual-to-auditory perceptual mappings, and (c) capturing perceptual similarities among psychoacoustic parameters. Our key findings reveal that (1) EEG embeddings can reliably measure cognitive load, achieving a peak F1-score of 0.98; (2) Extreme focus levels are easier to detect via auditory mappings than intermediate ones, and (3) psychoacoustic parameters inducing comparable cognitive load levels tend to generate similar EEG encodings. ",
        "title": "EEG-based Cognitive Load Estimation of Acoustic Parameters for Data  Sonification",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08165",
        "abstract_url": "http://arxiv.org/abs/2401.08165",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shupei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yutong"
            },
            {
                "last_name": "Di",
                "first_name": "Boya"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Recently, the rapid development of metasurface facilitates the growth of extremely large-scale antenna arrays, making the ultra-massive MIMO possible. In this paper, we study the codebook design and beam training for an intelligent omni-surface (IOS) aided multi-user system, where the IOS is a novel metasurface enabling simultaneous signal reflection and refraction. To deal with the near field expansion caused by the large-dimension of IOS, we design a near-far field codebook to serve users both in the near and far fields without prior knowledge of user distribution. Moreover, to fully exploit the dual functionality of the IOS, the coupling between the reflective and refractive signals is analyzed theoretically and utilized in the codebook design, thereby reducing the training overhead. On this basis, the multi-user beam training is adopted where each codeword covers multiple areas to enable all users to be trained simultaneously. Simulation results verify our theoretical analysis on the reflective-refractive coupling. Compared to the state-of-the-art schemes, the proposed scheme can improve the sum rate and throughput. ",
        "title": "Near-Far Field Codebook Design for IOS-Aided Multi-User Communications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08166",
        "abstract_url": "http://arxiv.org/abs/2401.08166",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Haobin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xulong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ning"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jing"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianzong"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Existing emotional speech synthesis methods often utilize an utterance-level style embedding extracted from reference audio, neglecting the inherent multi-scale property of speech prosody. We introduce ED-TTS, a multi-scale emotional speech synthesis model that leverages Speech Emotion Diarization (SED) and Speech Emotion Recognition (SER) to model emotions at different levels. Specifically, our proposed approach integrates the utterance-level emotion embedding extracted by SER with fine-grained frame-level emotion embedding obtained from SED. These embeddings are used to condition the reverse process of the denoising diffusion probabilistic model (DDPM). Additionally, we employ cross-domain SED to accurately predict soft labels, addressing the challenge of a scarcity of fine-grained emotion-annotated datasets for supervising emotional TTS training. ",
        "title": "ED-TTS: Multi-Scale Emotion Modeling using Cross-Domain Emotion  Diarization for Emotional Speech Synthesis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08167",
        "abstract_url": "http://arxiv.org/abs/2401.08167",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Lin",
                "first_name": "Buyu"
            },
            {
                "last_name": "Sen",
                "first_name": "Subhabrata"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "SI"
        ],
        "abstract": "  Multi-view data arises frequently in modern network analysis e.g. relations of multiple types among individuals in social network analysis, longitudinal measurements of interactions among observational units, annotated networks with noisy partial labeling of vertices etc. We study community detection in these disparate settings via a unified theoretical framework, and investigate the fundamental thresholds for community recovery. We characterize the mutual information between the data and the latent parameters, provided the degrees are sufficiently large. Based on this general result, (i) we derive a sharp threshold for community detection in an inhomogeneous multilayer block model \\citep{chen2022global}, (ii) characterize a sharp threshold for weak recovery in a dynamic stochastic block model \\citep{matias2017statistical}, and (iii) identify the limiting mutual information in an unbalanced partially labeled block model. Our first two results are derived modulo coordinate-wise convexity assumptions on specific functions -- we provide extensive numerical evidence for their correctness. Finally, we introduce iterative algorithms based on Approximate Message Passing for community detection in these problems. ",
        "title": "Fundamental limits of community detection from multi-view data:  multi-layer, dynamic and partially labeled block models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08169",
        "abstract_url": "http://arxiv.org/abs/2401.08169",
        "authors": [
            {
                "last_name": "Shiraishi",
                "first_name": "Tomohiro"
            },
            {
                "last_name": "Miwa",
                "first_name": "Daiki"
            },
            {
                "last_name": "Katsuoka",
                "first_name": "Teruyuki"
            },
            {
                "last_name": "Duy",
                "first_name": "Vo Nguyen Le"
            },
            {
                "last_name": "Taji",
                "first_name": "Koichi"
            },
            {
                "last_name": "Takeuchi",
                "first_name": "Ichiro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The Vision Transformer (ViT) demonstrates exceptional performance in various computer vision tasks. Attention is crucial for ViT to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. However, when utilizing the attention of ViT as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. In this study, we propose a statistical test for ViT's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for ViT's decision-making with a rigorously controlled error rate. Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentions. We demonstrate the validity and the effectiveness of the proposed method through numerical experiments and applications to brain image diagnoses. ",
        "title": "Statistical Test for Attention Map in Vision Transformer",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08171",
        "abstract_url": "http://arxiv.org/abs/2401.08171",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zida"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziran"
            },
            {
                "last_name": "Li",
                "first_name": "Haoying"
            },
            {
                "last_name": "Li",
                "first_name": "Menghao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yueting"
            },
            {
                "last_name": "Li",
                "first_name": "Qi"
            },
            {
                "last_name": "Feng",
                "first_name": "Huajun"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhihai"
            },
            {
                "last_name": "Chen",
                "first_name": "Shiqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Linear Array Pushbroom (LAP) imaging technology is widely used in the realm of remote sensing. However, images acquired through LAP always suffer from distortion and blur because of camera jitter. Traditional methods for restoring LAP images, such as algorithms estimating the point spread function (PSF), exhibit limited performance. To tackle this issue, we propose a Jitter-Aware Restoration Network (JARNet), to remove the distortion and blur in two stages. In the first stage, we formulate an Optical Flow Correction (OFC) block to refine the optical flow of the degraded LAP images, resulting in pre-corrected images where most of the distortions are alleviated. In the second stage, for further enhancement of the pre-corrected images, we integrate two jitter-aware techniques within the Spatial and Frequency Residual (SFRes) block: 1) introducing Coordinate Attention (CoA) to the SFRes block in order to capture the jitter state in orthogonal direction; 2) manipulating image features in both spatial and frequency domains to leverage local and global priors. Additionally, we develop a data synthesis pipeline, which applies Continue Dynamic Shooting Model (CDSM) to simulate realistic degradation in LAP images. Both the proposed JARNet and LAP image synthesis pipeline establish a foundation for addressing this intricate challenge. Extensive experiments demonstrate that the proposed two-stage method outperforms state-of-the-art image restoration models. Code is available at https://github.com/JHW2000/JARNet. ",
        "title": "Deep Linear Array Pushbroom Image Restoration: A Degradation Pipeline  and Jitter-Aware Restoration Network",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08174",
        "abstract_url": "http://arxiv.org/abs/2401.08174",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Zhen"
            },
            {
                "last_name": "Fan",
                "first_name": "Junfeng"
            },
            {
                "last_name": "Ma",
                "first_name": "Yunkai"
            },
            {
                "last_name": "Zhao",
                "first_name": "Sihan"
            },
            {
                "last_name": "Jing",
                "first_name": "Fengshui"
            },
            {
                "last_name": "Tan",
                "first_name": "Min"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Completely occluded and dense object instance segmentation (IS) is an important and challenging task. Although current amodal IS methods can predict invisible regions of occluded objects, they are difficult to directly predict completely occluded objects. For dense object IS, existing box-based methods are overly dependent on the performance of bounding box detection. In this paper, we propose CFNet, a coarse-to-fine IS framework for completely occluded and dense objects, which is based on box prompt-based segmentation foundation models (BSMs). Specifically, CFNet first detects oriented bounding boxes (OBBs) to distinguish instances and provide coarse localization information. Then, it predicts OBB prompt-related masks for fine segmentation. To predict completely occluded object instances, CFNet performs IS on occluders and utilizes prior geometric properties, which overcomes the difficulty of directly predicting completely occluded object instances. Furthermore, based on BSMs, CFNet reduces the dependence on bounding box detection performance, improving dense object IS performance. Moreover, we propose a novel OBB prompt encoder for BSMs. To make CFNet more lightweight, we perform knowledge distillation on it and introduce a Gaussian smoothing method for teacher targets. Experimental results demonstrate that CFNet achieves the best performance on both industrial and publicly available datasets. ",
        "title": "Completely Occluded and Dense Object Instance Segmentation Using Box  Prompt-Based Segmentation Foundation Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08178",
        "abstract_url": "http://arxiv.org/abs/2401.08178",
        "authors": [
            {
                "last_name": "Oh",
                "first_name": "Seok-Hwan"
            },
            {
                "last_name": "Jung",
                "first_name": "Guil"
            },
            {
                "last_name": "Kim",
                "first_name": "Myeong-Gee"
            },
            {
                "last_name": "Kim",
                "first_name": "Sang-Yun"
            },
            {
                "last_name": "Kim",
                "first_name": "Young-Min"
            },
            {
                "last_name": "Lee",
                "first_name": "Hyeon-Jik"
            },
            {
                "last_name": "Kwon",
                "first_name": "Hyuk-Sool"
            },
            {
                "last_name": "Bae",
                "first_name": "Hyeon-Min"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we introduce a Key-point-guided Diffusion probabilistic Model (KDM) that gains precise control over images by manipulating the object's key-point. We propose a two-stage generative model incorporating an optical flow map as an intermediate output. By doing so, a dense pixel-wise understanding of the semantic relation between the image and sparse key point is configured, leading to more realistic image generation. Additionally, the integration of optical flow helps regulate the inter-frame variance of sequential images, demonstrating an authentic sequential image generation. The KDM is evaluated with diverse key-point conditioned image synthesis tasks, including facial image generation, human pose synthesis, and echocardiography video prediction, demonstrating the KDM is proving consistency enhanced and photo-realistic images compared with state-of-the-art models. ",
        "title": "Key-point Guided Deformable Image Manipulation Using Diffusion Model",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08179",
        "abstract_url": "http://arxiv.org/abs/2401.08179",
        "authors": [
            {
                "last_name": "Peltekis",
                "first_name": "Christodoulos"
            },
            {
                "last_name": "Titopoulos",
                "first_name": "Vasileios"
            },
            {
                "last_name": "Nicopoulos",
                "first_name": "Chrysostomos"
            },
            {
                "last_name": "Dimitrakopoulos",
                "first_name": "Giorgos"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Deep Learning (DL) has achieved unprecedented success in various application domains. Meanwhile, model pruning has emerged as a viable solution to reduce the footprint of DL models in mobile applications, without compromising their accuracy. To enable the matrix engines built for dense DL models to also handle their pruned counterparts, pruned DL models follow a fine-grained structured sparsity pattern of 1:4, or 2:4, whereby in each group of four contiguous values, at least one, or two, respectively, must be non-zero. Structured sparsity has recently also moved to coarser (relaxed) cases of N:128, or N:256, for small values of N, targeting a wider range of sparsity (10%-90%) for the DL models. In this work, we design an accelerator that operates, by construction, on wide blocks with relaxed structured sparsity. In contrast to the conventional systolic array archetype, the new engine decouples the memory part of the systolic array from the multiply-add units. The memory block comprises 1 write and N read ports, with the number of read ports being equal to the number of non-zero elements per row. The multiply-add units connect directly to each read port and complete the multiplication in a row-wise product-first order. More importantly, simple reconfiguration facilitates more dense patterns. The experimental evaluation demonstrates substantial latency improvements over current state-of-the-art systolic array engines built for fine-grained and relaxed structured sparsity. ",
        "title": "DeMM: A Decoupled Matrix Multiplication Engine Supporting Relaxed  Structured Sparsity",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08180",
        "abstract_url": "http://arxiv.org/abs/2401.08180",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Tengji"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weipeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Luo",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Qiarong"
            },
            {
                "last_name": "Wang",
                "first_name": "Benshan"
            },
            {
                "last_name": "Luo",
                "first_name": "Mingcheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Xingyuan"
            },
            {
                "last_name": "Shastri",
                "first_name": "Bhavin J."
            },
            {
                "last_name": "Prucnal",
                "first_name": "Paul R."
            },
            {
                "last_name": "Huang",
                "first_name": "Chaoran"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Silicon photonic neural networks (PNNs) utilizing wavelength-multiplexing approaches, such as those based on micro-ring resonators (MRRs), offer superior performance due to their weight assignment, exceptional energy efficiency, and high computing density achieved through small device footprints. However, MRRs are highly sensitive to ambient disturbance, with even slight resonance drifts of tens of picometers potentially compromising MRRs-based PNNs. Current solutions often rely on complex control methods, resulting in high hardware complexity impractical for large-scale PNNs. Here, we propose a novel hardware-aware training and pruning approach. The core idea is to train the parameters of a physical neural network towards its noise-robust and energy-efficient region. This innovation enables control-free and energy-efficient photonic computing. Experimentally, this method improves computing precision by 4 bits without requiring intricate MRR control or power-consuming temperature stabilization circuits. It notably increases the accuracy of experimental handwritten digit classification from 67.0%, initially impacted by the thermal variances, to 95.0%, a figure comparable to the theoretical value and achieved without a thermoelectric controller. Additionally, this approach reduces the energy required to tune the MRRs ten-fold. Furthermore, our numerical verification indicates that this method is broadly applicable across various NN sizes and machine-learning tasks. Its effectiveness is particularly marked in larger NNs, where it boosts accuracy from 10.9% to 98.0% and slashes power consumption by 160 times. This advancement represents a significant step towards the practical, energy-efficient, and noise-resilient implementation of large-scale integrated PNNs. ",
        "title": "Control-free and efficient silicon photonic neural networks via  hardware-aware training and pruning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08181",
        "abstract_url": "http://arxiv.org/abs/2401.08181",
        "authors": [
            {
                "last_name": "Rixte",
                "first_name": "Alice"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  In Electronic Dance Music (EDM), many artists use DJing techniques in order to perform their own productions live. As a consequence, they do not have access during the performance to the internal structure of their tracks, and specifically to their equivalent of a partition: MIDI files. On the other hand, if an artist attempts to remix or interpret their own production live, the number of tracks that they can simultaneously control is limited without suitable software. This article introduces LiveScaler, a software that allows live control of the harmony and pitch of electronic music. A set of pitch transformations, termed affine transformations, is presented. These transformations are applied to all MIDI streams of a prepared track. A MaxMSP implementation, in conjunction with Ableton Live, is proposed. Special attention is given to control issues, mapping, and practical live experimentation in the context of EDM. ",
        "title": "LiveScaler: Live control of the harmony of an electronic music track",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08183",
        "abstract_url": "http://arxiv.org/abs/2401.08183",
        "authors": [
            {
                "last_name": "Dahl",
                "first_name": "Martin"
            },
            {
                "last_name": "Larsson",
                "first_name": "Erik G."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Wirelessly connected devices can collaborately train a machine learning model using federated learning, where the aggregation of model updates occurs using over-the-air computation. Carrier frequency offset caused by imprecise clocks in devices will cause the phase of the over-the-air channel to drift randomly, such that late symbols in a coherence block are transmitted with lower quality than early symbols. To mitigate the effect of degrading symbol quality, we propose a scheme where one of the permutations Roll, Flip and Sort are applied on gradients before transmission. Through simulations we show that the permutations can both improve and degrade learning performance. Furthermore, we derive the expectation and variance of the gradient estimate, which is shown to grow exponentially with the number of symbols in a coherence block. ",
        "title": "Over-the-Air Federated Learning with Phase Noise: Analysis and  Countermeasures",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08185",
        "abstract_url": "http://arxiv.org/abs/2401.08185",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Bingcai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Rainy weather will have a significant impact on the regular operation of the imaging system. Based on this premise, image rain removal has always been a popular branch of low-level visual tasks, especially methods using deep neural networks. However, most neural networks are but-branched, such as only using convolutional neural networks or Transformers, which is unfavourable for the multidimensional fusion of image features. In order to solve this problem, this paper proposes a dual-branch attention fusion network. Firstly, a two-branch network structure is proposed. Secondly, an attention fusion module is proposed to selectively fuse the features extracted by the two branches rather than simply adding them. Finally, complete ablation experiments and sufficient comparison experiments prove the rationality and effectiveness of the proposed method. ",
        "title": "DPAFNet:Dual Path Attention Fusion Network for Single Image Deraining",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08186",
        "abstract_url": "http://arxiv.org/abs/2401.08186",
        "authors": [
            {
                "last_name": "Elbir",
                "first_name": "Ahmet M."
            },
            {
                "last_name": "Celik",
                "first_name": "Abdulkadir"
            },
            {
                "last_name": "Eltawil",
                "first_name": "Ahmed M."
            },
            {
                "last_name": "Amin",
                "first_name": "Moeness G."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  A joint design of both sensing and communication can lead to substantial enhancement for both subsystems in terms of size, cost as well as spectrum and hardware efficiency. In the last decade, integrated sensing and communications (ISAC) has emerged as a means to efficiently utilize the spectrum on a single and shared hardware platform. Recent studies focused on developing multi-function approaches to share the spectrum between radar sensing and communications. Index modulation (IM) is one particular approach to incorporate information-bearing communication symbols into the emitted radar waveforms. While IM has been well investigated in communications-only systems, the implementation adoption of IM concept in ISAC has recently attracted researchers to achieve improved energy/spectral efficiency while maintaining satisfactory radar sensing performance. This article focuses on recent studies on IM-ISAC, and presents in detail the analytical background and relevance of the major IM-ISAC applications. ",
        "title": "Index Modulation for Integrated Sensing and Communications: A Signal  Processing Perspective",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08189",
        "abstract_url": "http://arxiv.org/abs/2401.08189",
        "authors": [
            {
                "last_name": "Kong",
                "first_name": "Weize"
            },
            {
                "last_name": "Hombaiah",
                "first_name": "Spurthi Amba"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Mei",
                "first_name": "Qiaozhu"
            },
            {
                "last_name": "Bendersky",
                "first_name": "Michael"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a \"trial and error\" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated tool leverages manually crafted prompts as starting points which makes the rewriting procedure more guided and efficient. The generated prompts are human readable, and self-explanatory, unlike some of those in previous works. We conducted extensive experiments on diverse datasets and found that the prompts generated with this new method not only outperform professionally crafted prompts, but also prompts generated with other previously proposed methods. ",
        "title": "PRewrite: Prompt Rewriting with Reinforcement Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08190",
        "abstract_url": "http://arxiv.org/abs/2401.08190",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Minpeng"
            },
            {
                "last_name": "Luo",
                "first_name": "Wei"
            },
            {
                "last_name": "Li",
                "first_name": "Chengxi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jing"
            },
            {
                "last_name": "Fan",
                "first_name": "Kai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets. We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we have made the model checkpoints and will make the dataset publicly available. We hope this will facilitate further research and development within the community. ",
        "title": "MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible  Pipeline",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08191",
        "abstract_url": "http://arxiv.org/abs/2401.08191",
        "authors": [
            {
                "last_name": "Valero",
                "first_name": "Francisco"
            },
            {
                "last_name": "Diaz-Rodriguez",
                "first_name": "Miguel"
            },
            {
                "last_name": "Valles",
                "first_name": "Marina"
            },
            {
                "last_name": "Besa",
                "first_name": "Antonio"
            },
            {
                "last_name": "Bernabeu",
                "first_name": "Enrique"
            },
            {
                "last_name": "Valera",
                "first_name": "Angel"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper aims to develop an approach for the reconfiguration of a parallel kinematic manipulator (PKM) with four degrees of freedom (DoF) designed to tackle tasks of diagnosis and rehabilitation in an injured knee. The original layout of the 4-DoF manipulator presents Type-II singular configurations within its workspace. Thus, we proposed to reconfigure the manipulator to avoid such singularities (owing to the Forward Jacobian of the PKM) during typical rehabilitation trajectories. We achieve the reconfiguration of the PKM through a minimization problem where the design variables correspond to the anchoring points of the robot limbs on fixed and mobile platforms. The objective function relies on the minimization of the forces exerted by the actuators for a specific trajectory. The minimization problem considers constraint equations to avoid Type-II singularities, which guarantee the feasibility of the active generalized coordinates for a particular path. To evaluate the proposed conceptual strategy, we build a prototype where reconfiguration occurs by moving the position of the anchoring points to holes bored in the fixed and mobile platforms. Simulations and experiments of several study cases enable testing the strategy performance. The results show that the reconfiguration strategy allows obtaining trajectories having minimum actuation forces without Type-II singularities. ",
        "title": "Reconfiguration of a parallel kinematic manipulator with 2T2R motions  for avoiding singularities through minimizing actuator forces",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08192",
        "abstract_url": "http://arxiv.org/abs/2401.08192",
        "authors": [
            {
                "last_name": "Valles",
                "first_name": "Marina"
            },
            {
                "last_name": "Araujo-Gomez",
                "first_name": "Pedro"
            },
            {
                "last_name": "Mata",
                "first_name": "Vicente"
            },
            {
                "last_name": "Valera",
                "first_name": "Angel"
            },
            {
                "last_name": "Diaz-Rodriguez",
                "first_name": "Miguel"
            },
            {
                "last_name": "Page",
                "first_name": "Alvaro"
            },
            {
                "last_name": "Farhat",
                "first_name": "Nidal M."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Although parallel manipulators (PMs) started with the introduction of architectures with 6 Degrees of Freedom (DoF), a vast number of applications require less than 6 DoF. Consequently, scholars have proposed architectures with 3 DoF and 4 DoF, but relatively few 4 DoF PMs have become prototypes, especially of the two rotation (2R) and two translation (2T) motion types. In this paper, we explain the mechatronics design, prototype and control architecture design of a 4 DoF PM with 2R2T motions. We chose to design a 4 DoF manipulator based on the motion needed to complete the tasks of lower limb rehabilitation.   To the author's best knowledge, PMs between 3 and 6 DoF for rehabilitation of lower limbs have not been proposed to date. The developed architecture enhances the three minimum DoF required by adding a 4 DoF which allows combinations of normal or tangential efforts in the joints, or torque acting on the knee. We put forward the inverse and forward displacement equations, describe the prototype, perform the experimental setup, and develop the hardware and control architecture. The tracking accuracy experiments from the proposed controller show that the manipulator can accomplish the required application. ",
        "title": "Mechatronic Design, Experimental Setup and Control Architecture Design  of a Novel 4 DoF Parallel Manipulator",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08194",
        "abstract_url": "http://arxiv.org/abs/2401.08194",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuefeng"
            },
            {
                "last_name": "Lin",
                "first_name": "Kai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  Image compression constitutes a significant challenge amidst the era of information explosion. Recent studies employing deep learning methods have demonstrated the superior performance of learning-based image compression methods over traditional codecs. However, an inherent challenge associated with these methods lies in their lack of interpretability. Following an analysis of the varying degrees of compression degradation across different frequency bands, we propose the end-to-end optimized image compression model facilitated by the frequency-oriented transform. The proposed end-to-end image compression model consists of four components: spatial sampling, frequency-oriented transform, entropy estimation, and frequency-aware fusion. The frequency-oriented transform separates the original image signal into distinct frequency bands, aligning with the human-interpretable concept. Leveraging the non-overlapping hypothesis, the model enables scalable coding through the selective transmission of arbitrary frequency components. Extensive experiments are conducted to demonstrate that our model outperforms all traditional codecs including next-generation standard H.266/VVC on MS-SSIM metric. Moreover, visual analysis tasks (i.e., object detection and semantic segmentation) are conducted to verify the proposed compression method could preserve semantic fidelity besides signal-level precision. ",
        "title": "End-to-End Optimized Image Compression with the Frequency-Oriented  Transform",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08195",
        "abstract_url": "http://arxiv.org/abs/2401.08195",
        "authors": [
            {
                "last_name": "Wan",
                "first_name": "Ruhao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shixin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we study the Hermitian hulls of (extended) generalized Reed-Solomon (GRS and EGRS) codes over finite fields. For a given class of (extended) GRS codes, by increasing the length, increasing the dimensions and increasing both the length and the dimensions, we obtain three new classes of (extended) GRS codes with Hermitian hulls of arbitrary dimensions. Furthermore, we obtain several new classes of $q^2$-ary maximum distance separable (MDS) codes with Hermitian hulls of arbitrary dimensions. And the dimension of these MDS codes can be taken from $1$ to $\\frac{n}{2}$. By propagation rules, the parameters of the obtained code can be more flexible. As an application, a lot of new (MDS) entanglement-assisted quantum error correction codes (EAQECCs) can be constructed from previous known (extended) GRS codes. We derive three new propagation rules on (MDS) EAQECCs constructed from (extended) GRS codes. Finally, we present several new classes of (MDS) EAQECCs with flexible parameters. Notably, the distance parameters of our codes can range from $2$ to $\\frac{n+2}{2}$. ",
        "title": "Three classes of propagation rules for GRS and EGRS codes and their  applications to EAQECCs",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08196",
        "abstract_url": "http://arxiv.org/abs/2401.08196",
        "authors": [
            {
                "last_name": "Flamini",
                "first_name": "Andrea"
            },
            {
                "last_name": "Sciarretta",
                "first_name": "Giada"
            },
            {
                "last_name": "Scuro",
                "first_name": "Mario"
            },
            {
                "last_name": "Sharif",
                "first_name": "Amir"
            },
            {
                "last_name": "Tomasi",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Ranise",
                "first_name": "Silvio"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Verifiable credentials are a digital analogue of physical credentials. Their authenticity and integrity are protected by means of cryptographic techniques, and they can be presented to verifiers to reveal attributes or even predicates about the attributes included in the credential. One way to preserve privacy during presentation consists in selectively disclosing the attributes in a credential. In this paper we present the most widespread cryptographic mechanisms used to enable selective disclosure of attributes identifying two categories: the ones based on hiding commitments - e.g., mdl ISO/IEC 18013-5 - and the ones based on non-interactive zero-knowledge proofs - e.g., BBS signatures. We also include a description of the cryptographic primitives used to design such cryptographic mechanisms. We describe the design of the cryptographic mechanisms and compare them by performing an analysis on their standard maturity in terms of standardization, cryptographic agility and quantum safety, then we compare the features that they support with main focus on the unlinkability of presentations, the ability to create predicate proofs and support for threshold credential issuance. Finally we perform an experimental evaluation based on the Rust open source implementations that we have considered most relevant. In particular we evaluate the size of credentials and presentations built using different cryptographic mechanisms and the time needed to generate and verify them. We also highlight some trade-offs that must be considered in the instantiation of the cryptographic mechanisms. ",
        "title": "On Cryptographic Mechanisms for the Selective Disclosure of Verifiable  Credentials",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08202",
        "abstract_url": "http://arxiv.org/abs/2401.08202",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Kai"
            },
            {
                "last_name": "He",
                "first_name": "Zihao"
            },
            {
                "last_name": "Burghardt",
                "first_name": "Keith"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingxin"
            },
            {
                "last_name": "Lerman",
                "first_name": "Kristina"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "CY",
            "DL"
        ],
        "abstract": "  The conflict between Israel and Palestinians significantly escalated after the October 7, 2023 Hamas attack, capturing global attention. To understand the public discourse on this conflict, we present a meticulously compiled dataset--IsamasRed--comprising nearly 400,000 conversations and over 8 million comments from Reddit, spanning from August 2023 to November 2023. We introduce an innovative keyword extraction framework leveraging a large language model to effectively identify pertinent keywords, ensuring a comprehensive data collection. Our initial analysis on the dataset, examining topics, controversy, emotional and moral language trends over time, highlights the emotionally charged and complex nature of the discourse. This dataset aims to enrich the understanding of online discussions, shedding light on the complex interplay between ideology, sentiment, and community engagement in digital spaces. ",
        "title": "IsamasRed: A Public Dataset Tracking Reddit Discussions on Israel-Hamas  Conflict",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08206",
        "abstract_url": "http://arxiv.org/abs/2401.08206",
        "authors": [
            {
                "last_name": "Long",
                "first_name": "Xinwei"
            },
            {
                "last_name": "Zeng",
                "first_name": "Jiali"
            },
            {
                "last_name": "Meng",
                "first_name": "Fandong"
            },
            {
                "last_name": "Ma",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kaiyan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bowen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jie"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Knowledge retrieval with multi-modal queries plays a crucial role in supporting knowledge-intensive multi-modal applications. However, existing methods face challenges in terms of their effectiveness and training efficiency, especially when it comes to training and integrating multiple retrievers to handle multi-modal queries. In this paper, we propose an innovative end-to-end generative framework for multi-modal knowledge retrieval. Our framework takes advantage of the fact that large language models (LLMs) can effectively serve as virtual knowledge bases, even when trained with limited data. We retrieve knowledge via a two-step process: 1) generating knowledge clues related to the queries, and 2) obtaining the relevant document by searching databases using the knowledge clue. In particular, we first introduce an object-aware prefix-tuning technique to guide multi-grained visual learning. Then, we align multi-grained visual features into the textual feature space of the LLM, employing the LLM to capture cross-modal interactions. Subsequently, we construct instruction data with a unified format for model training. Finally, we propose the knowledge-guided generation strategy to impose prior constraints in the decoding steps, thereby promoting the generation of distinctive knowledge clues. Through experiments conducted on three benchmarks, we demonstrate significant improvements ranging from 3.0% to 14.6% across all evaluation metrics when compared to strong baselines. ",
        "title": "Generative Multi-Modal Knowledge Retrieval with Large Language Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08209",
        "abstract_url": "http://arxiv.org/abs/2401.08209",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Leheng"
            },
            {
                "last_name": "Li",
                "first_name": "Yawei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xiaorui"
            },
            {
                "last_name": "Gu",
                "first_name": "Shuhang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks. ",
        "title": "Transcending the Limit of Local Window: Advanced Super-Resolution  Transformer with Adaptive Token Dictionary",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08210",
        "abstract_url": "http://arxiv.org/abs/2401.08210",
        "authors": [
            {
                "last_name": "Fang",
                "first_name": "Zhongbin"
            },
            {
                "last_name": "Li",
                "first_name": "Xia"
            },
            {
                "last_name": "Li",
                "first_name": "Xiangtai"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shen"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengyuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, 3D point cloud classification has made significant progress with the help of many datasets. However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods. To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras. ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods. Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner. We term our method PointMLS. Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective. More experiments also demonstrate the robustness and effectiveness of PointMLS. ",
        "title": "ModelNet-O: A Large-Scale Synthetic Dataset for Occlusion-Aware Point  Cloud Classification",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08212",
        "abstract_url": "http://arxiv.org/abs/2401.08212",
        "authors": [
            {
                "last_name": "Lyu",
                "first_name": "Hanjia"
            },
            {
                "last_name": "Qi",
                "first_name": "Weihong"
            },
            {
                "last_name": "Wei",
                "first_name": "Zhongyu"
            },
            {
                "last_name": "Luo",
                "first_name": "Jiebo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications. Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions. Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction. This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis. The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures. ",
        "title": "Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and  Usage in Digital Communication",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08216",
        "abstract_url": "http://arxiv.org/abs/2401.08216",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Yu"
            },
            {
                "last_name": "Shen",
                "first_name": "Jiyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziyao"
            },
            {
                "last_name": "Tan",
                "first_name": "Chee Wei"
            },
            {
                "last_name": "Lam",
                "first_name": "Kwok-Yan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaining comparable recovery performance, we can accelerate the recovery speed and decrease memory consumption. Following this concept, we introduce Crab, an efficient and certified recovery method, which relies on selective information storage and adaptive model rollback. Theoretically, we demonstrate that the difference between the global model recovered by Crab and the one recovered by train-from-scratch can be bounded under certain assumptions. Our empirical evaluation, conducted across three datasets over multiple machine learning models, and a variety of untargeted and targeted poisoning attacks reveals that Crab is both accurate and efficient, and consistently outperforms previous approaches in terms of both recovery speed and memory consumption. ",
        "title": "Towards Efficient and Certified Recovery from Poisoning Attacks in  Federated Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08217",
        "abstract_url": "http://arxiv.org/abs/2401.08217",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Zhixuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Cui",
                "first_name": "Qing"
            },
            {
                "last_name": "Li",
                "first_name": "Longfei"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenqing"
            },
            {
                "last_name": "Li",
                "first_name": "Sheng"
            },
            {
                "last_name": "Qin",
                "first_name": "Zhan"
            },
            {
                "last_name": "Ren",
                "first_name": "Kui"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests. To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks. By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability. We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets. The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications. ",
        "title": "LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable  Recommendation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08219",
        "abstract_url": "http://arxiv.org/abs/2401.08219",
        "authors": [
            {
                "last_name": "Birkmann",
                "first_name": "Fabian"
            },
            {
                "last_name": "Urbat",
                "first_name": "Henning"
            },
            {
                "last_name": "Milius",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  Extensions of Stone-type dualities have a long history in algebraic logic and have also been instrumental for proving results in algebraic language theory. We show how to extend abstract categorical dualities via monoidal adjunctions, subsuming various incarnations of classical extended Stone and Priestley duality as a special case. Guided by these categorical foundations, we investigate residuation algebras, which are algebraic models of language derivatives, and show the subcategory of derivation algebras to be dually equivalent to the category of profinite ordered monoids, restricting to a duality between boolean residuation algebras and profinite monoids. We further extend this duality to capture relational morphisms of profinite ordered monoids, which dualize to natural morphisms of residuation algebras. ",
        "title": "Monoidal Extended Stone Duality",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08221",
        "abstract_url": "http://arxiv.org/abs/2401.08221",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hang"
            },
            {
                "last_name": "Yang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Du",
                "first_name": "Keqing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Integrating deep learning and causal discovery has encouraged us to spot that learning causal structures and representations in dialogue and video is full of challenges. We defined These data forms as \"Indefinite Data\", characterized by multi-structure data and multi-value representations. Unlike existing adaptable data forms, Indefinite Data still faces gaps in datasets and methods. To address the dataset gap, we release two high-quality datasets - Causalogue and Causaction, containing text dialogue samples and video action samples with causal annotations respectively. Moreover, the method gap arises from the coexistence of multi-structure data and multi-value representations, breaking the assumptions of all current methods and rendering them infeasible on Indefinite Data. To this end, we propose a probabilistic framework as a baseline, incorporating three designed highlights for this gap: 1) establishing Causation Condition of representations using the independence of noise terms under non-fixed causal structures, 2) treating causal strength as a latent variable and measuring the reconstruction loss in the correlation space, and 3) estimating the effects of latent confounders. These highpoints make the probabilistic model capable of overcoming challenges brought by the coexistence of multi-structure data and multi-value representations and pave the way for the extension of latent confounders. Comprehensive experiments have evaluated baseline results of causal structures, causal representations, and confounding disentanglement. ",
        "title": "Towards Causal Relationship in Indefinite Data: Baseline Model and New  Datasets",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08224",
        "abstract_url": "http://arxiv.org/abs/2401.08224",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jiachun"
            },
            {
                "last_name": "Simchi-Levi",
                "first_name": "David"
            },
            {
                "last_name": "Shi",
                "first_name": "Kaining"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimality condition. Furthermore, we propose differentially private algorithms which still matches the lower bound, showing that privacy is \"almost free\". Additionally, we derive the asymptotic normality of the estimator, which is essential in statistical inference and hypothesis testing. ",
        "title": "Differentially Private Estimation of CATE in Adaptive Experiment",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08225",
        "abstract_url": "http://arxiv.org/abs/2401.08225",
        "authors": [
            {
                "last_name": "Geyer",
                "first_name": "Fabien"
            },
            {
                "last_name": "Freitag",
                "first_name": "Johannes"
            },
            {
                "last_name": "Schulz",
                "first_name": "Tobias"
            },
            {
                "last_name": "Uhrig",
                "first_name": "Sascha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "AR"
        ],
        "abstract": "  In recent years, machine learning (ML) and neural networks (NNs) have gained widespread use and attention across various domains, particularly in transportation for achieving autonomy, including the emergence of flying taxis for urban air mobility (UAM). However, concerns about certification have come up, compelling the development of standardized processes encompassing the entire ML and NN pipeline. This paper delves into the inference stage and the requisite hardware, highlighting the challenges associated with IEEE 754 floating-point arithmetic and proposing alternative number representations. By evaluating diverse summation and dot product algorithms, we aim to mitigate issues related to non-associativity. Additionally, our exploration of fixed-point arithmetic reveals its advantages over floating-point methods, demonstrating significant hardware efficiencies. Employing an empirical approach, we ascertain the optimal bit-width necessary to attain an acceptable level of accuracy, considering the inherent complexity of bit-width optimization. ",
        "title": "Efficient and Mathematically Robust Operations for Certified Neural  Networks Inference",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08227",
        "abstract_url": "http://arxiv.org/abs/2401.08227",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zhonghao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ru"
            },
            {
                "last_name": "Fu",
                "first_name": "Jiaye"
            },
            {
                "last_name": "Wong",
                "first_name": "Ka-Chun"
            },
            {
                "last_name": "Peng",
                "first_name": "Chengbin"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Core-periphery structure is an essential mesoscale feature in complex networks. Previous researches mostly focus on discriminative approaches while in this work, we propose a generative model called masked Bayesian non-negative matrix factorization. We build the model using two pair affiliation matrices to indicate core-periphery pair associations and using a mask matrix to highlight connections to core nodes. We propose an approach to infer the model parameters, and prove the convergence of variables with our approach. Besides the abilities as traditional approaches, it is able to identify core scores with overlapping core-periphery pairs. We verify the effectiveness of our method using randomly generated networks and real-world networks. Experimental results demonstrate that the proposed method outperforms traditional approaches. ",
        "title": "Core-periphery Detection Based on Masked Bayesian Non-negative Matrix  Factorization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08228",
        "abstract_url": "http://arxiv.org/abs/2401.08228",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Hao"
            },
            {
                "last_name": "Guo",
                "first_name": "Lei"
            },
            {
                "last_name": "Zhu",
                "first_name": "Lei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yongqiang"
            },
            {
                "last_name": "Gao",
                "first_name": "Min"
            },
            {
                "last_name": "Yin",
                "first_name": "Hongzhi"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Cross-domain Recommendation (CR) is the task that tends to improve the recommendations in the sparse target domain by leveraging the information from other rich domains. Existing methods of cross-domain recommendation mainly focus on overlapping scenarios by assuming users are totally or partially overlapped, which are taken as bridges to connect different domains. However, this assumption does not always hold since it is illegal to leak users' identity information to other domains. Conducting Non-overlapping MCR (NMCR) is challenging since 1) The absence of overlapping information prevents us from directly aligning different domains, and this situation may get worse in the MCR scenario. 2) The distribution between source and target domains makes it difficult for us to learn common information across domains. To overcome the above challenges, we focus on NMCR, and devise MCRPL as our solution. To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts, and pre-train them in the pre-training stage. To address Challenge 2, we further update the domain-dependent prompts with other parameters kept fixed to transfer the domain knowledge to the target domain. We conduct experiments on five real-world domains, and the results show the advance of our MCRPL method compared with several recent SOTA baselines. ",
        "title": "MCRPL: A Pretrain, Prompt & Fine-tune Paradigm for Non-overlapping  Many-to-one Cross-domain Recommendation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08229",
        "abstract_url": "http://arxiv.org/abs/2401.08229",
        "authors": [
            {
                "last_name": "Pulloquinga",
                "first_name": "Jose L."
            },
            {
                "last_name": "Mata",
                "first_name": "Vicente"
            },
            {
                "last_name": "Valera",
                "first_name": "Angel"
            },
            {
                "last_name": "Zamora-Ortiz",
                "first_name": "Pau"
            },
            {
                "last_name": "Diaz-Rodriguez",
                "first_name": "Miguel"
            },
            {
                "last_name": "Zambrano",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Parallel robots (PRs) have singular configurations where the robot gains at least one degree of freedom and loses control. Theoretically, such singularity occurs when the Forward Jacobian-matrix determinant becomes zero (Type II). However, actual PRs could lose control owing to Type II singularities for determinant values near zero, but not zero, because manufacturing tolerances introduce errors that are complex to model due to their low repeatability.   Thus, using an actual 3UPS+RPU PR, this paper presents three contributions: i) a proximity detection index for Type II singularities based on the angle between two Output Twist Screws. The index can identify which kinematic chains contribute to the singularity. ii) an experimental benchmark to study Type II singularities. iii) PR configurations where the proposed index is zero and the Forward Jacobian determinant is not. In this last configuration, the findings show that the actual robot is unable to handle external actions applied to the PR. ",
        "title": "Experimental Analysis of Type II Singularities and Assembly Change  Points in a 3UPS+RPU Parallel Robot",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08232",
        "abstract_url": "http://arxiv.org/abs/2401.08232",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Chongzhi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyuan"
            },
            {
                "last_name": "Teng",
                "first_name": "Zhiyang"
            },
            {
                "last_name": "Li",
                "first_name": "Jiayi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xizhou"
            },
            {
                "last_name": "Lu",
                "first_name": "Lewei"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Sun",
                "first_name": "Aixin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding. Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data. In this study, we present a novel approach to NLVL that aims to address this issue. Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query. The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder. To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder. Our approach effectively encapsulates the interaction between the query and video data across various time scales. Experiments on the Charades and DiDeMo datasets underscore the potency of our design. ",
        "title": "Multi-scale 2D Temporal Map Diffusion Models for Natural Language Video  Localization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08233",
        "abstract_url": "http://arxiv.org/abs/2401.08233",
        "authors": [
            {
                "last_name": "Christian",
                "first_name": "Mulomba Mukendi"
            },
            {
                "last_name": "Kim",
                "first_name": "Yun Seon"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            },
            {
                "last_name": "Lee",
                "first_name": "Jaeyoung"
            },
            {
                "last_name": "You",
                "first_name": "SongHee"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Accurate prediction of wind speed and power is vital for enhancing the efficiency of wind energy systems. Numerous solutions have been implemented to date, demonstrating their potential to improve forecasting. Among these, deep learning is perceived as a revolutionary approach in the field. However, despite their effectiveness, the noise present in the collected data remains a significant challenge. This noise has the potential to diminish the performance of these algorithms, leading to inaccurate predictions. In response to this, this study explores a novel feature engineering approach. This approach involves altering the data input shape in both Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various forecasting horizons. The results reveal substantial enhancements in model resilience against noise resulting from step increases in data. The approach could achieve an impressive 83% accuracy in predicting unseen data up to the 24th steps. Furthermore, this method consistently provides high accuracy for short, mid, and long-term forecasts, outperforming the performance of individual models. These findings pave the way for further research on noise reduction strategies at different forecasting horizons through shape-wise feature engineering. ",
        "title": "Enhancing Wind Speed and Wind Power Forecasting Using Shape-Wise Feature  Engineering: A Novel Approach for Improved Accuracy and Robustness",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08236",
        "abstract_url": "http://arxiv.org/abs/2401.08236",
        "authors": [
            {
                "last_name": "Shakespeare",
                "first_name": "Dougal"
            },
            {
                "last_name": "Roth",
                "first_name": "Camille"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In the field of node representation learning the task of interpreting latent dimensions has become a prominent, well-studied research topic. The contribution of this work focuses on appraising the interpretability of another rarely-exploited feature of node embeddings increasingly utilised in recommendation and consumption diversity studies: inter-node embedded distances. Introducing a new method to measure how understandable the distances between nodes are, our work assesses how well the proximity weights derived from a network before embedding relate to the node closeness measurements after embedding. Testing several classical node embedding models, our findings reach a conclusion familiar to practitioners albeit rarely cited in literature - the matrix factorisation model SVD is the most interpretable through 1, 2 and even higher-order proximities. ",
        "title": "Interpreting Node Embedding Distances Through $n$-order Proximity  Neighbourhoods",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08237",
        "abstract_url": "http://arxiv.org/abs/2401.08237",
        "authors": [
            {
                "last_name": "Delbari",
                "first_name": "Mohamadreza"
            },
            {
                "last_name": "Alexandropoulos",
                "first_name": "George C."
            },
            {
                "last_name": "Schober",
                "first_name": "Robert"
            },
            {
                "last_name": "Jamali",
                "first_name": "Vahid"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this chapter, we investigate the mathematical foundation of the modeling and design of reconfigurable intelligent surfaces (RIS) in both the far- and near-field regimes. More specifically, we first present RIS-assisted wireless channel models for the far- and near-field regimes, discussing relevant phenomena, such as line-of-sight (LOS) and non-LOS links, rich and poor scattering, channel correlation, and array manifold. Subsequently, we introduce two general approaches for the RIS reflective beam design, namely optimization-based and analytical, which offer different degrees of design flexibility and computational complexity. Furthermore, we provide a comprehensive set of simulation results for the performance evaluation of the studied RIS beam designs and the investigation of the impact of the system parameters. ",
        "title": "Far- versus Near-Field RIS Modeling and Beam Design",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08238",
        "abstract_url": "http://arxiv.org/abs/2401.08238",
        "authors": [
            {
                "last_name": "Braglia",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Tebaldi",
                "first_name": "Davide"
            },
            {
                "last_name": "Biagiotti",
                "first_name": "Luigi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  When there is a need to define and adapt a robotic task based on a reference motion, Dynamic Movement Primitives (DMP) is a standard and efficient method for encoding it. The nominal trajectory is typically obtained through a Programming by Demonstration (PbD) approach, where the robot is taught a specific task through kinesthetic guidance. Subsequently, the motion is reproduced by the manipulator in terms of both geometric path and timing law. The basic approach for modifying the duration of the execution involves adjusting a time constant characterizing the model. On the contrary, the goal of this paper is to achieve complete decoupling between the geometric information of the task, encoded into the DMP, and the phase law governing the execution, allowing them to be chosen independently. This enables the optimization of the task duration to satisfy constraints such as velocity or acceleration or even to define a phase law dependent on external inputs, such as the force applied by a user in a co-manipulation task. As an example, this mechanism will be exploited to define a rehabilitation activity where the cobot assists humans in performing various pre-planned exercises. ",
        "title": "Phase-free Dynamic Movement Primitives Applied to Kinesthetic Guidance  in Robotic Co-manipulation Tasks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08241",
        "abstract_url": "http://arxiv.org/abs/2401.08241",
        "authors": [
            {
                "last_name": "M\u00fcller",
                "first_name": "Romy"
            },
            {
                "last_name": "Blunk",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In complex systems, decision makers often have to consider qualitatively different risks when choosing between options. Do their strategies of integrating these risks depend on the framing of problem contents? In the present study, participants were either instructed that they were choosing between two ways of solving a complex problem, or between two generic options. The former was framed as a modular plant scenario that required choices between modifying parameter settings in a current module (Adapt) and replacing the module by another one (Exchange). The risk was higher for Adapt to harm the product and for Exchange to harm the plant. These risks were presented as probabilities, and participants were either told that the consequences of both risks were equally severe (content-same group), or that harming the plant was much worse (content-different group). A third group made decisions based on the same probabilities, but received a generic task framing (no-content group). We expected framing to affect risk integration, leading the content-same group to make different choices than the no-content group. Contrary to this hypothesis, these two groups were strikingly similar in their decision outcomes and strategies, but clearly differed from the content-different group. These findings question whether ecological validity can be enhanced merely by framing a task in terms of real-world problem contents. ",
        "title": "Adapt/Exchange decisions or generic choices: Does framing influence how  people integrate qualitatively different risks?",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08242",
        "abstract_url": "http://arxiv.org/abs/2401.08242",
        "authors": [
            {
                "last_name": "Sawai",
                "first_name": "Sora"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Kazuaki"
            },
            {
                "last_name": "Ozaki",
                "first_name": "Katsuhisa"
            },
            {
                "last_name": "Oishi",
                "first_name": "Shin'ichi"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  Two-dimensional Delaunay triangulation is a fundamental aspect of computational geometry. This paper presents a novel algorithm that is specifically designed to ensure the correctness of 2D Delaunay triangulation, namely the Polygonal Sequence-driven Triangulation Validator (PSTV). Our research highlights the paramount importance of proper triangulation and the often overlooked, yet profound, impact of rounding errors in numerical computations on the precision of triangulation. The primary objective of the PSTV algorithm is to identify these computational errors and ensure the accuracy of the triangulation output. In addition to validating the correctness of triangulation, this study underscores the significance of the Delaunay property for the quality of finite element methods. Effective strategies are proposed to verify this property for a triangulation and correct it when necessary. While acknowledging the difficulty of rectifying complex triangulation errors such as overlapping triangles, these strategies provide valuable insights on identifying the locations of these errors and remedying them. The unique feature of the PSTV algorithm lies in its adoption of floating-point filters in place of interval arithmetic, striking an effective balance between computational efficiency and precision. This research sets a vital precedent for error reduction and precision enhancement in computational geometry. ",
        "title": "Polygonal Sequence-driven Triangulation Validator: An Incremental  Approach to 2D Triangulation Verification",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08245",
        "abstract_url": "http://arxiv.org/abs/2401.08245",
        "authors": [
            {
                "last_name": "Tamaru",
                "first_name": "Asuka"
            },
            {
                "last_name": "Hara",
                "first_name": "Junya"
            },
            {
                "last_name": "Higashi",
                "first_name": "Hiroshi"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Yuichi"
            },
            {
                "last_name": "Ortega",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we propose a method, based on graph signal processing, to optimize the choice of $k$ in $k$-nearest neighbor graphs ($k$NNGs). $k$NN is one of the most popular approaches and is widely used in machine learning and signal processing. The parameter $k$ represents the number of neighbors that are connected to the target node; however, its appropriate selection is still a challenging problem. Therefore, most $k$NNGs use ad hoc selection methods for $k$. In the proposed method, we assume that a different $k$ can be chosen for each node. We formulate a discrete optimization problem to seek the best $k$ with a constraint on the sum of distances of the connected nodes. The optimal $k$ values are efficiently obtained without solving a complex optimization. Furthermore, we reveal that the proposed method is closely related to existing graph learning methods. In experiments on real datasets, we demonstrate that the $k$NNGs obtained with our method are sparse and can determine an appropriate variable number of edges per node. We validate the effectiveness of the proposed method for point cloud denoising, comparing our denoising performance with achievable graph construction methods that can be scaled to typical point cloud sizes (e.g., thousands of nodes). ",
        "title": "Optimizing $k$ in $k$NN Graphs with Graph Learning Perspective",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08249",
        "abstract_url": "http://arxiv.org/abs/2401.08249",
        "authors": [
            {
                "last_name": "Rosenberger",
                "first_name": "Hans"
            },
            {
                "last_name": "Bereyhi",
                "first_name": "Ali"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Ralf R."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We revisit existing linear computation coding (LCC) algorithms, and introduce a new framework that measures the computational cost of computing multidimensional linear functions, not only in terms of the number of additions, but also with respect to their suitability for parallel processing. Utilizing directed acyclic graphs, which correspond to signal flow graphs in hardware, we propose a novel LCC algorithm that controls the trade-off between the total number of operations and their parallel executability. Numerical evaluations show that the proposed algorithm, constrained to a fully parallel structure, outperforms existing schemes. ",
        "title": "Graph-based Algorithms for Linear Computation Coding",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08251",
        "abstract_url": "http://arxiv.org/abs/2401.08251",
        "authors": [
            {
                "last_name": "Marug\u00e1n",
                "first_name": "Alberto Pliego"
            },
            {
                "last_name": "M\u00e1rquez",
                "first_name": "Fausto Pedro Garc\u00eda"
            },
            {
                "last_name": "P\u00e9rez",
                "first_name": "Jes\u00fas Mar\u00eda Pinar"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Currently, wind energy is one of the most important sources of renewable energy. Offshore locations for wind turbines are increasingly exploited because of their numerous advantages. However, offshore wind farms require high investment in maintenance service. Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners. In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers. We created a complete techno-economic model to address this problem from an impartial point of view. An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives. Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests. The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier. This analysis evaluates the conflicts of interest of both parties. In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests. ",
        "title": "A techno-economic model for avoiding conflicts of interest between  owners of offshore wind farms and maintenance suppliers",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08255",
        "abstract_url": "http://arxiv.org/abs/2401.08255",
        "authors": [
            {
                "last_name": "Roth",
                "first_name": "Tom"
            },
            {
                "last_name": "Unanue",
                "first_name": "Inigo Jauregi"
            },
            {
                "last_name": "Abuadbba",
                "first_name": "Alsharif"
            },
            {
                "last_name": "Piccardi",
                "first_name": "Massimo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Current adversarial attack algorithms, where an adversary changes a text to fool a victim model, have been repeatedly shown to be effective against text classifiers. These attacks, however, generally assume that the victim model is monolingual and cannot be used to target multilingual victim models, a significant limitation given the increased use of these models. For this reason, in this work we propose an approach to fine-tune a multilingual paraphrase model with an adversarial objective so that it becomes able to generate effective adversarial examples against multilingual classifiers. The training objective incorporates a set of pre-trained models to ensure text quality and language consistency of the generated text. In addition, all the models are suitably connected to the generator by vocabulary-mapping matrices, allowing for full end-to-end differentiability of the overall training pipeline. The experimental validation over two multilingual datasets and five languages has shown the effectiveness of the proposed approach compared to existing baselines, particularly in terms of query efficiency. We also provide a detailed analysis of the generated attacks and discuss limitations and opportunities for future research. ",
        "title": "A Generative Adversarial Attack for Multilingual Text Classifiers",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08256",
        "abstract_url": "http://arxiv.org/abs/2401.08256",
        "authors": [
            {
                "last_name": "Alabi",
                "first_name": "Oluwatosin"
            },
            {
                "last_name": "Vercauteren",
                "first_name": "Tom"
            },
            {
                "last_name": "Shi",
                "first_name": "Miaojing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos. Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships. Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data. This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS. Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems. Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments. ",
        "title": "Multitask Learning in Minimally Invasive Surgical Vision: A Review",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08258",
        "abstract_url": "http://arxiv.org/abs/2401.08258",
        "authors": [
            {
                "last_name": "Popovski",
                "first_name": "Petar"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "IT"
        ],
        "abstract": "  Wireless systems beyond 5G evolve towards embracing both sensing and communication, resulting in increased convergence of the digital and the physical world. The existence of fused digital-physical realms raises critical questions regarding temporal ordering, causality, and the synchronization of events. This paper addresses the temporal challenges arising from the fact that the wireless infrastructure becomes an entity with multisensory perception. With the growing reliance on real-time interactions and applications such as digital twins, extended reality, and the metaverse, the need for accurate timestamping and temporal forensics becomes crucial. The paper introduces a model that incorporates Temporal Windows of Integration (TWI) to emulate human multisensory perception and discusses the implications for setting timing constraints in real-time applications and enabling temporal forensics. The analysis explores trade-offs, probabilities, and bounds for simultaneity and causality violation in the context of wireless systems evolving towards perceptive networks. This work underscores the significance of timestamping in the evolving wireless landscape, provide insights into system-level implications, and points out new research avenues for systems that combine sensing and communications. ",
        "title": "Time, Simultaneity, and Causality in Wireless Networks with Sensing and  Communications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08260",
        "abstract_url": "http://arxiv.org/abs/2401.08260",
        "authors": [
            {
                "last_name": "Hertrich",
                "first_name": "Johannes"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Kernel-based methods are heavily used in machine learning. However, they suffer from $O(N^2)$ complexity in the number $N$ of considered data points. In this paper, we propose an approximation procedure, which reduces this complexity to $O(N)$. Our approach is based on two ideas. First, we prove that any radial kernel with analytic basis function can be represented as sliced version of some one-dimensional kernel and derive an analytic formula for the one-dimensional counterpart. It turns out that the relation between one- and $d$-dimensional kernels is given by a generalized Riemann-Liouville fractional integral. Hence, we can reduce the $d$-dimensional kernel summation to a one-dimensional setting. Second, for solving these one-dimensional problems efficiently, we apply fast Fourier summations on non-equispaced data, a sorting algorithm or a combination of both. Due to its practical importance we pay special attention to the Gaussian kernel, where we show a dimension-independent error bound and represent its one-dimensional counterpart via a closed-form Fourier transform. We provide a run time comparison and error estimate of our fast kernel summations. ",
        "title": "Fast Kernel Summation in High Dimensions via Slicing and Fourier  Transforms",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08261",
        "abstract_url": "http://arxiv.org/abs/2401.08261",
        "authors": [
            {
                "last_name": "Pautov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Bogdanov",
                "first_name": "Nikita"
            },
            {
                "last_name": "Pyatkin",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Rogov",
                "first_name": "Oleg"
            },
            {
                "last_name": "Oseledets",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups. ",
        "title": "Probabilistically Robust Watermarking of Neural Networks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08263",
        "abstract_url": "http://arxiv.org/abs/2401.08263",
        "authors": [
            {
                "last_name": "Arcanjo",
                "first_name": "Bruno"
            },
            {
                "last_name": "Ferrarini",
                "first_name": "Bruno"
            },
            {
                "last_name": "Milford",
                "first_name": "Michael"
            },
            {
                "last_name": "McDonald-Maier",
                "first_name": "Klaus D."
            },
            {
                "last_name": "Ehsan",
                "first_name": "Shoaib"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data. VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints. Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance. Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets. Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis. For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image. The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment. ",
        "title": "Multi-Technique Sequential Information Consistency For Dynamic Visual  Place Recognition In Changing Environments",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08264",
        "abstract_url": "http://arxiv.org/abs/2401.08264",
        "authors": [
            {
                "last_name": "Tripuramallu",
                "first_name": "Dhiren"
            },
            {
                "last_name": "Singh",
                "first_name": "Swapnil"
            },
            {
                "last_name": "Deshmukh",
                "first_name": "Shrirang"
            },
            {
                "last_name": "Pinisetty",
                "first_name": "Srinivas"
            },
            {
                "last_name": "Shivaji",
                "first_name": "Shinde Arjun"
            },
            {
                "last_name": "Balusamy",
                "first_name": "Raja"
            },
            {
                "last_name": "Bandeppa",
                "first_name": "Ajaganna"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "PL"
        ],
        "abstract": "  Rust is a multi-paradigm programming language developed by Mozilla that focuses on performance and safety. Rust code is arguably known best for its speed and memory safety, a property essential while developing embedded systems. Thus, it becomes one of the alternatives when developing operating systems for embedded devices. How to convert an existing C++ code base to Rust is also gaining greater attention. In this work, we focus on the process of transpiling C++ code to a Rust codebase in a robust and safe manner. The manual transpilation process is carried out to understand the different constructs of the Rust language and how they correspond to C++ constructs. Based on the learning from the manual transpilation, a transpilation table is created to aid in future transpilation efforts and to develop an automated transpiler. We also studied the existing automated transpilers and identified the problems and inefficiencies they involved. The results of the transpilation process were closely monitored and evaluated, showing improved memory safety without compromising performance and reliability of the resulting codebase. The study concludes with a comprehensive analysis of the findings, an evaluation of the implications for future research, and recommendations for the same in this area. ",
        "title": "Towards a Transpiler for C/C++ to Safer Rust",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08267",
        "abstract_url": "http://arxiv.org/abs/2401.08267",
        "authors": [
            {
                "last_name": "Pathak",
                "first_name": "Kanaad"
            },
            {
                "last_name": "Azzopardi",
                "first_name": "Leif"
            },
            {
                "last_name": "Halvey",
                "first_name": "Martin"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The Probability Ranking Principle (PRP) ranks search results based on their expected utility derived solely from document contents, often overlooking the nuances of presentation and user interaction. However, with the evolution of Search Engine Result Pages (SERPs), now comprising a variety of result cards, the manner in which these results are presented is pivotal in influencing user engagement and satisfaction. This shift prompts the question: How does the PRP and its user-centric counterpart, the Interactive Probability Ranking Principle (iPRP), compare in the context of these heterogeneous SERPs? Our study draws a comparison between the PRP and the iPRP, revealing significant differences in their output. The iPRP, accounting for item-specific costs and interaction probabilities to determine the ``Expected Perceived Utility\" (EPU), yields different result orderings compared to the PRP. We evaluate the effect of the EPU on the ordering of results by observing changes in the ranking within a heterogeneous SERP compared to the traditional ``ten blue links''. We find that changing the presentation affects the ranking of items according to the (iPRP) by up to 48\\% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the TREC WaPo Collection. This work suggests that the iPRP should be employed when ranking heterogeneous SERPs to provide a user-centric ranking that adapts the ordering based on the presentation and user engagement. ",
        "title": "Ranking Heterogeneous Search Result Pages using the Interactive  Probability Ranking Principle",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08272",
        "abstract_url": "http://arxiv.org/abs/2401.08272",
        "authors": [
            {
                "last_name": "Tabatabaei",
                "first_name": "Zahra"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Moll",
                "first_name": "JAvier Oliver"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "IR",
            "LG"
        ],
        "abstract": "  Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making. Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features. In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor. The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs. The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy. Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists. To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images. The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels. So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics. ",
        "title": "Siamese Content-based Search Engine for a More Transparent Skin and  Breast Cancer Diagnosis through Histological Imaging",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08273",
        "abstract_url": "http://arxiv.org/abs/2401.08273",
        "authors": [
            {
                "last_name": "Taveekitworachai",
                "first_name": "Pittawat"
            },
            {
                "last_name": "Abdullah",
                "first_name": "Febri"
            },
            {
                "last_name": "Thawonmas",
                "first_name": "Ruck"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the \"Examples\" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results. ",
        "title": "Large Language Models are Null-Shot Learners",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08275",
        "abstract_url": "http://arxiv.org/abs/2401.08275",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Bin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Lei",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems. Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image. But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods. In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image. The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing. We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization. ",
        "title": "Modeling Spoof Noise by De-spoofing Diffusion and its Application in  Face Anti-spoofing",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08276",
        "abstract_url": "http://arxiv.org/abs/2401.08276",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yipo"
            },
            {
                "last_name": "Yuan",
                "first_name": "Quan"
            },
            {
                "last_name": "Sheng",
                "first_name": "Xiangfei"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhichao"
            },
            {
                "last_name": "Wu",
                "first_name": "Haoning"
            },
            {
                "last_name": "Chen",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Yang",
                "first_name": "Yuzhe"
            },
            {
                "last_name": "Li",
                "first_name": "Leida"
            },
            {
                "last_name": "Lin",
                "first_name": "Weisi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development. However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications. An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity. To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets. (1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts. (2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI). Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans. We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs. Source data will be available at https://github.com/yipoh/AesBench. ",
        "title": "AesBench: An Expert Benchmark for Multimodal Large Language Models on  Image Aesthetics Perception",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08281",
        "abstract_url": "http://arxiv.org/abs/2401.08281",
        "authors": [
            {
                "last_name": "Douze",
                "first_name": "Matthijs"
            },
            {
                "last_name": "Guzhva",
                "first_name": "Alexandr"
            },
            {
                "last_name": "Deng",
                "first_name": "Chengqi"
            },
            {
                "last_name": "Johnson",
                "first_name": "Jeff"
            },
            {
                "last_name": "Szilvasy",
                "first_name": "Gergely"
            },
            {
                "last_name": "Mazar\u00e9",
                "first_name": "Pierre-Emmanuel"
            },
            {
                "last_name": "Lomeli",
                "first_name": "Maria"
            },
            {
                "last_name": "Hosseini",
                "first_name": "Lucas"
            },
            {
                "last_name": "J\u00e9gou",
                "first_name": "Herv\u00e9"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "SE"
        ],
        "abstract": "  Vector databases manage large collections of embedding vectors. As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability. ",
        "title": "The Faiss library",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08282",
        "abstract_url": "http://arxiv.org/abs/2401.08282",
        "authors": [
            {
                "last_name": "Ossadnik",
                "first_name": "Dennis"
            },
            {
                "last_name": "Jensen",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Haddadin",
                "first_name": "Sami"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Template models are frequently used to simplify the control dynamics for robot hopping or running. Passive limit cycles can emerge for such systems and be exploited for energy-efficient control. A grand challenge in locomotion is trunk stabilization when the hip is offset from the center of mass (CoM). The swing phase plays a major role in this process due to the moment of inertia of the leg; however, many template models ignore the leg mass. In this work, the authors consider a robot hopper model (RHM) with a rigid trunk and leg plus a hip that is displaced from the CoM. It has been previously shown that no passive limit cycle exists for such a model given a linear hip spring. In this work, we show that passive limit cycles can be found when a nonlinear hip spring is used instead. To the authors' knowledge, this is the first time that a passive limit cycle has been found for this type of system. ",
        "title": "Nonlinear stiffness allows passive dynamic hopping for one-legged robots  with an upright trunk",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08287",
        "abstract_url": "http://arxiv.org/abs/2401.08287",
        "authors": [
            {
                "last_name": "Paraskevopoulou",
                "first_name": "Zoe"
            },
            {
                "last_name": "Fitzgibbons",
                "first_name": "Michael"
            },
            {
                "last_name": "Thalakottur",
                "first_name": "Michelle"
            },
            {
                "last_name": "Mushtak",
                "first_name": "Noble"
            },
            {
                "last_name": "Mazur",
                "first_name": "Jose Sulaiman"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Amal"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  Safe, shared-memory interoperability between languages with different type systems and memory-safety guarantees is an intricate problem as crossing language boundaries may result in memory-safety violations. In this paper, we present RichWasm, a novel richly typed intermediate language designed to serve as a compilation target for typed high-level languages with different memory-safety guarantees. RichWasm is based on WebAssembly and enables safe shared-memory interoperability by incorporating a variety of type features that support fine-grained memory ownership and sharing. RichWasm is rich enough to serve as a typed compilation target for both typed garbage-collected languages and languages with an ownership-based type system and manually managed memory. We demonstrate this by providing compilers from core ML and L3, a type-safe language with strong updates, to RichWasm. RichWasm is compiled to regular Wasm, allowing for use in existing environments. We formalize RichWasm in Coq and prove type safety. ",
        "title": "RichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability  Down to WebAssembly",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08292",
        "abstract_url": "http://arxiv.org/abs/2401.08292",
        "authors": [
            {
                "last_name": "Ossadnik",
                "first_name": "Dennis"
            },
            {
                "last_name": "Jensen",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Haddadin",
                "first_name": "Sami"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  While many advancements have been made in the development of template models for describing upright-trunk locomotion, the majority of the effort has been focused on the stance phase. In this paper, we develop a new compact dynamic model as a first step toward a fully unified locomotion template model (ULT-model) of an upright-trunk forward hopping system, which will also require a unified control law in the next step. We demonstrate that all locomotion subfunctions are enabled by adding just a point foot mass and a parallel leg actuator to the well-known trunk SLIP model and that a stable limit cycle can be achieved. This brings us closer toward the ultimate goal of enabling closed-loop dynamics for anchor matching and thus achieving simple, efficient, robust and stable upright-trunk gait control, as observed in biological systems. ",
        "title": "ULT-model: Towards a one-legged unified locomotion template model for  forward hopping with an upright trunk",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08294",
        "abstract_url": "http://arxiv.org/abs/2401.08294",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Shuming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Enbo"
            },
            {
                "last_name": "Cai",
                "first_name": "Deng"
            },
            {
                "last_name": "Cui",
                "first_name": "Leyang"
            },
            {
                "last_name": "Huang",
                "first_name": "Xinting"
            },
            {
                "last_name": "Li",
                "first_name": "Huayang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs). With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code. Compared with most existing inference engines, Inferflow has some key features. First, by implementing a modular framework of atomic build-blocks and technologies, Inferflow is compositionally generalizable to new models. Second, 3.5-bit quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit quantization. Third, hybrid model partitioning for multi-GPU inference is introduced in Inferflow to better balance inference speed and throughput than the existing partition-by-layer and partition-by-tensor strategies. ",
        "title": "Inferflow: an Efficient and Highly Configurable Inference Engine for  Large Language Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08295",
        "abstract_url": "http://arxiv.org/abs/2401.08295",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Weixiang"
            },
            {
                "last_name": "Wang",
                "first_name": "Shilong"
            },
            {
                "last_name": "Hu",
                "first_name": "Yulin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yanyan"
            },
            {
                "last_name": "Qin",
                "first_name": "Bing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xuanyu"
            },
            {
                "last_name": "Yang",
                "first_name": "Qing"
            },
            {
                "last_name": "Xu",
                "first_name": "Dongliang"
            },
            {
                "last_name": "Che",
                "first_name": "Wanxiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Based on parameter-efficient tuning (PET), existing methods devise the learning module and the selection module to handle the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in CL. The learning module allocates separate PET blocks for each continually emerged task and the selection module function to choose the correct one for the input at testing time. However, there are limitations in their deigns of both modules and they ignore the potential of aligning the two module to address CF and KT simultaneously. To this end, we propose a novel Dual Attention Framework , to align the PET learning and selection via the Dual Attentive Learning\\&Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT at the same time. Moreover, DAPT exhibits the superiority when we scale it to different model sizes (from 770M to 11B) and unseen tasks. ",
        "title": "DAPT: A Dual Attention Framework for Parameter-Efficient Continual  Learning of Large Language Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08297",
        "abstract_url": "http://arxiv.org/abs/2401.08297",
        "authors": [
            {
                "last_name": "Beckenbach",
                "first_name": "Isabel"
            },
            {
                "last_name": "Hulek",
                "first_name": "Klaus"
            },
            {
                "last_name": "Teschke",
                "first_name": "Olaf"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  zbMATH Open has started a new feature -- relevant preprints posted at arXiv will also be displayed in the database. In this article we introduce this new feature and the underlying editorial policy. We also describe some of the technical issues involved and discuss the challenges this presents for future developments. ",
        "title": "The extension of zbMATH Open by arXiv preprints",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08298",
        "abstract_url": "http://arxiv.org/abs/2401.08298",
        "authors": [
            {
                "last_name": "Patni",
                "first_name": "Shubhan P."
            },
            {
                "last_name": "Stoudek",
                "first_name": "Pavel"
            },
            {
                "last_name": "Chlup",
                "first_name": "Hynek"
            },
            {
                "last_name": "Hoffmann",
                "first_name": "Matej"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Standard robot grippers are not designed for elasticity estimation. In this work, a professional biaxial compression device was used as a control setup to study the accuracy with which material properties can be estimated by two standard parallel jaw grippers and a force/torque sensor mounted at the robot wrist. Using three sets of deformable objects, different parameters were varied to observe their effect on measuring material characteristics: (1) repeated compression cycles, (2) compression speed, and (3) the surface area of the gripper jaws. Gripper effort versus position curves were obtained and transformed into stress/strain curves. The modulus of elasticity was estimated at different strain points. Viscoelasticity was assessed using the energy absorbed in a compression/decompression cycle, the Kelvin-Voigt, and Hunt-Crossley models. Our results can be summarized as follows: (1) better results were obtained with slower compression speeds, while additional compression cycles or surface area did not improve estimation; (2) the robot grippers, even after calibration, were found to have a limited capability of delivering accurate estimates of absolute values of Young's modulus and viscoelasticity; (3) relative ordering of material characteristics was largely consistent across different grippers; (4) despite the nonlinear characteristics of deformable objects, fitting linear stress/strain approximations led to more stable results than local estimates of Young's modulus; (5) to assess viscoelasticity, the Hunt-Crossley model worked best. Finally, we show that a two-dimensional space representing elasticity and viscoelasticity estimates is advantageous for the discrimination of deformable objects. A single-grasp, online, classification and sorting of such objects is thus possible. An additional contribution is the dataset and data processing codes that we make publicly available. ",
        "title": "Evaluating online elasticity estimation of soft objects using standard  robot grippers",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08301",
        "abstract_url": "http://arxiv.org/abs/2401.08301",
        "authors": [
            {
                "last_name": "Yeganeh",
                "first_name": "Rahman Saadat"
            },
            {
                "last_name": "Omidi",
                "first_name": "Mohammad Javad"
            },
            {
                "last_name": "Zeinali",
                "first_name": "Farshad"
            },
            {
                "last_name": "Robatmili",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Ghavami",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we employ active simultaneously transmitting and reflecting reconfigurable intelligent surface (ASRIS) to aid in establishing and enhancing communication within a commensal symbiotic radio (CSR) network. Unlike traditional RIS, ASRIS not only ensures coverage in an omni directional manner but also amplifies received signals, consequently elevating overall network performance. in the first phase, base station (BS) with active massive MIMO antennas, send ambient signal to SBDs. In the first phase, the BS transmits ambient signals to the symbiotic backscatter devices (SBDs), and after harvesting the energy and modulating their information onto the signal carrier, the SBDs send Backscatter signals back to the BS. In this scheme, we employ the Backscatter Relay system to facilitate the transmission of information from the SBDs to the symbiotic User Equipments (SUEs) with the assistance of the BS. In the second phase, the BS transmits information signals to the SUEs after eliminating interference using the Successive Interference Cancellation (SIC) method. ASRIS is employed to establish communication among SUEs lacking a line of sight (LoS) and to amplify power signals for SUEs with a LoS connection to the BS. It is worth noting that we use NOMA for multiple access in all network.   The main goal of this paper is to maximize the sum throughput between all users. To achieve this, we formulate an optimization problem with variables including active beamforming coefficients at the BS and ASRIS, as well as the phase adjustments of ASRIS and scheduling parameters between the first and second phases. To model this optimization problem, we employ three deep reinforcement learning (DRL) methods, namely PPO, TD3, and A3C. Finally, the mentioned methods are simulated and compared with each other. ",
        "title": "Sum Throughput Maximization in Multi-BD Symbiotic Radio NOMA Network  Assisted by Active-STAR-RIS",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08302",
        "abstract_url": "http://arxiv.org/abs/2401.08302",
        "authors": [
            {
                "last_name": "Macpherson",
                "first_name": "Andrew W."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "GT"
        ],
        "abstract": "  We study a new \"laminated\" queueing model for orders on batched trading venues such as decentralised exchanges. The model aims to capture and generalise transaction queueing infrastructure that has arisen to organise MEV activity on public blockchains such as Ethereum, providing convenient channels for sophisticated agents to extract value by acting on end-user order flow by performing arbitrage and related HFT activities. In our model, market orders are interspersed with orders created by arbitrageurs that under idealised conditions reset the marginal price to a global equilibrium between each trade, improving predictability of execution for liquidity traders.   If an arbitrageur has a chance to land multiple opportunities in a row, he may attempt to manipulate the execution price of the intervening market order by a probabilistic blind sandwiching strategy. To study how bad this manipulation can get, we introduce and bound a price manipulation coefficient that measures the deviation from global equilibrium of local pricing quoted by a rational arbitrageur. We exhibit cases in which this coefficient is well approximated by a \"zeta value' with interpretable and empirically measurable parameters. ",
        "title": "Do backrun auctions protect traders?",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08307",
        "abstract_url": "http://arxiv.org/abs/2401.08307",
        "authors": [
            {
                "last_name": "Sequeira",
                "first_name": "Andr\u00e9"
            },
            {
                "last_name": "Santos",
                "first_name": "Luis Paulo"
            },
            {
                "last_name": "Barbosa",
                "first_name": "Luis Soares"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This research delves into the role of the quantum Fisher Information Matrix (FIM) in enhancing the performance of Parameterized Quantum Circuit (PQC)-based reinforcement learning agents. While previous studies have highlighted the effectiveness of PQC-based policies preconditioned with the quantum FIM in contextual bandits, its impact in broader reinforcement learning contexts, such as Markov Decision Processes, is less clear. Through a detailed analysis of L\\\"owner inequalities between quantum and classical FIMs, this study uncovers the nuanced distinctions and implications of using each type of FIM. Our results indicate that a PQC-based agent using the quantum FIM without additional insights typically incurs a larger approximation error and does not guarantee improved performance compared to the classical FIM. Empirical evaluations in classic control benchmarks suggest even though quantum FIM preconditioning outperforms standard gradient ascent, in general it is not superior to classical FIM preconditioning. ",
        "title": "On Quantum Natural Policy Gradients",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08309",
        "abstract_url": "http://arxiv.org/abs/2401.08309",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhongwang"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Yao",
                "first_name": "Junjie"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhangchen"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaolong"
            },
            {
                "last_name": "E",
                "first_name": "Weinan"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhi-Qin John"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Understanding transformer-based language models is becoming increasingly crucial, particularly as they play pivotal roles in advancing towards artificial general intelligence. However, language model research faces significant challenges, especially for academic research groups with constrained resources. These challenges include complex data structures, unknown target functions, high computational costs and memory requirements, and a lack of interpretability in the inference process, etc. Drawing a parallel to the use of simple models in scientific research, we propose the concept of an anchor function. This is a type of benchmark function designed for studying language models in learning tasks that follow an \"anchor-key\" pattern. By utilizing the concept of an anchor function, we can construct a series of functions to simulate various language tasks. The anchor function plays a role analogous to that of mice in diabetes research, particularly suitable for academic research. We demonstrate the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions. These operations are also commonly observed in large language models. The anchor function framework, therefore, opens up a series of valuable and accessible research questions for further exploration, especially for theoretical study. ",
        "title": "Anchor function: a type of benchmark functions for studying language  models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08315",
        "abstract_url": "http://arxiv.org/abs/2401.08315",
        "authors": [
            {
                "last_name": "Gan",
                "first_name": "Chengguang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinghao"
            },
            {
                "last_name": "Mori",
                "first_name": "Tatsunori"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks. Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios. This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews. To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes. ",
        "title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume  Screening",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08318",
        "abstract_url": "http://arxiv.org/abs/2401.08318",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Yizhuo"
            },
            {
                "last_name": "Singh",
                "first_name": "Gagan Deep"
            },
            {
                "last_name": "Beikmirza",
                "first_name": "Mohammadreza"
            },
            {
                "last_name": "de Vreede",
                "first_name": "Leo"
            },
            {
                "last_name": "Alavi",
                "first_name": "Morteza"
            },
            {
                "last_name": "Gao",
                "first_name": "Chang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent. Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison. This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA DPA in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs. Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD. ",
        "title": "OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for  Wideband Power Amplifier Modeling and Digital Pre-Distortion",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08326",
        "abstract_url": "http://arxiv.org/abs/2401.08326",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Junjie"
            },
            {
                "last_name": "Wu",
                "first_name": "Yilong"
            },
            {
                "last_name": "Gao",
                "first_name": "Songyang"
            },
            {
                "last_name": "Li",
                "first_name": "Sixian"
            },
            {
                "last_name": "Li",
                "first_name": "Guanyu"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiaoran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            },
            {
                "last_name": "Gui",
                "first_name": "Tao"
            },
            {
                "last_name": "Huang",
                "first_name": "Xuanjing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench. ",
        "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large  Language Models in Tool Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08327",
        "abstract_url": "http://arxiv.org/abs/2401.08327",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Kexin"
            },
            {
                "last_name": "Ye",
                "first_name": "Rui"
            },
            {
                "last_name": "Huang",
                "first_name": "Xiaolin"
            },
            {
                "last_name": "Yang",
                "first_name": "Jie"
            },
            {
                "last_name": "Chen",
                "first_name": "Siheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Personalized federated learning aims to address data heterogeneity across local clients in federated learning. However, current methods blindly incorporate either full model parameters or predefined partial parameters in personalized federated learning. They fail to customize the collaboration manner according to each local client's data characteristics, causing unpleasant aggregation results. To address this essential issue, we propose $\\textit{Learn2pFed}$, a novel algorithm-unrolling-based personalized federated learning framework, enabling each client to adaptively select which part of its local model parameters should participate in collaborative training. The key novelty of the proposed $\\textit{Learn2pFed}$ is to optimize each local model parameter's degree of participant in collaboration as learnable parameters via algorithm unrolling methods. This approach brings two benefits: 1) mathmatically determining the participation degree of local model parameters in the federated collaboration, and 2) obtaining more stable and improved solutions. Extensive experiments on various tasks, including regression, forecasting, and image classification, demonstrate that $\\textit{Learn2pFed}$ significantly outperforms previous personalized federated learning methods. ",
        "title": "Learn What You Need in Personalized Federated Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08328",
        "abstract_url": "http://arxiv.org/abs/2401.08328",
        "authors": [
            {
                "last_name": "Tomar",
                "first_name": "Devavrat"
            },
            {
                "last_name": "Vray",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Thiran",
                "first_name": "Jean-Philippe"
            },
            {
                "last_name": "Bozorgtabar",
                "first_name": "Behzad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions. This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing Test-Time Normalization Statistics\" (UnMix-TNS). UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment. The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch. Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts. UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples. Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks. ",
        "title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal  Correlation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08329",
        "abstract_url": "http://arxiv.org/abs/2401.08329",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiayin"
            },
            {
                "last_name": "Ma",
                "first_name": "Weizhi"
            },
            {
                "last_name": "Sun",
                "first_name": "Peijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            },
            {
                "last_name": "Nie",
                "first_name": "Jian-Yun"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence. This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration. This gap in research becomes increasingly critical as LLMs become more integrated into people's everyday and professional interactions. This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations. Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification. Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns. This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs. Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments. This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications. ",
        "title": "Understanding User Experience in Large Language Model Interactions",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08330",
        "abstract_url": "http://arxiv.org/abs/2401.08330",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Qixin"
            },
            {
                "last_name": "Wan",
                "first_name": "Zongqi"
            },
            {
                "last_name": "Deng",
                "first_name": "Zengde"
            },
            {
                "last_name": "Chen",
                "first_name": "Zaiyi"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiaoming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jialin"
            },
            {
                "last_name": "Yang",
                "first_name": "Yu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Projected Gradient Ascent (PGA) is the most commonly used optimization scheme in machine learning and operations research areas. Nevertheless, numerous studies and examples have shown that the PGA methods may fail to achieve the tight approximation ratio for continuous DR-submodular maximization problems. To address this challenge, we present a boosting technique in this paper, which can efficiently improve the approximation guarantee of the standard PGA to \\emph{optimal} with only small modifications on the objective function. The fundamental idea of our boosting technique is to exploit non-oblivious search to derive a novel auxiliary function $F$, whose stationary points are excellent approximations to the global maximum of the original DR-submodular objective $f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we propose an auxiliary function $F$ whose stationary points can provide a better $(1-e^{-\\gamma})$-approximation than the $(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of $f$ itself. Similarly, for the non-monotone case, we devise another auxiliary function $F$ whose stationary points can achieve an optimal $\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation guarantee where $\\mathcal{C}$ is a convex constraint set. In contrast, the stationary points of the original non-monotone DR-submodular function can be arbitrarily bad~\\citep{chen2023continuous}. Furthermore, we demonstrate the scalability of our boosting technique on four problems. In all of these four problems, our resulting variants of boosting PGA algorithm beat the previous standard PGA in several aspects such as approximation ratio and efficiency. Finally, we corroborate our theoretical findings with numerical experiments, which demonstrate the effectiveness of our boosting PGA methods. ",
        "title": "Boosting Gradient Ascent for Continuous DR-submodular Maximization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08333",
        "abstract_url": "http://arxiv.org/abs/2401.08333",
        "authors": [
            {
                "last_name": "Huttner",
                "first_name": "Michael"
            },
            {
                "last_name": "Simeth",
                "first_name": "Jakob"
            },
            {
                "last_name": "Liguori",
                "first_name": "Renato"
            },
            {
                "last_name": "Ferrazzi",
                "first_name": "Fulvia"
            },
            {
                "last_name": "Spang",
                "first_name": "Rainer"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research. Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.   Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management. dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format. Its approach to data security involves a two-stage envelope encryption process. We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism. The private key necessary for decrypting the data remains exclusively on the owner's device. Thus, accessing data is impossible without explicit permission from the keyholder.   Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries. Documentation is available as part of the web application.   Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions. All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files. ",
        "title": "dabih -- encrypted data storage and sharing platform",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08341",
        "abstract_url": "http://arxiv.org/abs/2401.08341",
        "authors": [
            {
                "last_name": "Cinmere",
                "first_name": "Idris"
            },
            {
                "last_name": "Mehmood",
                "first_name": "Kashif"
            },
            {
                "last_name": "Kralevska",
                "first_name": "Katina"
            },
            {
                "last_name": "Mahmoodi",
                "first_name": "Toktam"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  As network systems evolve, there is an escalating demand for automated tools to facilitate efficient management and configuration. This paper explores conflict resolution in Intent-Based Network (IBN) management, an innovative approach that holds promise for effective network administration, especially within radio access domain. Nevertheless, when multiple intents are in operation concurrently, conflicts may emerge, presenting a significant issue that remains under-addressed in the current literature. In response to this challenge, our research expands the range of conflict resolution strategies beyond the established Nash Bargaining Solution (NBS), to incorporate the Weighted Nash Bargaining Solution (WNBS), the Kalai-Smorodinsky Bargaining Solution (KSBS), and the Shannon Entropy Bargaining Solution (SEBS). These methods are employed with the objective to identify optimal parameter values, aiming to ensure fairness in conflict resolution. Through simulations, it is demonstrated that distinct antenna tilt values are yielded as the respective solutions for each method. Ultimately, based on Jain Fairness Index, the KSBS is identified as the most equitable method under the given conditions. ",
        "title": "Direct-Conflict Resolution in Intent-Driven Autonomous Networks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08345",
        "abstract_url": "http://arxiv.org/abs/2401.08345",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Fei"
            },
            {
                "last_name": "Wang",
                "first_name": "YiKang"
            },
            {
                "last_name": "Qi",
                "first_name": "Han"
            },
            {
                "last_name": "Jin",
                "first_name": "WenPing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, few-shot action recognition has attracted increasing attention. It generally adopts the paradigm of meta-learning. In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples. We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity. Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion. Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query. Secondly, we establish a Multi-view. In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers. Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias. Our code is available at the URL: \\url{https://github.com/cofly2014/MDMF}. ",
        "title": "Multi-view Distillation based on Multi-modal Fusion for Few-shot Action  Recognition(CLIP-$\\mathrm{M^2}$DF)",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08348",
        "abstract_url": "http://arxiv.org/abs/2401.08348",
        "authors": [
            {
                "last_name": "Bia\u0142ek",
                "first_name": "Jakub"
            },
            {
                "last_name": "Kuberski",
                "first_name": "Wojtek"
            },
            {
                "last_name": "Perrakis",
                "first_name": "Nikolaos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The performance of machine learning models often degrades after deployment due to data distribution shifts. In many use cases, it is impossible to calculate the post-deployment performance because labels are unavailable or significantly delayed. Proxy methods for evaluating model performance stability, like drift detection techniques, do not properly quantify data distribution shift impact. As a solution, we propose a robust and accurate performance estimation method for evaluating ML classification models on unlabeled data that accurately quantifies the impact of covariate shift on model performance. We call it multi-calibrated confidence-based performance estimation (M-CBPE). It is model and data-type agnostic and works for any performance metric. It does not require access to the monitored model - it uses the model predictions and probability estimates. M-CBPE does not need user input on the nature of the covariate shift as it fully learns from the data. We evaluate it with over 600 dataset-model pairs from US census data and compare it with multiple benchmarks using several evaluation metrics. Results show that M-CBPE is the best method to estimate the performance of classification models in any evaluation context. ",
        "title": "We don't need no labels: Estimating post-deployment model performance  under covariate shift without ground truth",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08351",
        "abstract_url": "http://arxiv.org/abs/2401.08351",
        "authors": [
            {
                "last_name": "Boroujeni",
                "first_name": "Mahrokh Ghoddousi"
            },
            {
                "last_name": "Krause",
                "first_name": "Andreas"
            },
            {
                "last_name": "Trecate",
                "first_name": "Giancarlo Ferrari"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Federated learning aims to infer a shared model from private and decentralized data stored locally by multiple clients. Personalized federated learning (PFL) goes one step further by adapting the global model to each client, enhancing the model's fit for different clients. A significant level of personalization is required for highly heterogeneous clients, but can be challenging to achieve especially when they have small datasets. To address this problem, we propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework that utilizes differential privacy to handle data-dependent priors. Our algorithm collaboratively learns a shared hyper-posterior and regards each client's posterior inference as the personalization step. By establishing and minimizing a generalization bound on the average true risk of clients, PAC-PFL effectively combats over-fitting. PACPFL achieves accurate and well-calibrated predictions, supported by experiments on a dataset of photovoltaic panel power generation, FEMNIST dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen et al., 2017). ",
        "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian  Approach",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08352",
        "abstract_url": "http://arxiv.org/abs/2401.08352",
        "authors": [
            {
                "last_name": "Zabegaev",
                "first_name": "Yury"
            },
            {
                "last_name": "Keilegavlen",
                "first_name": "Eirik"
            },
            {
                "last_name": "Iversen",
                "first_name": "Einar"
            },
            {
                "last_name": "Berre",
                "first_name": "Inga"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Porous media processes involve various physical phenomena such as mechanical deformation, transport, and fluid flow. Accurate simulations must capture the strong couplings between these phenomena. Choosing an efficient solver for the multiphysics problem usually entails the decoupling into subproblems related to separate physical phenomena. Then, the suitable solvers for each subproblem and the iteration scheme must be chosen. The wide range of options for the solver components makes finding the optimum difficult and time-consuming; moreover, solvers come with numerical parameters that need to be optimized. As a further complication, the solver performance may depend on the physical regime of the simulation model, which may vary with time. Switching a solver with respect to the dominant process can be beneficial, but the threshold of when to switch solver is unclear and complicated to analyze. We address this challenge by developing a machine learning framework that automatically searches for the optimal solver for a given multiphysics simulation setup, based on statistical data from previously solved problems. For a series of problems, exemplified by successive time steps in a time-dependent simulation, the framework updates and improves its decision model online during the simulation. We show how it outperforms preselected state-of-the-art solvers for test problem setups. The examples are based on simulations of poromechanics and simulations of flow and transport. For the quasi-static linear Biot model, we demonstrate automated tuning of numerical solver parameters by showing how the L-parameter of the so-called Fixed-Stress preconditioner can be optimized. Motivated by a test example where the main heat transfer mechanism changes between convection and diffusion, we discuss how the solver selector can dynamically switch solvers when the dominant physical phenomenon changes with time. ",
        "title": "Automated solver selection for simulation of multiphysics processes in  porous media",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08355",
        "abstract_url": "http://arxiv.org/abs/2401.08355",
        "authors": [
            {
                "last_name": "Jeffery",
                "first_name": "Stacey"
            },
            {
                "last_name": "Pass",
                "first_name": "Galina"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We introduce an object called a subspace graph that formalizes the technique of multidimensional quantum walks. Composing subspace graphs allows one to seamlessly combine quantum and classical reasoning, keeping a classical structure in mind, while abstracting quantum parts into subgraphs with simple boundaries as need. As an example, we show how to combine a switching network with arbitrary quantum subroutines, to compute a composed function. As another application, we give a time-efficient implementation of quantum Divide & Conquer when the sub-problems are combined via a symmetric Boolean formula. We use this to quadratically speed up Savitch's algorithm for directed $st$-connectivity. ",
        "title": "Multidimensional Quantum Walks, Recursion, and Quantum Divide & Conquer",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08357",
        "abstract_url": "http://arxiv.org/abs/2401.08357",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xilai"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaosong"
            },
            {
                "last_name": "Tan",
                "first_name": "Haishu"
            },
            {
                "last_name": "Li",
                "first_name": "Jinyang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately. To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability. First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels. To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy. Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results. Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations. The source code is available at https://github.com/ixilai/SAMF. ",
        "title": "SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08358",
        "abstract_url": "http://arxiv.org/abs/2401.08358",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Junliang"
            },
            {
                "last_name": "Li",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Wu",
                "first_name": "Di"
            },
            {
                "last_name": "Jenkin",
                "first_name": "Michael"
            },
            {
                "last_name": "Liu",
                "first_name": "Steve"
            },
            {
                "last_name": "Dudek",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks. ",
        "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08360",
        "abstract_url": "http://arxiv.org/abs/2401.08360",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Qi"
            },
            {
                "last_name": "Tung",
                "first_name": "Tze-Yang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Recently, deep autoencoders have gained traction as a powerful method for implementing goal-oriented semantic communications systems. The idea is to train a mapping from the source domain directly to channel symbols, and vice versa. However, prior studies often focused on rate-distortion tradeoff and transmission delay, at the cost of increasing end-to-end complexity and thus latency. Moreover, the datasets used are often not reflective of real-world environments, and the results were not validated against real-world baseline systems, leading to an unfair comparison. In this paper, we study the problem of remote camera pose estimation and propose AdaSem, an adaptive semantic communications approach that optimizes the tradeoff between inference accuracy and end-to-end latency. We develop an adaptive semantic codec model, which encodes the source data into a dynamic number of symbols, based on the latent space distribution and the channel state feedback. We utilize a lightweight model for both transmitter and receiver to ensure comparable complexity to the baseline implemented in a real-world system. Extensive experiments on real-environment data show the effectiveness of our approach. When compared to a real implementation of a client-server camera relocalization service, AdaSem outperforms the baseline by reducing the end-to-end delay and estimation error by over 75% and 63%, respectively. ",
        "title": "AdaSem: Adaptive Goal-Oriented Semantic Communications for End-to-End  Camera Relocalization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08361",
        "abstract_url": "http://arxiv.org/abs/2401.08361",
        "authors": [
            {
                "last_name": "Caflisch",
                "first_name": "Russel"
            },
            {
                "last_name": "Yang",
                "first_name": "Yunan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This survey explores the development of adjoint Monte Carlo methods for solving optimization problems governed by kinetic equations, a common challenge in areas such as plasma control and device design. These optimization problems are particularly demanding due to the high dimensionality of the phase space and the randomness in evaluating the objective functional, a consequence of using a forward Monte Carlo solver. To overcome these difficulties, a range of ``adjoint Monte Carlo methods'' have been devised. These methods skillfully combine Monte Carlo gradient estimators with PDE-constrained optimization, introducing innovative solutions tailored for kinetic applications. In this review, we begin by examining three primary strategies for Monte Carlo gradient estimation: the score function approach, the reparameterization trick, and the coupling method. We also delve into the adjoint-state method, an essential element in PDE-constrained optimization. Focusing on applications in the radiative transfer equation and the nonlinear Boltzmann equation, we provide a comprehensive guide on how to integrate Monte Carlo gradient techniques within both the optimize-then-discretize and the discretize-then-optimize frameworks from PDE-constrained optimization. This approach leads to the formulation of effective adjoint Monte Carlo methods, enabling efficient gradient estimation in complex, high-dimensional optimization problems. ",
        "title": "Adjoint Monte Carlo Method",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08363",
        "abstract_url": "http://arxiv.org/abs/2401.08363",
        "authors": [
            {
                "last_name": "Kulkarni",
                "first_name": "Aditya"
            },
            {
                "last_name": "Balachandran",
                "first_name": "Vivek"
            },
            {
                "last_name": "Divakaran",
                "first_name": "Dinil Mon"
            },
            {
                "last_name": "Das",
                "first_name": "Tamal"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The widespread accessibility of the Internet has led to a surge in online fraudulent activities, underscoring the necessity of shielding users' sensitive information from cybercriminals. Phishing, a well-known cyberattack, revolves around the creation of phishing webpages and the dissemination of corresponding URLs, aiming to deceive users into sharing their sensitive information, often for identity theft or financial gain. Various techniques are available for preemptively categorizing zero-day phishing URLs by distilling unique attributes and constructing predictive models. However, these existing techniques encounter unresolved issues. This proposal delves into persistent challenges within phishing detection solutions, particularly concentrated on the preliminary phase of assembling comprehensive datasets, and proposes a potential solution in the form of a tool engineered to alleviate bias in ML models. Such a tool can generate phishing webpages for any given set of legitimate URLs, infusing randomly selected content and visual-based phishing features. Furthermore, we contend that the tool holds the potential to assess the efficacy of existing phishing detection solutions, especially those trained on confined datasets. ",
        "title": "Mitigating Bias in Machine Learning Models for Phishing Webpage  Detection",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08364",
        "abstract_url": "http://arxiv.org/abs/2401.08364",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiaotong"
            },
            {
                "last_name": "Wang",
                "first_name": "Jinxin"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            },
            {
                "last_name": "Lin",
                "first_name": "Shao-Bo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Spherical radial-basis-based kernel interpolation abounds in image sciences including geophysical image reconstruction, climate trends description and image rendering due to its excellent spatial localization property and perfect approximation performance. However, in dealing with noisy data, kernel interpolation frequently behaves not so well due to the large condition number of the kernel matrix and instability of the interpolation process. In this paper, we introduce a weighted spectral filter approach to reduce the condition number of the kernel matrix and then stabilize kernel interpolation. The main building blocks of the proposed method are the well developed spherical positive quadrature rules and high-pass spectral filters. Using a recently developed integral operator approach for spherical data analysis, we theoretically demonstrate that the proposed weighted spectral filter approach succeeds in breaking through the bottleneck of kernel interpolation, especially in fitting noisy data. We provide optimal approximation rates of the new method to show that our approach does not compromise the predicting accuracy. Furthermore, we conduct both toy simulations and two real-world data experiments with synthetically added noise in geophysical image reconstruction and climate image processing to verify our theoretical assertions and show the feasibility of the weighted spectral filter approach. ",
        "title": "Weighted Spectral Filters for Kernel Interpolation on Spheres: Estimates  of Prediction Accuracy for Noisy Data",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08366",
        "abstract_url": "http://arxiv.org/abs/2401.08366",
        "authors": [
            {
                "last_name": "Middelburg",
                "first_name": "C. A."
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "DS",
            "LO"
        ],
        "abstract": "  The starting point of this paper is a collection of properties of an algorithm that have been distilled from the informal descriptions of what an algorithm is that are given in standard works from the mathematical and computer science literature. Based on that, the notion of a proto-algorithm is introduced. The thought is that algorithms are equivalence classes of proto-algorithms under some equivalence relation. Three equivalence relations are defined. Two of them give bounds between which an appropriate equivalence relation must lie. The third lies in between these two and is likely an appropriate equivalence relation. A sound method is presented to prove, using an imperative process algebra based on ACP, that this equivalence relation holds between two proto-algorithms. ",
        "title": "On the formalization of the notion of an algorithm",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08367",
        "abstract_url": "http://arxiv.org/abs/2401.08367",
        "authors": [
            {
                "last_name": "Sarveswaran",
                "first_name": "Kengatharaiyer"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper provides an overview of the morphology and syntax of the Tamil language, focusing on its contemporary usage. The paper also highlights the complexity and richness of Tamil in terms of its morphological and syntactic features, which will be useful for linguists analysing the language and conducting comparative studies. In addition, the paper will be useful for those developing computational resources for the Tamil language. It is proven as a rule-based morphological analyser cum generator and a computational grammar for Tamil have already been developed based on this paper. To enhance accessibility for a broader audience, the analysis is conducted without relying on any specific grammatical formalism. ",
        "title": "Morphology and Syntax of the Tamil Language",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08374",
        "abstract_url": "http://arxiv.org/abs/2401.08374",
        "authors": [
            {
                "last_name": "Espl\u00e0-Gomis",
                "first_name": "Miquel"
            },
            {
                "last_name": "S\u00e1nchez-Cartagena",
                "first_name": "V\u00edctor M."
            },
            {
                "last_name": "P\u00e9rez-Ortiz",
                "first_name": "Juan Antonio"
            },
            {
                "last_name": "S\u00e1nchez-Mart\u00ednez",
                "first_name": "Felipe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators. However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation tasks. In this paper, we introduce a novel neural approach aimed at overcoming this limitation by exploiting not only TMs, but also in-domain target-language (TL) monolingual corpora, and still enabling a similar functionality to that offered by conventional TM-based CAT tools. Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora, and on a neural model to estimate their post-editing effort. The paper presents an automatic evaluation of these techniques on four language pairs that shows that our approach can successfully exploit monolingual texts in a TM-based CAT environment, increasing the amount of useful translation proposals, and that our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way. A human evaluation performed on a single language pair confirms the results of the automatic evaluation and seems to indicate that the translation proposals retrieved with our approach are more useful than what the automatic evaluation shows. ",
        "title": "Cross-lingual neural fuzzy matching for exploiting target-language  monolingual corpora in computer-aided translation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08375",
        "abstract_url": "http://arxiv.org/abs/2401.08375",
        "authors": [
            {
                "last_name": "Machkour",
                "first_name": "Jasin"
            },
            {
                "last_name": "Breloy",
                "first_name": "Arnaud"
            },
            {
                "last_name": "Muma",
                "first_name": "Michael"
            },
            {
                "last_name": "Palomar",
                "first_name": "Daniel P."
            },
            {
                "last_name": "Pascal",
                "first_name": "Fr\u00e9d\u00e9ric"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Sparse principal component analysis (PCA) aims at mapping large dimensional data to a linear subspace of lower dimension. By imposing loading vectors to be sparse, it performs the double duty of dimension reduction and variable selection. Sparse PCA algorithms are usually expressed as a trade-off between explained variance and sparsity of the loading vectors (i.e., number of selected variables). As a high explained variance is not necessarily synonymous with relevant information, these methods are prone to select irrelevant variables. To overcome this issue, we propose an alternative formulation of sparse PCA driven by the false discovery rate (FDR). We then leverage the Terminating-Random Experiments (T-Rex) selector to automatically determine an FDR-controlled support of the loading vectors. A major advantage of the resulting T-Rex PCA is that no sparsity parameter tuning is required. Numerical experiments and a stock market data example demonstrate a significant performance improvement. ",
        "title": "Sparse PCA with False Discovery Rate Controlled Variable Selection",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08376",
        "abstract_url": "http://arxiv.org/abs/2401.08376",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanlin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Haofen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenqiang"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods. Our source code and data are available at https://github.com/DeepSoftwareAnalytics/KADEL ",
        "title": "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08377",
        "abstract_url": "http://arxiv.org/abs/2401.08377",
        "authors": [
            {
                "last_name": "Watanabe",
                "first_name": "Kazuki"
            },
            {
                "last_name": "van der Vegt",
                "first_name": "Marck"
            },
            {
                "last_name": "Hasuo",
                "first_name": "Ichiro"
            },
            {
                "last_name": "Rot",
                "first_name": "Jurriaan"
            },
            {
                "last_name": "Junges",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Computing schedulers that optimize reachability probabilities in MDPs is a standard verification task. To address scalability concerns, we focus on MDPs that are compositionally described in a high-level description formalism. In particular, this paper considers string diagrams, which specify an algebraic, sequential composition of subMDPs. Towards their compositional verification, the key challenge is to locally optimize schedulers on subMDPs without considering their context in the string diagram. This paper proposes to consider the schedulers in a subMDP which form a Pareto curve on a combination of local objectives. While considering all such schedulers is intractable, it gives rise to a highly efficient sound approximation algorithm. The prototype on top of the model checker Storm demonstrates the scalability of this approach. ",
        "title": "Pareto Curves for Compositionally Model Checking String Diagrams of MDPs",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08381",
        "abstract_url": "http://arxiv.org/abs/2401.08381",
        "authors": [
            {
                "last_name": "Spisak",
                "first_name": "Josua"
            },
            {
                "last_name": "Kerzel",
                "first_name": "Matthias"
            },
            {
                "last_name": "Wermter",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Imitation can allow us to quickly gain an understanding of a new task. Through a demonstration, we can gain direct knowledge about which actions need to be performed and which goals they have. In this paper, we introduce a new approach to imitation learning that tackles the challenges of a robot imitating a human, such as the change in perspective and body schema. Our approach can use a single human demonstration to abstract information about the demonstrated task, and use that information to generalise and replicate it. We facilitate this ability by a new integration of two state-of-the-art methods: a diffusion action segmentation model to abstract temporal information from the demonstration and an open vocabulary object detector for spatial information. Furthermore, we refine the abstracted information and use symbolic reasoning to create an action plan utilising inverse kinematics, to allow the robot to imitate the demonstrated action. ",
        "title": "Robotic Imitation of Human Actions",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08383",
        "abstract_url": "http://arxiv.org/abs/2401.08383",
        "authors": [
            {
                "last_name": "Yao",
                "first_name": "Jinghan"
            },
            {
                "last_name": "Anthony",
                "first_name": "Quentin"
            },
            {
                "last_name": "Shafi",
                "first_name": "Aamir"
            },
            {
                "last_name": "Subramoni",
                "first_name": "Hari"
            },
            {
                "last_name": "K.",
                "first_name": "Dhabaleswar"
            },
            {
                "last_name": "Panda",
                "first_name": ""
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism on distributed systems, our design only uses one Alltoall communication to deliver the same functionality while previous methods all require two Alltoalls. By carefully examining the conditional probability in tokens' routing across multiple layers, we proved that pre-trained GPT MoE models implicitly exhibit a strong inter-layer expert affinity. We then design an efficient integer programming model to capture such features and show that by properly placing the experts on corresponding GPUs, we can reduce up to 67% cross-GPU routing latency. Our solution beats the cutting-edge MoE implementations with experts from 8 to 64, with up to 2.2x improvement in inference throughput. We further provide a detailed study of how the model implicitly acquires this expert affinity at the very early training stage and how this affinity evolves and stabilizes during training. ",
        "title": "Exploiting Inter-Layer Expert Affinity for Accelerating  Mixture-of-Experts Model Inference",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08385",
        "abstract_url": "http://arxiv.org/abs/2401.08385",
        "authors": [
            {
                "last_name": "Blatter",
                "first_name": "Lionel"
            },
            {
                "last_name": "Kosmatov",
                "first_name": "Nikolai"
            },
            {
                "last_name": "Prevosto",
                "first_name": "Virgile"
            },
            {
                "last_name": "Gall",
                "first_name": "Pascale Le"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Deductive verification typically relies on function contracts that specify the behavior of each function for a single function call. Relational properties link several function calls together within a single specification. They can express more advanced properties of a given function, such as non-interference, continuity, or monotonicity, or relate calls to different functions, possibly run in parallel, for instance, to show the equivalence of two implementations. However, relational properties cannot be expressed and verified directly in the traditional setting of modular deductive verification. Recent work proposed a new technique for relational property verification that relies on a verification condition generator to produce logical formulas that must be verified to ensure a given relational property. This paper presents an overview of this approach and proposes important enhancements. We integrate an optimized verification condition generator and extend the underlying theory to show how relational properties can be proved in a modular way, where one relational property can be used to prove another one, like in modular verification of function contracts. Our results have been fully formalized and proved sound in the Coq proof assistant. ",
        "title": "An Efficient VCGen-based Modular Verification of Relational Properties",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08386",
        "abstract_url": "http://arxiv.org/abs/2401.08386",
        "authors": [
            {
                "last_name": "Ahmad",
                "first_name": "Wasim"
            },
            {
                "last_name": "Shadaydeh",
                "first_name": "Maha"
            },
            {
                "last_name": "Denzler",
                "first_name": "Joachim"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Causal inference in a nonlinear system of multivariate timeseries is instrumental in disentangling the intricate web of relationships among variables, enabling us to make more accurate predictions and gain deeper insights into real-world complex systems. Causality methods typically identify the causal structure of a multivariate system by considering the cause-effect relationship of each pair of variables while ignoring the collective effect of a group of variables or interactions involving more than two-time series variables. In this work, we test model invariance by group-level interventions on the trained deep networks to infer causal direction in groups of variables, such as climate and ecosystem, brain networks, etc. Extensive testing with synthetic and real-world time series data shows a significant improvement of our method over other applied group causality methods and provides us insights into real-world time series. The code for our method can be found at:https://github.com/wasimahmadpk/gCause. ",
        "title": "Deep Learning-based Group Causal Inference in Multivariate Time-series",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08392",
        "abstract_url": "http://arxiv.org/abs/2401.08392",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Zongxin"
            },
            {
                "last_name": "Chen",
                "first_name": "Guikun"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaodi"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenguan"
            },
            {
                "last_name": "Yang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  The field of AI agents is advancing at an unprecedented rate due to the capabilities of large language models (LLMs). However, LLM-driven visual agents mainly focus on solving tasks for the image modality, which limits their ability to understand the dynamic nature of the real world, making it still far from real-life applications, e.g., guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing and perceptually intensive nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video with massive content into a symbolic memory that stores \\textit{task-related} attributes. This structured representation allows for spatial-temporal querying and reasoning by sub-task tools, resulting in concise and relevant intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorporate plug-and-play tools to assess external knowledge and address tasks across different domains. Moreover, we introduce a novel LLM-driven planner based on Monte Carlo Tree Search to efficiently explore the large planning space for scheduling various tools. The planner iteratively finds feasible solutions by backpropagating the result's reward, and multiple solutions can be summarized into an improved final answer. We extensively evaluate DoraemonGPT in dynamic scenes and provide in-the-wild showcases demonstrating its ability to handle more complex questions than previous studies. ",
        "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language  Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08396",
        "abstract_url": "http://arxiv.org/abs/2401.08396",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Qiao"
            },
            {
                "last_name": "Chen",
                "first_name": "Fangyuan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiliang"
            },
            {
                "last_name": "Xu",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Cheung",
                "first_name": "Justin M."
            },
            {
                "last_name": "Chen",
                "first_name": "Robert"
            },
            {
                "last_name": "Summers",
                "first_name": "Ronald M."
            },
            {
                "last_name": "Rousseau",
                "first_name": "Justin F."
            },
            {
                "last_name": "Ni",
                "first_name": "Peiyun"
            },
            {
                "last_name": "Landsman",
                "first_name": "Marc J"
            },
            {
                "last_name": "Baxter",
                "first_name": "Sally L."
            },
            {
                "last_name": "Al'Aref",
                "first_name": "Subhi J."
            },
            {
                "last_name": "Li",
                "first_name": "Yijia"
            },
            {
                "last_name": "Chiang",
                "first_name": "Michael F."
            },
            {
                "last_name": "Peng",
                "first_name": "Yifan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhiyong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows. ",
        "title": "Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08397",
        "abstract_url": "http://arxiv.org/abs/2401.08397",
        "authors": [
            {
                "last_name": "Magliano",
                "first_name": "Enrico"
            },
            {
                "last_name": "Carpegna",
                "first_name": "Alessio"
            },
            {
                "last_name": "Savino",
                "first_name": "Alessadro"
            },
            {
                "last_name": "Di Carlo",
                "first_name": "Stefano"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  In contemporary times, the increasing complexity of the system poses significant challenges to the reliability, trustworthiness, and security of the SACRES. Key issues include the susceptibility to phenomena such as instantaneous voltage spikes, electromagnetic interference, neutron strikes, and out-of-range temperatures. These factors can induce switch state changes in transistors, resulting in bit-flipping, soft errors, and transient corruption of stored data in memory. The occurrence of soft errors, in turn, may lead to system faults that can propel the system into a hazardous state. Particularly in critical sectors like automotive, avionics, or aerospace, such malfunctions can have real-world implications, potentially causing harm to individuals.   This paper introduces a novel fault injector designed to facilitate the monitoring, aggregation, and examination of micro-architectural events. This is achieved by harnessing the microprocessor's PMU and the debugging interface, specifically focusing on ensuring the repeatability of fault injections. The fault injection methodology targets bit-flipping within the memory system, affecting CPU registers and RAM. The outcomes of these fault injections enable a thorough analysis of the impact of soft errors and establish a robust correlation between the identified faults and the essential timing predictability demanded by SACRES. ",
        "title": "A Micro Architectural Events Aware Real-Time Embedded System Fault  Injector",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08398",
        "abstract_url": "http://arxiv.org/abs/2401.08398",
        "authors": [
            {
                "last_name": "Ming",
                "first_name": "Xin"
            },
            {
                "last_name": "Li",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Ling",
                "first_name": "Jingwang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Libo"
            },
            {
                "last_name": "Xu",
                "first_name": "Feng"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CV"
        ],
        "abstract": "  Readily editable mesh blendshapes have been widely used in animation pipelines, while recent advancements in neural geometry and appearance representations have enabled high-quality inverse rendering. Building upon these observations, we introduce a novel technique that reconstructs mesh-based blendshape rigs from single or sparse multi-view videos, leveraging state-of-the-art neural inverse rendering. We begin by constructing a deformation representation that parameterizes vertex displacements into differential coordinates with tetrahedral connections, allowing for high-quality vertex deformation on high-resolution meshes. By constructing a set of semantic regulations in this representation, we achieve joint optimization of blendshapes and expression coefficients. Furthermore, to enable a user-friendly multi-view setup with unsynchronized cameras, we propose a neural regressor to model time-varying motion parameters. This approach implicitly considers the time difference across multiple cameras, enhancing the accuracy of motion modeling. Experiments demonstrate that, with the flexible input of single or sparse multi-view videos, we reconstruct personalized high-fidelity blendshapes. These blendshapes are both geometrically and semantically accurate, and they are compatible with industrial animation pipelines. Code and data will be released. ",
        "title": "High-Quality Mesh Blendshape Generation from Face Videos via Neural  Inverse Rendering",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08399",
        "abstract_url": "http://arxiv.org/abs/2401.08399",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yun"
            },
            {
                "last_name": "Yang",
                "first_name": "Haolin"
            },
            {
                "last_name": "Si",
                "first_name": "Xu"
            },
            {
                "last_name": "Liu",
                "first_name": "Ling"
            },
            {
                "last_name": "Li",
                "first_name": "Zipeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yebin"
            },
            {
                "last_name": "Yi",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Humans commonly work with multiple objects in daily life and can intuitively transfer manipulation skills to novel objects by understanding object functional regularities. However, existing technical approaches for analyzing and synthesizing hand-object manipulation are mostly limited to handling a single hand and object due to the lack of data support. To address this, we construct TACO, an extensive bimanual hand-object-interaction dataset spanning a large variety of tool-action-object compositions for daily human activities. TACO contains 2.5K motion sequences paired with third-person and egocentric views, precise hand-object 3D meshes, and action labels. To rapidly expand the data scale, we present a fully-automatic data acquisition pipeline combining multi-view sensing with an optical motion capture system. With the vast research fields provided by TACO, we benchmark three generalizable hand-object-interaction tasks: compositional action recognition, generalizable hand-object motion forecasting, and cooperative grasp synthesis. Extensive experiments reveal new insights, challenges, and opportunities for advancing the studies of generalizable hand-object motion analysis and synthesis. Our data and code are available at https://taco2024.github.io. ",
        "title": "TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object  Understanding",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08402",
        "abstract_url": "http://arxiv.org/abs/2401.08402",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junren"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhaoqiang"
            },
            {
                "last_name": "Ding",
                "first_name": "Meng"
            },
            {
                "last_name": "Ng",
                "first_name": "Michael K."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper studies quantized corrupted sensing where the measurements are contaminated by unknown corruption and then quantized by a dithered uniform quantizer. We establish uniform guarantees for Lasso that ensure the accurate recovery of all signals and corruptions using a single draw of the sub-Gaussian sensing matrix and uniform dither. For signal and corruption with structured priors (e.g., sparsity, low-rankness), our uniform error rate for constrained Lasso typically coincides with the non-uniform one [Sun, Cui and Liu, 2022] up to logarithmic factors. By contrast, our uniform error rate for unconstrained Lasso exhibits worse dependence on the structured parameters due to regularization parameters larger than the ones for non-uniform recovery. For signal and corruption living in the ranges of some Lipschitz continuous generative models (referred to as generative priors), we achieve uniform recovery via constrained Lasso with a measurement number proportional to the latent dimensions of the generative models. Our treatments to the two kinds of priors are (nearly) unified and share the common key ingredients of (global) quantized product embedding (QPE) property, which states that the dithered uniform quantization (universally) preserves inner product. As a by-product, our QPE result refines the one in [Xu and Jacques, 2020] under sub-Gaussian random matrix, and in this specific instance we are able to sharpen the uniform error decaying rate (for the projected-back projection estimator with signals in some convex symmetric set) presented therein from $O(m^{-1/16})$ to $O(m^{-1/8})$. ",
        "title": "Uniform Recovery Guarantees for Quantized Corrupted Sensing Using  Structured or Generative Priors",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08404",
        "abstract_url": "http://arxiv.org/abs/2401.08404",
        "authors": [
            {
                "last_name": "Vossough",
                "first_name": "Arastoo"
            },
            {
                "last_name": "Khalili",
                "first_name": "Nastaran"
            },
            {
                "last_name": "Familiar",
                "first_name": "Ariana M."
            },
            {
                "last_name": "Gandhi",
                "first_name": "Deep"
            },
            {
                "last_name": "Viswanathan",
                "first_name": "Karthik"
            },
            {
                "last_name": "Tu",
                "first_name": "Wenxin"
            },
            {
                "last_name": "Haldar",
                "first_name": "Debanjan"
            },
            {
                "last_name": "Bagheri",
                "first_name": "Sina"
            },
            {
                "last_name": "Anderson",
                "first_name": "Hannah"
            },
            {
                "last_name": "Haldar",
                "first_name": "Shuvanjan"
            },
            {
                "last_name": "Storm",
                "first_name": "Phillip B."
            },
            {
                "last_name": "Resnick",
                "first_name": "Adam"
            },
            {
                "last_name": "Ware",
                "first_name": "Jeffrey B."
            },
            {
                "last_name": "Nabavizadeh",
                "first_name": "Ali"
            },
            {
                "last_name": "Kazerooni",
                "first_name": "Anahita Fathi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Brain tumors are the most common solid tumors and the leading cause of cancer-related death among children. Tumor segmentation is essential in surgical and treatment planning, and response assessment and monitoring. However, manual segmentation is time-consuming and has high inter-operator variability, underscoring the need for more efficient methods. We compared two deep learning-based 3D segmentation models, DeepMedic and nnU-Net, after training with pediatric-specific multi-institutional brain tumor data using based on multi-parametric MRI scans.Multi-parametric preoperative MRI scans of 339 pediatric patients (n=293 internal and n=46 external cohorts) with a variety of tumor subtypes, were preprocessed and manually segmented into four tumor subregions, i.e., enhancing tumor (ET), non-enhancing tumor (NET), cystic components (CC), and peritumoral edema (ED). After training, performance of the two models on internal and external test sets was evaluated using Dice scores, sensitivity, and Hausdorff distance with reference to ground truth manual segmentations. Dice score for nnU-Net internal test sets was (mean +/- SD (median)) 0.9+/-0.07 (0.94) for WT, 0.77+/-0.29 for ET, 0.66+/-0.32 for NET, 0.71+/-0.33 for CC, and 0.71+/-0.40 for ED, respectively. For DeepMedic the Dice scores were 0.82+/-0.16 for WT, 0.66+/-0.32 for ET, 0.48+/-0.27, for NET, 0.48+/-0.36 for CC, and 0.19+/-0.33 for ED, respectively. Dice scores were significantly higher for nnU-Net (p<=0.01). External validation of the trained nnU-Net model on the multi-institutional BraTS-PEDs 2023 dataset revealed high generalization capability in segmentation of whole tumor and tumor core with Dice scores of 0.87+/-0.13 (0.91) and 0.83+/-0.18 (0.89), respectively. Pediatric-specific data trained nnU-Net model is superior to DeepMedic for whole tumor and subregion segmentation of pediatric brain tumors. ",
        "title": "Training and Comparison of nnU-Net and DeepMedic Methods for  Autosegmentation of Pediatric Brain Tumors",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08405",
        "abstract_url": "http://arxiv.org/abs/2401.08405",
        "authors": [
            {
                "last_name": "Nikghalb",
                "first_name": "Mohammad Ronagh"
            },
            {
                "last_name": "Cheng",
                "first_name": "Jinghui"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI as mere tools. Playful interactions with AI systems naturally emerged as a way for users to make sense of the ever-changing technology. However, these emergent and playful interactions are underexamined. We target this gap by investigating playful interactions exhibited by users of a recently trending powerful AI technology, ChatGPT. Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that a substantial portion of user discourse revolves around playful interactions. The analysis further allowed us to construct a preliminary taxonomy to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories. Overall, this study contributes to the field of HCI and CSCW by illuminating the multifaceted nature of playful interactions with AI, underlining their significance in shaping the human-AI relationship. ",
        "title": "Interrogating AI: Characterizing Emergent Playful Interactions with  ChatGPT",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08406",
        "abstract_url": "http://arxiv.org/abs/2401.08406",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Aman"
            },
            {
                "last_name": "Shirgaonkar",
                "first_name": "Anup"
            },
            {
                "last_name": "Balaguer",
                "first_name": "Angels de Luis"
            },
            {
                "last_name": "Silva",
                "first_name": "Bruno"
            },
            {
                "last_name": "Holstein",
                "first_name": "Daniel"
            },
            {
                "last_name": "Li",
                "first_name": "Dawei"
            },
            {
                "last_name": "Marsman",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Nunes",
                "first_name": "Leonardo O."
            },
            {
                "last_name": "Rouzbahman",
                "first_name": "Mahsa"
            },
            {
                "last_name": "Sharp",
                "first_name": "Morris"
            },
            {
                "last_name": "Mecklenburg",
                "first_name": "Nick"
            },
            {
                "last_name": "Padilha",
                "first_name": "Rafael"
            },
            {
                "last_name": "Chandra",
                "first_name": "Ranveer"
            },
            {
                "last_name": "Cunha",
                "first_name": "Renato Luiz de Freitas"
            },
            {
                "last_name": "Filho",
                "first_name": "Roberto de M. Estev\u00e3o"
            },
            {
                "last_name": "Tsang",
                "first_name": "Ryan"
            },
            {
                "last_name": "Malvar",
                "first_name": "Sara"
            },
            {
                "last_name": "Sharma",
                "first_name": "Swati"
            },
            {
                "last_name": "Hendry",
                "first_name": "Todd"
            },
            {
                "last_name": "Aski",
                "first_name": "Vijay"
            },
            {
                "last_name": "Vijayendran",
                "first_name": "Vijetha"
            },
            {
                "last_name": "Benara",
                "first_name": "Vinamra"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains. ",
        "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on  Agriculture",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08407",
        "abstract_url": "http://arxiv.org/abs/2401.08407",
        "authors": [
            {
                "last_name": "Nie",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Xing",
                "first_name": "Yun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Gongjie"
            },
            {
                "last_name": "Yan",
                "first_name": "Pei"
            },
            {
                "last_name": "Xiao",
                "first_name": "Aoran"
            },
            {
                "last_name": "Tan",
                "first_name": "Yap-Peng"
            },
            {
                "last_name": "Kot",
                "first_name": "Alex C."
            },
            {
                "last_name": "Lu",
                "first_name": "Shijian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting novel categories from a distinct domain using only limited exemplars. In this paper, we undertake a comprehensive study of CD-FSS and uncover two crucial insights: (i) the necessity of a fine-tuning stage to effectively transfer the learned meta-knowledge across domains, and (ii) the overfitting risk during the na\\\"ive fine-tuning due to the scarcity of novel category examples. With these insights, we propose a novel cross-domain fine-tuning strategy that addresses the challenging CD-FSS tasks. We first design Bi-directional Few-shot Prediction (BFP), which establishes support-query correspondence in a bi-directional manner, crafting augmented supervision to reduce the overfitting risk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which is a recursive framework to capture the support-query correspondence iteratively, targeting maximal exploitation of supervisory signals from the sparse novel category samples. Extensive empirical evaluations show that our method significantly outperforms the state-of-the-arts (+7.8\\%), which verifies that IFA tackles the cross-domain challenges and mitigates the overfitting simultaneously. Code will be made available. ",
        "title": "Cross-Domain Few-Shot Segmentation via Iterative Support-Query  Correspondence Mining",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08409",
        "abstract_url": "http://arxiv.org/abs/2401.08409",
        "authors": [
            {
                "last_name": "Bassi",
                "first_name": "Pedro R. A. S."
            },
            {
                "last_name": "Decherchi",
                "first_name": "Sergio"
            },
            {
                "last_name": "Cavalli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CY",
            "LG"
        ],
        "abstract": "  Image background features can constitute background bias (spurious correlations) and impact deep classifiers decisions, causing shortcut learning (Clever Hans effect) and reducing the generalization skill on real-world data. The concept of optimizing Layer-wise Relevance Propagation (LRP) heatmaps, to improve classifier behavior, was recently introduced by a neural network architecture named ISNet. It minimizes background relevance in LRP maps, to mitigate the influence of image background features on deep classifiers decisions, hindering shortcut learning and improving generalization. For each training image, the original ISNet produces one heatmap per possible class in the classification task, hence, its training time scales linearly with the number of classes. Here, we introduce reformulated architectures that allow the training time to become independent from this number, rendering the optimization process much faster. We challenged the enhanced models utilizing the MNIST dataset with synthetic background bias, and COVID-19 detection in chest X-rays, an application that is prone to shortcut learning due to background bias. The trained models minimized background attention and hindered shortcut learning, while retaining high accuracy. Considering external (out-of-distribution) test datasets, they consistently proved more accurate than multiple state-of-the-art deep neural network architectures, including a dedicated image semantic segmenter followed by a classifier. The architectures presented here represent a potentially massive improvement in training speed over the original ISNet, thus introducing LRP optimization into a gamut of applications that could not be feasibly handled by the original model. ",
        "title": "Faster ISNet for Background Bias Mitigation on Deep Neural Networks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08411",
        "abstract_url": "http://arxiv.org/abs/2401.08411",
        "authors": [
            {
                "last_name": "Borland",
                "first_name": "David"
            },
            {
                "last_name": "Wang",
                "first_name": "Arran Zeyu"
            },
            {
                "last_name": "Gotz",
                "first_name": "David"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Traditional approaches to data visualization have often focused on comparing different subsets of data, and this is reflected in the many techniques developed and evaluated over the years for visual comparison. Similarly, common workflows for exploratory visualization are built upon the idea of users interactively applying various filter and grouping mechanisms in search of new insights. This paradigm has proven effective at helping users identify correlations between variables that can inform thinking and decision-making. However, recent studies show that consumers of visualizations often draw causal conclusions even when not supported by the data. Motivated by these observations, this article highlights recent advances from a growing community of researchers exploring methods that aim to directly support visual causal inference. However, many of these approaches have their own limitations which limit their use in many real-world scenarios. This article therefore also outlines a set of key open challenges and corresponding priorities for new research to advance the state of the art in visual causal inference. ",
        "title": "Using Counterfactuals to Improve Causal Inferences from Visualizations",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08414",
        "abstract_url": "http://arxiv.org/abs/2401.08414",
        "authors": [
            {
                "last_name": "Jacobsen",
                "first_name": "Christian"
            },
            {
                "last_name": "Dong",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Khalloufi",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Huan",
                "first_name": "Xun"
            },
            {
                "last_name": "Duraisamy",
                "first_name": "Karthik"
            },
            {
                "last_name": "Akram",
                "first_name": "Maryam"
            },
            {
                "last_name": "Liu",
                "first_name": "Wanjiao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a comprehensive data-driven framework aimed at enhancing the modeling of physical systems, employing inference techniques and machine learning enhancements. As a demonstrative application, we pursue the modeling of cathodic electrophoretic deposition (EPD), commonly known as e-coating. Our approach illustrates a systematic procedure for enhancing physical models by identifying their limitations through inference on experimental data and introducing adaptable model enhancements to address these shortcomings. We begin by tackling the issue of model parameter identifiability, which reveals aspects of the model that require improvement. To address generalizability , we introduce modifications which also enhance identifiability. However, these modifications do not fully capture essential experimental behaviors. To overcome this limitation, we incorporate interpretable yet flexible augmentations into the baseline model. These augmentations are parameterized by simple fully-connected neural networks (FNNs), and we leverage machine learning tools, particularly Neural Ordinary Differential Equations (Neural ODEs), to learn these augmentations. Our simulations demonstrate that the machine learning-augmented model more accurately captures observed behaviors and improves predictive accuracy. Nevertheless, we contend that while the model updates offer superior performance and capture the relevant physics, we can reduce off-line computational costs by eliminating certain dynamics without compromising accuracy or interpretability in downstream predictions of quantities of interest, particularly film thickness predictions. The entire process outlined here provides a structured approach to leverage data-driven methods. Firstly, it helps us comprehend the root causes of model inaccuracies, and secondly, it offers a principled method for enhancing model performance. ",
        "title": "Enhancing Dynamical System Modeling through Interpretable Machine  Learning Augmentations: A Case Study in Cathodic Electrophoretic Deposition",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08415",
        "abstract_url": "http://arxiv.org/abs/2401.08415",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Jiu"
            },
            {
                "last_name": "Erol",
                "first_name": "Mehmet Hamza"
            },
            {
                "last_name": "Chung",
                "first_name": "Joon Son"
            },
            {
                "last_name": "Senocak",
                "first_name": "Arda"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  Transformers have become central to recent advances in audio classification. However, training an audio spectrogram transformer, e.g. AST, from scratch can be resource and time-intensive. Furthermore, the complexity of transformers heavily depends on the input audio spectrogram size. In this work, we aim to optimize AST training by linking to the resolution in the time-axis. We introduce multi-phase training of audio spectrogram transformers by connecting the seminal idea of coarse-to-fine with transformer models. To achieve this, we propose a set of methods for temporal compression. By employing one of these methods, the transformer model learns from lower-resolution (coarse) data in the initial phases, and then is fine-tuned with high-resolution data later in a curriculum learning strategy. Experimental results demonstrate that the proposed training mechanism for AST leads to improved (or on-par) performance with faster convergence, i.e. requiring fewer computational resources and less time. This approach is also generalizable to other AST-based methods regardless of their learning paradigms. ",
        "title": "From Coarse to Fine: Efficient Training for Audio Spectrogram  Transformers",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08417",
        "abstract_url": "http://arxiv.org/abs/2401.08417",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Haoran"
            },
            {
                "last_name": "Sharaf",
                "first_name": "Amr"
            },
            {
                "last_name": "Chen",
                "first_name": "Yunmo"
            },
            {
                "last_name": "Tan",
                "first_name": "Weiting"
            },
            {
                "last_name": "Shen",
                "first_name": "Lingfeng"
            },
            {
                "last_name": "Van Durme",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Murray",
                "first_name": "Kenton"
            },
            {
                "last_name": "Kim",
                "first_name": "Young Jin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets. ",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM  Performance in Machine Translation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08420",
        "abstract_url": "http://arxiv.org/abs/2401.08420",
        "authors": [
            {
                "last_name": "Balloccu",
                "first_name": "Simone"
            },
            {
                "last_name": "Reiter",
                "first_name": "Ehud"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vivek"
            },
            {
                "last_name": "Recupero",
                "first_name": "Diego Reforgiato"
            },
            {
                "last_name": "Riboni",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs), with their flexible generation abilities, can be powerful data sources in domains with few or no available corpora. However, problems like hallucinations and biases limit such applications. In this case study, we pick nutrition counselling, a domain lacking any public resource, and show that high-quality datasets can be gathered by combining LLMs, crowd-workers and nutrition experts. We first crowd-source and cluster a novel dataset of diet-related issues, then work with experts to prompt ChatGPT into producing related supportive text. Finally, we let the experts evaluate the safety of the generated text. We release HAI-coaching, the first expert-annotated nutrition counselling dataset containing ~2.4K dietary struggles from crowd workers, and ~97K related supportive texts generated by ChatGPT. Extensive analysis shows that ChatGPT while producing highly fluent and human-like text, also manifests harmful behaviours, especially in sensitive topics like mental health, making it unsuitable for unsupervised use. ",
        "title": "Ask the experts: sourcing high-quality datasets for nutritional  counselling through Human-AI collaboration",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08422",
        "abstract_url": "http://arxiv.org/abs/2401.08422",
        "authors": [
            {
                "last_name": "Kuo",
                "first_name": "Shang-Jui"
            },
            {
                "last_name": "Huang",
                "first_name": "Po-Han"
            },
            {
                "last_name": "Lin",
                "first_name": "Chia-Ching"
            },
            {
                "last_name": "Li",
                "first_name": "Jeng-Lin"
            },
            {
                "last_name": "Chang",
                "first_name": "Ming-Ching"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diabetic foot ulcers pose health risks, including higher morbidity, mortality, and amputation rates. Monitoring wound areas is crucial for proper care, but manual segmentation is subjective due to complex wound features and background variation. Expert annotations are costly and time-intensive, thus hampering large dataset creation. Existing segmentation models relying on extensive annotations are impractical in real-world scenarios with limited annotated data. In this paper, we propose a cross-domain augmentation method named TransMix that combines Augmented Global Pre-training AGP and Localized CutMix Fine-tuning LCF to enrich wound segmentation data for model learning. TransMix can effectively improve the foot ulcer segmentation model training by leveraging other dermatology datasets not on ulcer skins or wounds. AGP effectively increases the overall image variability, while LCF increases the diversity of wound regions. Experimental results show that TransMix increases the variability of wound regions and substantially improves the Dice score for models trained with only 40 annotated images under various proportions. ",
        "title": "Improving Limited Supervised Foot Ulcer Segmentation Using Cross-Domain  Augmentation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08423",
        "abstract_url": "http://arxiv.org/abs/2401.08423",
        "authors": [
            {
                "last_name": "Lai",
                "first_name": "Ming-Jun"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper begins by reviewing numerous theoretical advancements in the field of multivariate splines, primarily contributed by Professor Larry L. Schumaker. These foundational results have paved the way for a wide range of applications and computational techniques. The paper then proceeds to highlight various practical applications of multivariate splines. These include scattered data fitting and interpolation, the construction of smooth curves and surfaces, and the numerical solutions of various partial differential equations, encompassing both linear and nonlinear PDEs. Beyond these conventional and well-established uses, the paper introduces a novel application of multivariate splines in function value denoising. This innovative approach facilitates the creation of LKB splines, which are instrumental in approximating high-dimensional functions and effectively circumventing the curse of dimensionality. ",
        "title": "Multivariate Splines and Their Applications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08424",
        "abstract_url": "http://arxiv.org/abs/2401.08424",
        "authors": [
            {
                "last_name": "Sayis",
                "first_name": "Batuhan"
            },
            {
                "last_name": "Beardsley",
                "first_name": "Marc"
            },
            {
                "last_name": "Portero-Tresserra",
                "first_name": "Marta"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Best possible self (BPS) is a positive psychological intervention shown to enhance well-being which involves writing a description of an ideal future scenario. This paper presents a comparison of psychophysiological effects of a BPS activity that has been adapted for classroom settings and a time-matched control activity (NA). Thirty-three undergraduate students participated in the study that assessed state anxiety (State-Trait Anxiety Inventory, STAI), affect (Affective Slider, AS), and cardiac vagal activity (heart-rate variability, HRV) as an indicator of self-regulatory resource usage, at three time periods (PRE, DURING, POST). Results show that BPS led to a significantly greater increase in positive valence (DURING) and overall higher levels of cardiac vagal activity (HRV) compared to NA. These findings suggest that BPS has promising characteristics as a self-regulatory technique aimed at fostering positive affect and positively impacting self-regulatory resources. As BPS does not require expert knowledge nor specialized technology to administer, it may be a suitable activity for educators to use when teaching and having students practice self-regulation. This study presents evidence collected in a replicable multimodal approach of the self-regulatory effects of a brief BPS activity on undergraduate students. ",
        "title": "Multimodal assessment of best possible self as a self-regulatory  activity for the classroom",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08425",
        "abstract_url": "http://arxiv.org/abs/2401.08425",
        "authors": [
            {
                "last_name": "Zottin",
                "first_name": "Silvia"
            },
            {
                "last_name": "De Nardin",
                "first_name": "Axel"
            },
            {
                "last_name": "Colombi",
                "first_name": "Emanuela"
            },
            {
                "last_name": "Piciarelli",
                "first_name": "Claudio"
            },
            {
                "last_name": "Pavan",
                "first_name": "Filippo"
            },
            {
                "last_name": "Foresti",
                "first_name": "Gian Luca"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Document Layout Analysis, which is the task of identifying different semantic regions inside of a document page, is a subject of great interest for both computer scientists and humanities scholars as it represents a fundamental step towards further analysis tasks for the former and a powerful tool to improve and facilitate the study of the documents for the latter. However, many of the works currently present in the literature, especially when it comes to the available datasets, fail to meet the needs of both worlds and, in particular, tend to lean towards the needs and common practices of the computer science side, leading to resources that are not representative of the humanities real needs. For this reason, the present paper introduces U-DIADS-Bib, a novel, pixel-precise, non-overlapping and noiseless document layout analysis dataset developed in close collaboration between specialists in the fields of computer vision and humanities. Furthermore, we propose a novel, computer-aided, segmentation pipeline in order to alleviate the burden represented by the time-consuming process of manual annotation, necessary for the generation of the ground truth segmentation maps. Finally, we present a standardized few-shot version of the dataset (U-DIADS-BibFS), with the aim of encouraging the development of models and solutions able to address this task with as few samples as possible, which would allow for more effective use in a real-world scenario, where collecting a large number of segmentations is not always feasible. ",
        "title": "U-DIADS-Bib: a full and few-shot pixel-precise dataset for document  layout analysis of ancient manuscripts",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08426",
        "abstract_url": "http://arxiv.org/abs/2401.08426",
        "authors": [
            {
                "last_name": "Kumar",
                "first_name": "Siddharth Krishna"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  This paper investigates how non-differentiability affects three different aspects of the neural network training process. We first analyze fully connected neural networks with ReLU activations, for which we show that the continuously differentiable neural networks converge faster than non-differentiable neural networks. Next, we analyze the problem of $L_{1}$ regularization and show that the solutions produced by deep learning solvers are incorrect and counter-intuitive even for the $L_{1}$ penalized linear model. Finally, we analyze the Edge of Stability problem, where we show that all convex, non-smooth, Lipschitz continuous functions display unstable convergence, and provide an example of a result derived using twice differentiable functions which fails in the once differentiable setting. More generally, our results suggest that accounting for the non-linearity of neural networks in the training process is essential for us to develop better algorithms, and to get a better understanding of the training process in general. ",
        "title": "Three ways that non-differentiability affects neural network training",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08429",
        "abstract_url": "http://arxiv.org/abs/2401.08429",
        "authors": [
            {
                "last_name": "Pourkamali",
                "first_name": "Nooshin"
            },
            {
                "last_name": "Sharifi",
                "first_name": "Shler Ebrahim"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC",
            "LG"
        ],
        "abstract": "  Generative large language models (LLMs) have demonstrated exceptional proficiency in various natural language processing (NLP) tasks, including machine translation, question answering, text summarization, and natural language understanding.   To further enhance the performance of LLMs in machine translation, we conducted an investigation into two popular prompting methods and their combination, focusing on cross-language combinations of Persian, English, and Russian. We employed n-shot feeding and tailored prompting frameworks. Our findings indicate that multilingual LLMs like PaLM exhibit human-like machine translation outputs, enabling superior fine-tuning of desired translation nuances in accordance with style guidelines and linguistic considerations. These models also excel in processing and applying prompts. However, the choice of language model, machine translation task, and the specific source and target languages necessitate certain considerations when adopting prompting frameworks and utilizing n-shot in-context learning.   Furthermore, we identified errors and limitations inherent in popular LLMs as machine translation tools and categorized them based on various linguistic metrics. This typology of errors provides valuable insights for utilizing LLMs effectively and offers methods for designing prompts for in-context learning. Our report aims to contribute to the advancement of machine translation with LLMs by improving both the accuracy and reliability of evaluation metrics. ",
        "title": "Machine Translation with Large Language Models: Prompt Engineering for  Persian, English, and Russian Directions",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08430",
        "abstract_url": "http://arxiv.org/abs/2401.08430",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Zhoujie"
            },
            {
                "last_name": "Luo",
                "first_name": "Cai"
            },
            {
                "last_name": "Guan",
                "first_name": "Zhong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a dynamic capacitance matching (DCM)-based RC current response algorithm for calculating the current waveform of a signal line without performing SPICE simulation. Specifically, unlike previous method such as CCS model, driver linear representation, waveform functional fitting or equivalent load capacitance, our algorithm does not rely on fixed reduced model of both standard cell driver and RC load. Instead, our algorithm approaches the current waveform dynamically by computing current responses of the target driver for various load scenarios. Besides, we creatively use symbolic expression to combine the y-parameter of RC network with the pre-characterized driver library in order to perform capacitance matching by considering over/under-shoot effect. Our algorithm is experimentally verified on 40nm CMOS technology and has been partially adopted by latest commercial tool for other nodes. Experimental results show that our algorithm has excellent resolution and promising efficiency compared with traditional methods and SPICE golden result, especially for application in computing delay, power and signal line electromigration. ",
        "title": "A Dynamic Capacitance Matching (DCM)-based Current Response Algorithm  for Signal Line RC Network",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08433",
        "abstract_url": "http://arxiv.org/abs/2401.08433",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Peijia"
            },
            {
                "last_name": "Xia",
                "first_name": "Bingyi"
            },
            {
                "last_name": "Hu",
                "first_name": "Anjun"
            },
            {
                "last_name": "Zhao",
                "first_name": "Ziqi"
            },
            {
                "last_name": "Meng",
                "first_name": "Lingxiao"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhirui"
            },
            {
                "last_name": "Gao",
                "first_name": "Xuheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiankun"
            },
            {
                "last_name": "Meng",
                "first_name": "Max Q. -H."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The intricate and multi-stage task in dynamic public spaces like luggage trolley collection in airports presents both a promising opportunity and an ongoing challenge for automated service robots. Previous research has primarily focused on handling a single trolley or individual functional components, creating a gap in providing cost-effective and efficient solutions for practical scenarios. In this paper, we propose a mobile manipulation robot incorporated with an autonomy framework for the collection and transportation of multiple trolleys that can significantly enhance operational efficiency. We address the key challenges in the trolley collection problem through the novel design of the mechanical system and the vision-based control strategy. We design a lightweight manipulator and docking mechanism, optimized for the sequential stacking and transportation of multiple trolleys. Additionally, based on the Control Lyapunov Function and Control Barrier Function, we propose a novel vision-based control with the online Quadratic Programming which significantly improves the accuracy and efficiency of the collection process. The practical application of our system is demonstrated in real world scenarios, where it successfully executes multiple-trolley collection tasks. ",
        "title": "Autonomous Multiple-Trolley Collection System with Nonholonomic Robots:  Design, Control, and Implementation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08434",
        "abstract_url": "http://arxiv.org/abs/2401.08434",
        "authors": [
            {
                "last_name": "Yashvanth",
                "first_name": "L."
            },
            {
                "last_name": "Murthy",
                "first_name": "Chandra R."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We investigate the impact of multiple distributed intelligent reflecting surfaces (IRSs), which are deployed and optimized by a mobile operator (MO), on the performance of user equipments (UEs) served by other co-existing out-of-band (OOB) MOs that do not control the IRSs. We show that, under round-robin scheduling, in mmWave frequencies, the ergodic sum spectral efficiency (SE) of an OOB MO is monotonic in the total number of IRS elements with a pre-log factor that depends on the channel properties of the OOB UE. We further show that the maximum achievable SE of OOB MO scales log-linearly in IRS elements. Then, by specifying the minimum number of IRSs as a function of the channel parameters, we design a distributed IRS system in which an OOB MO almost surely obtains the maximum SE. Finally, we prove that the outage probability at an OOB UE decreases exponentially in the number of IRSs, even though they are randomly configured from the UE's viewpoint. We numerically verify our theory and conclude that distributed IRSs always help every MO, but the MO controlling the IRSs benefits the most. ",
        "title": "Distributed IRSs Always Benefit Every Mobile Operator",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08438",
        "abstract_url": "http://arxiv.org/abs/2401.08438",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Yaojia"
            },
            {
                "last_name": "Pan",
                "first_name": "Haojie"
            },
            {
                "last_name": "Fu",
                "first_name": "Ruiji"
            },
            {
                "last_name": "Liu",
                "first_name": "Ming"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhongyuan"
            },
            {
                "last_name": "Qin",
                "first_name": "Bing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows. ",
        "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language  Models",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08442",
        "abstract_url": "http://arxiv.org/abs/2401.08442",
        "authors": [
            {
                "last_name": "Alleman",
                "first_name": "Tijs W."
            },
            {
                "last_name": "Baetens",
                "first_name": "Jan M."
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  During the COVID-19 pandemic, governments faced the challenge of managing population behavior to prevent their healthcare systems from collapsing. Sweden adopted a strategy centered on voluntary sanitary recommendations while Belgium resorted to mandatory measures. Their consequences on pandemic progression and associated economic impacts remain insufficiently understood. This study leverages the divergent policies of Belgium and Sweden during the COVID-19 pandemic to relax the unrealistic -- but persistently used -- assumption that social contacts are not influenced by an epidemic's dynamics. We develop an epidemiological-economic co-simulation model where pandemic-induced behavioral changes are a superposition of voluntary actions driven by fear, prosocial behavior or social pressure, and compulsory compliance with government directives. Our findings emphasize the importance of early responses, which reduce the stringency of measures necessary to safeguard healthcare systems and minimize ensuing economic damage. Voluntary behavioral changes lead to a pattern of recurring epidemics, which should be regarded as the natural long-term course of pandemics. Governments should carefully consider prolonging lockdown longer than necessary because this leads to higher economic damage and a potentially higher second surge when measures are released. Our model can aid policymakers in the selection of an appropriate long-term strategy that minimizes economic damage. ",
        "title": "Assessing the impact of forced and voluntary behavioral changes on  economic-epidemiological co-dynamics: A comparative case study between  Belgium and Sweden during the 2020 COVID-19 pandemic",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08443",
        "abstract_url": "http://arxiv.org/abs/2401.08443",
        "authors": [
            {
                "last_name": "Wittmann",
                "first_name": "Jonas"
            },
            {
                "last_name": "Ochsenfarth",
                "first_name": "Franziska"
            },
            {
                "last_name": "Sonneville",
                "first_name": "Valentin"
            },
            {
                "last_name": "Rixen",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The aim of coordinated planning is to avoid robot-to-robot collisions in a multi-robot system, and there are two standard solution approaches: centralized planning and decoupled planning. Our first contribution is a decoupled planning approach that ensures C2-continuous control commands with zero velocities at the start and goal. We benchmark our decoupled approach with a centralized approach. Contrary to literature, we show that for a standard motion planning pipeline, such as the one used by MoveIt!, centralized planning is superior to decoupled planning in dual-arm manipulation: It has a lower computation time and a higher robustness. Our second contribution is an optimization that minimizes the rotational motion of an end-effector while considering obstacle avoidance. We derive the analytic gradients of this optimization problem, making the algorithm suitable for online motion planning. Our optimization extends an existing path quality improvement method. Integrating it into our decoupled approach overcomes its shortcomings and provides a motion planning pipeline that is robust at up to 99.9% with a planning time of less than 1s and that computes high-quality paths. ",
        "title": "Centralized vs. Decoupled Dual-Arm Planning Taking into Account Path  Quality",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08444",
        "abstract_url": "http://arxiv.org/abs/2401.08444",
        "authors": [
            {
                "last_name": "Wegmeth",
                "first_name": "Lukas"
            },
            {
                "last_name": "Vente",
                "first_name": "Tobias"
            },
            {
                "last_name": "Purucker",
                "first_name": "Lennart"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The hyperparameters of recommender systems for top-n predictions are typically optimized to enhance the predictive performance of algorithms. Thereby, the optimization algorithm, e.g., grid search or random search, searches for the best hyperparameter configuration according to an optimization-target metric, like nDCG or Precision. In contrast, the optimized algorithm, internally optimizes a different loss function during training, like squared error or cross-entropy. To tackle this discrepancy, recent work focused on generating loss functions better suited for recommender systems. Yet, when evaluating an algorithm using a top-n metric during optimization, another discrepancy between the optimization-target metric and the training loss has so far been ignored. During optimization, the top-n items are selected for computing a top-n metric; ignoring that the top-n items are selected from the recommendations of a model trained with an entirely different loss function. Item recommendations suitable for optimization-target metrics could be outside the top-n recommended items; hiddenly impacting the optimization performance. Therefore, we were motivated to analyze whether the top-n items are optimal for optimization-target top-n metrics. In pursuit of an answer, we exhaustively evaluate the predictive performance of 250 selection strategies besides selecting the top-n. We extensively evaluate each selection strategy over twelve implicit feedback and eight explicit feedback data sets with eleven recommender systems algorithms. Our results show that there exist selection strategies other than top-n that increase predictive performance for various algorithms and recommendation domains. However, the performance of the top ~43% of selection strategies is not significantly different. We discuss the impact of our findings on optimization and re-ranking in recommender systems and feasible solutions. ",
        "title": "Revealing the Hidden Impact of Top-N Metrics on Optimization in  Recommender Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08445",
        "abstract_url": "http://arxiv.org/abs/2401.08445",
        "authors": [
            {
                "last_name": "Jurka",
                "first_name": "Jan"
            },
            {
                "last_name": "Milius",
                "first_name": "Stefan"
            },
            {
                "last_name": "Urbat",
                "first_name": "Henning"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Many important computational structures involve an intricate interplay between algebraic features (given by operations on the underlying set) and relational features (taking account of notions such as order or distance). This paper investigates algebras over relational structures axiomatized by an infinitary Horn theory, which subsume, for example, partial algebras, various incarnations of ordered algebras, quantitative algebras introduced by Mardare, Panangaden, and Plotkin, and their recent extension to generalized metric spaces and lifted algebraic signatures by Mio, Sarkis, and Vignudelli. To this end, we develop the notion of clustered equation, which is inspired by Mardare et al.'s basic conditional equations in the theory of quantitative algebras, at the level of generality of arbitrary relational structures, and we prove it to be equivalent to an abstract categorical form of equation earlier introduced by Milius and Urbat. Our main results are a family of Birkhoff-type variety theorems (classifying the expressive power of clustered equations) and an exactness theorem (classifying abstract equations by a congruence property). ",
        "title": "Algebraic Reasoning over Relational Structures",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08447",
        "abstract_url": "http://arxiv.org/abs/2401.08447",
        "authors": [
            {
                "last_name": "Dosimont",
                "first_name": "Damien"
            },
            {
                "last_name": "Houzeaux",
                "first_name": "Guillaume"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We tackle the challenging tasks of monitoring on unstable HPC platforms the performance of CFD applications all along their development. We have designed and implemented a monitoring framework, integrated at the end of a CI-CD pipeline. Measures retrieved during the automatic execution of production simulations are analyzed within a visual analytics interface we developed, providing advanced visualizations and interaction. We have validated this approach by monitoring the CFD code Alya over two years, detecting and resolving issues related to the platform, and highlighting performance improvement. ",
        "title": "Monitoring the development of CFD applications on unstable HPC platforms",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08449",
        "abstract_url": "http://arxiv.org/abs/2401.08449",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Aozhu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Fangming"
            },
            {
                "last_name": "Wang",
                "first_name": "Ziyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Xirong"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Ad-hoc Video Search (AVS) enables users to search for unlabeled video content using on-the-fly textual queries. Current deep learning-based models for AVS are trained to optimize holistic similarity between short videos and their associated descriptions. However, due to the diversity of ad-hoc queries, even for a short video, its truly relevant part w.r.t. a given query can be of shorter duration. In such a scenario, the holistic similarity becomes suboptimal. To remedy the issue, we propose in this paper CLIPRerank, a fine-grained re-scoring method. We compute cross-modal similarities between query and video frames using a pre-trained CLIP model, with multi-frame scores aggregated by max pooling. The fine-grained score is weightedly added to the initial score for search result reranking. As such, CLIPRerank is agnostic to the underlying video retrieval models and extremely simple, making it a handy plug-in for boosting AVS. Experiments on the challenging TRECVID AVS benchmarks (from 2016 to 2021) justify the effectiveness of the proposed strategy. CLIPRerank consistently improves the TRECVID top performers and multiple existing models including SEA, W2VV++, Dual Encoding, Dual Task, LAFF, CLIP2Video, TS2-Net and X-CLIP. Our method also works when substituting BLIP-2 for CLIP. ",
        "title": "CLIPRerank: An Extremely Simple Method for Improving Ad-hoc Video Search",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08453",
        "abstract_url": "http://arxiv.org/abs/2401.08453",
        "authors": [
            {
                "last_name": "Okati",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Barreto",
                "first_name": "Andre Noll"
            },
            {
                "last_name": "Garcia",
                "first_name": "Luis Uzeda"
            },
            {
                "last_name": "Wigard",
                "first_name": "Jeroen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Co-existence of terrestrial and non-terrestrial networks (NTN) is foreseen as an important component to fulfill the global coverage promised for sixth-generation (6G) of cellular networks. Due to ever rising spectrum demand, using dedicated frequency bands for terrestrial network (TN) and NTN may not be feasible. As a result, certain S-band frequency bands allocated by radio regulations to NTN networks are overlapping with those already utilized by cellular TN, leading to significant performance degradation due to the potential co-channel interference. Early simulation-based studies on different co-existence scenarios failed to offer a comprehensive and insightful understanding of these networks' overall performance. Besides, the complexity of a brute force performance evaluation increases exponentially with the number of nodes and their possible combinations in the network. In this paper, we utilize stochastic geometry to analytically derive the performance of TN-NTN integrated networks in terms of the probability of coverage and average achievable data rate for two co-existence scenarios. From the numerical results, it can be observed that, depending on the network parameters, TN and NTN users' distributions, and traffic load, one co-existence case may outperform the other, resulting in optimal performance of the integrated network. The analytical results presented herein pave the way for designing state-of-the-art methods for spectrum sharing between TN and NTN and optimizing the integrated network performance. ",
        "title": "Co-existence of Terrestrial and Non-Terrestrial Networks in S-band",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08455",
        "abstract_url": "http://arxiv.org/abs/2401.08455",
        "authors": [
            {
                "last_name": "van Hoeij",
                "first_name": "Mark"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC"
        ],
        "abstract": "  This paper proposes ideas to speed up the process of creative telescoping, particularly when the telescoper is reducible. One can interpret telescoping as computing an annihilator $L \\in D$ for an element $m$ in a $D$-module $M$. The main idea is to look for submodules of $M$. If $N$ is a non-trivial submodule of $M$, constructing the minimal operator $R$ of the image of $m$ in $M/N$ gives a right-factor of $L$ in $D$. Then $L = L' R$ where the left-factor $L'$ is the telescoper of $R(m) \\in N$. To expedite computing $L'$, compute the action of $D$ on a natural basis of $N$, then obtain $L'$ with a cyclic vector computation.   The next main idea is that when $N$ has automorphisms, use them to construct submodules. An automorphism with distinct eigenvalues can be used to decompose $N$ as a direct sum $N_1 \\oplus \\cdots \\oplus N_k$. Then $L'$ is the LCLM (Least Common Left Multiple) of $L_1, \\ldots, L_k$ where $L_i$ is the telescoper of the projection of $R(m)$ on $N_i$. An LCLM can greatly increase the degrees of coefficients, so $L'$ and $L$ can be much larger expressions than the factors $L_1,\\ldots,L_k$ and $R$. Examples show that computing each factor $L_i$ and $R$ seperately can save a lot of CPU time compared to computing $L$ in expanded form with standard creative telescoping. ",
        "title": "Submodule approach to creative telescoping",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08458",
        "abstract_url": "http://arxiv.org/abs/2401.08458",
        "authors": [
            {
                "last_name": "Jeong",
                "first_name": "Hyejun"
            },
            {
                "last_name": "Chung",
                "first_name": "Tai-Myoung"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The advent of Federated Learning has enabled the creation of a high-performing model as if it had been trained on a considerable amount of data. A multitude of participants and a server cooperatively train a model without the need for data disclosure or collection. The healthcare industry, where security and privacy are paramount, can substantially benefit from this new learning paradigm, as data collection is no longer feasible due to stringent data policies. Nonetheless, unaddressed challenges and insufficient attack mitigation are hampering its adoption. Attack surfaces differ from traditional centralized learning in that the server and clients communicate between each round of training. In this paper, we thus present vulnerabilities, attacks, and defenses based on the widened attack surfaces, as well as suggest promising new research directions toward a more robust FL. ",
        "title": "Security and Privacy Issues and Solutions in Federated Learning for  Digital Healthcare",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08460",
        "abstract_url": "http://arxiv.org/abs/2401.08460",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Mi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Conversational question answering (ConvQA) over law knowledge bases (KBs) involves answering multi-turn natural language questions about law and hope to find answers in the law knowledge base. Despite many methods have been proposed. Existing law knowledge base ConvQA model assume that the input question is clear and can perfectly reflect user's intention. However, in real world, the input questions are noisy and inexplict. This makes the model hard to find the correct answer in the law knowledge bases. In this paper, we try to use reinforcement learning to solve this problem. The reinforcement learning agent can automatically learn how to find the answer based on the input question and the conversation history, even when the input question is inexplicit. We test the proposed method on several real world datasets and the results show the effectivenss of the proposed model. ",
        "title": "Reinforcement Learning for Conversational Question Answering over  Knowledge Graph",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08461",
        "abstract_url": "http://arxiv.org/abs/2401.08461",
        "authors": [
            {
                "last_name": "Ekila",
                "first_name": "J\u00e9r\u00f4me Botoko"
            },
            {
                "last_name": "Nevens",
                "first_name": "Jens"
            },
            {
                "last_name": "Verheyen",
                "first_name": "Lara"
            },
            {
                "last_name": "Beuls",
                "first_name": "Katrien"
            },
            {
                "last_name": "Van Eecke",
                "first_name": "Paul"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "NE"
        ],
        "abstract": "  This paper introduces a methodology through which a population of autonomous agents can establish a linguistic convention that enables them to refer to arbitrary entities that they observe in their environment. The linguistic convention emerges in a decentralised manner through local communicative interactions between pairs of agents drawn from the population. The convention consists of symbolic labels (word forms) associated to concept representations (word meanings) that are grounded in a continuous feature space. The concept representations of each agent are individually constructed yet compatible on a communicative level. Through a range of experiments, we show (i) that the methodology enables a population to converge on a communicatively effective, coherent and human-interpretable linguistic convention, (ii) that it is naturally robust against sensor defects in individual agents, (iii) that it can effectively deal with noisy observations, uncalibrated sensors and heteromorphic populations, (iv) that the method is adequate for continual learning, and (v) that the convention self-adapts to changes in the environment and communicative needs of the agents. ",
        "title": "Decentralised Emergence of Robust and Adaptive Linguistic Conventions in  Populations of Autonomous Agents Grounded in Continuous Worlds",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08464",
        "abstract_url": "http://arxiv.org/abs/2401.08464",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Binghui"
            },
            {
                "last_name": "Chen",
                "first_name": "Yongqiang"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kaiwen"
            },
            {
                "last_name": "Han",
                "first_name": "Bo"
            },
            {
                "last_name": "Meng",
                "first_name": "Wei"
            },
            {
                "last_name": "Cheng",
                "first_name": "James"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Domain generalization is a critical challenge for machine learning systems. Prior domain generalization methods focus on extracting domain-invariant features across several stationary domains to enable generalization to new domains. However, in non-stationary tasks where new domains evolve in an underlying continuous structure, such as time, merely extracting the invariant features is insufficient for generalization to the evolving new domains. Nevertheless, it is non-trivial to learn both evolving and invariant features within a single model due to their conflicts. To bridge this gap, we build causal models to characterize the distribution shifts concerning the two patterns, and propose to learn both dynamic and invariant features via a new framework called Mutual Information-Based Sequential Autoencoders (MISTS). MISTS adopts information theoretic constraints onto sequential autoencoders to disentangle the dynamic and invariant features, and leverage a domain adaptive classifier to make predictions based on both evolving and invariant information. Our experimental results on both synthetic and real-world datasets demonstrate that MISTS succeeds in capturing both evolving and invariant information, and present promising results in evolving domain generalization tasks. ",
        "title": "Enhancing Evolving Domain Generalization through Dynamic Latent  Representations",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08465",
        "abstract_url": "http://arxiv.org/abs/2401.08465",
        "authors": [
            {
                "last_name": "Iqbal",
                "first_name": "Subhyal Bin"
            },
            {
                "last_name": "Nadaf",
                "first_name": "Salman"
            },
            {
                "last_name": "Karabulut",
                "first_name": "Umur"
            },
            {
                "last_name": "Schulz",
                "first_name": "Philipp"
            },
            {
                "last_name": "Prado",
                "first_name": "Anna"
            },
            {
                "last_name": "Fettweis",
                "first_name": "Gerhard P."
            },
            {
                "last_name": "Kellerer",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The hand blockage effect of the human hand around the user equipment (UE) is too considerable to be ignored in frequency range 2 (FR2). This adds another layer of complexity to the link budget design in FR2 for 5G networks, which already suffer from high path and diffraction loss. More recently, multipanel UEs (MPUEs) have been proposed as a way to address this problem, whereby multiple distinct antenna panels are integrated into the UE body as a way to leverage gains from antenna directivity. MPUEs also enhance the Rx-beamforming gain because it is now subject to each individual antenna panel. In this paper, the mobility performance of hand blockage induced by three practical hand grips is analyzed in a system-level simulation, where in each grip both the UE orientation and the hand positioning around the UE is different. It is seen that each hand grip has a significant impact on mobility performance of the network, where in the worst case mobility failures increase by 43% compared to the non-hand blockage case. Moreover, a detailed analysis of the tradeoff between the mobility key performance indicators and the panel and Rx beam switching frequency is also studied. Results have shown that both the panel and Rx beam switches can be reduced considerably without compromising on the mobility performance. This is beneficial because it helps in reducing UE power consumption. ",
        "title": "A Mobility Analysis of UE-Side Beamforming for Multi-Panel User  Equipment with Hand Blockage",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08468",
        "abstract_url": "http://arxiv.org/abs/2401.08468",
        "authors": [
            {
                "last_name": "Kumar",
                "first_name": "Syamantak"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Purnamrita"
            },
            {
                "last_name": "Bickel",
                "first_name": "Peter"
            },
            {
                "last_name": "Bean",
                "first_name": "Derek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we propose a non-parametric score to evaluate the quality of the solution to an iterative algorithm for Independent Component Analysis (ICA) with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. We also provide a new characteristic function-based contrast function for ICA and propose a fixed point iteration to optimize the corresponding objective function. Finally, we propose a theoretical framework to obtain sufficient conditions for the local and global optima of a family of contrast functions for ICA. This framework uses quasi-orthogonalization inherently, and our results extend the classical analysis of cumulant-based objective functions to noisy ICA. We demonstrate the efficacy of our algorithms via experimental results on simulated datasets. ",
        "title": "Keep or toss? A nonparametric score to evaluate solutions for noisy ICA",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08469",
        "abstract_url": "http://arxiv.org/abs/2401.08469",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiamin"
            },
            {
                "last_name": "Li",
                "first_name": "Xuhong"
            },
            {
                "last_name": "Xu",
                "first_name": "Yanwu"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            },
            {
                "last_name": "Xiong",
                "first_name": "Haoyi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Medical image segmentation aims to identify and locate abnormal structures in medical images, such as chest radiographs, using deep neural networks. These networks require a large number of annotated images with fine-grained masks for the regions of interest, making pre-training strategies based on classification datasets essential for sample efficiency. Based on a large-scale medical image classification dataset, our work collects explanations from well-trained classifiers to generate pseudo labels of segmentation tasks. Specifically, we offer a case study on chest radiographs and train image classifiers on the CheXpert dataset to identify 14 pathological observations in radiology. We then use Integrated Gradients (IG) method to distill and boost the explanations obtained from the classifiers, generating massive diagnosis-oriented localization labels (DoLL). These DoLL-annotated images are used for pre-training the model before fine-tuning it for downstream segmentation tasks, including COVID-19 infectious areas, lungs, heart, and clavicles. Our method outperforms other baselines, showcasing significant advantages in model performance and training efficiency across various segmentation settings. ",
        "title": "Explanations of Classifiers Enhance Medical Image Segmentation via  End-to-end Pre-training",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08470",
        "abstract_url": "http://arxiv.org/abs/2401.08470",
        "authors": [
            {
                "last_name": "Barkatou",
                "first_name": "Moulay"
            },
            {
                "last_name": "van Hoeij",
                "first_name": "Mark"
            },
            {
                "last_name": "Middeke",
                "first_name": "Johannes"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yi"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC"
        ],
        "abstract": "  We extend Petkov\\v{s}ek's algorithm for computing hypergeometric solutions of scalar difference equations to the case of difference systems $\\tau(Y) = M Y$, with $M \\in {\\rm GL}_n(C(x))$, where $\\tau$ is the shift operator. Hypergeometric solutions are solutions of the form $\\gamma P$ where $P \\in C(x)^n$ and $\\gamma$ is a hypergeometric term over $C(x)$, i.e. ${\\tau(\\gamma)}/{\\gamma} \\in C(x)$. Our contributions concern efficient computation of a set of candidates for ${\\tau(\\gamma)}/{\\gamma}$ which we write as $\\lambda = c\\frac{A}{B}$ with monic $A, B \\in C[x]$, $c \\in C^*$. Factors of the denominators of $M^{-1}$ and $M$ give candidates for $A$ and $B$, while another algorithm is needed for $c$. We use the super-reduction algorithm to compute candidates for $c$, as well as other ingredients to reduce the list of candidates for $A/B$. To further reduce the number of candidates $A/B$, we bound the so-called type of $A/B$ by bounding local types. Our algorithm has been implemented in Maple and experiments show that our implementation can handle systems of high dimension, which is useful for factoring operators. ",
        "title": "Hypergeometric Solutions of Linear Difference Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08472",
        "abstract_url": "http://arxiv.org/abs/2401.08472",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Lidong"
            },
            {
                "last_name": "Zheng",
                "first_name": "Zhedong"
            },
            {
                "last_name": "Wei",
                "first_name": "Yinwei"
            },
            {
                "last_name": "Chua",
                "first_name": "Tat-seng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we study the text-guided image generation task. Our focus lies in the modification of a reference image, given user text feedback, to imbue it with specific desired properties. Despite recent strides in this field, a persistent challenge remains that single-round optimization often overlooks crucial details, particularly in the realm of fine-grained changes like shoes or sleeves. This misalignment accumulation significantly hampers multi-round customization during interaction. In an attempt to address this challenge, we introduce a new self-supervised regularization into the existing framework, i.e., multi-round regularization. It builds upon the observation that the modification order does not affect the final result. As the name suggests, the multi-round regularization encourages the model to maintain consistency across different modification orders. Specifically, our proposed approach addresses the issue where an initial failure to capture fine-grained details leads to substantial discrepancies after multiple rounds, as opposed to traditional one-round learning. Both qualitative and quantitative experiments show the proposed method achieves high-fidelity generation quality over the text-guided generation task, especially the local modification. Furthermore, we extend the evaluation to semantic alignment with text by applying our method to text-guided retrieval datasets, such as FahisonIQ, where it demonstrates competitive performance. ",
        "title": "Instilling Multi-round Thinking to Text-guided Image Generation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08474",
        "abstract_url": "http://arxiv.org/abs/2401.08474",
        "authors": [
            {
                "last_name": "Cre\u00df",
                "first_name": "Christian"
            },
            {
                "last_name": "Zimmer",
                "first_name": "Walter"
            },
            {
                "last_name": "Purschke",
                "first_name": "Nils"
            },
            {
                "last_name": "Doan",
                "first_name": "Bach Ngoc"
            },
            {
                "last_name": "Lakshminarasimhan",
                "first_name": "Venkatnarayanan"
            },
            {
                "last_name": "Strand",
                "first_name": "Leah"
            },
            {
                "last_name": "Knoll",
                "first_name": "Alois C."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Event-based cameras are predestined for Intelligent Transportation Systems (ITS). They provide very high temporal resolution and dynamic range, which can eliminate motion blur and make objects easier to recognize at night. However, event-based images lack color and texture compared to images from a conventional rgb camera. Considering that, data fusion between event-based and conventional cameras can combine the strengths of both modalities. For this purpose, extrinsic calibration is necessary. To the best of our knowledge, no targetless calibration between event-based and rgb cameras can handle multiple moving objects, nor data fusion optimized for the domain of roadside ITS exists, nor synchronized event-based and rgb camera datasets in the field of ITS are known. To fill these research gaps, based on our previous work, we extend our targetless calibration approach with clustering methods to handle multiple moving objects. Furthermore, we develop an early fusion, simple late fusion, and a novel spatiotemporal late fusion method. Lastly, we publish the TUMTraf Event Dataset, which contains more than 4k synchronized event-based and rgb images with 21.9k labeled 2D boxes. During our extensive experiments, we verified the effectiveness of our calibration method with multiple moving objects. Furthermore, compared to a single rgb camera, we increased the detection performance of up to +16% mAP in the day and up to +12% mAP in the challenging night with our presented event-based sensor fusion methods. The TUMTraf Event Dataset is available at https://innovation-mobility.com/tumtraf-dataset. ",
        "title": "TUMTraf Event: Calibration and Fusion Resulting in a Dataset for  Roadside Event-Based and RGB Cameras",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08476",
        "abstract_url": "http://arxiv.org/abs/2401.08476",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Ziyuan"
            },
            {
                "last_name": "Bicz\u00f3k",
                "first_name": "Gergely"
            },
            {
                "last_name": "Liu",
                "first_name": "Mingyan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Misaligned incentives in secure software development have long been the focus of research in the economics of security. Product liability, a powerful legal framework in other industries, has been largely ineffective for software products until recent times. However, the rapid regulatory responses to recent global cyberattacks by both the United States and the European Union, together with the (relative) success of the General Data Protection Regulation in defining both duty and standard of care for software vendors, may just enable regulators to use liability to re-align incentives for the benefit of the digital society. Specifically, the recently proposed United States National Cybersecurity Strategy shifts responsibility for cyber incidents back to software vendors. In doing so, the strategy also puts forward the concept of the liability waiver: if a software company voluntarily undergoes and passes an IT security audit, its liability is waived.   In this paper, we analyze this audit scenario from the aspect of the software vendor. We propose a mechanism where a software vendor should first undergo a repeated auditing process in each stage of which the vendor decides whether to quit early or stay with additional security investment. We show that the optimal strategy for an opt-in vendor is to never quit; and exert cumulative investments in either \"one-and-done\" or \"incremental\" manner. We relate the audit mechanism to a liability waiver insurance policy and revealed its effect on reshaping the vendor's risk perception. We also discuss influence of audit quality on the vendor's incentives and pinpoint that a desirable audit rule should be highly accurate and less strict. ",
        "title": "Incentivizing Secure Software Development: The Role of Liability  (Waiver) and Audit",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08478",
        "abstract_url": "http://arxiv.org/abs/2401.08478",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Kaixin"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chen"
            },
            {
                "last_name": "Yuan",
                "first_name": "Chun"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Continuous offline reinforcement learning (CORL) combines continuous and offline reinforcement learning, enabling agents to learn multiple tasks from static datasets without forgetting prior tasks. However, CORL faces challenges in balancing stability and plasticity. Existing methods, employing Actor-Critic structures and experience replay (ER), suffer from distribution shifts, low efficiency, and weak knowledge-sharing. We aim to investigate whether Decision Transformer (DT), another offline RL paradigm, can serve as a more suitable offline continuous learner to address these issues. We first compare AC-based offline algorithms with DT in the CORL framework. DT offers advantages in learning efficiency, distribution shift mitigation, and zero-shot generalization but exacerbates the forgetting problem during supervised parameter updates. We introduce multi-head DT (MH-DT) and low-rank adaptation DT (LoRA-DT) to mitigate DT's forgetting problem. MH-DT stores task-specific knowledge using multiple heads, facilitating knowledge sharing with common components. It employs distillation and selective rehearsal to enhance current task learning when a replay buffer is available. In buffer-unavailable scenarios, LoRA-DT merges less influential weights and fine-tunes DT's decisive MLP layer to adapt to the current task. Extensive experiments on MoJuCo and Meta-World benchmarks demonstrate that our methods outperform SOTA CORL baselines and showcase enhanced learning capabilities and superior memory efficiency. ",
        "title": "Solving Continual Offline Reinforcement Learning with Decision  Transformer",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08484",
        "abstract_url": "http://arxiv.org/abs/2401.08484",
        "authors": [
            {
                "last_name": "Jard",
                "first_name": "Timoth\u00e9"
            },
            {
                "last_name": "Snaiki",
                "first_name": "Reda"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Downstream wind turbines operating behind upstream turbines face significant performance challenges due to reduced wind speeds and increased turbulence. This leads to decreased wind energy production and higher dynamic loads on downwind turbines. Consequently, real-time monitoring and control have become crucial for improving wind farm performance. One promising solution involves optimizing wind farm layouts in real-time, taking advantage of the added flexibility offered by floating offshore wind turbines (FOWTs). This study explores a dynamic layout optimization strategy to minimize wake effects in wind farms while meeting power requirements. Two scenarios are considered: power maximization and power set-point tracking. The methodology involves a centralized wind farm controller optimizing the layout, followed by wind turbine controllers to meet the prescribed targets. Each FOWT employs model predictive control to adjust aerodynamic thrust force. The control strategy integrates a dynamic wind farm model that considers floating platform motion and wake transport in changing wind conditions. In a case study with a 1x3 wind farm layout of 5 MW FOWTs, the results show a 25% increase in stable energy production compared to a static layout in one hour for the first scenario. In the second scenario, desired power production was swiftly and consistently achieved. ",
        "title": "Real-Time Dynamic Layout Optimization for Floating Offshore Wind Farm  Control",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08491",
        "abstract_url": "http://arxiv.org/abs/2401.08491",
        "authors": [
            {
                "last_name": "Klein",
                "first_name": "Tassilo"
            },
            {
                "last_name": "Nabi",
                "first_name": "Moin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful. ",
        "title": "Contrastive Perplexity for Controlled Generation: An Application in  Detoxifying Large Language Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08495",
        "abstract_url": "http://arxiv.org/abs/2401.08495",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Messi H. J."
            },
            {
                "last_name": "Montgomery",
                "first_name": "Jacob M."
            },
            {
                "last_name": "Lai",
                "first_name": "Calvin K."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have become pervasive in everyday life, yet their inner workings remain opaque. While scholarly efforts have demonstrated LLMs' propensity to reproduce biases in their training data, they have primarily focused on the association of social groups with stereotypic attributes. In this paper, we extend this line of inquiry to investigate a bias akin to the social-psychological phenomenon where socially dominant groups are perceived to be less homogeneous than socially subordinate groups as it is reproduced by LLMs. We had ChatGPT, a state-of-the-art LLM, generate a diversity of texts about intersectional group identities and compared text homogeneity. We consistently find that LLMs portray African, Asian, and Hispanic Americans as more homogeneous than White Americans. They also portray women as more homogeneous than men, but these differences are small. Finally, we find that the effect of gender differs across racial/ethnic groups such that the effect of gender is consistent within African and Hispanic Americans but not within Asian and White Americans. We speculate possible sources of this bias in LLMs and posit that the bias has the potential to amplify biases in future LLM training and to reinforce stereotypes. ",
        "title": "The Effect of Group Status on the Variability of Group Representations  in LLM-generated Text",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08497",
        "abstract_url": "http://arxiv.org/abs/2401.08497",
        "authors": [
            {
                "last_name": "Holand",
                "first_name": "Ethan"
            },
            {
                "last_name": "Homer",
                "first_name": "Jarrod"
            },
            {
                "last_name": "Storrer",
                "first_name": "Alex"
            },
            {
                "last_name": "Khandeker",
                "first_name": "Musheeera"
            },
            {
                "last_name": "Muhlon",
                "first_name": "Ethan F."
            },
            {
                "last_name": "Patel",
                "first_name": "Maulik"
            },
            {
                "last_name": "Vainqueur",
                "first_name": "Ben-oni"
            },
            {
                "last_name": "Antaki",
                "first_name": "David"
            },
            {
                "last_name": "Cooke",
                "first_name": "Naomi"
            },
            {
                "last_name": "Wilson",
                "first_name": "Chloe"
            },
            {
                "last_name": "Shafai",
                "first_name": "Bahram"
            },
            {
                "last_name": "Hanson",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Pad\u0131r",
                "first_name": "Ta\u015fk\u0131n"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We propose a novel, heterogeneous multi-agent architecture that miniaturizes rovers by outsourcing power generation to a central hub. By delegating power generation and distribution functions to this hub, the size, weight, power, and cost (SWAP-C) per rover are reduced, enabling efficient fleet scaling. As these rovers conduct mission tasks around the terrain, the hub charges an array of replacement battery modules. When a rover requires charging, it returns to the hub to initiate an autonomous docking sequence and exits with a fully charged battery. This confers an advantage over direct charging methods, such as wireless or wired charging, by replenishing a rover in minutes as opposed to hours, increasing net rover uptime.   This work shares an open-source platform developed to demonstrate battery swapping on unknown field terrain. We detail our design methodologies utilized for increasing system reliability, with a focus on optimization, robust mechanical design, and verification. Optimization of the system is discussed, including the design of passive guide rails through simulation-based optimization methods which increase the valid docking configuration space by 258%. The full system was evaluated during integrated testing, where an average servicing time of 98 seconds was achieved on surfaces with a gradient up to 10{\\deg}. We conclude by briefly proposing flight considerations for advancing the system toward a space-ready design. In sum, this prototype represents a proof of concept for autonomous docking and battery transfer on field terrain, advancing its Technology Readiness Level (TRL) from 1 to 3. ",
        "title": "Battery-Swapping Multi-Agent System for Sustained Operation of Large  Planetary Fleets",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08500",
        "abstract_url": "http://arxiv.org/abs/2401.08500",
        "authors": [
            {
                "last_name": "Ridnik",
                "first_name": "Tal"
            },
            {
                "last_name": "Kredo",
                "first_name": "Dedy"
            },
            {
                "last_name": "Friedman",
                "first_name": "Itamar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "SE"
        ],
        "abstract": "  Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium ",
        "title": "Code Generation with AlphaCodium: From Prompt Engineering to Flow  Engineering",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08501",
        "abstract_url": "http://arxiv.org/abs/2401.08501",
        "authors": [
            {
                "last_name": "Kahl",
                "first_name": "Kim-Celine"
            },
            {
                "last_name": "L\u00fcth",
                "first_name": "Carsten T."
            },
            {
                "last_name": "Zenk",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Maier-Hein",
                "first_name": "Klaus"
            },
            {
                "last_name": "Jaeger",
                "first_name": "Paul F."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Uncertainty estimation is an essential and heavily-studied component for the reliable application of semantic segmentation methods. While various studies exist claiming methodological advances on the one hand, and successful application on the other hand, the field is currently hampered by a gap between theory and practice leaving fundamental questions unanswered: Can data-related and model-related uncertainty really be separated in practice? Which components of an uncertainty method are essential for real-world performance? Which uncertainty method works well for which application? In this work, we link this research gap to a lack of systematic and comprehensive evaluation of uncertainty methods. Specifically, we identify three key pitfalls in current literature and present an evaluation framework that bridges the research gap by providing 1) a controlled environment for studying data ambiguities as well as distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for the five predominant uncertainty applications: OoD-detection, active learning, failure detection, calibration, and ambiguity modeling. Empirical results on simulated as well as real-world data demonstrate how the proposed framework is able to answer the predominant questions in the field revealing for instance that 1) separation of uncertainty types works on simulated data but does not necessarily translate to real-world data, 2) aggregation of scores is a crucial but currently neglected component of uncertainty methods, 3) While ensembles are performing most robustly across the different downstream tasks and settings, test-time augmentation often constitutes a light-weight alternative. Code is at: https://github.com/IML-DKFZ/values ",
        "title": "ValUES: A Framework for Systematic Validation of Uncertainty Estimation  in Semantic Segmentation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08503",
        "abstract_url": "http://arxiv.org/abs/2401.08503",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Zhenhui"
            },
            {
                "last_name": "Zhong",
                "first_name": "Tianyun"
            },
            {
                "last_name": "Ren",
                "first_name": "Yi"
            },
            {
                "last_name": "Yang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Weichuang"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Ziyue"
            },
            {
                "last_name": "He",
                "first_name": "Jinzheng"
            },
            {
                "last_name": "Huang",
                "first_name": "Rongjie"
            },
            {
                "last_name": "Liu",
                "first_name": "Jinglin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chen"
            },
            {
                "last_name": "Yin",
                "first_name": "Xiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Zejun"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zhou"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. ",
        "title": "Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08505",
        "abstract_url": "http://arxiv.org/abs/2401.08505",
        "authors": [
            {
                "last_name": "Coquelin",
                "first_name": "Daniel"
            },
            {
                "last_name": "Fl\u00fcgel",
                "first_name": "Katharina"
            },
            {
                "last_name": "Weiel",
                "first_name": "Marie"
            },
            {
                "last_name": "Kiefer",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Debus",
                "first_name": "Charlotte"
            },
            {
                "last_name": "Streit",
                "first_name": "Achim"
            },
            {
                "last_name": "G\u00f6tz",
                "first_name": "Markus"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study explores the learning dynamics of neural networks by analyzing the singular value decomposition (SVD) of their weights throughout training. Our investigation reveals that an orthogonal basis within each multidimensional weight's SVD representation stabilizes during training. Building upon this, we introduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a novel training method exploiting the intrinsic orthogonality of neural networks. OIALR seamlessly integrates into existing training workflows with minimal accuracy loss, as demonstrated by benchmarking on various datasets and well-established network architectures. With appropriate hyperparameter tuning, OIALR can surpass conventional training setups, including those of state-of-the-art models. ",
        "title": "Harnessing Orthogonality to Train Low-Rank Neural Networks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08506",
        "abstract_url": "http://arxiv.org/abs/2401.08506",
        "authors": [
            {
                "last_name": "Ajao",
                "first_name": "Oluwaseun"
            },
            {
                "last_name": "Bhowmik",
                "first_name": "Deepayan"
            },
            {
                "last_name": "Zargari",
                "first_name": "Shahrzad"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Inferring locations from user texts on social media platforms is a non-trivial and challenging problem relating to public safety. We propose a novel non-uniform grid-based approach for location inference from Twitter messages using Quadtree spatial partitions. The proposed algorithm uses natural language processing (NLP) for semantic understanding and incorporates Cosine similarity and Jaccard similarity measures for feature vector extraction and dimensionality reduction. We chose Twitter as our experimental social media platform due to its popularity and effectiveness for the dissemination of news and stories about recent events happening around the world. Our approach is the first of its kind to make location inference from tweets using Quadtree spatial partitions and NLP, in hybrid word-vector representations. The proposed algorithm achieved significant classification accuracy and outperformed state-of-the-art grid-based content-only location inference methods by up to 24% in correctly predicting tweet locations within a 161km radius and by 300km in median error distance on benchmark datasets. ",
        "title": "Content-Aware Tweet Location Inference using Quadtree Spatial  Partitioning and Jaccard-Cosine Word Embedding",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08508",
        "abstract_url": "http://arxiv.org/abs/2401.08508",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Yang",
                "first_name": "Kailai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianlin"
            },
            {
                "last_name": "Xie",
                "first_name": "Qianqian"
            },
            {
                "last_name": "Yu",
                "first_name": "Zeping"
            },
            {
                "last_name": "Ananiadou",
                "first_name": "Sophia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of LLMs, researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on various classification and regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 14 tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our model with a variety of LLMs on AEB, where our models outperform all other open-sourced LLMs, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools. ",
        "title": "EmoLLMs: A Series of Emotional Large Language Models and Annotation  Tools for Comprehensive Affective Analysis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08511",
        "abstract_url": "http://arxiv.org/abs/2401.08511",
        "authors": [
            {
                "last_name": "Kaneko",
                "first_name": "Masahiro"
            },
            {
                "last_name": "Bollegala",
                "first_name": "Danushka"
            },
            {
                "last_name": "Baldwin",
                "first_name": "Timothy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The output tendencies of Pre-trained Language Models (PLM) vary markedly before and after Fine-Tuning (FT) due to the updates to the model parameters. These divergences in output tendencies result in a gap in the social biases of PLMs. For example, there exits a low correlation between intrinsic bias scores of a PLM and its extrinsic bias scores under FT-based debiasing methods. Additionally, applying FT-based debiasing methods to a PLM leads to a decline in performance in downstream tasks. On the other hand, PLMs trained on large datasets can learn without parameter updates via In-Context Learning (ICL) using prompts. ICL induces smaller changes to PLMs compared to FT-based debiasing methods. Therefore, we hypothesize that the gap observed in pre-trained and FT models does not hold true for debiasing methods that use ICL. In this study, we demonstrate that ICL-based debiasing methods show a higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods. Moreover, the performance degradation due to debiasing is also lower in the ICL case compared to that in the FT case. ",
        "title": "The Gaps between Pre-train and Downstream Settings in Bias Evaluation  and Debiasing",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08513",
        "abstract_url": "http://arxiv.org/abs/2401.08513",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Rahul"
            },
            {
                "last_name": "Redyuk",
                "first_name": "Sergey"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Sumantrak"
            },
            {
                "last_name": "Sipka",
                "first_name": "Andrea"
            },
            {
                "last_name": "Vollmer",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Selby",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Explainable AI (XAI) and interpretable machine learning methods help to build trust in model predictions and derived insights, yet also present a perverse incentive for analysts to manipulate XAI metrics to support pre-specified conclusions. This paper introduces the concept of X-hacking, a form of p-hacking applied to XAI metrics such as Shap values. We show how an automated machine learning pipeline can be used to search for 'defensible' models that produce a desired explanation while maintaining superior predictive performance to a common baseline. We formulate the trade-off between explanation and accuracy as a multi-objective optimization problem and illustrate the feasibility and severity of X-hacking empirically on familiar real-world datasets. Finally, we suggest possible methods for detection and prevention, and discuss ethical implications for the credibility and reproducibility of XAI research. ",
        "title": "X Hacking: The Threat of Misguided AutoML",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08514",
        "abstract_url": "http://arxiv.org/abs/2401.08514",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Bohang"
            },
            {
                "last_name": "Gai",
                "first_name": "Jingchu"
            },
            {
                "last_name": "Du",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Ye",
                "first_name": "Qiwei"
            },
            {
                "last_name": "He",
                "first_name": "Di"
            },
            {
                "last_name": "Wang",
                "first_name": "Liwei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DM",
            "DS"
        ],
        "abstract": "  Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a unified framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric. ",
        "title": "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN  Expressiveness",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08517",
        "abstract_url": "http://arxiv.org/abs/2401.08517",
        "authors": [
            {
                "last_name": "Abu-Rasheed",
                "first_name": "Hasan"
            },
            {
                "last_name": "Abdulsalam",
                "first_name": "Mohamad Hussam"
            },
            {
                "last_name": "Weber",
                "first_name": "Christian"
            },
            {
                "last_name": "Fathi",
                "first_name": "Madjid"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's context. A group chat approach is developed to connect students with human mentors, either on demand or in cases that exceed the chatbot's pre-defined tasks. We evaluate the chatbot with a user study, to provide a proof-of-concept and highlight the potential requirements and limitations of utilizing chatbots in conversational explainability. ",
        "title": "Supporting Student Decisions on Learning Recommendations: An LLM-Based  Chatbot with Knowledge Graph Contextualization for Conversational  Explainability and Mentoring",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08518",
        "abstract_url": "http://arxiv.org/abs/2401.08518",
        "authors": [
            {
                "last_name": "Erler",
                "first_name": "Philipp"
            },
            {
                "last_name": "Fuentes",
                "first_name": "Lizeth"
            },
            {
                "last_name": "Hermosilla",
                "first_name": "Pedro"
            },
            {
                "last_name": "Guerrero",
                "first_name": "Paul"
            },
            {
                "last_name": "Wimmer",
                "first_name": "Renato Pajarola Michael"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  3D surface reconstruction from point clouds is a key step in areas such as content creation, archaeology, digital cultural heritage, and engineering. Current approaches either try to optimize a non-data-driven surface representation to fit the points, or learn a data-driven prior over the distribution of commonly occurring surfaces and how they correlate with potentially noisy point clouds. Data-driven methods enable robust handling of noise and typically either focus on a global or a local prior, which trade-off between robustness to noise on the global end and surface detail preservation on the local end. We propose PPSurf as a method that combines a global prior based on point convolutions and a local prior based on processing local point cloud patches. We show that this approach is robust to noise while recovering surface details more accurately than the current state-of-the-art.   Our source code, pre-trained model and dataset are available at: https://github.com/cg-tuwien/ppsurf ",
        "title": "PPSURF: Combining Patches and Point Convolutions for Detailed Surface  Reconstruction",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08519",
        "abstract_url": "http://arxiv.org/abs/2401.08519",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yanbang"
            },
            {
                "last_name": "Kleinberg",
                "first_name": "Jon"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR",
            "SI"
        ],
        "abstract": "  We study the implications of the modeling choice to use a graph, instead of a hypergraph, to represent real-world interconnected systems whose constituent relationships are of higher order by nature. Such a modeling choice typically involves an underlying projection process that maps the original hypergraph onto a graph, and is common in graph-based analysis. While hypergraph projection can potentially lead to loss of higher-order relations, there exists very limited studies on the consequences of doing so, as well as its remediation. This work fills this gap by doing two things: (1) we develop analysis based on graph and set theory, showing two ubiquitous patterns of hyperedges that are root to structural information loss in all hypergraph projections; we also quantify the combinatorial impossibility of recovering the lost higher-order structures if no extra help is provided; (2) we still seek to recover the lost higher-order structures in hypergraph projection, and in light of (1)'s findings we propose to relax the problem into a learning-based setting. Under this setting, we develop a learning-based hypergraph reconstruction method based on an important statistic of hyperedge distributions that we find. Our reconstruction method is evaluated on 8 real-world datasets under different settings, and exhibits consistently good performance. We also demonstrate benefits of the reconstructed hypergraphs via use cases of protein rankings and link predictions. ",
        "title": "From Graphs to Hypergraphs: Hypergraph Projection and its Remediation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08520",
        "abstract_url": "http://arxiv.org/abs/2401.08520",
        "authors": [
            {
                "last_name": "Arora",
                "first_name": "Sanidhay"
            },
            {
                "last_name": "Li",
                "first_name": "Yingjiu"
            },
            {
                "last_name": "Feng",
                "first_name": "Yebo"
            },
            {
                "last_name": "Xu",
                "first_name": "Jiahua"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CE"
        ],
        "abstract": "  The evolving landscape of Decentralized Finance (DeFi) has raised critical security concerns, especially pertaining to Protocols for Loanable Funds (PLFs) and their dependency on price oracles, which are susceptible to manipulation. The emergence of flash loans has further amplified these risks, enabling increasingly complex oracle manipulation attacks that can lead to significant financial losses. Responding to this threat, we first dissect the attack mechanism by formalizing the standard operational and adversary models for PLFs. Based on our analysis, we propose SecPLF, a robust and practical solution designed to counteract oracle manipulation attacks efficiently. SecPLF operates by tracking a price state for each crypto-asset, including the recent price and the timestamp of its last update. By imposing price constraints on the price oracle usage, SecPLF ensures a PLF only engages a price oracle if the last recorded price falls within a defined threshold, thereby negating the profitability of potential attacks. Our evaluation based on historical market data confirms SecPLF's efficacy in providing high-confidence prevention against arbitrage attacks that arise due to minor price differences. SecPLF delivers proactive protection against oracle manipulation attacks, offering ease of implementation, oracle-agnostic property, and resource and cost efficiency. ",
        "title": "SecPLF: Secure Protocols for Loanable Funds against Oracle Manipulation  Attacks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08522",
        "abstract_url": "http://arxiv.org/abs/2401.08522",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Zihao"
            },
            {
                "last_name": "Guan",
                "first_name": "Fengbin"
            },
            {
                "last_name": "Lu",
                "first_name": "Yiting"
            },
            {
                "last_name": "Li",
                "first_name": "Xin"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhibo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  The objective of non-reference video quality assessment is to evaluate the quality of distorted video without access to reference high-definition references. In this study, we introduce an enhanced spatial perception module, pre-trained on multiple image quality assessment datasets, and a lightweight temporal fusion module to address the no-reference visual quality assessment (NR-VQA) task. This model implements Swin Transformer V2 as a local-level spatial feature extractor and fuses these multi-stage representations through a series of transformer layers. Furthermore, a temporal transformer is utilized for spatiotemporal feature fusion across the video. To accommodate compressed videos of varying bitrates, we incorporate a coarse-to-fine contrastive strategy to enrich the model's capability to discriminate features from videos of different bitrates. This is an expanded version of the one-page abstract. ",
        "title": "Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine  Strategy",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08525",
        "abstract_url": "http://arxiv.org/abs/2401.08525",
        "authors": [
            {
                "last_name": "Zolna",
                "first_name": "Konrad"
            },
            {
                "last_name": "Cabi",
                "first_name": "Serkan"
            },
            {
                "last_name": "Chen",
                "first_name": "Yutian"
            },
            {
                "last_name": "Lau",
                "first_name": "Eric"
            },
            {
                "last_name": "Fantacci",
                "first_name": "Claudio"
            },
            {
                "last_name": "Pasukonis",
                "first_name": "Jurgis"
            },
            {
                "last_name": "Springenberg",
                "first_name": "Jost Tobias"
            },
            {
                "last_name": "Colmenarejo",
                "first_name": "Sergio Gomez"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "RO"
        ],
        "abstract": "  As the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems. ",
        "title": "GATS: Gather-Attend-Scatter",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08527",
        "abstract_url": "http://arxiv.org/abs/2401.08527",
        "authors": [
            {
                "last_name": "Bie",
                "first_name": "Yequan"
            },
            {
                "last_name": "Luo",
                "first_name": "Luyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Black-box deep learning approaches have showcased significant potential in the realm of medical image analysis. However, the stringent trustworthiness requirements intrinsic to the medical field have catalyzed research into the utilization of Explainable Artificial Intelligence (XAI), with a particular focus on concept-based methods. Existing concept-based methods predominantly apply concept annotations from a single perspective (e.g., global level), neglecting the nuanced semantic relationships between sub-regions and concepts embedded within medical images. This leads to underutilization of the valuable medical information and may cause models to fall short in harmoniously balancing interpretability and performance when employing inherently interpretable architectures such as Concept Bottlenecks. To mitigate these shortcomings, we propose a multi-modal explainable disease diagnosis framework that meticulously aligns medical images and clinical-related concepts semantically at multiple strata, encompassing the image level, token level, and concept level. Moreover, our method allows for model intervention and offers both textual and visual explanations in terms of human-interpretable concepts. Experimental results on three skin image datasets demonstrate that our method, while preserving model interpretability, attains high performance and label efficiency for concept detection and disease diagnosis. ",
        "title": "MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level  Image-Concept Alignment",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08534",
        "abstract_url": "http://arxiv.org/abs/2401.08534",
        "authors": [
            {
                "last_name": "Moreira",
                "first_name": "Ricardo"
            },
            {
                "last_name": "Bono",
                "first_name": "Jacopo"
            },
            {
                "last_name": "Cardoso",
                "first_name": "M\u00e1rio"
            },
            {
                "last_name": "Saleiro",
                "first_name": "Pedro"
            },
            {
                "last_name": "Figueiredo",
                "first_name": "M\u00e1rio A. T."
            },
            {
                "last_name": "Bizarro",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "HC"
        ],
        "abstract": "  Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Because of this, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts. ",
        "title": "DiConStruct: Causal Concept-based Explanations through Black-Box  Distillation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08536",
        "abstract_url": "http://arxiv.org/abs/2401.08536",
        "authors": [
            {
                "last_name": "Pal",
                "first_name": "Anuj"
            },
            {
                "last_name": "He",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiang"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Koopman operator approach for data-driven control design of a nonlinear system is on the rise because of its capability to capture the behaviours of global dynamics. However, the measurement noises of inputs and outputs will bias the Koopman model identification and cause model mismatch from the actual nonlinear dynamics. The current work evaluates the bounds of the noise-induced model bias of the Koopman operator model and proposes a data-driven robust dual-loop control framework (Koopman based robust control-KROC) for the biased model. First, the model mismatch is found bounded under radial basis functions (RBF) and the bounded noises, and the bound of model mismatch is assessed. Second, the pitfalls of linear quadratic Gaussian (LQG) control based on the biased Koopman model of Van Der Pol oscillator are shown. Motivated from the pitfalls, the dual-loop control is proposed, which consist of an observer-based state-feedback control based on the nominal Koopman model and an additional robust loop to compensate model mismatch. A linear matrix inequality (LMI) is derived, which can guarantee robust stability and performance under bounded noises for the finite-dimensional Koopman operator model. Finally, the proposed framework is implemented to a nonlinear Van Der Pol oscillator to demonstrate enhanced control performance by the dual-loop robust control. ",
        "title": "Dual-Loop Robust Control of Biased Koopman Operator Model by Noisy Data  of Nonlinear Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08537",
        "abstract_url": "http://arxiv.org/abs/2401.08537",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Emily"
            },
            {
                "last_name": "Widdows",
                "first_name": "Dominic"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As a tech company, Grab has expanded from transportation to food delivery, aiming to serve Southeast Asia with hyperlocalized applications. Information about places as transportation destinations can help to improve our knowledge about places as restaurants, so long as the spatial entity resolution problem between these datasets can be solved. In this project, we attempted to recognize identical place entities from databases of Points-of-Interest (POI) and GrabFood restaurants, using their spatial and textual attributes, i.e., latitude, longitude, place name, and street address.   Distance metrics were calculated for these attributes and fed to tree-based classifiers. POI-restaurant matching was conducted separately for Singapore, Philippines, Indonesia, and Malaysia. Experimental estimates demonstrate that a matching POI can be found for over 35% of restaurants in these countries. As part of these estimates, test datasets were manually created, and RandomForest, AdaBoost, Gradient Boosting, and XGBoost perform well, with most accuracy, precision, and recall scores close to or higher than 90% for matched vs. unmatched classification. To the authors' knowledge, there are no previous published scientific papers devoted to matching of spatial entities for the Southeast Asia region. ",
        "title": "Spatial Entity Resolution between Restaurant Locations and  Transportation Destinations in Southeast Asia",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08539",
        "abstract_url": "http://arxiv.org/abs/2401.08539",
        "authors": [
            {
                "last_name": "Legay",
                "first_name": "Bastien"
            },
            {
                "last_name": "Latapy",
                "first_name": "Matthieu"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  We consider the following problem : we have a high-resolution street network of a given city, and low-resolution measurements of traffic within this city. We want to associate to each measurement the set of streets corresponding to the observed traffic. To do so, we take benefit of specific properties of these data to match measured links to links in the street network. We propose several success criteria for the obtained matching. They show that the matching algorithm generally performs very well, and they give complementary ways to detect data discrepancies that makes any matching highly dubious. ",
        "title": "Mapping low-resolution edges to high-resolution paths: the case of  traffic measurements in cities",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08541",
        "abstract_url": "http://arxiv.org/abs/2401.08541",
        "authors": [
            {
                "last_name": "El-Nouby",
                "first_name": "Alaaeldin"
            },
            {
                "last_name": "Klein",
                "first_name": "Michal"
            },
            {
                "last_name": "Zhai",
                "first_name": "Shuangfei"
            },
            {
                "last_name": "Bautista",
                "first_name": "Miguel Angel"
            },
            {
                "last_name": "Toshev",
                "first_name": "Alexander"
            },
            {
                "last_name": "Shankar",
                "first_name": "Vaishaal"
            },
            {
                "last_name": "Susskind",
                "first_name": "Joshua M"
            },
            {
                "last_name": "Joulin",
                "first_name": "Armand"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2 billion images, that achieves 84.0% on ImageNet-1k with a frozen trunk. Interestingly, even at this scale, we observe no sign of saturation in performance, suggesting that AIM potentially represents a new frontier for training large-scale vision models. The pre-training of AIM is similar to the pre-training of LLMs, and does not require any image-specific strategy to stabilize the training at scale. ",
        "title": "Scalable Pre-training of Large Autoregressive Image Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08544",
        "abstract_url": "http://arxiv.org/abs/2401.08544",
        "authors": [
            {
                "last_name": "Baek",
                "first_name": "Jonghyuk"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanran"
            },
            {
                "last_name": "Chen",
                "first_name": "J. S."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Conventional finite element methods are known to be tedious in adaptive refinements due to their conformal regularity requirements. Further, the enrichment functions for adaptive refinements are often not readily available in general applications. This work introduces a novel neural network-enriched Partition of Unity (NN-PU) approach for solving boundary value problems via artificial neural networks with a potential energy-based loss function minimization. The flexibility and adaptivity of the NN function space are utilized to capture complex solution patterns that the conventional Galerkin methods fail to capture. The NN enrichment is constructed by combining pre-trained feature-encoded NN blocks with an additional untrained NN block. The pre-trained NN blocks learn specific local features during the offline stage, enabling efficient enrichment of the approximation space during the online stage through the Ritz-type energy minimization. The NN enrichment is introduced under the Partition of Unity (PU) framework, ensuring convergence of the proposed method. The proposed NN-PU approximation and feature-encoded transfer learning forms an adaptive approximation framework, termed the neural-refinement (n-refinement), for solving boundary value problems. Demonstrated by solving various elasticity problems, the proposed method offers accurate solutions while notably reducing the computational cost compared to the conventional adaptive refinement in the mesh-based methods. ",
        "title": "N-Adaptive Ritz Method: A Neural Network Enriched Partition of Unity for  Boundary Value Problems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08550",
        "abstract_url": "http://arxiv.org/abs/2401.08550",
        "authors": [
            {
                "last_name": "Leng",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Joseph"
            },
            {
                "last_name": "Peng",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Xiaodi"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Many promising quantum applications depend on the efficient quantum simulation of an exponentially large sparse Hamiltonian, a task known as sparse Hamiltonian simulation, which is fundamentally important in quantum computation. Although several theoretically appealing quantum algorithms have been proposed for this task, they typically require a black-box query model of the sparse Hamiltonian, rendering them impractical for near-term implementation on quantum devices.   In this paper, we propose a technique named Hamiltonian embedding. This technique simulates a desired sparse Hamiltonian by embedding it into the evolution of a larger and more structured quantum system, allowing for more efficient simulation through hardware-efficient operations. We conduct a systematic study of this new technique and demonstrate significant savings in computational resources for implementing prominent quantum applications. As a result, we can now experimentally realize quantum walks on complicated graphs (e.g., binary trees, glued-tree graphs), quantum spatial search, and the simulation of real-space Schr\\\"odinger equations on current trapped-ion and neutral-atom platforms. Given the fundamental role of Hamiltonian evolution in the design of quantum algorithms, our technique markedly expands the horizon of implementable quantum advantages in the NISQ era. ",
        "title": "Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian  Embedding",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08552",
        "abstract_url": "http://arxiv.org/abs/2401.08552",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zichuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yingying"
            },
            {
                "last_name": "Wang",
                "first_name": "Tianchun"
            },
            {
                "last_name": "Wang",
                "first_name": "Zefan"
            },
            {
                "last_name": "Luo",
                "first_name": "Dongsheng"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            },
            {
                "last_name": "Wu",
                "first_name": "Min"
            },
            {
                "last_name": "Wang",
                "first_name": "Yi"
            },
            {
                "last_name": "Chen",
                "first_name": "Chunlin"
            },
            {
                "last_name": "Fan",
                "first_name": "Lunting"
            },
            {
                "last_name": "Wen",
                "first_name": "Qingsong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The code is available for review: https://anonymous.4open.science/r/ContraLSP-1146/ ",
        "title": "Explaining Time Series via Contrastive and Locally Sparse Perturbations",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08553",
        "abstract_url": "http://arxiv.org/abs/2401.08553",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Jianlan"
            },
            {
                "last_name": "Xu",
                "first_name": "Charles"
            },
            {
                "last_name": "Liu",
                "first_name": "Fangchen"
            },
            {
                "last_name": "Tan",
                "first_name": "Liam"
            },
            {
                "last_name": "Lin",
                "first_name": "Zipeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Abbeel",
                "first_name": "Pieter"
            },
            {
                "last_name": "Levine",
                "first_name": "Sergey"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we propose a real-world benchmark for studying robotic learning in the context of functional manipulation: a robot needs to accomplish complex long-horizon behaviors by composing individual manipulation skills in functionally relevant ways. The core design principles of our Functional Manipulation Benchmark (FMB) emphasize a harmonious balance between complexity and accessibility. Tasks are deliberately scoped to be narrow, ensuring that models and datasets of manageable scale can be utilized effectively to track progress. Simultaneously, they are diverse enough to pose a significant generalization challenge. Furthermore, the benchmark is designed to be easily replicable, encompassing all essential hardware and software components. To achieve this goal, FMB consists of a variety of 3D-printed objects designed for easy and accurate replication by other researchers. The objects are procedurally generated, providing a principled framework to study generalization in a controlled fashion. We focus on fundamental manipulation skills, including grasping, repositioning, and a range of assembly behaviors. The FMB can be used to evaluate methods for acquiring individual skills, as well as methods for combining and ordering such skills to solve complex, multi-stage manipulation tasks. We also offer an imitation learning framework that includes a suite of policies trained to solve the proposed tasks. This enables researchers to utilize our tasks as a versatile toolkit for examining various parts of the pipeline. For example, researchers could propose a better design for a grasping controller and evaluate it in combination with our baseline reorientation and assembly policies as part of a pipeline for solving multi-stage tasks. Our dataset, object CAD files, code, and evaluation videos can be found on our project website: https://functional-manipulation-benchmark.github.io ",
        "title": "FMB: a Functional Manipulation Benchmark for Generalizable Robotic  Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08558",
        "abstract_url": "http://arxiv.org/abs/2401.08558",
        "authors": [
            {
                "last_name": "Lamarre",
                "first_name": "Olivier"
            },
            {
                "last_name": "Malhotra",
                "first_name": "Shantanu"
            },
            {
                "last_name": "Kelly",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Exploration of the lunar south pole with a solar-powered rover is challenging due to the highly dynamic solar illumination conditions and the presence of permanently shadowed regions (PSRs). In turn, careful planning in space and time is essential. Mission-level path planning is a global, spatiotemporal paradigm that addresses this challenge, taking into account rover resources and mission requirements. However, existing approaches do not proactively account for random disturbances, such as recurring faults, that may temporarily delay rover traverse progress. In this paper, we formulate a chance-constrained mission-level planning problem for the exploration of PSRs by a solar-powered rover affected by random faults. The objective is to find a policy that visits as many waypoints of scientific interest as possible while respecting an upper bound on the probability of mission failure.   Our approach assumes that faults occur randomly, but at a known, constant average rate. Each fault is resolved within a fixed time, simulating the recovery period of an autonomous system or the time required for a team of human operators to intervene. Unlike solutions based upon dynamic programming alone, our method breaks the chance-constrained optimization problem into smaller offline and online subtasks to make the problem computationally tractable. Specifically, our solution combines existing mission-level path planning techniques with a stochastic reachability analysis component. We find mission plans that remain within reach of safety throughout large state spaces. To empirically validate our algorithm, we simulate mission scenarios using orbital terrain and illumination maps of Cabeus Crater. Results from simulations of multi-day, long-range drives in the LCROSS impact region are also presented. ",
        "title": "Safe Mission-Level Path Planning for Exploration of Lunar Shadowed  Regions by a Solar-Powered Rover",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08559",
        "abstract_url": "http://arxiv.org/abs/2401.08559",
        "authors": [
            {
                "last_name": "Petrovich",
                "first_name": "Mathis"
            },
            {
                "last_name": "Litany",
                "first_name": "Or"
            },
            {
                "last_name": "Iqbal",
                "first_name": "Umar"
            },
            {
                "last_name": "Black",
                "first_name": "Michael J."
            },
            {
                "last_name": "Varol",
                "first_name": "G\u00fcl"
            },
            {
                "last_name": "Peng",
                "first_name": "Xue Bin"
            },
            {
                "last_name": "Rempe",
                "first_name": "Davis"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "LG"
        ],
        "abstract": "  Recent advances in generative modeling have led to promising progress on synthesizing 3D human motion from text, with methods that can generate character animations from short prompts and specified durations. However, using a single text prompt as input lacks the fine-grained control needed by animators, such as composing multiple actions and defining precise durations for parts of the motion. To address this, we introduce the new problem of timeline control for text-driven motion synthesis, which provides an intuitive, yet fine-grained, input interface for users. Instead of a single prompt, users can specify a multi-track timeline of multiple prompts organized in temporal intervals that may overlap. This enables specifying the exact timings of each action and composing multiple actions in sequence or at overlapping intervals. To generate composite animations from a multi-track timeline, we propose a new test-time denoising method. This method can be integrated with any pre-trained motion diffusion model to synthesize realistic motions that accurately reflect the timeline. At every step of denoising, our method processes each timeline interval (text prompt) individually, subsequently aggregating the predictions with consideration for the specific body parts engaged in each action. Experimental comparisons and ablations validate that our method produces realistic motions that respect the semantics and timing of given text prompts. Our code and models are publicly available at https://mathis.petrovich.fr/stmc. ",
        "title": "Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08561",
        "abstract_url": "http://arxiv.org/abs/2401.08561",
        "authors": [
            {
                "last_name": "D'Angelo",
                "first_name": "Andrea"
            },
            {
                "last_name": "Di Sipio",
                "first_name": "Claudio"
            },
            {
                "last_name": "Politowsky",
                "first_name": "Cristiano"
            },
            {
                "last_name": "Rubei",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Being predominant in digital entertainment for decades, video games have been recognized as valuable software artifacts by the software engineering (SE) community just recently. Such an acknowledgment has unveiled several research opportunities, spanning from empirical studies to the application of AI techniques for classification tasks. In this respect, several curated game datasets have been disclosed for research purposes even though the collected data are insufficient to support the application of advanced models or to enable interdisciplinary studies. Moreover, the majority of those are limited to PC games, thus excluding notorious gaming platforms, e.g., PlayStation, Xbox, and Nintendo. In this paper, we propose PlayMyData, a curated dataset composed of 99,864 multi-platform games gathered by IGDB website. By exploiting a dedicated API, we collect relevant metadata for each game, e.g., description, genre, rating, gameplay video URLs, and screenshots. Furthermore, we enrich PlayMyData with the timing needed to complete each game by mining the HLTB website. To the best of our knowledge, this is the most comprehensive dataset in the domain that can be used to support different automated tasks in SE. More importantly, PlayMyData can be used to foster cross-domain investigations built on top of the provided multimedia data. ",
        "title": "PlayMyData: a curated dataset of multi-platform video games",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08562",
        "abstract_url": "http://arxiv.org/abs/2401.08562",
        "authors": [
            {
                "last_name": "Goyens",
                "first_name": "Florentin"
            },
            {
                "last_name": "Cartis",
                "first_name": "Coralia"
            },
            {
                "last_name": "Chr\u00e9tien",
                "first_name": "St\u00e9phane"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We consider the point cloud registration problem, the task of finding a transformation between two point clouds that represent the same object but are expressed in different coordinate systems. Our approach is not based on a point-to-point correspondence, matching every point in the source point cloud to a point in the target point cloud. Instead, we assume and leverage a low-dimensional nonlinear geometric structure of the data. Firstly, we approximate each point cloud by an algebraic variety (a set defined by finitely many polynomial equations). This is done by solving an optimization problem on the Grassmann manifold, using a connection between algebraic varieties and polynomial bases. Secondly, we solve an optimization problem on the orthogonal group to find the transformation (rotation $+$ translation) which makes the two algebraic varieties overlap. We use second-order Riemannian optimization methods for the solution of both steps. Numerical experiments on real and synthetic data are provided, with encouraging results. Our approach is particularly useful when the two point clouds describe different parts of an objects (which may not even be overlapping), on the condition that the surface of the object may be well approximated by a set of polynomial equations. The first procedure -- the approximation -- is of independent interest, as it can be used for denoising data that belongs to an algebraic variety. We provide statistical guarantees for the estimation error of the denoising using Stein's unbiased estimator. ",
        "title": "Registration of algebraic varieties using Riemannian optimization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08564",
        "abstract_url": "http://arxiv.org/abs/2401.08564",
        "authors": [
            {
                "last_name": "Baharlouei",
                "first_name": "Hamideh"
            },
            {
                "last_name": "Makanju",
                "first_name": "Adetokunbo"
            },
            {
                "last_name": "Zincir-Heywood",
                "first_name": "Nur"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  In the domain of Vehicular Ad hoc Networks (VANETs), where the imperative of having a real-world malicious detector capable of detecting attacks in real-time and unveiling their perpetrators is crucial, our study introduces a system with this goal. This system is designed for real-time detection of malicious behavior, addressing the critical need to first identify the onset of attacks and subsequently the responsible actors. Prior work in this area have never addressed both requirements, which we believe are necessary for real world deployment, simultaneously. By seamlessly integrating statistical and machine learning techniques, the proposed system prioritizes simplicity and efficiency. It excels in swiftly detecting attack onsets with a remarkable F1-score of 99.66%, subsequently identifying malicious vehicles with an average F1-score of approximately 97.85%. Incorporating federated learning in both stages enhances privacy and improves the efficiency of malicious node detection, effectively reducing the false negative rate. ",
        "title": "ADVENT: Attack/Anomaly Detection in VANETs",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08565",
        "abstract_url": "http://arxiv.org/abs/2401.08565",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Alisa"
            },
            {
                "last_name": "Han",
                "first_name": "Xiaochuang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yizhong"
            },
            {
                "last_name": "Tsvetkov",
                "first_name": "Yulia"
            },
            {
                "last_name": "Choi",
                "first_name": "Yejin"
            },
            {
                "last_name": "Smith",
                "first_name": "Noah A."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance. ",
        "title": "Tuning Language Models by Proxy",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08567",
        "abstract_url": "http://arxiv.org/abs/2401.08567",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuhui"
            },
            {
                "last_name": "Sui",
                "first_name": "Elaine"
            },
            {
                "last_name": "Yeung-Levy",
                "first_name": "Serena"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "CV"
        ],
        "abstract": "  Building cross-modal applications is challenging due to limited paired multi-modal data. Recent works have shown that leveraging a pre-trained multi-modal contrastive representation space enables cross-modal tasks to be learned from uni-modal data. This is based on the assumption that contrastive optimization makes embeddings from different modalities interchangeable. However, this assumption is under-explored due to the poorly understood geometry of the multi-modal contrastive space, where a modality gap exists. In our study, we provide a theoretical explanation of this space's geometry and introduce a three-step method, $C^3$ (Connect, Collapse, Corrupt), to bridge the modality gap, enhancing the interchangeability of embeddings. Our $C^3$ method significantly improves cross-modal learning from uni-modal data, achieving state-of-the-art results on zero-shot image / audio / video captioning and text-to-image generation. ",
        "title": "Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal  Data",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08570",
        "abstract_url": "http://arxiv.org/abs/2401.08570",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Siwei"
            },
            {
                "last_name": "Bhatnagar",
                "first_name": "Bharat Lal"
            },
            {
                "last_name": "Xu",
                "first_name": "Yuanlu"
            },
            {
                "last_name": "Winkler",
                "first_name": "Alexander"
            },
            {
                "last_name": "Kadlecek",
                "first_name": "Petr"
            },
            {
                "last_name": "Tang",
                "first_name": "Siyu"
            },
            {
                "last_name": "Bogo",
                "first_name": "Federica"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose RoHM, an approach for robust 3D human motion reconstruction from monocular RGB(-D) videos in the presence of noise and occlusions. Most previous approaches either train neural networks to directly regress motion in 3D or learn data-driven motion priors and combine them with optimization at test time. The former do not recover globally coherent motion and fail under occlusions; the latter are time-consuming, prone to local minima, and require manual tuning. To overcome these shortcomings, we exploit the iterative, denoising nature of diffusion models. RoHM is a novel diffusion-based motion model that, conditioned on noisy and occluded input data, reconstructs complete, plausible motions in consistent global coordinates. Given the complexity of the problem -- requiring one to address different tasks (denoising and infilling) in different solution spaces (local and global motion) -- we decompose it into two sub-tasks and learn two models, one for global trajectory and one for local motion. To capture the correlations between the two, we then introduce a novel conditioning module, combining it with an iterative inference scheme. We apply RoHM to a variety of tasks -- from motion reconstruction and denoising to spatial and temporal infilling. Extensive experiments on three popular datasets show that our method outperforms state-of-the-art approaches qualitatively and quantitatively, while being faster at test time. The code will be available at https://sanweiliti.github.io/ROHM/ROHM.html. ",
        "title": "RoHM: Robust Human Motion Reconstruction via Diffusion",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08572",
        "abstract_url": "http://arxiv.org/abs/2401.08572",
        "authors": [
            {
                "last_name": "Agnew",
                "first_name": "William"
            },
            {
                "last_name": "Bergman",
                "first_name": "A. Stevie"
            },
            {
                "last_name": "Chien",
                "first_name": "Jennifer"
            },
            {
                "last_name": "D\u00edaz",
                "first_name": "Mark"
            },
            {
                "last_name": "El-Sayed",
                "first_name": "Seliem"
            },
            {
                "last_name": "Pittman",
                "first_name": "Jaylen"
            },
            {
                "last_name": "Mohamed",
                "first_name": "Shakir"
            },
            {
                "last_name": "McKee",
                "first_name": "Kevin R."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Human participants play a central role in the development of modern artificial intelligence (AI) technology, in psychological science, and in user research. Recent advances in generative AI have attracted growing interest to the possibility of replacing human participants in these domains with AI surrogates. We survey several such \"substitution proposals\" to better understand the arguments for and against substituting human participants with modern generative AI. Our scoping review indicates that the recent wave of these proposals is motivated by goals such as reducing the costs of research and development work and increasing the diversity of collected data. However, these proposals ignore and ultimately conflict with foundational values of work with human participants: representation, inclusion, and understanding. This paper critically examines the principles and goals underlying human participation to help chart out paths for future work that truly centers and empowers participants. ",
        "title": "The illusion of artificial inclusion",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08573",
        "abstract_url": "http://arxiv.org/abs/2401.08573",
        "authors": [
            {
                "last_name": "An",
                "first_name": "Bang"
            },
            {
                "last_name": "Ding",
                "first_name": "Mucong"
            },
            {
                "last_name": "Rabbani",
                "first_name": "Tahseen"
            },
            {
                "last_name": "Agrawal",
                "first_name": "Aakriti"
            },
            {
                "last_name": "Xu",
                "first_name": "Yuancheng"
            },
            {
                "last_name": "Deng",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Sicheng"
            },
            {
                "last_name": "Mohamed",
                "first_name": "Abdirisak"
            },
            {
                "last_name": "Wen",
                "first_name": "Yuxin"
            },
            {
                "last_name": "Goldstein",
                "first_name": "Tom"
            },
            {
                "last_name": "Huang",
                "first_name": "Furong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR",
            "LG"
        ],
        "abstract": "  This paper investigates the weaknesses of image watermarking techniques. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel benchmark for assessing watermark robustness, overcoming the limitations of current evaluation methods.WAVES integrates detection and identification tasks, and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced and novel variations of adversarial, diffusive, and embedding-based attacks. We introduce a normalized score of attack potency which incorporates several widely used image quality metrics and allows us to produce of an ordered ranking of attacks. Our comprehensive evaluation over reveals previously undetected vulnerabilities of several modern watermarking algorithms. WAVES is envisioned as a toolkit for the future development of robust watermarking systems. ",
        "title": "Benchmarking the Robustness of Image Watermarks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08574",
        "abstract_url": "http://arxiv.org/abs/2401.08574",
        "authors": [
            {
                "last_name": "Aky\u00fcrek",
                "first_name": "Afra Feyza"
            },
            {
                "last_name": "Aky\u00fcrek",
                "first_name": "Ekin"
            },
            {
                "last_name": "Choshen",
                "first_name": "Leshem"
            },
            {
                "last_name": "Wijaya",
                "first_name": "Derry"
            },
            {
                "last_name": "Andreas",
                "first_name": "Jacob"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs' reasoning capabilities during inference can be leveraged during training to improve their reliability. ",
        "title": "Deductive Closure Training of Language Models for Coherence, Accuracy,  and Updatability",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08577",
        "abstract_url": "http://arxiv.org/abs/2401.08577",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Yining"
            },
            {
                "last_name": "Zheng",
                "first_name": "Zishuo"
            },
            {
                "last_name": "Chen",
                "first_name": "Peihao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yian"
            },
            {
                "last_name": "Li",
                "first_name": "Junyan"
            },
            {
                "last_name": "Gan",
                "first_name": "Chuang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "LG",
            "RO"
        ],
        "abstract": "  Human beings possess the capability to multiply a melange of multisensory cues while actively exploring and interacting with the 3D world. Current multi-modal large language models, however, passively absorb sensory data as inputs, lacking the capacity to actively interact with the objects in the 3D environment and dynamically collect their multisensory information. To usher in the study of this area, we propose MultiPLY, a multisensory embodied large language model that could incorporate multisensory interactive data, including visual, audio, tactile, and thermal information into large language models, thereby establishing the correlation among words, actions, and percepts. To this end, we first collect Multisensory Universe, a large-scale multisensory interaction dataset comprising 500k data by deploying an LLM-powered embodied agent to engage with the 3D environment. To perform instruction tuning with pre-trained LLM on such generated data, we first encode the 3D scene as abstracted object-centric representations and then introduce action tokens denoting that the embodied agent takes certain actions within the environment, as well as state tokens that represent the multisensory state observations of the agent at each time step. In the inference time, MultiPLY could generate action tokens, instructing the agent to take the action in the environment and obtain the next multisensory state observation. The observation is then appended back to the LLM via state tokens to generate subsequent text or action tokens. We demonstrate that MultiPLY outperforms baselines by a large margin through a diverse set of embodied tasks involving object retrieval, tool use, multisensory captioning, and task decomposition. ",
        "title": "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in  3D World",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08579",
        "abstract_url": "http://arxiv.org/abs/2401.08579",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yu-hsuan"
            },
            {
                "last_name": "Kara",
                "first_name": "Levent Burak"
            },
            {
                "last_name": "Cagan",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  This research presents a new parametric style transfer framework specifically designed for curve-based design sketches. In this research, traditional challenges faced by neural style transfer methods in handling binary sketch transformations are effectively addressed through the utilization of parametric shape-editing rules, efficient curve-to-pixel conversion techniques, and the fine-tuning of VGG19 on ImageNet-Sketch, enhancing its role as a feature pyramid network for precise style extraction. By harmonizing intuitive curve-based imagery with rule-based editing, this study holds the potential to significantly enhance design articulation and elevate the practice of style transfer within the realm of product design. ",
        "title": "Curve-based Neural Style Transfer",
        "date": "2023-10-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08581",
        "abstract_url": "http://arxiv.org/abs/2401.08581",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Yi"
            },
            {
                "last_name": "Ganguli",
                "first_name": "Swetava"
            },
            {
                "last_name": "Pandey",
                "first_name": "Vipul"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas. Temporal embeddings transform sequential, spatiotemporal motion trajectory data into semantically meaningful image-like tensor representations that can be combined (multimodal fusion) with other data modalities that are or can be transformed into image-like tensor representations (for e.g., RBG imagery, graph embeddings of road networks, passively collected imagery like SAR, etc.) to facilitate multimodal learning in geospatial computer vision. Multimodal computer vision is critical for training machine learning models for geospatial feature detection to keep a geospatial mapping service up-to-date in real-time and can significantly improve user experience and above all, user safety. ",
        "title": "Temporal Embeddings: Scalable Self-Supervised Temporal Representation  Learning from Spatiotemporal Data for Multimodal Computer Vision",
        "date": "2023-10-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08583",
        "abstract_url": "http://arxiv.org/abs/2401.08583",
        "authors": [
            {
                "last_name": "Adin",
                "first_name": "Veysi"
            },
            {
                "last_name": "Kim",
                "first_name": "Chunwoo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  As the more sensors and actuators are used in the robotic systems to provide more features, complexity of the system is increasing. When it comes to medical robotics, it becomes harder to ensure safety and determinism in the system. To deal with increasing complexity and ensure precise periodicity and execution timing for a medical robot, in this paper we report development of EtherCAT master as a part of software framework for spine surgery robot. We implemented multi-axis controller using open-source EtherCAT master running in real-time preemptive Linux. We evaluated the real-time performance of the system in terms of periodicity, jitter and execution time in our first prototype of spine surgery robot. ",
        "title": "Development of Control Framework for Spine Surgery Robot Using EtherCAT",
        "date": "2023-10-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08584",
        "abstract_url": "http://arxiv.org/abs/2401.08584",
        "authors": [
            {
                "last_name": "Saadati",
                "first_name": "Sina"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "NE",
            "RO"
        ],
        "abstract": "  In this paper, for the first time, a method is presented that can provide a fully automated surgery based on software and computer vision techniques. Then, the advantages and challenges of computerization of medical surgery are examined. Finally, the surgery related to isolated ovarian endometriosis disease has been examined, and based on the presented method, a more detailed algorithm is presented that is capable of automatically diagnosing and treating this disease during surgery as proof of our proposed method where a U-net is trained to detect the endometriosis during surgery. ",
        "title": "Nahid: AI-based Algorithm for operating fully-automatic surgery",
        "date": "2023-11-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08585",
        "abstract_url": "http://arxiv.org/abs/2401.08585",
        "authors": [
            {
                "last_name": "Tull",
                "first_name": "Sean"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Razin A."
            },
            {
                "last_name": "Zemljic",
                "first_name": "Sara Sabrina"
            },
            {
                "last_name": "Clark",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this article we present a new modelling framework for structured concepts using a category-theoretic generalisation of conceptual spaces, and show how the conceptual representations can be learned automatically from data, using two very different instantiations: one classical and one quantum. A contribution of the work is a thorough category-theoretic formalisation of our framework. We claim that the use of category theory, and in particular the use of string diagrams to describe quantum processes, helps elucidate some of the most important features of our approach. We build upon Gardenfors' classical framework of conceptual spaces, in which cognition is modelled geometrically through the use of convex spaces, which in turn factorise in terms of simpler spaces called domains. We show how concepts from the domains of shape, colour, size and position can be learned from images of simple shapes, where concepts are represented as Gaussians in the classical implementation, and quantum effects in the quantum one. In the classical case we develop a new model which is inspired by the Beta-VAE model of concepts, but is designed to be more closely connected with language, so that the names of concepts form part of the graphical model. In the quantum case, concepts are learned by a hybrid classical-quantum network trained to perform concept classification, where the classical image processing is carried out by a convolutional neural network and the quantum representations are produced by a parameterised quantum circuit. Finally, we consider the question of whether our quantum models of concepts can be considered conceptual spaces in the Gardenfors sense. ",
        "title": "From Conceptual Spaces to Quantum Concepts: Formalising and Learning  Structured Conceptual Models",
        "date": "2023-11-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08586",
        "abstract_url": "http://arxiv.org/abs/2401.08586",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Zirui"
            },
            {
                "last_name": "Li",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Hu",
                "first_name": "Shenyang"
            },
            {
                "last_name": "Gopalakrishnan",
                "first_name": "Ganesh"
            },
            {
                "last_name": "Li",
                "first_name": "Ang"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Smoothed Particle Hydrodynamics (SPH) is essential for modeling complex large-deformation problems across various applications, requiring significant computational power. A major portion of SPH computation time is dedicated to the Nearest Neighboring Particle Search (NNPS) process. While advanced NNPS algorithms have been developed to enhance SPH efficiency, the potential efficiency gains from modern computation hardware remain underexplored. This study investigates the impact of GPU parallel architecture, low-precision computing on GPUs, and GPU memory management on NNPS efficiency. Our approach employs a GPU-accelerated mixed-precision SPH framework, utilizing low-precision float-point 16 (FP16) for NNPS while maintaining high precision for other components. To ensure FP16 accuracy in NNPS, we introduce a Relative Coordinated-based Link List (RCLL) algorithm, storing FP16 relative coordinates of particles within background cells. Our testing results show three significant speedup rounds for CPU-based NNPS algorithms. The first comes from parallel GPU computations, with up to a 1000x efficiency gain. The second is achieved through low-precision GPU computing, where the proposed FP16-based RCLL algorithm offers a 1.5x efficiency improvement over the FP64-based approach on GPUs. By optimizing GPU memory bandwidth utilization, the efficiency of the FP16 RCLL algorithm can be further boosted by 2.7x, as demonstrated in an example with 1 million particles. Our code is released at https://github.com/pnnl/lpNNPS4SPH. ",
        "title": "A GPU accelerated mixed-precision Smoothed Particle Hydrodynamics  framework with cell-based relative coordinates",
        "date": "2023-11-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08587",
        "abstract_url": "http://arxiv.org/abs/2401.08587",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Jia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In order to solve the problem of point cloud data splitting improved by DPC algorithm, a research on automatic separation and 3D reconstruction of point cloud data split lines is proposed. First, the relative coordinates of each point in the cloud point are calculated. Second, it is planned to develop a relative ensemble-based DPC swarm algorithm for analyzing the number of separation lines to determine all parts in the cloud content. Finally, fit each separator using the least squares method. iron. The cloud point of the resulting split subconductors has a clear demarcation line, and the distance between adjacent split subconductors is 0.45 m, divided by the four vertices of the square. ",
        "title": "Automatic extraction and 3D reconstruction of split wire from point  cloud data based on improved DPC algorithm",
        "date": "2023-11-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08588",
        "abstract_url": "http://arxiv.org/abs/2401.08588",
        "authors": [
            {
                "last_name": "Rout",
                "first_name": "Nirmal Kumar"
            },
            {
                "last_name": "Dutta",
                "first_name": "Gyanateet"
            },
            {
                "last_name": "Sinha",
                "first_name": "Varun"
            },
            {
                "last_name": "Dey",
                "first_name": "Arghadeep"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Subhrangshu"
            },
            {
                "last_name": "Gupta",
                "first_name": "Gopal"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Potholes are common road hazards that is causing damage to vehicles and posing a safety risk to drivers. The introduction of Convolutional Neural Networks (CNNs) is widely used in the industry for object detection based on Deep Learning methods and has achieved significant progress in hardware improvement and software implementations. In this paper, a unique better algorithm is proposed to warrant the use of low-resolution cameras or low-resolution images and video feed for automatic pothole detection using Super Resolution (SR) through Super Resolution Generative Adversarial Networks (SRGANs). Then we have proceeded to establish a baseline pothole detection performance on low quality and high quality dashcam images using a You Only Look Once (YOLO) network, namely the YOLOv7 network. We then have illustrated and examined the speed and accuracy gained above the benchmark after having upscaling implementation on the low quality images. ",
        "title": "Improved Pothole Detection Using YOLOv7 and ESRGAN",
        "date": "2023-11-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08593",
        "abstract_url": "http://arxiv.org/abs/2401.08593",
        "authors": [
            {
                "last_name": "Grazioso",
                "first_name": "Fabio"
            },
            {
                "last_name": "Atsapina",
                "first_name": "Anzhelika A."
            },
            {
                "last_name": "Obaeed",
                "first_name": "Gardoon L. O."
            },
            {
                "last_name": "Ivanova",
                "first_name": "Natalia A."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A method to efficiently and quantitatively study the delivery of a pesticide-surfactant formulation in water solution over plants leaves is presented. Instead of measuring the contact angle, the surface of the leaves wet area is used as key parameter. To this goal, a deep learning model has been trained and tested, to automatically measure the surface of area wet with water solution over cucumber leaves, processing the frames of video footage. We have individuated an existing deep learning model, reported in literature for other applications, and we have applied it to this different task. We present the measurement technique, some details of the deep learning model, its training procedure and its image segmentation performance. Finally, we report the results of the wet areas surface measurement as a function of the concentration of a surfactant in the pesticide solution. ",
        "title": "Automatic measurement of coverage area of water-based  pesticides-surfactant formulation on plant leaves using deep learning tools",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08595",
        "abstract_url": "http://arxiv.org/abs/2401.08595",
        "authors": [
            {
                "last_name": "Kabamba",
                "first_name": "Herve Mbikayi"
            },
            {
                "last_name": "Khouzam",
                "first_name": "Matthew"
            },
            {
                "last_name": "Dagenais",
                "first_name": "Michel"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "PF"
        ],
        "abstract": "  Adequate consideration is crucial to ensure that services in a distributed application context are running satisfactorily with the resources available. Due to the asynchronous nature of tasks and the need to work with multiple layers that deliver coordinated results in a single-threaded context, analysing performance is a challenging task in event-loop-based systems.   The existing performance analysis methods for environments such as Node.js rely on higher-level instrumentation but lack precision, as they cannot capture the relevant underlying application flow. As a solution, we propose a streamlined method for recovering the asynchronous execution path of requests called the Nested Bounded Context Algorithm. The proposed technique tracks the application execution flow through multiple layers and showcases it on an interactive interface for further assessment.   Furthermore, we introduce the vertical span concept. This representation of a span as a multidimensional object (horizontal and vertical) with a start and end of execution, along with its sub-layers and triggered operations, enables the granular identification and diagnosis of performance issues. We proposed a new technique called the Bounded Context Tracking Algorithm for event matching and request reassembling in a multi-layer trace . The two techniques allow aligning the executions of the request in a tree-based data structure for developed visualisations. These visualisations permit performance debugging of complex performance issues in Node.js. ",
        "title": "Node Compass: Multilevel Tracing and Debugging of Request Executions in  JavaScript-Based Web-Servers",
        "date": "2023-11-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08598",
        "abstract_url": "http://arxiv.org/abs/2401.08598",
        "authors": [
            {
                "last_name": "Tai",
                "first_name": "Chi-en Amy"
            },
            {
                "last_name": "Nair",
                "first_name": "Saeejith"
            },
            {
                "last_name": "Markham",
                "first_name": "Olivia"
            },
            {
                "last_name": "Keller",
                "first_name": "Matthew"
            },
            {
                "last_name": "Wu",
                "first_name": "Yifan"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuhao"
            },
            {
                "last_name": "Wong",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Dietary intake estimation plays a crucial role in understanding the nutritional habits of individuals and populations, aiding in the prevention and management of diet-related health issues. Accurate estimation requires comprehensive datasets of food scenes, including images, segmentation masks, and accompanying dietary intake metadata. In this paper, we introduce NutritionVerse-Real, an open access manually collected 2D food scene dataset for dietary intake estimation with 889 images of 251 distinct dishes and 45 unique food types. The NutritionVerse-Real dataset was created by manually collecting images of food scenes in real life, measuring the weight of every ingredient and computing the associated dietary content of each dish using the ingredient weights and nutritional information from the food packaging or the Canada Nutrient File. Segmentation masks were then generated through human labelling of the images. We provide further analysis on the data diversity to highlight potential biases when using this data to develop models for dietary intake estimation. NutritionVerse-Real is publicly available at https://www.kaggle.com/datasets/nutritionverse/nutritionverse-real as part of an open initiative to accelerate machine learning for dietary sensing. ",
        "title": "NutritionVerse-Real: An Open Access Manually Collected 2D Food Scene  Dataset for Dietary Intake Estimation",
        "date": "2023-11-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08599",
        "abstract_url": "http://arxiv.org/abs/2401.08599",
        "authors": [
            {
                "last_name": "Fan",
                "first_name": "Lei"
            },
            {
                "last_name": "Ding",
                "first_name": "Yiwen"
            },
            {
                "last_name": "Fan",
                "first_name": "Dongdong"
            },
            {
                "last_name": "Wu",
                "first_name": "Yong"
            },
            {
                "last_name": "Chu",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Pagnucco",
                "first_name": "Maurice"
            },
            {
                "last_name": "Song",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a machine vision-based database named GrainSet for the purpose of visual quality inspection of grain kernels. The database contains more than 350K single-kernel images with experts' annotations. The grain kernels used in the study consist of four types of cereal grains including wheat, maize, sorghum and rice, and were collected from over 20 regions in 5 countries. The surface information of each kernel is captured by our custom-built device equipped with high-resolution optic sensor units, and corresponding sampling information and annotations include collection location and time, morphology, physical size, weight, and Damage & Unsound grain categories provided by senior inspectors. In addition, we employed a commonly used deep learning model to provide classification results as a benchmark. We believe that our GrainSet will facilitate future research in fields such as assisting inspectors in grain quality inspections, providing guidance for grain storage and trade, and contributing to applications of smart agriculture. ",
        "title": "An annotated grain kernel image database for visual quality inspection",
        "date": "2023-11-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08602",
        "abstract_url": "http://arxiv.org/abs/2401.08602",
        "authors": [
            {
                "last_name": "Farsang",
                "first_name": "M\u00f3nika"
            },
            {
                "last_name": "Lechner",
                "first_name": "Mathias"
            },
            {
                "last_name": "Lung",
                "first_name": "David"
            },
            {
                "last_name": "Hasani",
                "first_name": "Ramin"
            },
            {
                "last_name": "Rus",
                "first_name": "Daniela"
            },
            {
                "last_name": "Grosu",
                "first_name": "Radu"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Bio-inspired neural networks have the potential to advance our understanding of neural computation and improve the state-of-the-art of AI systems. Bio-electrical synapses directly transmit neural signals, by enabling fast current flow between neurons. In contrast, bio-chemical synapses transmit neural signals indirectly, through neurotransmitters. Prior work showed that interpretable dynamics for complex robotic control, can be achieved by using chemical synapses, within a sparse, bio-inspired architecture, called Neural Circuit Policies (NCPs). However, a comparison of these two synaptic models, within the same architecture, remains an unexplored area. In this work we aim to determine the impact of using chemical synapses compared to electrical synapses, in both sparse and all-to-all connected networks. We conduct experiments with autonomous lane-keeping through a photorealistic autonomous driving simulator to evaluate their performance under diverse conditions and in the presence of noise. The experiments highlight the substantial influence of the architectural and synaptic-model choices, respectively. Our results show that employing chemical synapses yields noticeable improvements compared to electrical synapses, and that NCPs lead to better results in both synaptic models. ",
        "title": "Learning with Chemical versus Electrical Synapses -- Does it Make a  Difference?",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08603",
        "abstract_url": "http://arxiv.org/abs/2401.08603",
        "authors": [
            {
                "last_name": "Jaziri",
                "first_name": "Achref"
            },
            {
                "last_name": "Ditzel",
                "first_name": "Sina"
            },
            {
                "last_name": "Pliushch",
                "first_name": "Iuliia"
            },
            {
                "last_name": "Ramesh",
                "first_name": "Visvanathan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Modern data-driven machine learning system designs exploit inductive biases on architectural structure, invariance and equivariance requirements, task specific loss functions, and computational optimization tools. Previous works have illustrated that inductive bias in the early layers of the encoder in the form of human specified quasi-invariant filters can serve as a powerful inductive bias to attain better robustness and transparency in learned classifiers. This paper explores this further in the context of representation learning with local plasticity rules i.e. bio-inspired Hebbian learning . We propose a modular framework trained with a bio-inspired variant of contrastive predictive coding (Hinge CLAPP Loss). Our framework is composed of parallel encoders each leveraging a different invariant visual descriptor as an inductive bias. We evaluate the representation learning capacity of our system in a classification scenario on image data of various difficulties (GTSRB, STL10, CODEBRIM) as well as video data (UCF101). Our findings indicate that this form of inductive bias can be beneficial in closing the gap between models with local plasticity rules and backpropagation models as well as learning more robust representations in general. ",
        "title": "Representation Learning in a Decomposed Encoder Design for Bio-inspired  Hebbian Learning",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08604",
        "abstract_url": "http://arxiv.org/abs/2401.08604",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Weihao"
            },
            {
                "last_name": "Qian",
                "first_name": "Yeqiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Xingyuan"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Hanyang"
            },
            {
                "last_name": "Wang",
                "first_name": "Chunxiang"
            },
            {
                "last_name": "Yang",
                "first_name": "Ming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semantic segmentation plays a critical role in enabling intelligent vehicles to comprehend their surrounding environments. However, deep learning-based methods usually perform poorly in domain shift scenarios due to the lack of labeled data for training. Unsupervised domain adaptation (UDA) techniques have emerged to bridge the gap across different driving scenes and enhance model performance on unlabeled target environments. Although self-training UDA methods have achieved state-of-the-art results, the challenge of generating precise pseudo-labels persists. These pseudo-labels tend to favor majority classes, consequently sacrificing the performance of rare classes or small objects like traffic lights and signs. To address this challenge, we introduce SAM4UDASS, a novel approach that incorporates the Segment Anything Model (SAM) into self-training UDA methods for refining pseudo-labels. It involves Semantic-Guided Mask Labeling, which assigns semantic labels to unlabeled SAM masks using UDA pseudo-labels. Furthermore, we devise fusion strategies aimed at mitigating semantic granularity inconsistency between SAM masks and the target domain. SAM4UDASS innovatively integrate SAM with UDA for semantic segmentation in driving scenes and seamlessly complements existing self-training UDA methodologies. Extensive experiments on synthetic-to-real and normal-to-adverse driving datasets demonstrate its effectiveness. It brings more than 3% mIoU gains on GTA5-to-Cityscapes, SYNTHIA-to-Cityscapes, and Cityscapes-to-ACDC when using DAFormer and achieves SOTA when using MIC. The code will be available at https://github.com/ywher/SAM4UDASS. ",
        "title": "SAM4UDASS: When SAM Meets Unsupervised Domain Adaptive Semantic  Segmentation in Intelligent Vehicles",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08609",
        "abstract_url": "http://arxiv.org/abs/2401.08609",
        "authors": [
            {
                "last_name": "Al-Saad",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Ramaswamy",
                "first_name": "Lakshmish"
            },
            {
                "last_name": "Bhandarkar",
                "first_name": "Suchendra"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent studies have shown that video-level representation learning is crucial to the capture and understanding of the long-range temporal structure for video action recognition. Most existing 3D convolutional neural network (CNN)-based methods for video-level representation learning are clip-based and focus only on short-term motion and appearances. These CNN-based methods lack the capacity to incorporate and model the long-range spatiotemporal representation of the underlying video and ignore the long-range video-level context during training. In this study, we propose a factorized 4D CNN architecture with attention (F4D) that is capable of learning more effective, finer-grained, long-term spatiotemporal video representations. We demonstrate that the proposed F4D architecture yields significant performance improvements over the conventional 2D, and 3D CNN architectures proposed in the literature. Experiment evaluation on five action recognition benchmark datasets, i.e., Something-Something-v1, SomethingSomething-v2, Kinetics-400, UCF101, and HMDB51 demonstrate the effectiveness of the proposed F4D network architecture for video-level action recognition. ",
        "title": "F4D: Factorized 4D Convolutional Neural Network for Efficient  Video-level Representation Learning",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08610",
        "abstract_url": "http://arxiv.org/abs/2401.08610",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Xihan"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Knottenbelt",
                "first_name": "William"
            },
            {
                "last_name": "Huth",
                "first_name": "Michael"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Lido, the leading Liquid Staking Derivative (LSD) provider on Ethereum, allows users to stake an arbitrary amount of ETH to receive stETH, which can be integrated with Decentralized Finance (DeFi) protocols such as Aave. The composability between Lido and Aave enables a novel strategy called \"leverage staking\", where users stake ETH on Lido to acquire stETH, utilize stETH as collateral on Aave to borrow ETH, and then restake the borrowed ETH on Lido. Users can iteratively execute this process to optimize potential returns based on their risk profile.   This paper systematically studies the opportunities and risks associated with leverage staking. We are the first to formalize the leverage staking strategy within the Lido-Aave ecosystem. Our empirical study identifies 262 leverage staking positions on Ethereum, with an aggregated staking amount of 295,243 ETH (482M USD). We discover that 90.13% of leverage staking positions have achieved higher returns than conventional staking. Furthermore, we perform stress tests to evaluate the risk introduced by leverage staking under extreme conditions. We find that leverage staking significantly amplifies the risk of cascading liquidations. We hope this paper can inform and encourage the development of robust risk management approaches to protect the Lido-Aave LSD ecosystem. ",
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities  and Risks",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08613",
        "abstract_url": "http://arxiv.org/abs/2401.08613",
        "authors": [
            {
                "last_name": "Luu",
                "first_name": "Quang-Hung"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Thai M."
            },
            {
                "last_name": "Zheng",
                "first_name": "Nan"
            },
            {
                "last_name": "Vu",
                "first_name": "Hai L."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Connected and automated vehicles (CAV) are expected to deliver a much safer, more efficient, and eco-friendlier mobility. Being an indispensable component of the future transportation, their key driving features of CAVs include not only the automated functionality but also the cooperative capability. Despite the CAVs themselves are emerging and active research areas, there is a lack of a comprehensive literature review on the digital infrastructure that enables them. In this paper, we review the requirements and benefits of digital infrastructures for the CAVs including the vehicle built-in, roadside-based, operational and planning infrastructures. We then highlight challenges and opportunities on digital infrastructure research for the CAVs. Our study sheds lights on seamless integration of digital infrastructure for safe operations of CAVs. ",
        "title": "Digital Infrastructure for Connected and Automated Vehicles",
        "date": "2023-11-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08615",
        "abstract_url": "http://arxiv.org/abs/2401.08615",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Chengkun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiangmin"
            },
            {
                "last_name": "Wang",
                "first_name": "Chen"
            },
            {
                "last_name": "Gondal",
                "first_name": "Iqbal"
            },
            {
                "last_name": "Shao",
                "first_name": "Jie"
            },
            {
                "last_name": "Yi",
                "first_name": "Xun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Social video anomaly is an observation in video streams that does not conform to a common pattern of dataset's behaviour. Social video anomaly detection plays a critical role in applications from e-commerce to e-learning. Traditionally, anomaly detection techniques are applied to find anomalies in video broadcasting. However, they neglect the live social video streams which contain interactive talk, speech, or lecture with audience. In this paper, we propose a generic framework for effectively online detecting Anomalies Over social Video LIve Streaming (AOVLIS). Specifically, we propose a novel deep neural network model called Coupling Long Short-Term Memory (CLSTM) that adaptively captures the history behaviours of the presenters and audience, and their mutual interactions to predict their behaviour at next time point over streams. Then we well integrate the CLSTM with a decoder layer, and propose a new reconstruction error-based scoring function $RE_{IA}$ to calculate the anomaly score of each video segment for anomaly detection. After that, we propose a novel model update scheme that incrementally maintains CLSTM and decoder. Moreover, we design a novel upper bound and ADaptive Optimisation Strategy (ADOS) for improving the efficiency of our solution. Extensive experiments are conducted to prove the superiority of AOVLIS. ",
        "title": "Online Anomaly Detection over Live Social Video Streaming",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08619",
        "abstract_url": "http://arxiv.org/abs/2401.08619",
        "authors": [
            {
                "last_name": "Goffinet",
                "first_name": "Etienne"
            },
            {
                "last_name": "Mall",
                "first_name": "Raghvendra"
            },
            {
                "last_name": "Singh",
                "first_name": "Ankita"
            },
            {
                "last_name": "Kaushik",
                "first_name": "Rahul"
            },
            {
                "last_name": "Castiglione",
                "first_name": "Filippo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  An accurate binding affinity prediction between T-cell receptors and epitopes contributes decisively to develop successful immunotherapy strategies. Some state-of-the-art computational methods implement deep learning techniques by integrating evolutionary features to convert the amino acid residues of cell receptors and epitope sequences into numerical values, while some other methods employ pre-trained language models to summarize the embedding vectors at the amino acid residue level to obtain sequence-wise representations.   Here, we propose a highly reliable novel method, MATE-Pred, that performs multi-modal attention-based prediction of T-cell receptors and epitopes binding affinity. The MATE-Pred is compared and benchmarked with other deep learning models that leverage multi-modal representations of T-cell receptors and epitopes. In the proposed method, the textual representation of proteins is embedded with a pre-trained bi-directional encoder model and combined with two additional modalities: a) a comprehensive set of selected physicochemical properties; b) predicted contact maps that estimate the 3D distances between amino acid residues in the sequences.   The MATE-Pred demonstrates the potential of multi-modal model in achieving state-of-the-art performance (+8.4\\% MCC, +5.5\\% AUC compared to baselines) and efficiently capturing contextual, physicochemical, and structural information from amino acid residues. The performance of MATE-Pred projects its potential application in various drug discovery regimes. ",
        "title": "MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor",
        "date": "2023-12-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08623",
        "abstract_url": "http://arxiv.org/abs/2401.08623",
        "authors": [
            {
                "last_name": "Sorrenti",
                "first_name": "Amelia"
            },
            {
                "last_name": "Bellitto",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Salanitri",
                "first_name": "Federica Proietto"
            },
            {
                "last_name": "Pennisi",
                "first_name": "Matteo"
            },
            {
                "last_name": "Palazzo",
                "first_name": "Simone"
            },
            {
                "last_name": "Spampinato",
                "first_name": "Concetto"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "CV",
            "LG"
        ],
        "abstract": "  We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy leveraging Complementary Learning System theory and the wake-sleep phases of the human brain to improve the performance of deep neural networks for visual classification tasks in continual learning settings. Our method learns continually via the synchronization between distinct wake and sleep phases. During the wake phase, the model is exposed to sensory input and adapts its representations, ensuring stability through a dynamic parameter freezing mechanism and storing episodic memories in a short-term temporary memory (similarly to what happens in the hippocampus). During the sleep phase, the training process is split into NREM and REM stages. In the NREM stage, the model's synaptic weights are consolidated using replayed samples from the short-term and long-term memory and the synaptic plasticity mechanism is activated, strengthening important connections and weakening unimportant ones. In the REM stage, the model is exposed to previously-unseen realistic visual sensory experience, and the dreaming process is activated, which enables the model to explore the potential feature space, thus preparing synapses to future knowledge. We evaluate the effectiveness of our approach on three benchmark datasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method outperforms the baselines and prior work, yielding a significant performance gain on continual visual classification tasks. Furthermore, we demonstrate the usefulness of all processing stages and the importance of dreaming to enable positive forward transfer. ",
        "title": "Wake-Sleep Consolidated Learning",
        "date": "2023-12-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08624",
        "abstract_url": "http://arxiv.org/abs/2401.08624",
        "authors": [
            {
                "last_name": "T\u00e4rneberg",
                "first_name": "William"
            },
            {
                "last_name": "Fedorov",
                "first_name": "Aleksei"
            },
            {
                "last_name": "Callebaut",
                "first_name": "Gilles"
            },
            {
                "last_name": "Van der Perre",
                "first_name": "Liesbet"
            },
            {
                "last_name": "Fitzgerald",
                "first_name": "Emma"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The advent of 6G wireless communication marks a transformative era in technological connectivity, bringing forth challenges and opportunities alike. This paper unveils an innovative, open-source simulator, meticulously crafted for cell-free 6G wireless networks. This simulator is not just a tool but a gateway to the future, blending cutting-edge channel models with the simulation of both physical propagation effects and intricate system-level protocols. It stands at the forefront of technological advancement by integrating LIS and MIMO technologies, harnessing the power of the Unity game engine for efficient ray-tracing and GPU-accelerated computations. The unparalleled flexibility in scenario configuration, coupled with its unique ability to dynamically simulate interactions across network layers, establishes this simulator as an indispensable asset in pioneering &G systems' research and development. ",
        "title": "Towards Practical Cell-Free 6G Network Deployments: An Open-Source  End-to-End Ray Tracing Simulator",
        "date": "2023-12-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08625",
        "abstract_url": "http://arxiv.org/abs/2401.08625",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Shitian"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junyue"
            },
            {
                "last_name": "Liang",
                "first_name": "Yilai"
            },
            {
                "last_name": "Chu",
                "first_name": "Xiaoyang"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  In the field of Electronic Design Automation (EDA), logic synthesis plays a pivotal role in optimizing hardware resources. Traditional logic synthesis algorithms, such as the Quine-McCluskey method, face challenges in scalability and efficiency, particularly for higher-dimension problems. This paper introduces a novel heuristic algorithm based on Conditional Flood Fill Method aimed at addressing these limitations. Our method employs count-based adjacent element handling and introduces nine new theorems to guide the logic synthesis process. Experimental results validate the efficacy of our approach, showing significant improvements in computational efficiency and scalability compared to existing algorithms. The algorithm holds potential for future advancements in circuit development and Boolean function optimization. ",
        "title": "Conditional Flood Fill Method in Logic Synthesis",
        "date": "2023-12-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08627",
        "abstract_url": "http://arxiv.org/abs/2401.08627",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shichang"
            },
            {
                "last_name": "Tang",
                "first_name": "Longwen"
            },
            {
                "last_name": "Bauchy",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Sun",
                "first_name": "Yizhou"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Metallic Glasses (MGs) are widely used disordered materials. Understanding the relationship between the local structure and physical properties of MGs is one of the greatest challenges for both material science and condensed matter physics. In this work, we utilize Graph Neural Networks (GNNs) to model the atomic graph structure and study the connection between the structure and the corresponding local energy barrier, which is believed to govern many critical physical properties in MGs. One of our key contributions is to propose a novel Symmetrized GNN (SymGNN) model for predicting the energy barriers, which is invariant under orthogonal transformations of the structure, e.g., rotations and reflections. Such invariance is a desired property that standard GNNs like Graph Convolutional Networks cannot capture. SymGNNs handle the invariance by aggregating over orthogonal transformations of the graph structure for representation learning, and an optimal distribution over all 3D orthogonal transformations $\\mathcal{O}_3$ is learned to maximize the benefit of invariance. We demonstrate in our experiments that SymGNN can significantly improve the energy barrier prediction over other GNNs and non-graph machine learning models. With such an accurate model, we also apply graph explanation algorithms to better reveal the structure-property relationship of MGs. Our GNN framework allows effective prediction of material physical properties and bolsters material science research through the use of AI models. ",
        "title": "Predicting and Interpreting Energy Barriers of Metallic Glasses with  Graph Neural Networks",
        "date": "2023-12-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08628",
        "abstract_url": "http://arxiv.org/abs/2401.08628",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Jiho"
            },
            {
                "last_name": "Kang",
                "first_name": "Sangwoo"
            },
            {
                "last_name": "Lim",
                "first_name": "Mikyoung"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider the imaging of a planar extended target from far-field data under a monostatic measurement configuration, in which the data is measured by a single moving transducer, as frequently encountered in practical application. In this paper, we develop a Bayesian approach to recover the shape of the extended target with MCMC sampling, where a new shape basis selection is proposed based on the shape derivative analysis for the measurement data. In order to optimize the center and radius of the initial disk, we use the monostatic sampling method for the center and the explicit scattered field expression for disks for the radius. Numerical simulations are presented to validate the proposed method. ",
        "title": "Monostatic imaging of an extended target with MCMC sampling",
        "date": "2023-12-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08629",
        "abstract_url": "http://arxiv.org/abs/2401.08629",
        "authors": [
            {
                "last_name": "Sapkota",
                "first_name": "Ranjan"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Dawood"
            },
            {
                "last_name": "Churuvija",
                "first_name": "Martin"
            },
            {
                "last_name": "Karkee",
                "first_name": "Manoj"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Detecting and estimating size of apples during the early stages of growth is crucial for predicting yield, pest management, and making informed decisions related to crop-load management, harvest and post-harvest logistics, and marketing. Traditional fruit size measurement methods are laborious and time-consuming. This study employs the state-of-the-art YOLOv8 object detection and instance segmentation algorithm in conjunction with geometric shape fitting techniques on 3D point cloud data to accurately determine the size of immature green apples (or fruitlet) in a commercial orchard environment. The methodology utilized two RGB-D sensors: the Intel RealSense D435i and the Microsoft Azure Kinect DK. Notably, the YOLOv8 instance segmentation models exhibited proficiency in immature green apple detection, with the YOLOv8m-seg model clinching the highest AP@0.5 and AP@0.75 scores of 0.94 and 0.91, respectively. Leveraging the ellipsoid fitting technique on images from the Azure Kinect, we observed remarkable metrics, including an RMSE of 2.35, MAE of 1.66, MAPE of 6.15, and an R-squared value of 0.9. Challenges such as partial occlusion, where YOLOv8 sometimes misinterpreted immature green apple clusters, were recognized. In a comparison of 102 outdoor samples, the Microsoft Azure Kinect showed better performance than the Intel Realsense D435i, as supported by the MAE data. This study emphasizes the combined effectiveness of shape-fitting methods and 3D sensors in improving fruitlet sizing for agriculture. ",
        "title": "Immature Green Apple Detection and Sizing in Commercial Orchards using  YOLOv8 and Shape Fitting Techniques",
        "date": "2023-12-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08631",
        "abstract_url": "http://arxiv.org/abs/2401.08631",
        "authors": [
            {
                "last_name": "Lahoz-Beltra",
                "first_name": "Rafael"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "ET"
        ],
        "abstract": "  In recent years, the emergence of the first quantum computers at a time when AI is undergoing a fruitful era has led many AI researchers to be tempted into adapting their algorithms to run on a quantum computer. However, in many cases the initial enthusiasm has ended in frustration, since the features and principles underlying quantum computing are very different from traditional computers. In this paper, we present a discussion of the difficulties arising when designing a quantum version of an evolutionary algorithm based on Darwin's evolutionary mechanism, the so-called genetic algorithms. The paper includes the code in both Python and QISKIT of the quantum version of one of these evolutionary algorithms allowing the reader to experience the setbacks arising when translating a classical algorithm to its quantum version. The algorithm studied in this paper, termed RQGA (Reduced Quantum Genetic Algorithm), has been chosen as an example that clearly shows these difficulties, which are common to other AI algorithms. ",
        "title": "The Conquest of Quantum Genetic Algorithms: The Adventure to Cross the  Valley of Death",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08632",
        "abstract_url": "http://arxiv.org/abs/2401.08632",
        "authors": [
            {
                "last_name": "Faldor",
                "first_name": "Maxence"
            },
            {
                "last_name": "Chalumeau",
                "first_name": "F\u00e9lix"
            },
            {
                "last_name": "Flageat",
                "first_name": "Manon"
            },
            {
                "last_name": "Cully",
                "first_name": "Antoine"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG",
            "RO"
        ],
        "abstract": "  A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-Elites is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-Elites performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we present three contributions: (1) we enhance the Policy Gradient variation operator with a descriptor-conditioned critic that reconciles diversity search with gradient-based methods, (2) we leverage the actor-critic training to learn a descriptor-conditioned policy at no additional cost, distilling the knowledge of the population into one single versatile policy that can execute a diversity of behaviors, (3) we exploit the descriptor-conditioned actor by injecting it in the population, despite network architecture differences. Our method, DCG-MAP-Elites, achieves equal or higher QD score and coverage compared to all baselines on seven challenging continuous control locomotion tasks. ",
        "title": "Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement  Learning",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08633",
        "abstract_url": "http://arxiv.org/abs/2401.08633",
        "authors": [
            {
                "last_name": "Vachha",
                "first_name": "Cyrus"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We present a pipeline for integrating NeRFs into traditional compositing VFX pipelines using Nerfstudio, an open-source framework for training and rendering NeRFs. Our approach involves using Blender, a widely used open-source 3D creation software, to align camera paths and composite NeRF renders with meshes and other NeRFs, allowing for seamless integration of NeRFs into traditional VFX pipelines. Our NeRF Blender add-on allows for more controlled camera trajectories of photorealistic scenes, compositing meshes and other environmental effects with NeRFs, and compositing multiple NeRFs in a single scene.This approach of generating NeRF aligned camera paths can be adapted to other 3D tool sets and workflows, enabling a more seamless integration of NeRFs into visual effects and film production. Documentation can be found here: https://docs.nerf.studio/extensions/blender_addon.html ",
        "title": "Creating Visual Effects with Neural Radiance Fields",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08634",
        "abstract_url": "http://arxiv.org/abs/2401.08634",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xueyuan"
            },
            {
                "last_name": "Gursoy",
                "first_name": "M. Cenk"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In this paper, we investigate jamming-resilient UAV path planning strategies for data collection in Internet of Things (IoT) networks, in which the typical UAV can learn the optimal trajectory to elude such jamming attacks. Specifically, the typical UAV is required to collect data from multiple distributed IoT nodes under collision avoidance, mission completion deadline, and kinematic constraints in the presence of jamming attacks. We first design a fixed ground jammer with continuous jamming attack and periodical jamming attack strategies to jam the link between the typical UAV and IoT nodes. Defensive strategies involving a reinforcement learning (RL) based virtual jammer and the adoption of higher SINR thresholds are proposed to counteract against such attacks. Secondly, we design an intelligent UAV jammer, which utilizes the RL algorithm to choose actions based on its observation. Then, an intelligent UAV anti-jamming strategy is constructed to deal with such attacks, and the optimal trajectory of the typical UAV is obtained via dueling double deep Q-network (D3QN). Simulation results show that both non-intelligent and intelligent jamming attacks have significant influence on the UAV's performance, and the proposed defense strategies can recover the performance close to that in no-jammer scenarios. ",
        "title": "Resilient Path Planning for UAVs in Data Collection under Adversarial  Attacks",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08636",
        "abstract_url": "http://arxiv.org/abs/2401.08636",
        "authors": [
            {
                "last_name": "Chennamsetti",
                "first_name": "Varshitha"
            },
            {
                "last_name": "von Laszewski",
                "first_name": "Gregor"
            },
            {
                "last_name": "Gu",
                "first_name": "Ruochen"
            },
            {
                "last_name": "Mehnaz",
                "first_name": "Laiba"
            },
            {
                "last_name": "Papay",
                "first_name": "Juri"
            },
            {
                "last_name": "Jackson",
                "first_name": "Samuel"
            },
            {
                "last_name": "Thiyagalingam",
                "first_name": "Jeyan"
            },
            {
                "last_name": "Samsonau",
                "first_name": "Sergey V."
            },
            {
                "last_name": "Fox",
                "first_name": "Geoffrey C."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  In this paper, we report on work performed for the MLCommons Science Working Group on the cloud masking benchmark. MLCommons is a consortium that develops and maintains several scientific benchmarks that aim to benefit developments in AI. The benchmarks are conducted on the High Performance Computing (HPC) Clusters of New York University and University of Virginia, as well as a commodity desktop. We provide a description of the cloud masking benchmark, as well as a summary of our submission to MLCommons on the benchmark experiment we conducted. It includes a modification to the reference implementation of the cloud masking benchmark enabling early stopping. This benchmark is executed on the NYU HPC through a custom batch script that runs the various experiments through the batch queuing system while allowing for variation on the number of epochs trained. Our submission includes the modified code, a custom batch script to modify epochs, documentation, and the benchmark results. We report the highest accuracy (scientific metric) and the average time taken (performance metric) for training and inference that was achieved on NYU HPC Greene. We also provide a comparison of the compute capabilities between different systems by running the benchmark for one epoch. Our submission can be found in a Globus repository that is accessible to MLCommons Science Working Group. ",
        "title": "MLCommons Cloud Masking Benchmark with Early Stopping",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08637",
        "abstract_url": "http://arxiv.org/abs/2401.08637",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Taesik"
            },
            {
                "last_name": "Jang",
                "first_name": "Si Young"
            },
            {
                "last_name": "Acer",
                "first_name": "Utku G\u00fcnay"
            },
            {
                "last_name": "Kawsar",
                "first_name": "Fahim"
            },
            {
                "last_name": "Min",
                "first_name": "Chulhong"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  The advent of tiny AI accelerators opens opportunities for deep neural network deployment at the extreme edge, offering reduced latency, lower power cost, and improved privacy in on-device ML inference. Despite these advancements, challenges persist due to inherent limitations of these accelerators, such as restricted onboard memory and single-device focus. This paper introduces Synergy, a system that dynamically composes tiny AI accelerators for multi-tenant models, effectively addressing tinyML's critical challenges for the increasing demand for on-device AI. A key feature of Synergy is its virtual computing space, providing a unified, virtualized view of resources and enabling efficient task mapping to physical devices. Synergy's runtime orchestration module ensures optimal inference across dynamic and heterogeneous accelerators. Our evaluations with 7 baselines and 8 models demonstrate that Synergy improves throughput by an average of 8.0X compared to baselines. ",
        "title": "Collaborative Inference via Dynamic Composition of Tiny AI Accelerators  on MCUs",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08639",
        "abstract_url": "http://arxiv.org/abs/2401.08639",
        "authors": [
            {
                "last_name": "Geng",
                "first_name": "Zhengyang"
            },
            {
                "last_name": "Pokle",
                "first_name": "Ashwini"
            },
            {
                "last_name": "Kolter",
                "first_name": "J. Zico"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models directly from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is crucial to this capability, as GET matches a $5\\times$ larger ViT in terms of FID scores while striking a critical balance of computational cost and image quality. Code, checkpoints, and datasets are available. ",
        "title": "One-Step Diffusion Distillation via Deep Equilibrium Models",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08640",
        "abstract_url": "http://arxiv.org/abs/2401.08640",
        "authors": [
            {
                "last_name": "Igneczi",
                "first_name": "Gergo"
            },
            {
                "last_name": "Dobay",
                "first_name": "Tamas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The rapid development of automated driving systems in recent years has led to improvements in road safety and travel comfort. One typical function of these systems is Lane Keep Assist, which generally does not take human driving preferences into account. In our previous work, we have demonstrated that it is possible to implement a Lane Keep Assist function that is appropriate to human preferences using a trajectory planning algorithm based on a linear driving model. In our current work, we investigated how to separate the driving styles of individual drivers. We assumed that there are three driving styles: sporty, neutral and defensive. To prove these relations, clustering methods were applied to previously recorded measurements . Simulations with parameters describing the average behaviour of the classes (re-simulated with clustered types) showed that the resulting paths successfully classified drivers, that the 3 classes are distinct in their behaviour and that our model reproduces these behaviours. ",
        "title": "Typification of Driver Models Using Clustering Methods",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08643",
        "abstract_url": "http://arxiv.org/abs/2401.08643",
        "authors": [
            {
                "last_name": "Favero",
                "first_name": "Renan"
            },
            {
                "last_name": "Elefteriadou",
                "first_name": "Lily"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Autonomous shuttles (AS) operate in several cities and have shown potential to improve the public transport network. However, there is no car following model that is based on field data and allows decision-makers to assess and plan for AS operations. To fill this gap, this study collected field data from AS, analyzed their driving performance, and suggested changes in the AS trajectory model to improve passenger comfort. A sample was collected with more than 4000 seconds of AS following a conventional car. The sample contained GPS positions from both AS and conventional vehicles. Latitude and longitude positions were used to calculate the speed, acceleration, and jerk of the leader and follower. The data analyses indicated that AS have higher jerk values that may impact the passengers comfort. Several existing models were evaluated, and the researchers concluded that the calibrated ACC model resulted in lower errors for AS spacing and speed. The results of the calibration indicate that the AS has lower peak acceleration and higher deceleration than the parameters that were calibrated for autonomous vehicle models in other research ",
        "title": "Exploratory Driving Performance and Car-Following Modeling for  Autonomous Shuttles Based on Field Data",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08644",
        "abstract_url": "http://arxiv.org/abs/2401.08644",
        "authors": [
            {
                "last_name": "Noeiaghdam",
                "first_name": "Samad"
            },
            {
                "last_name": "Dreglea",
                "first_name": "Aliona I."
            },
            {
                "last_name": "Sidorov",
                "first_name": "Denis N."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This study aims to discuss the existence and uniqueness of solution of fuzzy Volterra integral equation with piecewise continuous kernel. Such problems appears in many balance problems for hereditary dynamic systems, e.g. in electric load leveling. The method of successive approximations is applied and the main theorems are proved based on the method. Some examples are discussed and the results are presented for different values of $\\mu$ by plotting several graphs. ",
        "title": "Fuzzy Volterra Integral Equation with Piecewise Continuous Kernel:  Theory and Numerical Solution",
        "date": "2023-12-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08647",
        "abstract_url": "http://arxiv.org/abs/2401.08647",
        "authors": [
            {
                "last_name": "Barvenik",
                "first_name": "Kieran"
            },
            {
                "last_name": "Coogan",
                "first_name": "Zachary"
            },
            {
                "last_name": "Librandi",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Pezzulla",
                "first_name": "Matteo"
            },
            {
                "last_name": "Tubaldi",
                "first_name": "Eleonora"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Soft and lightweight grippers have greatly enhanced the performance of robotic manipulators in handling complex objects with varying shape, texture, and stiffness. However, the combination of universal grasping with passive sensing capabilities still presents challenges. To overcome this limitation, we introduce a fluidic soft gripper, named the ``Pac-Man'' gripper, based on the buckling of soft, thin hemispherical shells. Leveraging a single fluidic pressure input, the soft gripper can encapsulate slippery and delicate objects while passively providing information on this physical interaction. Guided by analytical, numerical, and experimental tools, we explore the novel grasping principle of this mechanics-based soft gripper. First, we characterize the buckling behavior of a free hemisphere as a function of its geometric parameters. Inspired by the free hemisphere's two-lobe mode shape ideal for grasping purposes, we demonstrate that the gripper can perform dexterous manipulation and gentle gripping of fragile objects in confined environments. Last, we prove the soft gripper's embedded capability of detecting contact, grasping, and release conditions during the interaction with an unknown object. This simple buckling-based soft gripper opens new avenues for the design of adaptive gripper morphologies with applications ranging from medical and agricultural robotics to space and underwater exploration. ",
        "title": "The \"Pac-Man'' Gripper: Tactile Sensing and Grasping through Thin-Shell  Buckling",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08649",
        "abstract_url": "http://arxiv.org/abs/2401.08649",
        "authors": [
            {
                "last_name": "Yi",
                "first_name": "Zexiang"
            },
            {
                "last_name": "Lian",
                "first_name": "Jing"
            },
            {
                "last_name": "Qi",
                "first_name": "Yunliang"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhaofei"
            },
            {
                "last_name": "Tang",
                "first_name": "Huajin"
            },
            {
                "last_name": "Ma",
                "first_name": "Yide"
            },
            {
                "last_name": "Liu",
                "first_name": "Jizhao"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Spiking Neural Networks (SNNs) capture the information processing mechanism of the brain by taking advantage of spiking neurons, such as the Leaky Integrate-and-Fire (LIF) model neuron, which incorporates temporal dynamics and transmits information via discrete and asynchronous spikes. However, the simplified biological properties of LIF ignore the neuronal coupling and dendritic structure of real neurons, which limits the spatio-temporal dynamics of neurons and thus reduce the expressive power of the resulting SNNs. In this work, we leverage a more biologically plausible neural model with complex dynamics, i.e., a pulse-coupled neural network (PCNN), to improve the expressiveness and recognition performance of SNNs for vision tasks. The PCNN is a type of cortical model capable of emulating the complex neuronal activities in the primary visual cortex. We construct deep pulse-coupled neural networks (DPCNNs) by replacing commonly used LIF neurons in SNNs with PCNN neurons. The intra-coupling in existing PCNN models limits the coupling between neurons only within channels. To address this limitation, we propose inter-channel coupling, which allows neurons in different feature maps to interact with each other. Experimental results show that inter-channel coupling can efficiently boost performance with fewer neurons, synapses, and less training time compared to widening the networks. For instance, compared to the LIF-based SNN with wide VGG9, DPCNN with VGG9 uses only 50%, 53%, and 73% of neurons, synapses, and training time, respectively. Furthermore, we propose receptive field and time dependent batch normalization (RFTD-BN) to speed up the convergence and performance of DPCNNs. ",
        "title": "Deep Pulse-Coupled Neural Networks",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08651",
        "abstract_url": "http://arxiv.org/abs/2401.08651",
        "authors": [
            {
                "last_name": "Monemi",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Fallah",
                "first_name": "Mohammad Amir"
            },
            {
                "last_name": "Rasti",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Latva-Aho",
                "first_name": "Matti"
            },
            {
                "last_name": "Debbah",
                "first_name": "Merouane"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Spot beamfocusing (SBF) is the process of focusing the signal power in a small spot-like region in the 3D space, which can be either hard-tuned (HT) using traditional tools like lenses and mirrors or electronically reconfigured (ER) using modern large-scale intelligent surface phased arrays. ER-SBF can be a key enabling technology (KET) for the next-generation 6G wireless networks offering benefits to many future wireless application areas such as wireless communication and security, mid-range wireless chargers, medical and health, physics, etc. Although near-field HT-SBF and ER-beamfocusing have been studied in the literature and applied in the industry, there is no comprehensive study of different aspects of ER-SBF and its future applications, especially for nonoptical (mmWave, sub-THz, and THz) electromagnetic waves in the next generation wireless technology, which is the aim of this paper. The theoretical concepts behind ER-SBF, different antenna technologies for implementing ER-SBF, employing machine learning (ML)-based schemes for enabling channel-state-information (CSI)-independent ER-SBF, and different practical application areas that can benefit from ER-SBF will be explored. ",
        "title": "Towards Near-Field 3D Spot Beamfocusing: Possibilities, Challenges, and  Use-cases",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08652",
        "abstract_url": "http://arxiv.org/abs/2401.08652",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Xiongfei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Gerui"
            },
            {
                "last_name": "Si",
                "first_name": "Yain-Whar"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "GT"
        ],
        "abstract": "  As coin-based rewards dwindle, transaction fees play an important role as mining incentives in Bitcoin. In this paper, we propose a novel mechanism called Efficient Dynamic Transaction Storage (EDTS) for dynamically allocating transactions among blocks to achieve efficient storage utilization. By leveraging a combination of Cuckoo Filter and Dynamic Transaction Storage (DTS) strategies, EDTS is able to improve the scalability while remaining sustainable even after the Bitcoin enters a transaction-fee regime. In addition to preventing deviant mining behaviors under the transaction-fee regime, EDTS can also provide differentiated transmission priorities based on transaction fees while allowing the investors to engage in pledging more transaction fees. In EDTS, we applied the multi-objective optimization algorithm U-NSGA-III to find the best DTS strategy and its corresponding attributes. Experimental results show that the EDTS mechanism together with the optimized DTS strategy can achieve a throughput of 325.3 TPS. The experimental results reveal that the scalability improvement of EDTS is superior to the performance of Bitcoin NG, which is the best known on-chain scaling solution, while maintaining the sustainability under the transaction-fee regime. ",
        "title": "An Efficient Dynamic Transaction Storage Mechanism for Sustainable High  Throughput Bitcoin",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08653",
        "abstract_url": "http://arxiv.org/abs/2401.08653",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Kui"
            },
            {
                "last_name": "Yu",
                "first_name": "Tao"
            },
            {
                "last_name": "Li",
                "first_name": "Zongdian"
            },
            {
                "last_name": "Sakaguchi",
                "first_name": "Kei"
            },
            {
                "last_name": "Hashash",
                "first_name": "Omar"
            },
            {
                "last_name": "Saad",
                "first_name": "Walid"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The concept of a digital twin (DT) plays a pivotal role in the ongoing digital transformation and has achieved significant strides for various wireless applications in recent years. In particular, the field of autonomous vehicles is a domain that is ripe for exploiting the concept of DT. Nevertheless, there are many challenges that include holistic consideration and integration of hardware, software, communication methods, and collaboration of edge/cloud computing. In this paper, an end-to-end (E2E) real-world smart mobility DT is designed and implemented for the purpose of autonomous driving. The proposed system utilizes roadside units (RSUs) and edge computing to capture real-world traffic information, which is then processed in the cloud to create a DT model. This DT model is then exploited to enable route planning services for the autonomous vehicle to avoid heavy traffic. Real-world experimental results show that the system reliability can reach 99.53% while achieving a latency that is 3.36% below the 3GPP recommended value of 100 ms for autonomous driving. These results clearly validate the effectiveness of the system according to practical 3GPP standards for sensor and state map sharing (SSMS) and information sharing. ",
        "title": "Digital Twins for Autonomous Driving: A Comprehensive Implementation and  Demonstration",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08654",
        "abstract_url": "http://arxiv.org/abs/2401.08654",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Kui"
            },
            {
                "last_name": "Li",
                "first_name": "Zongdian"
            },
            {
                "last_name": "Yu",
                "first_name": "Tao"
            },
            {
                "last_name": "Sakaguchi",
                "first_name": "Kei"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  During the past decade, smart mobility and intelligent vehicles have attracted increasing attention, because they promise to create a highly efficient and safe transportation system in the future. Meanwhile, digital twin, as an emerging technology, will play an important role in automated driving and intelligent transportation systems. This technology is applied in this paper to design a platform for smart mobility, providing large-scale route planning services. Utilizing sensing technologies and cloud/edge computing, we build a digital twin system model that reflects the static and dynamic objects from the real world in real time. With the smart mobility platform, we realize traffic monitoring and route planning through cooperative environment perception to help automated vehicles circumvent jams. A proof-of-concept test with a real vehicle in real traffic is conducted to validate the functions and the delay performance of the proposed platform. ",
        "title": "Smart Mobility Digital Twin for Automated Driving: Design and  Proof-of-Concept",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08655",
        "abstract_url": "http://arxiv.org/abs/2401.08655",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Inkyu"
            },
            {
                "last_name": "Cho",
                "first_name": "Jaewoong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "LG",
            "MM"
        ],
        "abstract": "  Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animation editing process. ",
        "title": "SAiD: Speech-driven Blendshape Facial Animation with Diffusion",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08656",
        "abstract_url": "http://arxiv.org/abs/2401.08656",
        "authors": [
            {
                "last_name": "Bartosz",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Szafraniec",
                "first_name": "Pawe\u0142"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we deal with a first order evolution inclusion involving a multivalued term generated by a Clarke subdifferential of a locally Lipschitz potential. For this problem we construct a double step time-semidiscrete approximation, known as the Rothe scheme. We study a sequence of solutions of the semidiscrete approximate problems and provide its weak convergence to a limit element that is a solution of the original problem. ",
        "title": "Convergence of double step scheme for a class of parabolic Clarke  subdifferential inclusions",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08658",
        "abstract_url": "http://arxiv.org/abs/2401.08658",
        "authors": [
            {
                "last_name": "Hao",
                "first_name": "Gongjin Lan an Qi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper aims to provide a quick review of the methods including the technologies in detail that are currently reported in industry and academia. Specifically, this paper reviews the end-to-end planning, including Tesla FSD V12, Momenta 2023, Horizon Robotics 2023, Motional RoboTaxi 2022, Woven Planet (Toyota): Urban Driver, and Nvidia. In addition, we review the state-of-the-art academic studies that investigate end-to-end planning of autonomous driving. This paper provides readers with a concise structure and fast learning of state-of-the-art end-to-end planning for 2022-2023. This article provides a meaningful overview as introductory material for beginners to follow the state-of-the-art end-to-end planning of autonomous driving in industry and academia, as well as supplementary material for advanced researchers. ",
        "title": "End-To-End Planning of Autonomous Driving in Industry and Academia:  2022-2023",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08659",
        "abstract_url": "http://arxiv.org/abs/2401.08659",
        "authors": [
            {
                "last_name": "\u0141odzikowski",
                "first_name": "Kacper"
            },
            {
                "last_name": "Foltz",
                "first_name": "Peter W."
            },
            {
                "last_name": "Behrens",
                "first_name": "John T."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  We discuss the implications of generative AI on education across four critical sections: the historical development of AI in education, its contemporary applications in learning, societal repercussions, and strategic recommendations for researchers. We propose ways in which generative AI can transform the educational landscape, primarily via its ability to conduct assessment of complex cognitive performances and create personalized content. We also address the challenges of effective educational tool deployment, data bias, design transparency, and accurate output verification. Acknowledging the societal impact, we emphasize the need for updating curricula, redefining communicative trust, and adjusting to transformed social norms. We end by outlining the ways in which educational stakeholders can actively engage with generative AI, develop fluency with its capacities and limitations, and apply these insights to steer educational practices in a rapidly advancing digital landscape. ",
        "title": "Generative AI and Its Educational Implications",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08660",
        "abstract_url": "http://arxiv.org/abs/2401.08660",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Gyeong-Geon"
            },
            {
                "last_name": "Latif",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Shi",
                "first_name": "Lehong"
            },
            {
                "last_name": "Zhai",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This study compared the classification performance of Gemini Pro and GPT-4V in educational settings. Employing visual question answering (VQA) techniques, the study examined both models' abilities to read text-based rubrics and then automatically score student-drawn models in science education. We employed both quantitative and qualitative analyses using a dataset derived from student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics for Image Feedback) prompting methods. The findings reveal that GPT-4V significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic Weighted Kappa. The qualitative analysis reveals that the differences may be due to the models' ability to process fine-grained texts in images and overall image classification performance. Even adapting the NERIF approach by further de-sizing the input images, Gemini Pro seems not able to perform as well as GPT-4V. The findings suggest GPT-4V's superior capability in handling complex multimodal educational tasks. The study concludes that while both models represent advancements in AI, GPT-4V's higher performance makes it a more suitable tool for educational applications involving multimodal data interpretation. ",
        "title": "Gemini Pro Defeated by GPT-4V: Evidence from Education",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08661",
        "abstract_url": "http://arxiv.org/abs/2401.08661",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Di"
            },
            {
                "last_name": "Li",
                "first_name": "Hao"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Tu",
                "first_name": "Huizhao"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Autonomous vehicles (AVs) have the potential to prevent accidents caused by drivers' error and reduce road traffic risks. Due to the nature of heavy vehicles, whose collisions cause more serious crashes, the weights of vehicles need to be considered when making driving strategies aimed at reducing the potential risks and their consequences in the context of autonomous driving. This study develops an autonomous driving strategy based on risk anticipation, considering the weights of surrounding vehicles and using hierarchical deep reinforcement learning. A risk indicator integrating surrounding vehicles' weights, based on the risk field theory, is proposed and incorporated into autonomous driving decisions. A hybrid action space is designed to allow for left lane changes, right lane changes and car-following, which enables AVs to act more freely and realistically whenever possible. To solve the above hybrid decision-making problem, a hierarchical proximal policy optimization (HPPO) algorithm is developed and an attention mechanism is incorporated, providing great advantages in maintaining stable performance. An indicator, potential collision energy in conflicts (PCEC), is newly proposed to evaluate the performance of the developed AV driving strategy from both the perspectives of the likelihood and the consequences of potential accidents. An application is carried out and the simulation results demonstrate that our model provides driving strategies that reduce both the likelihood and consequences of potential accidents, at the same time maintaining driving efficiency. The developed method is especially meaningful for AVs driving on highways, where heavy vehicles make up a high proportion of the traffic. ",
        "title": "Risk-anticipatory autonomous driving strategies considering vehicles'  weights, based on hierarchical deep reinforcement learning",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08662",
        "abstract_url": "http://arxiv.org/abs/2401.08662",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Ruikang"
            },
            {
                "last_name": "Mu",
                "first_name": "Xidong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yimeng"
            },
            {
                "last_name": "Jabor",
                "first_name": "Mona"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanwei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  A conception of mobile edge generation (MEG) is proposed, where generative artificial intelligence (GAI) models are distributed at edge servers (ESs) and user equipment (UE), enabling joint execution of generation tasks. Various distributed deployment schemes of the GAI model are proposed to alleviate the immense network load and long user queuing times for accessing GAI models. Two MEG frameworks are proposed, namely the single-ES framework and the multi-ESs framework. 1) A one-to-one joint generation framework between an ES and a UE is proposed, including four specific single-ES MEG protocols. These protocols allow distributed GAI models to transmit seeds or sketches for delivering information efficiently. 2) Several protocols are proposed for multi-ESs MEG, which enable multiple ESs to perform the generation task cooperatively or in parallel. Finally, a case study of a text-guided-image-to-image generation is provided, where a latent diffusion model is distributed at an ES and a UE. The simulation results demonstrate that the proposed protocols are able to generate high-quality images at extremely low signal-to-noise ratios. The proposed protocols can significantly reduce the communication overhead compared to the centralized model. ",
        "title": "Mobile Edge Generation: A New Era to 6G",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08663",
        "abstract_url": "http://arxiv.org/abs/2401.08663",
        "authors": [
            {
                "last_name": "Sever",
                "first_name": "Gulay Goktas"
            },
            {
                "last_name": "Demir",
                "first_name": "Umut"
            },
            {
                "last_name": "Satir",
                "first_name": "Abdullah Sadik"
            },
            {
                "last_name": "Sahin",
                "first_name": "Mustafa Cagatay"
            },
            {
                "last_name": "Ure",
                "first_name": "Nazim Kemal"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  In this paper, we present a methodology for constructing data-driven maneuver generation models for agile aircraft that can generalize across a wide range of trim conditions and aircraft model parameters. Maneuver generation models play a crucial role in the testing and evaluation of aircraft prototypes, providing insights into the maneuverability and agility of the aircraft. However, constructing the models typically requires extensive amounts of real pilot data, which can be time-consuming and costly to obtain. Moreover, models built with limited data often struggle to generalize beyond the specific flight conditions covered in the original dataset. To address these challenges, we propose a hybrid architecture that leverages a simulation model, referred to as the source model. This open-source agile aircraft simulator shares similar dynamics with the target aircraft and allows us to generate unlimited data for building a proxy maneuver generation model. We then fine-tune this model to the target aircraft using a limited amount of real pilot data. Our approach combines techniques from imitation learning, transfer learning, and reinforcement learning to achieve this objective. To validate our methodology, we utilize real agile pilot data provided by Turkish Aerospace Industries (TAI). By employing the F-16 as the source model, we demonstrate that it is possible to construct a maneuver generation model that generalizes across various trim conditions and aircraft parameters without requiring any additional real pilot data. Our results showcase the effectiveness of our approach in developing robust and adaptable models for agile aircraft. ",
        "title": "An Integrated Imitation and Reinforcement Learning Methodology for  Robust Agile Aircraft Control with Limited Pilot Demonstration Data",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08664",
        "abstract_url": "http://arxiv.org/abs/2401.08664",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Qingyao"
            },
            {
                "last_name": "Fu",
                "first_name": "Lingyue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weiming"
            },
            {
                "last_name": "Chen",
                "first_name": "Xianyu"
            },
            {
                "last_name": "Yu",
                "first_name": "Jingwei"
            },
            {
                "last_name": "Xia",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weinan"
            },
            {
                "last_name": "Tang",
                "first_name": "Ruiming"
            },
            {
                "last_name": "Yu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current development status, we further outline two approaches for an LLM-based education system: a unified approach and a mixture-of-expert (MoE) approach. Finally, we explore the challenges and future directions, providing new research opportunities and perspectives on adapting LLMs for education. ",
        "title": "Adapting Large Language Models for Education: Foundational Capabilities,  Potentials, and Challenges",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08666",
        "abstract_url": "http://arxiv.org/abs/2401.08666",
        "authors": [
            {
                "last_name": "Jaulin",
                "first_name": "Luc"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The rodwheel is a wheel equipped with a rod motorized on the axle. This paper proposes a Lagrangian approach to find the state equations of the rodwheel rolling on a plane without friction. The approach takes advantage of a symbolic computation. A controller is proposed to stabilize the rodwheel with the rod upward and going straight at a desired speed. ",
        "title": "Modeling and control of the rodwheel",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08667",
        "abstract_url": "http://arxiv.org/abs/2401.08667",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Sunwoong"
            },
            {
                "last_name": "Kim",
                "first_name": "Hojin"
            },
            {
                "last_name": "Hong",
                "first_name": "Yoonpyo"
            },
            {
                "last_name": "Yee",
                "first_name": "Kwanjung"
            },
            {
                "last_name": "Maulik",
                "first_name": "Romit"
            },
            {
                "last_name": "Kang",
                "first_name": "Namwoo"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "LG"
        ],
        "abstract": "  This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\\sim62\\%$ improvement over the single-fidelity approach. Finally, the uncertainty quantification performance of multi-fidelity DD-PINNs is investigated by the ensemble method to verify their potential in DT, where an accurate measure of predictive uncertainty is critical. The DD-PINN frameworks explored in this study are found to be more suitable for DT scenarios than traditional PINNs from the above perspectives, bringing engineers one step closer to seamless DT realization. ",
        "title": "Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08668",
        "abstract_url": "http://arxiv.org/abs/2401.08668",
        "authors": [
            {
                "last_name": "Neukart",
                "first_name": "Florian"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The P vs. NP problem, a cornerstone of computational theory, continues to elude resolution, traditionally approached through mathematical logic and algorithmic theory. This paper ventures beyond these confines, weaving together information theory, thermodynamics, and computational complexity to unveil a rich landscape of interdisciplinary intersections. Central to our discourse is the concept of entropy, examined through its information-theoretic subtleties and thermodynamic implications, particularly in relation to computational 'hardness'. We introduce Entropy-Driven Annealing (EDA) as a novel mechanism to elucidate the energy landscapes underlying computational problems, focusing on the inherent nature of NP problems. This approach hypothesizes a distinct thermodynamic profile for NP problems compared to P problems and speculates on possible thermodynamic pathways for polynomial-time solutions to NP challenges. The exploration extends to the practical application of EDA in studying protein-DNA complexes, grounding theoretical constructs in a biological context. While the P vs. NP conundrum remains unsolved, this interdisciplinary foray offers a fresh and insightful perspective on fundamental computational dilemmas, marking a step towards unraveling this complex puzzle. ",
        "title": "Thermodynamic Perspectives on Computational Complexity: Exploring the P  vs. NP Problem",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08669",
        "abstract_url": "http://arxiv.org/abs/2401.08669",
        "authors": [
            {
                "last_name": "Levin",
                "first_name": "Joshua"
            },
            {
                "last_name": "Correll",
                "first_name": "Randall"
            },
            {
                "last_name": "Ide",
                "first_name": "Takanori"
            },
            {
                "last_name": "Suzuki",
                "first_name": "Takafumi"
            },
            {
                "last_name": "Saito",
                "first_name": "Takaho"
            },
            {
                "last_name": "Arai",
                "first_name": "Alan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number of trucks and nodes, and then embedded into a large supply chain to yield solutions for larger numbers of trucks and nodes. We test our approach on a real supply chain environment arising in the operations of Japanese automotive parts manufacturer Aisin Corporation, and find that our algorithm outperforms Aisin's previous best solution. ",
        "title": "Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems  with Multi-Leg Demand Routes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08671",
        "abstract_url": "http://arxiv.org/abs/2401.08671",
        "authors": [
            {
                "last_name": "Holmes",
                "first_name": "Connor"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Masahiro"
            },
            {
                "last_name": "Wyatt",
                "first_name": "Michael"
            },
            {
                "last_name": "Awan",
                "first_name": "Ammar Ahmad"
            },
            {
                "last_name": "Rasley",
                "first_name": "Jeff"
            },
            {
                "last_name": "Rajbhandari",
                "first_name": "Samyam"
            },
            {
                "last_name": "Aminabadi",
                "first_name": "Reza Yazdani"
            },
            {
                "last_name": "Qin",
                "first_name": "Heyang"
            },
            {
                "last_name": "Bakhtiari",
                "first_name": "Arash"
            },
            {
                "last_name": "Kurilenko",
                "first_name": "Lev"
            },
            {
                "last_name": "He",
                "first_name": "Yuxiong"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF",
            "LG"
        ],
        "abstract": "  The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution. ",
        "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and  DeepSpeed-Inference",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08672",
        "abstract_url": "http://arxiv.org/abs/2401.08672",
        "authors": [
            {
                "last_name": "Rane",
                "first_name": "Sunayana"
            },
            {
                "last_name": "Bruna",
                "first_name": "Polyphony J."
            },
            {
                "last_name": "Sucholutsky",
                "first_name": "Ilia"
            },
            {
                "last_name": "Kello",
                "first_name": "Christopher"
            },
            {
                "last_name": "Griffiths",
                "first_name": "Thomas L."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment. ",
        "title": "Concept Alignment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08673",
        "abstract_url": "http://arxiv.org/abs/2401.08673",
        "authors": [
            {
                "last_name": "Tolnai",
                "first_name": "Bal\u00e1zs Andr\u00e1s"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Crowdsourcing data science competitions has become popular as a cost-effective alternative to solving complex energy-related challenges. How-ever, comprehensive reviews on hosting processes remain scarce. Therefore, this paper undertakes a detailed review of 33 existing data competitions and 12 hosting platforms, complemented by an in-depth case study of the ADRENALIN load disaggregation competition. The review identifies essential elements of competition procedure, including platform selection, timeline, datasets, and submission and evaluation mechanisms. Based on proposed 16 evaluation criteria, the similarities and differences between data competition hosting platforms can be categorized into platform scoring and popularity, platform features, community engagement, open-source platforms, region-specific platforms, platform-specific purposes, and multi-purpose platforms. The case study underscores strategic planning's critical role, particularly platform selection. The case study also shows the importance of defining competition scope which influences the whole com-petition content and procedure, especially the datasets. ",
        "title": "Standard energy data competition procedure: A comprehensive review with  a case study of the ADRENALIN load disaggregation competition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08678",
        "abstract_url": "http://arxiv.org/abs/2401.08678",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Han"
            },
            {
                "last_name": "Wang",
                "first_name": "Mou"
            },
            {
                "last_name": "Bai",
                "first_name": "Jisheng"
            },
            {
                "last_name": "Shi",
                "first_name": "Dongyuan"
            },
            {
                "last_name": "Gan",
                "first_name": "Woon-Seng"
            },
            {
                "last_name": "Chen",
                "first_name": "Jianfeng"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  This paper presents a detailed description of our proposed methods for the ICASSP 2024 Cadenza Challenge. Experimental results show that the proposed system can achieve better performance than official baselines. ",
        "title": "Sub-band and Full-band Interactive U-Net with DPRNN for Demixing  Cross-talk Stereo Music",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08682",
        "abstract_url": "http://arxiv.org/abs/2401.08682",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Akito"
            },
            {
                "last_name": "Mohri",
                "first_name": "Hitomi"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Although several methodologies for identifying the genealogy of video game genres and showing their relationships have been proposed in existing research, there have been few attempts to visualize the genealogy of a genre in a quantitative and qualitative manner. In this study, we propose a methodology to identify the scope of a specific game genre and to show how game specifications specific to that genre are related to each other. ",
        "title": "Visualizing the genealogy of video game specifications of video game  genres -- through a case study of the raising up game genre",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08683",
        "abstract_url": "http://arxiv.org/abs/2401.08683",
        "authors": [
            {
                "last_name": "Sandal",
                "first_name": "Selim"
            },
            {
                "last_name": "Akturk",
                "first_name": "Ismail"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG",
            "PL",
            "SE"
        ],
        "abstract": "  The design and optimization of hardware have traditionally been resource-intensive, demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies, this paper aims to use large language models that accepts high-level design specifications through a single prompt to generate corresponding Register-Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation, we demonstrate the shortcoming of existing attention mechanisms, and present the abilities of language models to produce functional, optimized, and industry-standard compliant RTL code when a novel attention mechanism is used. These findings underscore the expanding role of large language models in shaping the future landscape of architectural exploration and automation in hardware design. ",
        "title": "Zero-Shot RTL Code Generation with Attention Sink Augmented Large  Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08684",
        "abstract_url": "http://arxiv.org/abs/2401.08684",
        "authors": [
            {
                "last_name": "Naeini",
                "first_name": "Saeed Saviz"
            },
            {
                "last_name": "Snaiki",
                "first_name": "Reda"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Wave runup is a critical factor affecting coastal flooding, shoreline changes, and damage to coastal structures. Climate change is also expected to amplify wave runup's impact on coastal areas. Therefore, fast and accurate wave runup estimation is essential for effective coastal engineering design and management. However, predicting the time-dependent wave runup is challenging due to the intrinsic nonlinearities and non-stationarity of the process, even with the use of the most advanced machine learning techniques. In this study, a physics-informed machine learning-based approach is proposed to efficiently and accurately simulate time-series wave runup. The methodology combines the computational efficiency of the Surfbeat (XBSB) mode with the accuracy of the nonhydrostatic (XBNH) mode of the XBeach model. Specifically, a conditional generative adversarial network (cGAN) is used to map the image representation of wave runup from XBSB to the corresponding image from XBNH. These images are generated by first converting wave runup signals into time-frequency scalograms and then transforming them into image representations. The cGAN model achieves improved performance in image-to-image mapping tasks by incorporating physics-based knowledge from XBSB. After training the model, the high-fidelity XBNH-based scalograms can be predicted, which are then employed to reconstruct the time-series wave runup using the inverse wavelet transform. The simulation results underscore the efficiency and robustness of the proposed model in predicting wave runup, suggesting its potential value for applications in risk assessment and management. ",
        "title": "A Physics-informed machine learning model for time-dependent wave runup  prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08685",
        "abstract_url": "http://arxiv.org/abs/2401.08685",
        "authors": [
            {
                "last_name": "Santos",
                "first_name": "Ezequiel"
            },
            {
                "last_name": "Castillo",
                "first_name": "Vanessa"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This paper objectively analyzes the emerging discourse surrounding Apple Vision Pro's application in healthcare and medical education. Released in June 2023, Apple Vision Pro represents a significant advancement in spatial computing, combining augmented and virtual reality to create new possibilities in digital interaction. We aim to compile and present recent articles. We used PubMed, IEEE Xplore, Google Scholar, and JSTOR. Non-academic publications were excluded. The results were six commentaries, one a pre-print. All were majorly optimistic, with one mentioning VR/AR sickness. For future research directions, we stress the need for continued exploration of Apple Vision Pro's capabilities and limitations and expect expert opinions to englobe this discussion. ",
        "title": "Apple Vision Pro: Comments in Healthcare",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08686",
        "abstract_url": "http://arxiv.org/abs/2401.08686",
        "authors": [
            {
                "last_name": "Silva",
                "first_name": "Andr\u00e9 Luiz B. Vieira e"
            },
            {
                "last_name": "Sim\u00f5es",
                "first_name": "Francisco"
            },
            {
                "last_name": "Kowerko",
                "first_name": "Danny"
            },
            {
                "last_name": "Schlosser",
                "first_name": "Tobias"
            },
            {
                "last_name": "Battisti",
                "first_name": "Felipe"
            },
            {
                "last_name": "Teichrieb",
                "first_name": "Veronica"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Within (semi-)automated visual inspection, learning-based approaches for assessing visual defects, including deep neural networks, enable the processing of otherwise small defect patterns in pixel size on high-resolution imagery. The emergence of these often rarely occurring defect patterns explains the general need for labeled data corpora. To not only alleviate this issue but to furthermore advance the current state of the art in unsupervised visual inspection, this contribution proposes a DifferNet-based solution enhanced with attention modules utilizing SENet and CBAM as backbone - AttentDifferNet - to improve the detection and classification capabilities on three different visual inspection and anomaly detection datasets: MVTec AD, InsPLAD-fault, and Semiconductor Wafer. In comparison to the current state of the art, it is shown that AttentDifferNet achieves improved results, which are, in turn, highlighted throughout our quantitative as well as qualitative evaluation, indicated by a general improvement in AUC of 94.34 vs. 92.46, 96.67 vs. 94.69, and 90.20 vs. 88.74%. As our variants to AttentDifferNet show great prospects in the context of currently investigated approaches, a baseline is formulated, emphasizing the importance of attention for anomaly detection. ",
        "title": "Attention Modules Improve Modern Image-Level Anomaly Detection: A  DifferNet Case Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08687",
        "abstract_url": "http://arxiv.org/abs/2401.08687",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Kai"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            },
            {
                "last_name": "Shao",
                "first_name": "Ling"
            },
            {
                "last_name": "Lu",
                "first_name": "Shijian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Camera-only Bird's Eye View (BEV) has demonstrated great potential in environment perception in a 3D space. However, most existing studies were conducted under a supervised setup which cannot scale well while handling various new data. Unsupervised domain adaptive BEV, which effective learning from various unlabelled target data, is far under-explored. In this work, we design DA-BEV, the first domain adaptive camera-only BEV framework that addresses domain adaptive BEV challenges by exploiting the complementary nature of image-view features and BEV features. DA-BEV introduces the idea of query into the domain adaptation framework to derive useful information from image-view and BEV features. It consists of two query-based designs, namely, query-based adversarial learning (QAL) and query-based self-training (QST), which exploits image-view features or BEV features to regularize the adaptation of the other. Extensive experiments show that DA-BEV achieves superior domain adaptive BEV perception performance consistently across multiple datasets and tasks such as 3D object detection and 3D scene segmentation. ",
        "title": "DA-BEV: Unsupervised Domain Adaptation for Bird's Eye View Perception",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08688",
        "abstract_url": "http://arxiv.org/abs/2401.08688",
        "authors": [
            {
                "last_name": "Ganesan",
                "first_name": "Balaji"
            },
            {
                "last_name": "Ravikumar",
                "first_name": "Arjun"
            },
            {
                "last_name": "Piplani",
                "first_name": "Lakshay"
            },
            {
                "last_name": "Bhaumik",
                "first_name": "Rini"
            },
            {
                "last_name": "Padmanaban",
                "first_name": "Dhivya"
            },
            {
                "last_name": "Narasimhamurthy",
                "first_name": "Shwetha"
            },
            {
                "last_name": "Adhikary",
                "first_name": "Chetan"
            },
            {
                "last_name": "Deshapogu",
                "first_name": "Subhash"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Automated answer validation can help improve learning outcomes by providing appropriate feedback to learners, and by making question answering systems and online learning solutions more widely available. There have been some works in science question answering which show that information retrieval methods outperform neural methods, especially in the multiple choice version of this problem. We implement Siamese neural network models and produce a generalised solution to this problem. We compare our supervised model with other text similarity based solutions. ",
        "title": "Automated Answer Validation using Text Similarity",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08689",
        "abstract_url": "http://arxiv.org/abs/2401.08689",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Jingqiu"
            },
            {
                "last_name": "Zou",
                "first_name": "Aojun"
            },
            {
                "last_name": "Li",
                "first_name": "Hongshen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Out-of-distribution (OOD) detection is a crucial part of deploying machine learning models safely. It has been extensively studied with a plethora of methods developed in the literature. This problem is tackled with an OOD score computation, however, previous methods compute the OOD scores with limited usage of the in-distribution dataset. For instance, the OOD scores are computed with information from a small portion of the in-distribution data. Furthermore, these methods encode images with a neural image encoder. The robustness of these methods is rarely checked with respect to image encoders of different training methods and architectures. In this work, we introduce the diffusion process into the OOD task. The diffusion model integrates information on the whole training set into the predicted noise vectors. What's more, we deduce a closed-form solution for the noise vector (stable point). Then the noise vector is converted into our OOD score, we test both the deep model predicted noise vector and the closed-form noise vector on the OOD benchmarks \\cite{openood}. Our method outperforms previous OOD methods across all types of image encoders (Table. \\ref{main}). A $3.5\\%$ performance gain is achieved with the MAE-based image encoder. Moreover, we studied the robustness of OOD methods by applying different types of image encoders. Some OOD methods failed to generalize well when switching image encoders from ResNet to Vision Transformers, our method performs exhibits good robustness with all the image encoders. ",
        "title": "NODI: Out-Of-Distribution Detection with Noise from Diffusion",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08690",
        "abstract_url": "http://arxiv.org/abs/2401.08690",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lu"
            },
            {
                "last_name": "Du",
                "first_name": "Chao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Pu"
            },
            {
                "last_name": "Luo",
                "first_name": "Chuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zhangchi"
            },
            {
                "last_name": "Qiao",
                "first_name": "Bo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Lin",
                "first_name": "Qingwei"
            },
            {
                "last_name": "Rajmohan",
                "first_name": "Saravan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dongmei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  As one of the most effective self-supervised representation learning methods, contrastive learning (CL) relies on multiple negative pairs to contrast against each positive pair. In the standard practice of contrastive learning, data augmentation methods are utilized to generate both positive and negative pairs. While existing works have been focusing on improving the positive sampling, the negative sampling process is often overlooked. In fact, the generated negative samples are often polluted by positive samples, which leads to a biased loss and performance degradation. To correct the negative sampling bias, we propose a novel contrastive learning method named Positive-Unlabeled Contrastive Learning (PUCL). PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss. We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss. PUCL can be applied to general contrastive learning problems and outperforms state-of-the-art methods on various image and graph classification tasks. The code of PUCL is in the supplementary file. ",
        "title": "Contrastive Learning with Negative Sampling Correction",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08691",
        "abstract_url": "http://arxiv.org/abs/2401.08691",
        "authors": [
            {
                "last_name": "Castelnovo",
                "first_name": "Alessandro"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as \"Responsible AI\". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcoming European regulations and for promoting societal good. This thesis is structured around three fundamental pillars: understanding bias, mitigating bias, and accounting for bias. These contributions are validated through their practical application in real-world scenarios, in collaboration with Intesa Sanpaolo. This collaborative effort not only contributes to our understanding of fairness but also provides practical tools for the responsible implementation of AI-based decision-making systems. In line with open-source principles, we have released Bias On Demand and FairView as accessible Python packages, further promoting progress in the field of AI fairness. ",
        "title": "Towards Responsible AI in Banking: Addressing Bias for Fair  Decision-Making",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08694",
        "abstract_url": "http://arxiv.org/abs/2401.08694",
        "authors": [
            {
                "last_name": "Rivera",
                "first_name": "Mauricio"
            },
            {
                "last_name": "Godbout",
                "first_name": "Jean-Fran\u00e7ois"
            },
            {
                "last_name": "Rabbany",
                "first_name": "Reihaneh"
            },
            {
                "last_name": "Pelrine",
                "first_name": "Kellin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications. ",
        "title": "Combining Confidence Elicitation and Sample-based Methods for  Uncertainty Quantification in Misinformation Mitigation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08695",
        "abstract_url": "http://arxiv.org/abs/2401.08695",
        "authors": [
            {
                "last_name": "Fang",
                "first_name": "Zhengqing"
            },
            {
                "last_name": "Zhou",
                "first_name": "Shuowen"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zhouhang"
            },
            {
                "last_name": "Si",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Li",
                "first_name": "Mengze"
            },
            {
                "last_name": "Li",
                "first_name": "Jinxu"
            },
            {
                "last_name": "Xu",
                "first_name": "Yesheng"
            },
            {
                "last_name": "Xie",
                "first_name": "Wenjia"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Li",
                "first_name": "Yingming"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            },
            {
                "last_name": "Yao",
                "first_name": "Yu-Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC"
        ],
        "abstract": "  Although data-driven artificial intelligence (AI) in medical image diagnosis has shown impressive performance in silico, the lack of interpretability makes it difficult to incorporate the \"black box\" into clinicians' workflows. To make the diagnostic patterns learned from data understandable by clinicians, we develop an interpretable model, knowledge-guided diagnosis model (KGDM), that provides a visualized reasoning process containing AI-based biomarkers and retrieved cases that with the same diagnostic patterns. It embraces clinicians' prompts into the interpreted reasoning through human-AI interaction, leading to potentially enhanced safety and more accurate predictions. This study investigates the performance, interpretability, and clinical utility of KGDM in the diagnosis of infectious keratitis (IK), which is the leading cause of corneal blindness. The classification performance of KGDM is evaluated on a prospective validation dataset, an external testing dataset, and an publicly available testing dataset. The diagnostic odds ratios (DOR) of the interpreted AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit consistent diagnostic patterns with clinic experience. Moreover, a human-AI collaborative diagnosis test is conducted and the participants with collaboration achieved a performance exceeding that of both humans and AI. By synergistically integrating interpretability and interaction, this study facilitates the convergence of clinicians' expertise and data-driven intelligence. The promotion of inexperienced ophthalmologists with the aid of AI-based biomarkers, as well as increased AI prediction by intervention from experienced ones, demonstrate a promising diagnostic paradigm for infectious keratitis using KGDM, which holds the potential for extension to other diseases where experienced medical practitioners are limited and the safety of AI is concerned. ",
        "title": "Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by  Integrating Expert Knowledge and Interpretable Data-driven Intelligence",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08696",
        "abstract_url": "http://arxiv.org/abs/2401.08696",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Mingzhe"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jieru"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhe"
            },
            {
                "last_name": "Guo",
                "first_name": "Minyi"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  High-level synthesis (HLS) notably speeds up the hardware design process by avoiding RTL programming. However, the turnaround time of HLS increases significantly when post-route quality of results (QoR) are considered during optimization. To tackle this issue, we propose a hierarchical post-route QoR prediction approach for FPGA HLS, which features: (1) a modeling flow that directly estimates latency and post-route resource usage from C/C++ programs; (2) a graph construction method that effectively represents the control and data flow graph of source code and effects of HLS pragmas; and (3) a hierarchical GNN training and prediction method capable of capturing the impact of loop hierarchies. Experimental results show that our method presents a prediction error of less than 10% for different types of QoR metrics, which gains tremendous improvement compared with the state-of-the-art GNN methods. By adopting our proposed methodology, the runtime for design space exploration in HLS is shortened to tens of minutes and the achieved ADRS is reduced to 6.91% on average. ",
        "title": "Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis  with GNNs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08699",
        "abstract_url": "http://arxiv.org/abs/2401.08699",
        "authors": [
            {
                "last_name": "Tizhoosh",
                "first_name": "H. R."
            },
            {
                "last_name": "Pantanowitz",
                "first_name": "Liron"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "IR"
        ],
        "abstract": "  Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work. ",
        "title": "On Image Search in Histopathology",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08700",
        "abstract_url": "http://arxiv.org/abs/2401.08700",
        "authors": [
            {
                "last_name": "Sikirica",
                "first_name": "Ante"
            },
            {
                "last_name": "Lu\u010din",
                "first_name": "Ivana"
            },
            {
                "last_name": "Alvir",
                "first_name": "Marta"
            },
            {
                "last_name": "Kranj\u010devi\u0107",
                "first_name": "Lado"
            },
            {
                "last_name": "\u010carija",
                "first_name": "Zoran"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  This study aims to provide a comprehensive assessment of single-objective and multi-objective optimisation algorithms for the design of an elbow-type draft tube, as well as to introduce a computationally efficient optimisation workflow. The proposed workflow leverages deep neural network surrogates trained on data obtained from numerical simulations. The use of surrogates allows for a more flexible and faster evaluation of novel designs. The success history-based adaptive differential evolution with linear reduction and the multi-objective evolutionary algorithm based on decomposition were identified as the best-performing algorithms and used to determine the influence of different objectives in the single-objective optimisation and their combined impact on the draft tube design in the multi-objective optimisation. The results for the single-objective algorithm are consistent with those of the multi-objective algorithm when the objectives are considered separately. Multi-objective approach, however, should typically be chosen, especially for computationally inexpensive surrogates. A multi-criteria decision analysis method was used to obtain optimal multi-objective results, showing an improvement of 1.5% and 17% for the pressure recovery factor and drag coefficient, respectively. The difference between the predictions and the numerical results is less than 0.5% for the pressure recovery factor and 3% for the drag coefficient. As the demand for renewable energy continues to increase, the relevance of data-driven optimisation workflows, as discussed in this study, will become increasingly important, especially in the context of global sustainability efforts. ",
        "title": "Computationally Efficient Optimisation of Elbow-Type Draft Tube Using  Neural Network Surrogates",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08702",
        "abstract_url": "http://arxiv.org/abs/2401.08702",
        "authors": [
            {
                "last_name": "Hoffman",
                "first_name": "Kentaro"
            },
            {
                "last_name": "Salerno",
                "first_name": "Stephen"
            },
            {
                "last_name": "Afiaz",
                "first_name": "Awan"
            },
            {
                "last_name": "Leek",
                "first_name": "Jeffrey T."
            },
            {
                "last_name": "McCormick",
                "first_name": "Tyler H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g. rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as outcome variables. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to this so-called ``post-prediction inference'' problem and elucidate three potential sources of error: (i) the relationship between predicted outcomes and their true, unobserved counterparts, (ii) robustness of the machine learning model to resampling or uncertainty about the training data, and (iii) appropriately propagating not just bias but also uncertainty from predictions into the ultimate inference procedure. We also contrast the framework for post-prediction inference with classical work spanning several related fields, including survey sampling, missing data, and semi-supervised learning. This contrast elucidates the role of design in both classical and modern inference problems. ",
        "title": "Do We Really Even Need Data?",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08703",
        "abstract_url": "http://arxiv.org/abs/2401.08703",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Guowei"
            },
            {
                "last_name": "Ding",
                "first_name": "Changxing"
            },
            {
                "last_name": "Tan",
                "first_name": "Wentao"
            },
            {
                "last_name": "Tan",
                "first_name": "Mingkui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class's pseudo-feature from a memory in a momentum manner and insert an additional DPL loss. Finally, we introduce a consistency regularization-based approach to leverage samples with unconfident pseudo-labels. This approach transfers feature styles of samples with unconfident pseudo-labels to those with confident pseudo-labels. Thus, more reliable samples for TTA are created. The experimental results demonstrate that our methods achieve state-of-the-art performance on domain generalization benchmarks, and reliably improve the performance of self-training-based methods on image corruption benchmarks. The code will be released. ",
        "title": "Decoupled Prototype Learning for Reliable Test-Time Adaptation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08710",
        "abstract_url": "http://arxiv.org/abs/2401.08710",
        "authors": [
            {
                "last_name": "Maioli",
                "first_name": "Andrea"
            },
            {
                "last_name": "Quinones",
                "first_name": "Kevin A."
            },
            {
                "last_name": "Ahmed",
                "first_name": "Saad"
            },
            {
                "last_name": "Alizai",
                "first_name": "Muhammad H."
            },
            {
                "last_name": "Mottola",
                "first_name": "Luca"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "OS"
        ],
        "abstract": "  We present hardware/software techniques to intelligently regulate supply voltage and clock frequency of intermittently-computing devices. These devices rely on ambient energy harvesting to power their operation and small capacitors as energy buffers. Statically setting their clock frequency fails to capture the unique relations these devices expose between capacitor voltage, energy efficiency at a given operating frequency, and the corresponding operating range. Existing dynamic voltage and frequency scaling techniques are also largely inapplicable due to extreme energy scarcity and peculiar hardware features. We introduce two hardware/software co-designs that accommodate the distinct hardware features and function within a constrained energy envelope, offering varied trade-offs and functionalities. Our experimental evaluation combines tests on custom-manufactured hardware and detailed emulation experiments. The data gathered indicate that our approaches result in up to 3.75x reduced energy consumption and 12x swifter execution times compared to the considered baselines, all while utilizing smaller capacitors to accomplish identical workloads. ",
        "title": "Dynamic Voltage and Frequency Scaling for Intermittent Computing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08711",
        "abstract_url": "http://arxiv.org/abs/2401.08711",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Anuj"
            },
            {
                "last_name": "Atef",
                "first_name": "Yasser"
            },
            {
                "last_name": "Mills",
                "first_name": "Anna"
            },
            {
                "last_name": "Bali",
                "first_name": "Maha"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  This study explores how discussing metaphors for AI can help build awareness of the frames that shape our understanding of AI systems, particularly large language models (LLMs) like ChatGPT. Given the pressing need to teach \"critical AI literacy\", discussion of metaphor provides an opportunity for inquiry and dialogue with space for nuance, playfulness, and critique. Using a collaborative autoethnographic methodology, we analyzed metaphors from a range of sources, and reflected on them individually according to seven questions, then met and discussed our interpretations. We then analyzed how our reflections contributed to the three kinds of literacies delineated in Selber's multiliteracies framework: functional, critical, and rhetorical. These allowed us to analyze questions of ethics, equity, and accessibility in relation to AI. We explored each metaphor along the dimension of whether or not it was promoting anthropomorphizing, and to what extent such metaphors imply that AI is sentient. Our findings highlight the role of metaphor reflection in fostering a nuanced understanding of AI, suggesting that our collaborative autoethnographic approach as well as the heuristic model of plotting AI metaphors on dimensions of anthropomorphism and multiliteracies, might be useful for educators and researchers in the pursuit of advancing critical AI literacy. ",
        "title": "Assistant, Parrot, or Colonizing Loudspeaker? ChatGPT Metaphors for  Developing Critical AI Literacies",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08712",
        "abstract_url": "http://arxiv.org/abs/2401.08712",
        "authors": [
            {
                "last_name": "O",
                "first_name": "M. Mehdi Owrang"
            },
            {
                "last_name": "Horestani",
                "first_name": "Fariba Jafari"
            },
            {
                "last_name": "Schwarz",
                "first_name": "Ginger"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Breast cancer prognosis is crucial for effective treatment, with the disease more common in women over 40 years old but rare under 40 years old, where less than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in younger women, which varies by ethnicity. Breast cancers are classified based on receptors like estrogen, progesterone, and HER2. Triple-negative breast cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases and is more prevalent in younger patients, often resulting in poorer outcomes. Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like age, race, tumor grade, size, and lymph node status are studied for their role in TNBC's clinical outcomes, but current research is inconclusive about age-related differences. This study uses SEER data set to examine the influence of younger age on survivability in TNBC patients, aiming to determine if age is a significant prognostic factor. Our experimental results on SEER dataset confirm the existing research reports that TNBC patients have worse prognosis compared to non-TNBC based on age. Our main goal was to investigate whether younger age has any significance on the survivability of TNBC patients. Experimental results do not show that younger age has any significance on the prognosis and survival rate of the TNBC patients ",
        "title": "Survival Analysis of Young Triple-Negative Breast Cancer Patients",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08714",
        "abstract_url": "http://arxiv.org/abs/2401.08714",
        "authors": [
            {
                "last_name": "Bisio",
                "first_name": "Alessia"
            },
            {
                "last_name": "Yeguas-Bol\u00edvar",
                "first_name": "Enrique"
            },
            {
                "last_name": "Aparicio-Mart\u00ednez",
                "first_name": "Pilar"
            },
            {
                "last_name": "Redel-Mac\u00edas",
                "first_name": "Mar\u00eda Dolores"
            },
            {
                "last_name": "Pinzi",
                "first_name": "Sara"
            },
            {
                "last_name": "Rossi",
                "first_name": "Stefano"
            },
            {
                "last_name": "Taborri",
                "first_name": "Juri"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV",
            "GR"
        ],
        "abstract": "  Structured hand gestures that incorporate visual motions and signs are used in sign language. Sign language is a valuable means of daily communication for individuals who are deaf or have speech impairments, but it is still rare among hearing people, and fewer are capable of understand it. Within the academic context, parents and teachers play a crucial role in supporting deaf students from childhood by facilitating their learning of sign language. In the last years, among all the teaching tools useful for learning sign language, the use of Virtual Reality (VR) has increased, as it has been demonstrated to improve retention, memory and attention during the learning process. The ISENSE project has been created to assist students with deafness during their academic life by proposing different technological tools for teaching sign language to the hearing community in the academic context. As part of the ISENSE project, this work aims to develop an application for Spanish and Italian sign language recognition that exploits the VR environment to quickly and easily create a comprehensive database of signs and an Artificial Intelligence (AI)-based software to accurately classify and recognize static and dynamic signs: from letters to sentences. ",
        "title": "Training program on sign language: social inclusion through Virtual  Reality in ISENSE project",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08715",
        "abstract_url": "http://arxiv.org/abs/2401.08715",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Yifan"
            },
            {
                "last_name": "Dehaghani",
                "first_name": "M. Rahmani"
            },
            {
                "last_name": "Sajadi",
                "first_name": "Pouyan"
            },
            {
                "last_name": "Wang",
                "first_name": "G. Gary"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Considering data insufficiency in metal additive manufacturing (AM), transfer learning (TL) has been adopted to extract knowledge from source domains (e.g., completed printings) to improve the modeling performance in target domains (e.g., new printings). Current applications use all accessible source data directly in TL with no regard to the similarity between source and target data. This paper proposes a systematic method to find appropriate subsets of source data based on similarities between the source and target datasets for a given set of limited target domain data. Such similarity is characterized by the spatial and model distance metrics. A Pareto frontier-based source data selection method is developed, where the source data located on the Pareto frontier defined by two similarity distance metrics are selected iteratively. The method is integrated into an instance-based TL method (decision tree regression model) and a model-based TL method (fine-tuned artificial neural network). Both models are then tested on several regression tasks in metal AM. Comparison results demonstrate that 1) the source data selection method is general and supports integration with various TL methods and distance metrics, 2) compared with using all source data, the proposed method can find a small subset of source data from the same domain with better TL performance in metal AM regression tasks involving different processes and machines, and 3) when multiple source domains exist, the source data selection method could find the subset from one source domain to obtain comparable or better TL performance than the model constructed using data from all source domains. ",
        "title": "Selecting Subsets of Source Data for Transfer Learning with Applications  in Metal Additive Manufacturing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08717",
        "abstract_url": "http://arxiv.org/abs/2401.08717",
        "authors": [
            {
                "last_name": "Roman",
                "first_name": "Adrian S."
            },
            {
                "last_name": "Roman",
                "first_name": "Iran R."
            },
            {
                "last_name": "Bello",
                "first_name": "Juan P."
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Direction of arrival estimation (DoAE) aims at tracking a sound in azimuth and elevation. Recent advancements include data-driven models with inputs derived from ambisonics intensity vectors or correlations between channels in a microphone array. A spherical intensity map (SIM), or acoustic image, is an alternative input representation that remains underexplored. SIMs benefit from high-resolution microphone arrays, yet most DoAE datasets use low-resolution ones. Therefore, we first propose a super-resolution method to upsample low-resolution microphones. Next, we benchmark DoAE models that use SIMs as input. We arrive to a model that uses SIMs for DoAE estimation and outperforms a baseline and a state-of-the-art model. Our study highlights the relevance of acoustic imaging for DoAE tasks. ",
        "title": "Robust DOA estimation using deep acoustic imaging",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08718",
        "abstract_url": "http://arxiv.org/abs/2401.08718",
        "authors": [
            {
                "last_name": "Azmat",
                "first_name": "Adnan"
            },
            {
                "last_name": "Yi",
                "first_name": "Su Su"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper introduces the Expected Booking (xB) model, a novel metric designed to estimate the likelihood of a foul resulting in a yellow card in football. Through three iterative experiments, employing ensemble methods, the model demonstrates improved performance with additional features and an expanded dataset. Analysis of FIFA World Cup 2022 data validates the model's efficacy in providing insights into team and player fouling tactics, aligning with actual defensive performance. The xB model addresses a gap in fouling efficiency examination, emphasizing defensive strategies which often overlooked. Further enhancements are suggested through the incorporation of comprehensive data and spatial features. ",
        "title": "Investigating Fouling Efficiency in Football Using Expected Booking (xB)  Model",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08719",
        "abstract_url": "http://arxiv.org/abs/2401.08719",
        "authors": [
            {
                "last_name": "Baik",
                "first_name": "Seung-Yeop"
            },
            {
                "last_name": "Jeon",
                "first_name": "Mingi"
            },
            {
                "last_name": "Hahn",
                "first_name": "Joonghyuk"
            },
            {
                "last_name": "Kim",
                "first_name": "Jungin"
            },
            {
                "last_name": "Han",
                "first_name": "Yo-Sub"
            },
            {
                "last_name": "Ko",
                "first_name": "Sang-Ki"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CC"
        ],
        "abstract": "  Analyzing the worst-case time complexity of a code is a crucial task in computer science and software engineering for ensuring the efficiency, reliability, and robustness of software systems. However, it is well-known that the problem of determining the worst-case time complexity of a given code written in general-purpose programming language is theoretically undecidable by the famous Halting problem proven by Alan Turing. Thus, we move towards more realistic scenarios where the inputs and outputs of a program exist. This allows us to discern the correctness of given codes, challenging to analyze their time complexity exhaustively. In response to this challenge, we introduce CodeComplex, a novel source code dataset where each code is manually annotated with a corresponding worst-case time complexity. CodeComplex comprises 4,900 Java codes and an equivalent number of Python codes, all sourced from programming competitions and annotated with complexity labels by a panel of algorithmic experts. To the best of our knowledge, CodeComplex stands as the most extensive code dataset tailored for predicting complexity. Subsequently, we present the outcomes of our experiments employing various baseline models, leveraging state-of-the-art neural models in code comprehension like CodeBERT, GraphCodeBERT, UniXcoder, PLBART, CodeT5, CodeT5+, and ChatGPT. We analyze how the dataset impacts the model's learning in predicting time complexity. ",
        "title": "CodeComplex: A Time-Complexity Dataset for Bilingual Source Codes",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08720",
        "abstract_url": "http://arxiv.org/abs/2401.08720",
        "authors": [
            {
                "last_name": "Roggiolani",
                "first_name": "Gianmarco"
            },
            {
                "last_name": "Magistri",
                "first_name": "Federico"
            },
            {
                "last_name": "Guadagnino",
                "first_name": "Tiziano"
            },
            {
                "last_name": "Behley",
                "first_name": "Jens"
            },
            {
                "last_name": "Stachniss",
                "first_name": "Cyrill"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Crops for food, feed, fiber, and fuel are key natural resources for our society. Monitoring plants and measuring their traits is an important task in agriculture often referred to as plant phenotyping. Traditionally, this task is done manually, which is time- and labor-intensive. Robots can automate phenotyping providing reproducible and high-frequency measurements. Today's perception systems use deep learning to interpret these measurements, but require a substantial amount of annotated data to work well. Obtaining such labels is challenging as it often requires background knowledge on the side of the labelers. This paper addresses the problem of reducing the labeling effort required to perform leaf instance segmentation on 3D point clouds, which is a first step toward phenotyping in 3D. Separating all leaves allows us to count them and compute relevant traits as their areas, lengths, and widths. We propose a novel self-supervised task-specific pre-training approach to initialize the backbone of a network for leaf instance segmentation. We also introduce a novel automatic postprocessing that considers the difficulty of correctly segmenting the points close to the stem, where all the leaves petiole overlap. The experiments presented in this paper suggest that our approach boosts the performance over all the investigated scenarios. We also evaluate the embeddings to assess the quality of the fully unsupervised approach and see a higher performance of our domain-specific postprocessing. ",
        "title": "Unsupervised Pre-Training for 3D Leaf Instance Segmentation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08721",
        "abstract_url": "http://arxiv.org/abs/2401.08721",
        "authors": [
            {
                "last_name": "Anton",
                "first_name": "David"
            },
            {
                "last_name": "Berges",
                "first_name": "Idoia"
            },
            {
                "last_name": "Berm\u00fadez",
                "first_name": "Jes\u00fas"
            },
            {
                "last_name": "Go\u00f1i",
                "first_name": "Alfredo"
            },
            {
                "last_name": "Illarramendi",
                "first_name": "Arantza"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Telerehabilitation systems that support physical therapy sessions anywhere can help save healthcare costs while also improving the quality of life of the users that need rehabilitation. The main contribution of this paper is to present, as a whole, all the features supported by the innovative Kinect-based Telerehabilitation System (KiReS). In addition to the functionalities provided by current systems, it handles two new ones that could be incorporated into them, in order to give a step forward towards a new generation of telerehabilitation systems. The knowledge extraction functionality handles knowledge about the physical therapy record of patients and treatment protocols described in an ontology, named TRHONT, to select the adequate exercises for the rehabilitation of patients. The teleimmersion functionality provides a convenient, effective and user-friendly experience when performing the telerehabilitation, through a two-way real-time multimedia communication. The ontology contains about 2300 classes and 100 properties, and the system allows a reliable transmission of Kinect video depth, audio and skeleton data, being able to adapt to various network conditions. Moreover, the system has been tested with patients who suffered from shoulder disorders or total hip replacement. ",
        "title": "A Telerehabilitation System for the Selection, Evaluation and Remote  Management of Therapies",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08723",
        "abstract_url": "http://arxiv.org/abs/2401.08723",
        "authors": [
            {
                "last_name": "Quan",
                "first_name": "Minh K."
            },
            {
                "last_name": "Nguyen",
                "first_name": "Dinh C."
            },
            {
                "last_name": "Nguyen",
                "first_name": "Van-Dinh"
            },
            {
                "last_name": "Wijayasundara",
                "first_name": "Mayuri"
            },
            {
                "last_name": "Setunge",
                "first_name": "Sujeeva"
            },
            {
                "last_name": "Pathirana",
                "first_name": "Pubudu N."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CV",
            "DC",
            "LG"
        ],
        "abstract": "  Federated Learning is a promising approach for learning from user data while preserving data privacy. However, the high requirements of the model training process make it difficult for clients with limited memory or bandwidth to participate. To tackle this problem, Split Federated Learning is utilized, where clients upload their intermediate model training outcomes to a cloud server for collaborative server-client model training. This methodology facilitates resource-constrained clients' participation in model training but also increases the training time and communication overhead. To overcome these limitations, we propose a novel algorithm, called Hierarchical Split Federated Learning (HierSFL), that amalgamates models at the edge and cloud phases, presenting qualitative directives for determining the best aggregation timeframes to reduce computation and communication expenses. By implementing local differential privacy at the client and edge server levels, we enhance privacy during local model parameter updates. Our experiments using CIFAR-10 and MNIST datasets show that HierSFL outperforms standard FL approaches with better training accuracy, training time, and communication-computing trade-offs. HierSFL offers a promising solution to mobile edge computing's challenges, ultimately leading to faster content delivery and improved mobile service quality. ",
        "title": "HierSFL: Local Differential Privacy-aided Split Federated Learning in  Mobile Edge Computing",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08725",
        "abstract_url": "http://arxiv.org/abs/2401.08725",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Chenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Lanjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Anan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent developments in text-to-image models, particularly Stable Diffusion, have marked significant achievements in various applications. With these advancements, there are growing safety concerns about the vulnerability of the model that malicious entities exploit to generate targeted harmful images. However, the existing methods in the vulnerability of the model mainly evaluate the alignment between the prompt and generated images, but fall short in revealing the vulnerability associated with targeted image generation. In this study, we formulate the problem of targeted adversarial attack on Stable Diffusion and propose a framework to generate adversarial prompts. Specifically, we design a gradient-based embedding optimization method to craft reliable adversarial prompts that guide stable diffusion to generate specific images. Furthermore, after obtaining successful adversarial prompts, we reveal the mechanisms that cause the vulnerability of the model. Extensive experiments on two targeted attack tasks demonstrate the effectiveness of our method in targeted attacks. The code can be obtained in https://github.com/datar001/Revealing-Vulnerabilities-in-Stable-Diffusion-via-Targeted-Attacks. ",
        "title": "Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08727",
        "abstract_url": "http://arxiv.org/abs/2401.08727",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Zhengke"
            },
            {
                "last_name": "Ma",
                "first_name": "Yuliang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model, this paper also built a new adaptive adjacency matrix generation method and adjacency matrix attention module. This model mainly used gated temporal convolution and graph convolution to extract temporal and spatial information, respectively. Compared with multiple baselines, our model achieved the best performance on Shanghai taxi GPS trajectory dataset. The code is available at https://github.com/zachysun/Taxi Traffic Benchmark. ",
        "title": "MA2GCN: Multi Adjacency relationship Attention Graph Convolutional  Networks for Traffic Prediction using Trajectory data",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08728",
        "abstract_url": "http://arxiv.org/abs/2401.08728",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wenshuai"
            },
            {
                "last_name": "Wu",
                "first_name": "Lijun"
            },
            {
                "last_name": "Pajarinen",
                "first_name": "Joni"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Centralized training with decentralized execution (CTDE) is widely employed to stabilize partially observable multi-agent reinforcement learning (MARL) by utilizing a centralized value function during training. However, existing methods typically assume that agents make decisions based on their local observations independently, which may not lead to a correlated joint policy with sufficient coordination. Inspired by the concept of correlated equilibrium, we propose to introduce a \\textit{strategy modification} to provide a mechanism for agents to correlate their policies. Specifically, we present a novel framework, AgentMixer, which constructs the joint fully observable policy as a non-linear combination of individual partially observable policies. To enable decentralized execution, one can derive individual policies by imitating the joint policy. Unfortunately, such imitation learning can lead to \\textit{asymmetric learning failure} caused by the mismatch between joint policy and individual policy information. To mitigate this issue, we jointly train the joint policy and individual policies and introduce \\textit{Individual-Global-Consistency} to guarantee mode consistency between the centralized and decentralized policies. We then theoretically prove that AgentMixer converges to an $\\epsilon$-approximate Correlated Equilibrium. The strong experimental performance on three MARL benchmarks demonstrates the effectiveness of our method. ",
        "title": "AgentMixer: Multi-Agent Correlated Policy Factorization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08732",
        "abstract_url": "http://arxiv.org/abs/2401.08732",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Linfeng"
            },
            {
                "last_name": "Hamidi",
                "first_name": "Shayan Mohajer"
            },
            {
                "last_name": "Tan",
                "first_name": "Renhao"
            },
            {
                "last_name": "Yang",
                "first_name": "En-Hui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "IT"
        ],
        "abstract": "  It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-the-art KD frameworks, the student's classification accuracy consistently increases, with the gain of up to 3.32\\%. This suggests that the teacher's BCPD estimate provided by MCMI method is more accurate than that provided by MLL method. In addition, we show that such improvements in the student's accuracy are more drastic in zero-shot and few-shot settings. Notably, the student's accuracy increases with the gain of up to 5.72\\% when 5\\% of the training samples are available to the student (few-shot), and increases from 0\\% to as high as 84\\% for an omitted class (zero-shot). The code is available at \\url{https://github.com/iclr2024mcmi/ICLRMCMI}. ",
        "title": "Bayes Conditional Distribution Estimation for Knowledge Distillation  Based on Conditional Mutual Information",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08733",
        "abstract_url": "http://arxiv.org/abs/2401.08733",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Yiyao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hengyu"
            },
            {
                "last_name": "Dey",
                "first_name": "Babli"
            },
            {
                "last_name": "Tulga",
                "first_name": "Selenge"
            },
            {
                "last_name": "Lyu",
                "first_name": "Hanjia"
            },
            {
                "last_name": "Luo",
                "first_name": "Jiebo"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Public opinion on international conflicts, such as the concurrent Russia-Ukraine and Israel-Palestine crises, often reflects a society's values, beliefs, and history. These simultaneous conflicts have sparked heated global online discussions, offering a unique opportunity to explore the dynamics of public opinion in multiple international crises. This study investigates how public opinions toward one conflict might influence or relate to another, a relatively unexplored area in contemporary research. Focusing on Chinese netizens, who represent a significant online population, this study examines their perspectives, which are increasingly influential in global discourse due to China's unique cultural and political landscape. The research finds a range of opinions, including neutral stances towards both conflicts and a statistical correlation between attitudes towards each, indicating interconnected or mutually influenced viewpoints. The study also highlights the significant role of news media, particularly in China, where state policies and global politics shape conflict portrayal, in impacting public opinion. ",
        "title": "In the Eyes of the Bystander: Are the Stances on Different Conflicts  Correlated?",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08734",
        "abstract_url": "http://arxiv.org/abs/2401.08734",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zeliang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Rongyi"
            },
            {
                "last_name": "Yao",
                "first_name": "Wei"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaosen"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenliang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Deep neural networks are widely known to be vulnerable to adversarial examples. However, vanilla adversarial examples generated under the white-box setting often exhibit low transferability across different models. Since adversarial transferability poses more severe threats to practical applications, various approaches have been proposed for better transferability, including gradient-based, input transformation-based, and model-related attacks, \\etc. In this work, we find that several tiny changes in the existing adversarial attacks can significantly affect the attack performance, \\eg, the number of iterations and step size. Based on careful studies of existing adversarial attacks, we propose a bag of tricks to enhance adversarial transferability, including momentum initialization, scheduled step size, dual example, spectral-based input transformation, and several ensemble strategies. Extensive experiments on the ImageNet dataset validate the high effectiveness of our proposed tricks and show that combining them can further boost adversarial transferability. Our work provides practical insights and techniques to enhance adversarial transferability, and offers guidance to improve the attack performance on the real-world application through simple adjustments. ",
        "title": "Bag of Tricks to Boost Adversarial Transferability",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08735",
        "abstract_url": "http://arxiv.org/abs/2401.08735",
        "authors": [
            {
                "last_name": "Berrisford",
                "first_name": "Liam J"
            },
            {
                "last_name": "Neal",
                "first_name": "Lucy S"
            },
            {
                "last_name": "Buttery",
                "first_name": "Helen J"
            },
            {
                "last_name": "Evans",
                "first_name": "Benjamin R"
            },
            {
                "last_name": "Menezes",
                "first_name": "Ronaldo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Ambient air pollution remains a critical issue in the United Kingdom, where data on air pollution concentrations form the foundation for interventions aimed at improving air quality. However, the current air pollution monitoring station network in the UK is characterized by spatial sparsity, heterogeneous placement, and frequent temporal data gaps, often due to issues such as power outages. We introduce a scalable data-driven supervised machine learning model framework designed to address temporal and spatial data gaps by filling missing measurements. This approach provides a comprehensive dataset for England throughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning techniques and real-world data from the sparsely distributed monitoring stations, we generate 355,827 synthetic monitoring stations across the study area, yielding data valued at approximately \\pounds70 billion. Validation was conducted to assess the model's performance in forecasting, estimating missing locations, and capturing peak concentrations. The resulting dataset is of particular interest to a diverse range of stakeholders engaged in downstream assessments supported by outdoor air pollution concentration data for NO2, O3, PM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at a higher resolution than was previously possible. ",
        "title": "A Framework for Scalable Ambient Air Pollution Concentration Estimation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08738",
        "abstract_url": "http://arxiv.org/abs/2401.08738",
        "authors": [
            {
                "last_name": "Rezapour",
                "first_name": "Mostafa"
            },
            {
                "last_name": "Niazi",
                "first_name": "Muhammad Khalid Khan"
            },
            {
                "last_name": "Lu",
                "first_name": "Hao"
            },
            {
                "last_name": "Narayanan",
                "first_name": "Aarthi"
            },
            {
                "last_name": "Gurcan",
                "first_name": "Metin Nafi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting their essential roles in the immune response to EBOV. Our results underscore the efficacy of the SMAS method in revealing complex genetic interactions and response mechanisms during EBOV infection. This research provides valuable insights into EBOV pathogenesis and aids in developing more precise diagnostic tools and therapeutic strategies to address EBOV infection in particular and viral infection in general. ",
        "title": "Machine Learning-Based Analysis of Ebola Virus' Impact on Gene  Expression in Nonhuman Primates",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08739",
        "abstract_url": "http://arxiv.org/abs/2401.08739",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Gen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Kaifeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Siwei"
            },
            {
                "last_name": "Lyu",
                "first_name": "Xiaozhong"
            },
            {
                "last_name": "Dusmanu",
                "first_name": "Mihai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Pollefeys",
                "first_name": "Marc"
            },
            {
                "last_name": "Tang",
                "first_name": "Siyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/. ",
        "title": "EgoGen: An Egocentric Synthetic Data Generator",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08740",
        "abstract_url": "http://arxiv.org/abs/2401.08740",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Nanye"
            },
            {
                "last_name": "Goldstein",
                "first_name": "Mark"
            },
            {
                "last_name": "Albergo",
                "first_name": "Michael S."
            },
            {
                "last_name": "Boffi",
                "first_name": "Nicholas M."
            },
            {
                "last_name": "Vanden-Eijnden",
                "first_name": "Eric"
            },
            {
                "last_name": "Xie",
                "first_name": "Saining"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06. ",
        "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable  Interpolant Transformers",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08741",
        "abstract_url": "http://arxiv.org/abs/2401.08741",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Xingjian"
            },
            {
                "last_name": "Melas-Kyriazi",
                "first_name": "Luke"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an implicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efficiency. Compared to the state-of-the-art DiT model, FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation quality in situations where sampling computation or time is limited. Our code and pretrained models are available at https://lukemelas.github.io/fixed-point-diffusion-models. ",
        "title": "Fixed Point Diffusion Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08742",
        "abstract_url": "http://arxiv.org/abs/2401.08742",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Zijie"
            },
            {
                "last_name": "Yang",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiatian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Generating dynamic three-dimensional (3D) object from a single-view video is challenging due to the lack of 4D labeled data. Existing methods extend text-to-3D pipelines by transferring off-the-shelf image generation models such as score distillation sampling, but they are slow and expensive to scale (e.g., 150 minutes per object) due to the need for back-propagating the information-limited supervision signals through a large pretrained model. To address this limitation, we propose an efficient video-to-4D object generation framework called Efficient4D. It generates high-quality spacetime-consistent images under different camera views, and then uses them as labeled data to directly train a novel 4D Gaussian splatting model with explicit point cloud geometry, enabling real-time rendering under continuous camera trajectories. Extensive experiments on synthetic and real videos show that Efficient4D offers a remarkable 10-fold increase in speed when compared to prior art alternatives while preserving the same level of innovative view synthesis quality. For example, Efficient4D takes only 14 minutes to model a dynamic object. ",
        "title": "Fast Dynamic 3D Object Generation from a Single-view Video",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08743",
        "abstract_url": "http://arxiv.org/abs/2401.08743",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Chuanyang"
            },
            {
                "last_name": "Wu",
                "first_name": "Yutong"
            },
            {
                "last_name": "Cao",
                "first_name": "Jing"
            },
            {
                "last_name": "Xiang",
                "first_name": "Jiannan"
            },
            {
                "last_name": "Kuo",
                "first_name": "Yen-Ling"
            },
            {
                "last_name": "Hu",
                "first_name": "Zhiting"
            },
            {
                "last_name": "Ullman",
                "first_name": "Tomer"
            },
            {
                "last_name": "Torralba",
                "first_name": "Antonio"
            },
            {
                "last_name": "Tenenbaum",
                "first_name": "Joshua B."
            },
            {
                "last_name": "Shu",
                "first_name": "Tianmin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CV",
            "LG"
        ],
        "abstract": "  Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models. ",
        "title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08758",
        "abstract_url": "http://arxiv.org/abs/2401.08758",
        "authors": [
            {
                "last_name": "Da Silva",
                "first_name": "Lidia J. Gomes"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Dirac delta distributionally sourced differential equations emerge in many dynamical physical systems from neuroscience to black hole perturbation theory. Most of these lack exact analytical solutions and are thus best tackled numerically. This work describes a generic numerical algorithm which constructs discontinuous spatial and temporal discretisations by operating on discontinuous Lagrange and Hermite interpolation formulae recovering higher order accuracy. It is shown by solving the distributionally sourced wave equation, which has analytical solutions, that numerical weak-form solutions can be recovered to high order accuracy by solving a first-order reduced system of ordinary differential equations. The method-of-lines framework is applied to the DiscoTEX algorithm i.e through discontinuous collocation with implicit-turned-explicit (IMTEX) integration methods which are symmetric and conserve symplectic structure. Furthermore, the main application of the algorithm is proved, for the first-time, by calculating the amplitude at any desired location within the numerical grid, including at the position (and at its right and left limit) where the wave- (or wave-like) equation is discontinuous via interpolation using DiscoTEX. This is shown, firstly by solving the wave- (or wave-like) equation and comparing the numerical weak-form solution to the exact solution. Finally, one shows how to reconstruct the scalar and gravitational metric perturbations from weak-form numerical solutions of a non-rotating black hole, which do not have known exact analytical solutions, and compare against state-of-the-art frequency domain results. One concludes by motivating how DiscoTEX, and related algorithms, open a promising new alternative Extreme-Mass-Ratio-Inspiral (EMRI)s waveform generation route via a self-consistent evolution for the gravitational self-force programme in the time-domain. ",
        "title": "DiscoTEX: Discontinuous collocation and implicit-turned-explicit (IMTEX)  integration symplectic, symmetric numerical algorithms with higher order  jumps for differential equations with numerical black hole perturbation  theory applications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08763",
        "abstract_url": "http://arxiv.org/abs/2401.08763",
        "authors": [
            {
                "last_name": "Rogers",
                "first_name": "Brian"
            },
            {
                "last_name": "Lintott",
                "first_name": "Chris J."
            },
            {
                "last_name": "Croft",
                "first_name": "Steve"
            },
            {
                "last_name": "Schwamb",
                "first_name": "Megan E."
            },
            {
                "last_name": "Davenport",
                "first_name": "James R. A."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present a novel method for anomaly detection in Solar System object data, in preparation for the Legacy Survey of Space and Time. We train a deep autoencoder for anomaly detection and use the learned latent space to search for other interesting objects. We demonstrate the efficacy of the autoencoder approach by finding interesting examples, such as interstellar objects, and show that using the autoencoder, further examples of interesting classes can be found. We also investigate the limits of classic unsupervised approaches to anomaly detection through the generation of synthetic anomalies and evaluate the feasibility of using a supervised learning approach. Future work should consider expanding the feature space to increase the variety of anomalies that can be uncovered during the survey using an autoencoder. ",
        "title": "The weird and the wonderful in our Solar System: Searching for  serendipity in the Legacy Survey of Space and Time",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08772",
        "abstract_url": "http://arxiv.org/abs/2401.08772",
        "authors": [
            {
                "last_name": "Kong",
                "first_name": "Huanjun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Songyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Kai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this work, we present HuixiangDou, a technical assistant powered by Large Language Models (LLM). This system is designed to assist algorithm developers by providing insightful responses to questions related to open-source algorithm projects, such as computer vision and deep learning projects from OpenMMLab. We further explore the integration of this assistant into the group chats of instant messaging (IM) tools such as WeChat and Lark. Through several iterative improvements and trials, we have developed a sophisticated technical chat assistant capable of effectively answering users' technical questions without causing message flooding. This paper's contributions include: 1) Designing an algorithm pipeline specifically for group chat scenarios; 2) Verifying the reliable performance of text2vec in task rejection; 3) Identifying three critical requirements for LLMs in technical-assistant-like products, namely scoring ability, In-Context Learning (ICL), and Long Context. We have made the software and source code available at https://github.com/internlm/huixiangdou to aid in future research and application. HuixiangDou is applicable to any group chat within IM tools. ",
        "title": "HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical  Assistance",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08777",
        "abstract_url": "http://arxiv.org/abs/2401.08777",
        "authors": [
            {
                "last_name": "Gandrakota",
                "first_name": "Abhijith"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lily"
            },
            {
                "last_name": "Puli",
                "first_name": "Aahlad"
            },
            {
                "last_name": "Cranmer",
                "first_name": "Kyle"
            },
            {
                "last_name": "Ngadiuba",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Ranganath",
                "first_name": "Rajesh"
            },
            {
                "last_name": "Tran",
                "first_name": "Nhan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Anomaly, or out-of-distribution, detection is a promising tool for aiding discoveries of new particles or processes in particle physics. In this work, we identify and address two overlooked opportunities to improve anomaly detection for high-energy physics. First, rather than train a generative model on the single most dominant background process, we build detection algorithms using representation learning from multiple background types, thus taking advantage of more information to improve estimation of what is relevant for detection. Second, we generalize decorrelation to the multi-background setting, thus directly enforcing a more complete definition of robustness for anomaly detection. We demonstrate the benefit of the proposed robust multi-background anomaly detection algorithms on a high-dimensional dataset of particle decays at the Large Hadron Collider. ",
        "title": "Robust Anomaly Detection for Particle Physics Using Multi-Background  Representation Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08787",
        "abstract_url": "http://arxiv.org/abs/2401.08787",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenwen"
            },
            {
                "last_name": "Hsu",
                "first_name": "Chia-Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Sizhe"
            },
            {
                "last_name": "Yang",
                "first_name": "Yezhou"
            },
            {
                "last_name": "Lee",
                "first_name": "Hyunho"
            },
            {
                "last_name": "Liljedahl",
                "first_name": "Anna"
            },
            {
                "last_name": "Witharana",
                "first_name": "Chandi"
            },
            {
                "last_name": "Yang",
                "first_name": "Yili"
            },
            {
                "last_name": "Rogers",
                "first_name": "Brendan M."
            },
            {
                "last_name": "Arundel",
                "first_name": "Samantha T."
            },
            {
                "last_name": "Jones",
                "first_name": "Matthew B."
            },
            {
                "last_name": "McHenry",
                "first_name": "Kenton"
            },
            {
                "last_name": "Solis",
                "first_name": "Patricia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper assesses trending AI foundation models, especially emerging computer vision foundation models and their performance in natural landscape feature segmentation. While the term foundation model has quickly garnered interest from the geospatial domain, its definition remains vague. Hence, this paper will first introduce AI foundation models and their defining characteristics. Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks. To evaluate the performance of large AI vision models, especially Meta's Segment Anything Model (SAM), we implemented different instance segmentation pipelines that minimize the changes to SAM to leverage its power as a foundation model. A series of prompt strategies was developed to test SAM's performance regarding its theoretical upper bound of predictive accuracy, zero-shot performance, and domain adaptability through fine-tuning. The analysis used two permafrost feature datasets, ice-wedge polygons and retrogressive thaw slumps because (1) these landform features are more challenging to segment than manmade features due to their complicated formation mechanisms, diverse forms, and vague boundaries; (2) their presence and changes are important indicators for Arctic warming and climate change. The results show that although promising, SAM still has room for improvement to support AI-augmented terrain mapping. The spatial and domain generalizability of this finding is further validated using a more general dataset EuroCrop for agricultural field mapping. Finally, we discuss future research directions that strengthen SAM's applicability in challenging geospatial domains. ",
        "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation  Model's Generalizability in Permafrost Mapping",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08788",
        "abstract_url": "http://arxiv.org/abs/2401.08788",
        "authors": [
            {
                "last_name": "Akpinar",
                "first_name": "Nil-Jana"
            },
            {
                "last_name": "Lipton",
                "first_name": "Zachary C."
            },
            {
                "last_name": "Chouldechova",
                "first_name": "Alexandra"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. In this work, we present an analytically tractable model of differential feature under-reporting which we then use to characterize the impact of this kind of data bias on algorithmic fairness. We demonstrate how standard missing data methods typically fail to mitigate bias in this setting, and propose a new set of methods specifically tailored to differential feature under-reporting. Our results show that, in real world data settings, under-reporting typically leads to increasing disparities. The proposed solution methods show success in mitigating increases in unfairness. ",
        "title": "The Impact of Differential Feature Under-reporting on Algorithmic  Fairness",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08789",
        "abstract_url": "http://arxiv.org/abs/2401.08789",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Julie"
            },
            {
                "last_name": "Luceri",
                "first_name": "Luca"
            },
            {
                "last_name": "Ferrara",
                "first_name": "Emilio"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  The COVID-19 pandemic has triggered profound societal changes, extending beyond its health impacts to the moralization of behaviors. Leveraging insights from moral psychology, this study delves into the moral fabric shaping online discussions surrounding COVID-19 over a span of nearly two years. Our investigation identifies four distinct user groups characterized by differences in morality, political ideology, and communication styles. We underscore the intricate relationship between moral differences and political ideologies, revealing a nuanced picture where moral orientations do not rigidly separate users politically. Furthermore, we uncover patterns of moral homophily within the social network, highlighting the existence of one potential moral echo chamber. Analyzing the moral themes embedded in messages, we observe that messages featuring moral foundations not typically favored by their authors, as well as those incorporating multiple moral foundations, resonate more effectively with out-group members. This research contributes valuable insights into the complex interplay between moral foundations, communication dynamics, and network structures on Twitter. ",
        "title": "Moral Values Underpinning COVID-19 Online Communication Patterns",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08796",
        "abstract_url": "http://arxiv.org/abs/2401.08796",
        "authors": [
            {
                "last_name": "Guzm\u00e1n-Pro",
                "first_name": "Santiago"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  A well-established research line in structural and algorithmic graph theory is characterizing graph classes by listing their minimal obstructions. When this list is finite for some class $\\mathcal C$ we obtain a polynomial-time algorithm for recognizing graphs in $\\mathcal C$, and from a logic point of view, having finitely many obstructions corresponds to being definable by a universal sentence. However, in many cases we study classes with infinite sets of minimal obstructions, and this might have neither algorithmic nor logic implications for such a class. Some decades ago, Skrien (1982) and Damaschke (1990) introduced finite expressions of graph classes by means of forbidden orientations and forbidden linear orderings, and recently, similar research lines appeared in the literature, such as expressions by forbidden circular orders, by forbidden tree-layouts, and by forbidden edge-coloured graphs. In this paper, we introduce local expressions of graph classes; a general framework for characterizing graph classes by forbidden equipped graphs. In particular, it encompasses all research lines mentioned above, and we provide some new examples of such characterizations. Moreover, we see that every local expression of a class $\\mathcal C$ yields a polynomial-time certification algorithm for graphs in $\\mathcal C$. Finally, from a logic point of view, we show that being locally expressible corresponds to being definable in the logic SNP introduced by Feder and Vardi (1999). ",
        "title": "Local expressions of hereditary classes",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08804",
        "abstract_url": "http://arxiv.org/abs/2401.08804",
        "authors": [
            {
                "last_name": "Castell",
                "first_name": "Wolfgang zu"
            },
            {
                "last_name": "Dransch",
                "first_name": "Doris"
            },
            {
                "last_name": "Juckeland",
                "first_name": "Guido"
            },
            {
                "last_name": "Meistring",
                "first_name": "Marcel"
            },
            {
                "last_name": "Fritzsch",
                "first_name": "Bernadette"
            },
            {
                "last_name": "Gey",
                "first_name": "Ronny"
            },
            {
                "last_name": "H\u00f6pfner",
                "first_name": "Britta"
            },
            {
                "last_name": "K\u00f6hler",
                "first_name": "Martin"
            },
            {
                "last_name": "Mee\u00dfen",
                "first_name": "Christian"
            },
            {
                "last_name": "Mehrtens",
                "first_name": "Hela"
            },
            {
                "last_name": "M\u00fchlbauer",
                "first_name": "Felix"
            },
            {
                "last_name": "Schindler",
                "first_name": "Sirko"
            },
            {
                "last_name": "Schnicke",
                "first_name": "Thomas"
            },
            {
                "last_name": "Bertelmann",
                "first_name": "Roland"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL",
            "CY"
        ],
        "abstract": "  Research data and software are widely accepted as an outcome of scientific work. However, in comparison to text-based publications, there is not yet an established process to assess and evaluate quality of research data and research software publications. This paper presents an attempt to fill this gap. Initiated by the Working Group Open Science of the Helmholtz Association the Task Group Helmholtz Quality Indicators for Data and Software Publications currently develops a quality indicator for research data and research software publications to be used within the Association. This report summarizes the vision of the group of what all contributes to such an indicator. The proposed approach relies on generic well-established concepts for quality criteria, such as the FAIR Principles and the COBIT Maturity Model. It does - on purpose - not limit itself to technical implementation possibilities to avoid using an existing metric for a new purpose. The intention of this paper is to share the current state for further discussion with all stakeholders, particularly with other groups also working on similar metrics but also with entities that use the metrics. ",
        "title": "Towards a Quality Indicator for Research Data publications and Research  Software publications -- A vision from the Helmholtz Association",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08806",
        "abstract_url": "http://arxiv.org/abs/2401.08806",
        "authors": [
            {
                "last_name": "Williams",
                "first_name": "Harrison"
            },
            {
                "last_name": "Hicks",
                "first_name": "Matthew"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Batteryless energy harvesting systems enable a wide array of new sensing, computation, and communication platforms untethered by power delivery or battery maintenance demands. Energy harvesters charge a buffer capacitor from an unreliable environmental source until enough energy is stored to guarantee a burst of operation despite changes in power input. Current platforms use a fixed-size buffer chosen at design time to meet constraints on charge time or application longevity, but static energy buffers are a poor fit for the highly volatile power sources found in real-world deployments: fixed buffers waste energy both as heat when they reach capacity during a power surplus and as leakage when they fail to charge the system during a power deficit.   To maximize batteryless system performance in the face of highly dynamic input power, we propose REACT: a responsive buffering circuit which varies total capacitance according to net input power. REACT uses a variable capacitor bank to expand capacitance to capture incoming energy during a power surplus and reconfigures internal capacitors to reclaim additional energy from each capacitor as power input falls. Compared to fixed-capacity systems, REACT captures more energy, maximizes usable energy, and efficiently decouples system voltage from stored charge -- enabling low-power and high-performance designs previously limited by ambient power. Our evaluation on real-world platforms shows that REACT eliminates the tradeoff between responsiveness, efficiency, and longevity, increasing the energy available for useful work by an average 25.6% over static buffers optimized for reactivity and capacity, improving event responsiveness by an average 7.7x without sacrificing capacity, and enabling programmer directed longevity guarantees. ",
        "title": "Energy-adaptive Buffering for Efficient, Responsive, and Persistent  Batteryless Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08807",
        "abstract_url": "http://arxiv.org/abs/2401.08807",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Lezhi"
            },
            {
                "last_name": "Liu",
                "first_name": "Shangqing"
            },
            {
                "last_name": "Li",
                "first_name": "Yi"
            },
            {
                "last_name": "Xie",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Bu",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  In software development, formal program specifications play a crucial role in various stages. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models. Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM to generate appropriate specifications for a given program. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. To evaluate the performance of SpecGen, we manually construct a dataset containing 120 test cases. Our experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 100 out of 120 programs, outperforming the existing purely LLM-based approaches and conventional specification generation tools. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program. ",
        "title": "SpecGen: Automated Generation of Formal Program Specifications via Large  Language Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08808",
        "abstract_url": "http://arxiv.org/abs/2401.08808",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Shangmin"
            },
            {
                "last_name": "Ren",
                "first_name": "Yi"
            },
            {
                "last_name": "Albrecht",
                "first_name": "Stefano V."
            },
            {
                "last_name": "Smith",
                "first_name": "Kenny"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of samples and forgetting events during learning. Moreover, we also show that using lpNTK to identify and remove poisoning training samples does not hurt the generalisation performance of ANNs. ",
        "title": "Sample Relationship from Learning Dynamics Matters for Generalisation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08809",
        "abstract_url": "http://arxiv.org/abs/2401.08809",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Fang"
            },
            {
                "last_name": "Rawlekar",
                "first_name": "Samyak"
            },
            {
                "last_name": "Ahuja",
                "first_name": "Narendra"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  3D Reconstruction of moving articulated objects without additional information about object structure is a challenging problem. Current methods overcome such challenges by employing category-specific skeletal models. Consequently, they do not generalize well to articulated objects in the wild. We treat an articulated object as an unknown, semi-rigid skeletal structure surrounded by nonrigid material (e.g., skin). Our method simultaneously estimates the visible (explicit) representation (3D shapes, colors, camera parameters) and the implicit skeletal representation, from motion cues in the object video without 3D supervision. Our implicit representation consists of four parts. (1) Skeleton, which specifies how semi-rigid parts are connected. (2) \\textcolor{black}{Skinning Weights}, which associates each surface vertex with semi-rigid parts with probability. (3) Rigidity Coefficients, specifying the articulation of the local surface. (4) Time-Varying Transformations, which specify the skeletal motion and surface deformation parameters. We introduce an algorithm that uses physical constraints as regularization terms and iteratively estimates both implicit and explicit representations. Our method is category-agnostic, thus eliminating the need for category-specific skeletons, we show that our method outperforms state-of-the-art across standard video datasets. ",
        "title": "Learning Implicit Representation for Reconstructing Articulated Objects",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08814",
        "abstract_url": "http://arxiv.org/abs/2401.08814",
        "authors": [
            {
                "last_name": "Kouskiya",
                "first_name": "Uditnarayan"
            },
            {
                "last_name": "Acharya",
                "first_name": "Amit"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We demonstrate the feasibility of a scheme to obtain approximate weak solutions to the (inviscid) Burgers equation in conservation and Hamilton-Jacobi form, treated as degenerate elliptic problems. We show different variants recover non-unique weak solutions as appropriate, and also specific constructive approaches to recover the corresponding entropy solutions. ",
        "title": "Inviscid Burgers as a degenerate elliptic problem",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08815",
        "abstract_url": "http://arxiv.org/abs/2401.08815",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yumeng"
            },
            {
                "last_name": "Keuper",
                "first_name": "Margret"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dan"
            },
            {
                "last_name": "Khoreva",
                "first_name": "Anna"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in practice. To mitigate this, we propose to integrate adversarial supervision into the conventional training pipeline of L2I diffusion models (ALDM). Specifically, we employ a segmentation-based discriminator which provides explicit feedback to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. To encourage consistent adherence to the input layout over the sampling steps, we further introduce the multistep unrolling strategy. Instead of looking at a single timestep, we unroll a few steps recursively to imitate the inference process, and ask the discriminator to assess the alignment of denoised images with the layout over a certain time window. Our experiments show that ALDM enables layout faithfulness of the generated images, while allowing broad editability via text prompts. Moreover, we showcase its usefulness for practical applications: by synthesizing target distribution samples via text control, we improve domain generalization of semantic segmentation models by a large margin (~12 mIoU points). ",
        "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08818",
        "abstract_url": "http://arxiv.org/abs/2401.08818",
        "authors": [
            {
                "last_name": "Babul",
                "first_name": "Shazia'Ayn"
            },
            {
                "last_name": "Hristova",
                "first_name": "Desislava"
            },
            {
                "last_name": "Lima",
                "first_name": "Antonio"
            },
            {
                "last_name": "Lambiotte",
                "first_name": "Renaud"
            },
            {
                "last_name": "Beguerisse-D\u00edaz",
                "first_name": "Mariano"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "IR",
            "LG"
        ],
        "abstract": "  We explore the social and contextual factors that influence the outcome of person-to-person music recommendations and discovery. Specifically, we use data from Spotify to investigate how a link sent from one user to another results in the receiver engaging with the music of the shared artist. We consider several factors that may influence this process, such as the strength of the sender-receiver relationship, the user's role in the Spotify social network, their music social cohesion, and how similar the new artist is to the receiver's taste. We find that the receiver of a link is more likely to engage with a new artist when (1) they have similar music taste to the sender and the shared track is a good fit for their taste, (2) they have a stronger and more intimate tie with the sender, and (3) the shared artist is popular with the receiver's connections. Finally, we use these findings to build a Random Forest classifier to predict whether a shared music track will result in the receiver's engagement with the shared artist. This model elucidates which type of social and contextual features are most predictive, although peak performance is achieved when a diverse set of features are included. These findings provide new insights into the multifaceted mechanisms underpinning the interplay between music discovery and social processes. ",
        "title": "Link Me Baby One More Time: Social Music Discovery on Spotify",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08819",
        "abstract_url": "http://arxiv.org/abs/2401.08819",
        "authors": [
            {
                "last_name": "Cen",
                "first_name": "Zhepeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Zuxin"
            },
            {
                "last_name": "Wang",
                "first_name": "Zitong"
            },
            {
                "last_name": "Yao",
                "first_name": "Yihang"
            },
            {
                "last_name": "Lam",
                "first_name": "Henry"
            },
            {
                "last_name": "Zhao",
                "first_name": "Ding"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem in offline RL. ",
        "title": "Learning from Sparse Offline Datasets via Conservative Density  Estimation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08821",
        "abstract_url": "http://arxiv.org/abs/2401.08821",
        "authors": [
            {
                "last_name": "Raman",
                "first_name": "Ashutosh"
            },
            {
                "last_name": "Odion",
                "first_name": "Ren A."
            },
            {
                "last_name": "Yamamoto",
                "first_name": "Kent K."
            },
            {
                "last_name": "Ross",
                "first_name": "Weston"
            },
            {
                "last_name": "Vo-Dinh",
                "first_name": "Tuan"
            },
            {
                "last_name": "Codd",
                "first_name": "Patrick J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  Raman spectroscopy, a photonic modality based on the inelastic backscattering of coherent light, is a valuable asset to the intraoperative sensing space, offering non-ionizing potential and highly-specific molecular fingerprint-like spectroscopic signatures that can be used for diagnosis of pathological tissue in the dynamic surgical field. Though Raman suffers from weakness in intensity, Surface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to amplify Raman signals, can achieve detection sensitivities that rival traditional photonic modalities. In this study, we outline a robotic Raman system that can reliably pinpoint the location and boundaries of a tumor embedded in healthy tissue, modeled here as a tissue-mimicking phantom with selectively infused Gold Nanostar regions. Further, due to the relative dearth of collected biological SERS or Raman data, we implement transfer learning to achieve 100% validation classification accuracy for Gold Nanostars compared to Control Agarose, thus providing a proof-of-concept for Raman-based deep learning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2 minutes, and achieve 98.2% accuracy, preserving relative measurements between features in the phantom. We also achieve an 84.3% Intersection-over-Union score, which is the extent of overlap between the ground truth and predicted reconstructions. Lastly, we also demonstrate that the Raman system and classification algorithm do not discern based on sample color, but instead on presence of SERS agents. This study provides a crucial step in the translation of intelligent Raman systems in intraoperative oncological spaces. ",
        "title": "Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward  Accurate Reconstruction of the Surgical Zone",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08822",
        "abstract_url": "http://arxiv.org/abs/2401.08822",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Arran Zeyu"
            },
            {
                "last_name": "Borland",
                "first_name": "David"
            },
            {
                "last_name": "Gotz",
                "first_name": "David"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Counterfactuals -- expressing what might have been true under different circumstances -- have been widely applied in statistics and machine learning to help understand causal relationships. More recently, counterfactuals have begun to emerge as a technique being applied within visualization research. However, it remains unclear to what extent counterfactuals can aid with visual data communication. In this paper, we primarily focus on assessing the quality of users' understanding of data when provided with counterfactual visualizations. We propose a preliminary model of causality comprehension by connecting theories from causal inference and visual data communication. Leveraging this model, we conducted an empirical study to explore how counterfactuals can improve users' understanding of data in static visualizations. Our results indicate that visualizing counterfactuals had a positive impact on participants' interpretations of causal relations within datasets. These results motivate a discussion of how to more effectively incorporate counterfactuals into data visualizations. ",
        "title": "An Empirical Study of Counterfactual Visualization to Support Visual  Causal Inference",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08825",
        "abstract_url": "http://arxiv.org/abs/2401.08825",
        "authors": [
            {
                "last_name": "Gambetti",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Han",
                "first_name": "Qiwei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "CV"
        ],
        "abstract": "  Online reviews in the form of user-generated content (UGC) significantly impact consumer decision-making. However, the pervasive issue of not only human fake content but also machine-generated content challenges UGC's reliability. Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost. Leveraging OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a multi-modal dataset of 20,144 restaurant review-image pairs divided into authentic and machine-generated. We explore unimodal and multimodal detection models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from readability and photographic theories to score reviews and images, respectively, demonstrating their utility as hand-crafted features in scalable and interpretable detection models, with comparable performance. The paper contributes by open-sourcing the dataset and releasing fake review detectors, recommending its use in unimodal and multimodal fake review detection tasks, and evaluating linguistic and visual features in synthetic versus authentic data. ",
        "title": "AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant  Reviews and Images on Social Media",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08830",
        "abstract_url": "http://arxiv.org/abs/2401.08830",
        "authors": [
            {
                "last_name": "Whitaker",
                "first_name": "Tim"
            },
            {
                "last_name": "Whitley",
                "first_name": "Darrell"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Pruning methods have recently grown in popularity as an effective way to reduce the size and computational complexity of deep neural networks. Large numbers of parameters can be removed from trained models with little discernible loss in accuracy after a small number of continued training epochs. However, pruning too many parameters at once often causes an initial steep drop in accuracy which can undermine convergence quality. Iterative pruning approaches mitigate this by gradually removing a small number of parameters over multiple epochs. However, this can still lead to subnetworks that overfit local regions of the loss landscape. We introduce a novel and effective approach to tuning subnetworks through a regularization technique we call Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete manner, we instead represent subnetworks with stochastic masks where each parameter has a probabilistic chance of being included or excluded on any given forward pass. We anneal these probabilities over time such that subnetwork structure slowly evolves as mask values become more deterministic, allowing for a smoother and more robust optimization of subnetworks at high levels of sparsity. ",
        "title": "Stochastic Subnetwork Annealing: A Regularization Technique for Fine  Tuning Pruned Subnetworks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08832",
        "abstract_url": "http://arxiv.org/abs/2401.08832",
        "authors": [
            {
                "last_name": "Chuai",
                "first_name": "Yuwei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jichang"
            },
            {
                "last_name": "Lenzini",
                "first_name": "Gabriele"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  The engagement with online health misinformation, particularly during COVID-19, poses unprecedented threats to societal well-being. The susceptibility to misinformation is heightened within a multi-topic context during health crises. This paper addresses a critical gap in understanding online engagement with multi-topic misinformation related to COVID-19. We conduct a comprehensive analysis of 7273 fact-checked source news claims related to COVID-19 and their corresponding social engagement on X/Twitter through the lens of topic diversity and conspiracy theories. Our analysis yields several key findings: (i) False news, especially when accompanied by conspiracy theories, exhibits higher topic diversity compared to true news. (ii) In terms of engagement from source claims to online posts, false news has a longer lifetime and receives more posts on X/Twitter compared to true news. Additionally, the integration of conspiracy theories is associated with a longer lifetime of COVID-19 misinformation. (iii) News posts characterized by heightened topic diversity receive increased social engagement on X/Twitter in terms of reposts, likes, and replies. However, the effect of topic diversity is moderated by the news veracity. High topic diversity is linked to more engagement with true news posts compared to false news posts. (iiii) The integration of conspiracy theories is linked to more social engagement with misinformation on X/Twitter. False news posts that contain conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes, and 44.1% more replies compared to false news posts without conspiracy theories. These findings offer insights into understanding the engagement with multi-topic misinformation during health crises and highlight the importance of considering topic diversity and conspiracy theories in developing targeted interventions. ",
        "title": "Topic Diversity and Conspiracy Theories Shape Engagement with COVID-19  Misinformation on X/Twitter",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08833",
        "abstract_url": "http://arxiv.org/abs/2401.08833",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Alexander H."
            },
            {
                "last_name": "Yeh",
                "first_name": "Sung-Lin"
            },
            {
                "last_name": "Glass",
                "first_name": "James"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition. ",
        "title": "Revisiting Self-supervised Learning of Speech Representation from a  Mutual Information Perspective",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08835",
        "abstract_url": "http://arxiv.org/abs/2401.08835",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Jiyang"
            },
            {
                "last_name": "Kim",
                "first_name": "Kwangyoun"
            },
            {
                "last_name": "Shon",
                "first_name": "Suwon"
            },
            {
                "last_name": "Wu",
                "first_name": "Felix"
            },
            {
                "last_name": "Sridhar",
                "first_name": "Prashant"
            },
            {
                "last_name": "Watanabe",
                "first_name": "Shinji"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we propose a Guided Attention (GA) auxiliary training loss, which improves the effectiveness and robustness of automatic speech recognition (ASR) contextual biasing without introducing additional parameters. A common challenge in previous literature is that the word error rate (WER) reduction brought by contextual biasing diminishes as the number of bias phrases increases. To address this challenge, we employ a GA loss as an additional training objective besides the Transducer loss. The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames. Compared to studies with similar motivations, the proposed loss operates directly on the cross attention weights and is easier to implement. Through extensive experiments based on Conformer Transducer with Contextual Adapter, we demonstrate that the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases. Specifically, the GA loss decreases the WER of rare vocabularies by up to 19.2% on LibriSpeech compared to the contextual biasing baseline, and up to 49.3% compared to a vanilla Transducer. ",
        "title": "Improving ASR Contextual Biasing with Guided Attention",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08837",
        "abstract_url": "http://arxiv.org/abs/2401.08837",
        "authors": [
            {
                "last_name": "Albanwan",
                "first_name": "Hessah"
            },
            {
                "last_name": "Qin",
                "first_name": "Rongjun"
            },
            {
                "last_name": "Tang",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image fusion in Remote Sensing (RS) has been a consistent demand due to its ability to turn raw images of different resolutions, sources, and modalities into accurate, complete, and spatio-temporally coherent images. It greatly facilitates downstream applications such as pan-sharpening, change detection, land-cover classification, etc. Yet, image fusion solutions are highly disparate to various remote sensing problems and thus are often narrowly defined in existing reviews as topical applications, such as pan-sharpening, and spatial-temporal image fusion. Considering that image fusion can be theoretically applied to any gridded data through pixel-level operations, in this paper, we expanded its scope by comprehensively surveying relevant works with a simple taxonomy: 1) many-to-one image fusion; 2) many-to-many image fusion. This simple taxonomy defines image fusion as a mapping problem that turns either a single or a set of images into another single or set of images, depending on the desired coherence, e.g., spectral, spatial/resolution coherence, etc. We show that this simple taxonomy, despite the significant modality difference it covers, can be presented by a conceptually easy framework. In addition, we provide a meta-analysis to review the major papers studying the various types of image fusion and their applications over the years (from the 1980s to date), covering 5,926 peer-reviewed papers. Finally, we discuss the main benefits and emerging challenges to provide open research directions and potential future works. ",
        "title": "Image Fusion in Remote Sensing: An Overview and Meta Analysis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08840",
        "abstract_url": "http://arxiv.org/abs/2401.08840",
        "authors": [
            {
                "last_name": "Devkota",
                "first_name": "Sudarshan"
            },
            {
                "last_name": "Pattanaik",
                "first_name": "Sumanta"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  In this paper, we propose an efficient approach for the compression and representation of volumetric data utilizing coordinate-based networks and multi-resolution hash encoding. Efficient compression of volumetric data is crucial for various applications, such as medical imaging and scientific simulations. Our approach enables effective compression by learning a mapping between spatial coordinates and intensity values. We compare different encoding schemes and demonstrate the superiority of multi-resolution hash encoding in terms of compression quality and training efficiency. Furthermore, we leverage optimization-based meta-learning, specifically using the Reptile algorithm, to learn weight initialization for neural representations tailored to volumetric data, enabling faster convergence during optimization. Additionally, we compare our approach with state-of-the-art methods to showcase improved image quality and compression ratios. These findings highlight the potential of coordinate-based networks and multi-resolution hash encoding for an efficient and accurate representation of volumetric data, paving the way for advancements in large-scale data visualization and other applications. ",
        "title": "Efficient Neural Representation of Volumetric Data using  Coordinate-Based Networks",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08841",
        "abstract_url": "http://arxiv.org/abs/2401.08841",
        "authors": [
            {
                "last_name": "Ajao",
                "first_name": "Oluwaseun"
            },
            {
                "last_name": "Garg",
                "first_name": "Ashish"
            },
            {
                "last_name": "Da Costa-Abreu",
                "first_name": "Marjory"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The coronavirus pandemic (COVID-19) is probably the most disruptive global health disaster in recent history. It negatively impacted the whole world and virtually brought the global economy to a standstill. However, as the virus was spreading, infecting people and claiming thousands of lives so was the spread and propagation of fake news, misinformation and disinformation about the event. These included the spread of unconfirmed health advice and remedies on social media. In this paper, false information about the pandemic is identified using a content-based approach and metadata curated from messages posted to online social networks. A content-based approach combined with metadata as well as an initial feature analysis is used and then several supervised learning models are tested for identifying and predicting misleading posts. Our approach shows up to 93% accuracy in the detection of fake news related posts about the COVID-19 pandemic ",
        "title": "Exploring Content-Based and Meta-Data Analysis for Detecting Fake News  Infodemic: A case study on COVID-19",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08844",
        "abstract_url": "http://arxiv.org/abs/2401.08844",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Qiaoqiao"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this dissertation project, an actuation system was designed for the supersonic wind tunnel at the University of Manchester. The aim of this project is to build a remote control actuation system which could adjust the angle of attack for the aerodynamic shape to save researchers' time and improve the experimental efficiency. This project involves the model supporting system, a six component wind tunnel balance, a control system design, a virtual angle of attack adjustment interface and LabVIEW programming implementation, the angle of attack adjustment range is from -20 to 20 degree. The three-dimensional model of the mechanical part and its engineering drawing were finished in SolidWorks, and the control system including the sensors and rotary encoder control, the closed-loop control of the stepper motor and the wind tunnel balance feedback. The performance of the wind tunnel balance can be known in advance by finite element analysis. Finally, the virtual operating system was built based on the LabVIEW and Arduino interactive programs ",
        "title": "Wind tunnel actuation movement system",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08846",
        "abstract_url": "http://arxiv.org/abs/2401.08846",
        "authors": [
            {
                "last_name": "Thelasingha",
                "first_name": "Neelanga"
            },
            {
                "last_name": "Julius",
                "first_name": "Agung"
            },
            {
                "last_name": "Humann",
                "first_name": "James"
            },
            {
                "last_name": "Reddinger",
                "first_name": "Jean-Paul"
            },
            {
                "last_name": "Dotterweich",
                "first_name": "James"
            },
            {
                "last_name": "Childers",
                "first_name": "Marshal"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents an iterative planning framework for multi-agent systems with hybrid state spaces. The framework uses transition systems to mathematically represent planning tasks and employs multiple solvers to iteratively improve the plan until computation resources are exhausted. When integrating different solvers for iterative planning, we establish theoretical guarantees on the mathematical framework to ensure recursive feasibility. The proposed framework enables continual improvement of solution optimality, efficiently using allocated computation resources. The proposed method is validated by applying it to an energy-aware UGV-UAV cooperative task site assignment. The results demonstrate the continual solution improvement while preserving real-time implementation ability compared to algorithms proposed in the literature. ",
        "title": "Iterative Planning for Multi-agent Systems: An Application in  Energy-Aware UAV-UGV Cooperative Task Site Assignments",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08847",
        "abstract_url": "http://arxiv.org/abs/2401.08847",
        "authors": [
            {
                "last_name": "Maleki",
                "first_name": "Farhad"
            },
            {
                "last_name": "Moy",
                "first_name": "Linda"
            },
            {
                "last_name": "Forghani",
                "first_name": "Reza"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Tapotosh"
            },
            {
                "last_name": "Ovens",
                "first_name": "Katie"
            },
            {
                "last_name": "Langer",
                "first_name": "Steve"
            },
            {
                "last_name": "Rouzrokh",
                "first_name": "Pouria"
            },
            {
                "last_name": "Khosravi",
                "first_name": "Bardia"
            },
            {
                "last_name": "Ganjizadeh",
                "first_name": "Ali"
            },
            {
                "last_name": "Warren",
                "first_name": "Daniel"
            },
            {
                "last_name": "Daneshjou",
                "first_name": "Roxana"
            },
            {
                "last_name": "Moassefi",
                "first_name": "Mana"
            },
            {
                "last_name": "Avval",
                "first_name": "Atlas Haddadi"
            },
            {
                "last_name": "Sotardi",
                "first_name": "Susan"
            },
            {
                "last_name": "Tenenholtz",
                "first_name": "Neil"
            },
            {
                "last_name": "Kitamura",
                "first_name": "Felipe"
            },
            {
                "last_name": "Kline",
                "first_name": "Timothy"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Deep learning techniques, despite their potential, often suffer from a lack of reproducibility and generalizability, impeding their clinical adoption. Image segmentation is one of the critical tasks in medical image analysis, in which one or several regions/volumes of interest should be annotated. This paper introduces the RIDGE checklist, a framework for assessing the Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of deep learning-based medical image segmentation models. The checklist serves as a guide for researchers to enhance the quality and transparency of their work, ensuring that segmentation models are not only scientifically sound but also clinically relevant. ",
        "title": "RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and  Efficiency Assessment of Medical Image Segmentation Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08850",
        "abstract_url": "http://arxiv.org/abs/2401.08850",
        "authors": [
            {
                "last_name": "Ireland",
                "first_name": "David"
            },
            {
                "last_name": "Montana",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension. ",
        "title": "REValueD: Regularised Ensemble Value-Decomposition for Factorisable  Markov Decision Processes",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08851",
        "abstract_url": "http://arxiv.org/abs/2401.08851",
        "authors": [
            {
                "last_name": "Lasko",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Ma",
                "first_name": "Jeff"
            },
            {
                "last_name": "Nicoletti",
                "first_name": "Mike"
            },
            {
                "last_name": "Sussman-Fort",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Jeong",
                "first_name": "Sooyoung"
            },
            {
                "last_name": "Hartmann",
                "first_name": "William"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "SD"
        ],
        "abstract": "  Cognitive load classification is the task of automatically determining an individual's utilization of working memory resources during performance of a task based on physiologic measures such as electroencephalography (EEG). In this paper, we follow a cross-disciplinary approach, where tools and methodologies from speech processing are used to tackle this problem. The corpus we use was released publicly in 2021 as part of the first passive brain-computer interface competition on cross-session workload estimation. We present our approach which used i-vector-based neural network classifiers to accomplish inter-subject cross-session EEG transfer learning, achieving 18% relative improvement over equivalent subject-dependent models. We also report experiments showing how our subject-independent models perform competitively on held-out subjects and improve with additional subject data, suggesting that subject-dependent training is not required for effective cognitive load determination. ",
        "title": "Using i-vectors for subject-independent cross-session EEG transfer  learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08858",
        "abstract_url": "http://arxiv.org/abs/2401.08858",
        "authors": [
            {
                "last_name": "Conway",
                "first_name": "Alex"
            },
            {
                "last_name": "Bakshi",
                "first_name": "Ainesh"
            },
            {
                "last_name": "Bhattacharya",
                "first_name": "Arghya"
            },
            {
                "last_name": "Bennett",
                "first_name": "Rory"
            },
            {
                "last_name": "Jiao",
                "first_name": "Yizheng"
            },
            {
                "last_name": "Knorr",
                "first_name": "Eric"
            },
            {
                "last_name": "Zhan",
                "first_name": "Yang"
            },
            {
                "last_name": "Bender",
                "first_name": "Michael A."
            },
            {
                "last_name": "Jannen",
                "first_name": "William"
            },
            {
                "last_name": "Johnson",
                "first_name": "Rob"
            },
            {
                "last_name": "Kuszmaul",
                "first_name": "Bradley C."
            },
            {
                "last_name": "Porter",
                "first_name": "Donald E."
            },
            {
                "last_name": "Yuan",
                "first_name": "Jun"
            },
            {
                "last_name": "Farach-Colton",
                "first_name": "Martin"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS"
        ],
        "abstract": "  File systems must allocate space for files without knowing what will be added or removed in the future. Over the life of a file system, this may cause suboptimal file placement decisions that eventually lead to slower performance, or aging. Conventional wisdom suggests that file system aging is a solved problem in the common case; heuristics to avoid aging, such as colocating related files and data blocks, are effective until a storage device fills up, at which point space pressure exacerbates fragmentation-based aging. However, this article describes both realistic and synthetic workloads that can cause these heuristics to fail, inducing large performance declines due to aging, even when the storage device is nearly empty.   We argue that these slowdowns are caused by poor layout. We demonstrate a correlation between the read performance of a directory scan and the locality within a file system's access patterns, using a dynamic layout score. We complement these results with microbenchmarks that show that space pressure can cause a substantial amount of inter-file and intra-file fragmentation. However, our results suggest that the effect of free-space fragmentation on read performance is best described as accelerating the file system aging process. The effect on write performance is non-existent in some cases, and, in most cases, an order of magnitude smaller than the read degradation from fragmentation caused by normal usage.   In short, many file systems are exquisitely prone to read aging after a variety of write patterns. We show, however, that aging is not inevitable. BetrFS, a file system based on write-optimized dictionaries, exhibits almost no aging in our experiments. We present a framework for understanding and predicting aging, and identify the key features of BetrFS that avoid aging. ",
        "title": "File System Aging",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08859",
        "abstract_url": "http://arxiv.org/abs/2401.08859",
        "authors": [
            {
                "last_name": "Sinha",
                "first_name": "Prasoon"
            },
            {
                "last_name": "Kaffes",
                "first_name": "Kostis"
            },
            {
                "last_name": "Yadwadkar",
                "first_name": "Neeraja J."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  Serverless computing relieves developers from the burden of resource management, thus providing ease-of-use to the users and the opportunity to optimize resource utilization for the providers. However, today's serverless systems lack performance guarantees for function invocations, thus limiting support for performance-critical applications: we observed severe performance variability (up to 6x). Providers lack visibility into user functions and hence find it challenging to right-size them: we observed heavy resource underutilization (up to 80%). To understand the causes behind the performance variability and underutilization, we conducted a measurement study of commonly deployed serverless functions and learned that the function performance and resource utilization depend crucially on function semantics and inputs. Our key insight is to delay making resource allocation decisions until after the function inputs are available. We introduce Shabari, a resource management framework for serverless systems that makes decisions as late as possible to right-size each invocation to meet functions' performance objectives (SLOs) and improve resource utilization. Shabari uses an online learning agent to right-size each function invocation based on the features of the function input and makes cold-start-aware scheduling decisions. For a range of serverless functions and inputs, Shabari reduces SLO violations by 11-73% while not wasting any vCPUs and reducing wasted memory by 64-94% in the median case, compared to state-of-the-art systems, including Aquatope, Parrotfish, and Cypress. ",
        "title": "Shabari: Delayed Decision-Making for Faster and Efficient Serverless  Function",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08860",
        "abstract_url": "http://arxiv.org/abs/2401.08860",
        "authors": [
            {
                "last_name": "Bi",
                "first_name": "Qi"
            },
            {
                "last_name": "Ji",
                "first_name": "Wei"
            },
            {
                "last_name": "Yi",
                "first_name": "Jingjun"
            },
            {
                "last_name": "Zhan",
                "first_name": "Haolan"
            },
            {
                "last_name": "Xia",
                "first_name": "Gui-Song"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  High-quality annotation of fine-grained visual categories demands great expert knowledge, which is taxing and time consuming. Alternatively, learning fine-grained visual representation from enormous unlabeled images (e.g., species, brands) by self-supervised learning becomes a feasible solution. However, recent researches find that existing self-supervised learning methods are less qualified to represent fine-grained categories. The bottleneck lies in that the pre-text representation is built from every patch-wise embedding, while fine-grained categories are only determined by several key patches of an image. In this paper, we propose a Cross-level Multi-instance Distillation (CMD) framework to tackle the challenge. Our key idea is to consider the importance of each image patch in determining the fine-grained pre-text representation by multiple instance learning. To comprehensively learn the relation between informative patches and fine-grained semantics, the multi-instance knowledge distillation is implemented on both the region/image crop pairs from the teacher and student net, and the region-image crops inside the teacher / student net, which we term as intra-level multi-instance distillation and inter-level multi-instance distillation. Extensive experiments on CUB-200-2011, Stanford Cars and FGVC Aircraft show that the proposed method outperforms the contemporary method by upto 10.14% and existing state-of-the-art self-supervised learning approaches by upto 19.78% on both top-1 accuracy and Rank-1 retrieval metric. ",
        "title": "Cross-Level Multi-Instance Distillation for Self-Supervised Fine-Grained  Visual Categorization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08861",
        "abstract_url": "http://arxiv.org/abs/2401.08861",
        "authors": [
            {
                "last_name": "Nouri",
                "first_name": "Salar"
            },
            {
                "last_name": "Motalleb",
                "first_name": "Mojdeh Karbalaee"
            },
            {
                "last_name": "Shah-Mansouri",
                "first_name": "Vahid"
            },
            {
                "last_name": "Shariatpanahi",
                "first_name": "Seyed Pooya"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG",
            "MS"
        ],
        "abstract": "  The Open Radio Access Network (O-RAN) technology has emerged as a promising solution for network operators, providing them with an open and favorable environment. Ensuring effective coordination of x-applications (xAPPs) is crucial to enhance flexibility and optimize network performance within the O-RAN. In this paper, we introduce an innovative approach to the resource allocation problem, aiming to coordinate multiple independent xAPPs for network slicing and resource allocation in O-RAN. Our proposed method focuses on maximizing the weighted throughput among user equipments (UE), as well as allocating physical resource blocks (PRBs). We prioritize two service types, namely enhanced Mobile Broadband and Ultra Reliable Low Latency Communication. To achieve this, we have designed two xAPPs: a power control xAPP for each UE and a PRB allocation xAPP. The proposed method consists of a two-part training phase, where the first part uses supervised learning with a Variational Autoencoder trained to regress the power transmission as well as the user association and PRB allocation decisions, and the second part uses unsupervised learning with a contrastive loss approach to improve the generalization and robustness of the model. We evaluate the performance of our proposed method by comparing its results to those obtained from an exhaustive search algorithm, deep Q-network algorithm, and by reporting performance metrics for the regression task. We also evaluate the proposed model's performance in different scenarios among the service types. The results show that the proposed method is a more efficient and effective solution for network slicing problems compared to state-of-the-art methods. ",
        "title": "Semi-Supervised Learning Approach for Efficient Resource Allocation with  Network Slicing in O-RAN",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08863",
        "abstract_url": "http://arxiv.org/abs/2401.08863",
        "authors": [
            {
                "last_name": "Kolli",
                "first_name": "Abhiram"
            },
            {
                "last_name": "Casamassima",
                "first_name": "Filippo"
            },
            {
                "last_name": "Possegger",
                "first_name": "Horst"
            },
            {
                "last_name": "Bischof",
                "first_name": "Horst"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Using neural networks for localization of key fob within and surrounding a car as a security feature for keyless entry is fast emerging. In this paper we study: 1) the performance of pre-computed features of neural networks based UWB (ultra wide band) localization classification forming the baseline of our experiments. 2) Investigate the inherent robustness of various neural networks; therefore, we include the study of robustness of the adversarial examples without any adversarial training in this work. 3) Propose a multi-head self-supervised neural network architecture which outperforms the baseline neural networks without any adversarial training. The model's performance improved by 67% at certain ranges of adversarial magnitude for fast gradient sign method and 37% each for basic iterative method and projected gradient descent method. ",
        "title": "Robust Localization of Key Fob Using Channel Impulse Response of Ultra  Wide Band Sensors for Keyless Entry Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08864",
        "abstract_url": "http://arxiv.org/abs/2401.08864",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Yang"
            },
            {
                "last_name": "Sung",
                "first_name": "George"
            },
            {
                "last_name": "Shih",
                "first_name": "Shao-Fu"
            },
            {
                "last_name": "Erdogan",
                "first_name": "Hakan"
            },
            {
                "last_name": "Lee",
                "first_name": "Chehung"
            },
            {
                "last_name": "Grundmann",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD"
        ],
        "abstract": "  We propose a neural network model that can separate target speech sources from interfering sources at different angular regions using two microphones. The model is trained with simulated room impulse responses (RIRs) using omni-directional microphones without needing to collect real RIRs. By relying on specific angular regions and multiple room simulations, the model utilizes consistent time difference of arrival (TDOA) cues, or what we call delay contrast, to separate target and interference sources while remaining robust in various reverberation environments. We demonstrate the model is not only generalizable to a commercially available device with a slightly different microphone geometry, but also outperforms our previous work which uses one additional microphone on the same device. The model runs in real-time on-device and is suitable for low-latency streaming applications such as telephony and video conferencing. ",
        "title": "Binaural Angular Separation Network",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08865",
        "abstract_url": "http://arxiv.org/abs/2401.08865",
        "authors": [
            {
                "last_name": "Konz",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Mazurowski",
                "first_name": "Maciej A."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic \"label sharpness\" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model's adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our $d_{data}$ formalism to the related metric of learned representation intrinsic dimension ($d_{repr}$), derive a generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$ serves as an upper bound for $d_{repr}$. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks. ",
        "title": "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling  Learning Differences Between Natural and Medical Images",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08866",
        "abstract_url": "http://arxiv.org/abs/2401.08866",
        "authors": [
            {
                "last_name": "Di Paola",
                "first_name": "Ambra"
            },
            {
                "last_name": "Muraro",
                "first_name": "Serena"
            },
            {
                "last_name": "Marinelli",
                "first_name": "Roberto"
            },
            {
                "last_name": "Pilato",
                "first_name": "Christian"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Augmentative and Alternative Communication (AAC) are essential techniques that help people with communication disabilities. AAC demonstrates its transformative power by replacing spoken language with symbol sequences. However, to unlock its full potential, AAC materials must adhere to specific characteristics, placing the onus on educators to create custom-tailored materials and symbols. This paper introduces AMBRA (Pervasive and Personalized Augmentative and Alternative Communication based on Federated Learning and Generative AI), an open platform that aims to leverage the capabilities of foundation models to tackle many AAC issues, opening new opportunities (but also challenges) for AI-enhanced AAC. We thus present a compelling vision--a roadmap towards a more inclusive society. By leveraging the capabilities of modern technologies, we aspire to not only transform AAC but also guide the way toward a world where communication knows no bounds. ",
        "title": "Foundation Models in Augmentative and Alternative Communication:  Opportunities and Challenges",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08867",
        "abstract_url": "http://arxiv.org/abs/2401.08867",
        "authors": [
            {
                "last_name": "Ahamed",
                "first_name": "Md Atik"
            },
            {
                "last_name": "Cheng",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Tabular data remains ubiquitous across domains despite growing use of images and texts for machine learning. While deep learning models like convolutional neural networks and transformers achieve strong performance on tabular data, they require extensive data preprocessing, tuning, and resources, limiting accessibility and scalability. This work develops an innovative approach based on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have strong capabilities for efficiently extracting effective representations from data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM variant, for end-to-end supervised learning on tables. Compared to state-of-the-art baselines, MambaTab delivers superior performance while requiring significantly fewer parameters and minimal preprocessing, as empirically validated on diverse benchmark datasets. MambaTab's efficiency, scalability, generalizability, and predictive gains signify it as a lightweight, \"out-of-the-box\" solution for diverse tabular data with promise for enabling wider practical applications. ",
        "title": "MambaTab: A Simple Yet Effective Approach for Handling Tabular Data",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08868",
        "abstract_url": "http://arxiv.org/abs/2401.08868",
        "authors": [
            {
                "last_name": "Tran",
                "first_name": "Manuel"
            },
            {
                "last_name": "Lahiani",
                "first_name": "Amal"
            },
            {
                "last_name": "Cid",
                "first_name": "Yashin Dicente"
            },
            {
                "last_name": "Boxberg",
                "first_name": "Melanie"
            },
            {
                "last_name": "Lienemann",
                "first_name": "Peter"
            },
            {
                "last_name": "Matek",
                "first_name": "Christian"
            },
            {
                "last_name": "Wagner",
                "first_name": "Sophia J."
            },
            {
                "last_name": "Theis",
                "first_name": "Fabian J."
            },
            {
                "last_name": "Klaiman",
                "first_name": "Eldad"
            },
            {
                "last_name": "Peng",
                "first_name": "Tingying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vision Transformers (ViTs) and Swin Transformers (Swin) are currently state-of-the-art in computational pathology. However, domain experts are still reluctant to use these models due to their lack of interpretability. This is not surprising, as critical decisions need to be transparent and understandable. The most common approach to understanding transformers is to visualize their attention. However, attention maps of ViTs are often fragmented, leading to unsatisfactory explanations. Here, we introduce a novel architecture called the B-cos Vision Transformer (BvT) that is designed to be more interpretable. It replaces all linear transformations with the B-cos transform to promote weight-input alignment. In a blinded study, medical experts clearly ranked BvTs above ViTs, suggesting that our network is better at capturing biomedically relevant structures. This is also true for the B-cos Swin Transformer (Bwin). Compared to the Swin Transformer, it even improves the F1-score by up to 4.7% on two public datasets. ",
        "title": "B-Cos Aligned Transformers Learn Human-Interpretable Features",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08870",
        "abstract_url": "http://arxiv.org/abs/2401.08870",
        "authors": [
            {
                "last_name": "Blanco-Claraco",
                "first_name": "Jose Luis"
            },
            {
                "last_name": "Ma\u00f1as-Alvarez",
                "first_name": "Francisco"
            },
            {
                "last_name": "Torres-Moreno",
                "first_name": "Jose Luis"
            },
            {
                "last_name": "Rodriguez",
                "first_name": "Francisco"
            },
            {
                "last_name": "Gimenez-Fernandez",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Keeping a vehicle well-localized within a prebuilt-map is at the core of any autonomous vehicle navigation system. In this work, we show that both standard SIR sampling and rejection-based optimal sampling are suitable for efficient (10 to 20 ms) real-time pose tracking without feature detection that is using raw point clouds from a 3D LiDAR. Motivated by the large amount of information captured by these sensors, we perform a systematic statistical analysis of how many points are actually required to reach an optimal ratio between efficiency and positioning accuracy. Furthermore, initialization from adverse conditions, e.g., poor GPS signal in urban canyons, we also identify the optimal particle filter settings required to ensure convergence. Our findings include that a decimation factor between 100 and 200 on incoming point clouds provides a large savings in computational cost with a negligible loss in localization accuracy for a VLP-16 scanner. Furthermore, an initial density of $\\sim$2 particles/m$^2$ is required to achieve 100% convergence success for large-scale ($\\sim$100,000 m$^2$), outdoor global localization without any additional hint from GPS or magnetic field sensors. All implementations have been released as open-source software. ",
        "title": "Benchmarking Particle Filter Algorithms for Efficient Velodyne-Based  Vehicle Localization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08875",
        "abstract_url": "http://arxiv.org/abs/2401.08875",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Jiaming"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversion prediction models trained on observational data. This paper redefines the causal effect of user fea-tures on conversions and proposes a novel end-to-end approach, Deep Causal Representation for MTA (DCRMTA). Our model while eliminating confounding variables, extracts features with causal relations to conversions from users. Fur-thermore, Extensive experiments on both synthet-ic and real-world Criteo data demonstrate DCRMTA's superior performance in converting prediction across varying data distributions, while also effectively attributing value across dif-ferent advertising channels ",
        "title": "DCRMTA: Unbiased Causal Representation for Multi-touch Attribution",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08876",
        "abstract_url": "http://arxiv.org/abs/2401.08876",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dongping"
            },
            {
                "last_name": "Chatzimparmpas",
                "first_name": "Angelos"
            },
            {
                "last_name": "Kamali",
                "first_name": "Negar"
            },
            {
                "last_name": "Hullman",
                "first_name": "Jessica"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV",
            "LG"
        ],
        "abstract": "  As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications on how to incorporate them for real-world decision-making. ",
        "title": "Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image  Labeling",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08878",
        "abstract_url": "http://arxiv.org/abs/2401.08878",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Geon"
            },
            {
                "last_name": "Bu",
                "first_name": "Fanchen"
            },
            {
                "last_name": "Eliassi-Rad",
                "first_name": "Tina"
            },
            {
                "last_name": "Shin",
                "first_name": "Kijung"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "DB"
        ],
        "abstract": "  Hypergraphs are a natural and powerful choice for modeling group interactions in the real world, which are often referred to as higher-order networks. For example, when modeling collaboration networks, where collaborations can involve not just two but three or more people, employing hypergraphs allows us to explore beyond pairwise (dyadic) patterns and capture groupwise (polyadic) patterns. The mathematical complexity of hypergraphs offers both opportunities and challenges for learning and mining on hypergraphs, and hypergraph mining, which seeks to enhance our understanding of underlying systems through hypergraph modeling, gained increasing attention in research. Researchers have discovered various structural patterns in real-world hypergraphs, leading to the development of mining tools. Moreover, they have designed generators with the aim of reproducing and thereby shedding light on these patterns. In this survey, we provide a comprehensive overview of the current landscape of hypergraph mining, covering patterns, tools, and generators. We provide comprehensive taxonomies for them, and we also provide in-depth discussions to provide insights into future research on hypergraph mining. ",
        "title": "A Survey on Hypergraph Mining: Patterns, Tools, and Generators",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08879",
        "abstract_url": "http://arxiv.org/abs/2401.08879",
        "authors": [
            {
                "last_name": "Kampik",
                "first_name": "Timotheus"
            },
            {
                "last_name": "Potyka",
                "first_name": "Nico"
            },
            {
                "last_name": "Yin",
                "first_name": "Xiang"
            },
            {
                "last_name": "\u010cyras",
                "first_name": "Kristijonas"
            },
            {
                "last_name": "Toni",
                "first_name": "Francesca"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a principle-based analysis of contribution functions for quantitative bipolar argumentation graphs that quantify the contribution of one argument to another. The introduced principles formalise the intuitions underlying different contribution functions as well as expectations one would have regarding the behaviour of contribution functions in general. As none of the covered contribution functions satisfies all principles, our analysis can serve as a tool that enables the selection of the most suitable function based on the requirements of a given use case. ",
        "title": "Contribution Functions for Quantitative Bipolar Argumentation Graphs: A  Principle-based Analysis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08881",
        "abstract_url": "http://arxiv.org/abs/2401.08881",
        "authors": [
            {
                "last_name": "Pustelnik",
                "first_name": "Frederik Dermot"
            },
            {
                "last_name": "Sa\u00df",
                "first_name": "Xhani Marvin"
            },
            {
                "last_name": "Seifert",
                "first_name": "Jean-Pierre"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Graphic Processing Units (GPUs) have transcended their traditional use-case of rendering graphics and nowadays also serve as a powerful platform for accelerating ubiquitous, non-graphical rendering tasks. One prominent task is inference of neural networks, which process vast amounts of personal data, such as audio, text or images. Thus, GPUs became integral components for handling vast amounts of potentially confidential data, which has awakened the interest of security researchers. This lead to the discovery of various vulnerabilities in GPUs in recent years. In this paper, we uncover yet another vulnerability class in GPUs: We found that some GPU implementations lack proper register initialization routines before shader execution, leading to unintended register content leakage of previously executed shader kernels. We showcase the existence of the aforementioned vulnerability on products of 3 major vendors - Apple, NVIDIA and Qualcomm. The vulnerability poses unique challenges to an adversary due to opaque scheduling and register remapping algorithms present in the GPU firmware, complicating the reconstruction of leaked data. In order to illustrate the real-world impact of this flaw, we showcase how these challenges can be solved for attacking various workloads on the GPU. First, we showcase how uninitialized registers leak arbitrary pixel data processed by fragment shaders. We further implement information leakage attacks on intermediate data of Convolutional Neural Networks (CNNs) and present the attack's capability to leak and reconstruct the output of Large Language Models (LLMs). ",
        "title": "Whispering Pixels: Exploiting Uninitialized Register Accesses in Modern  GPUs",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08886",
        "abstract_url": "http://arxiv.org/abs/2401.08886",
        "authors": [
            {
                "last_name": "Peyvan",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Oommen",
                "first_name": "Vivek"
            },
            {
                "last_name": "Jagtap",
                "first_name": "Ameya D."
            },
            {
                "last_name": "Karniadakis",
                "first_name": "George Em"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Developing the proper representations for simulating high-speed flows with strong shock waves, rarefactions, and contact discontinuities has been a long-standing question in numerical analysis. Herein, we employ neural operators to solve Riemann problems encountered in compressible flows for extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we first consider the DeepONet that we train in a two-stage process, following the recent work of Lee and Shin, wherein the first stage, a basis is extracted from the trunk net, which is orthonormalized and subsequently is used in the second stage in training the branch net. This simple modification of DeepONet has a profound effect on its accuracy, efficiency, and robustness and leads to very accurate solutions to Riemann problems compared to the vanilla version. It also enables us to interpret the results physically as the hierarchical data-driven produced basis reflects all the flow features that would otherwise be introduced using ad hoc feature expansion layers. We also compare the results with another neural operator based on the U-Net for low, intermediate, and very high-pressure ratios that are very accurate for Riemann problems, especially for large pressure ratios, due to their multiscale nature but computationally more expensive. Overall, our study demonstrates that simple neural network architectures, if properly pre-trained, can achieve very accurate solutions of Riemann problems for real-time forecasting. ",
        "title": "RiemannONets: Interpretable Neural Operators for Riemann Problems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08887",
        "abstract_url": "http://arxiv.org/abs/2401.08887",
        "authors": [
            {
                "last_name": "Vinnikov",
                "first_name": "Alon"
            },
            {
                "last_name": "Ivry",
                "first_name": "Amir"
            },
            {
                "last_name": "Hurvitz",
                "first_name": "Aviv"
            },
            {
                "last_name": "Abramovski",
                "first_name": "Igor"
            },
            {
                "last_name": "Koubi",
                "first_name": "Sharon"
            },
            {
                "last_name": "Gurvich",
                "first_name": "Ilya"
            },
            {
                "last_name": "Pe`er",
                "first_name": "Shai"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xiong"
            },
            {
                "last_name": "Elizalde",
                "first_name": "Benjamin Martinez"
            },
            {
                "last_name": "Kanda",
                "first_name": "Naoyuki"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Shaer",
                "first_name": "Shalev"
            },
            {
                "last_name": "Yagev",
                "first_name": "Stav"
            },
            {
                "last_name": "Asher",
                "first_name": "Yossi"
            },
            {
                "last_name": "Sivasankaran",
                "first_name": "Sunit"
            },
            {
                "last_name": "Gong",
                "first_name": "Yifan"
            },
            {
                "last_name": "Tang",
                "first_name": "Min"
            },
            {
                "last_name": "Wang",
                "first_name": "Huaming"
            },
            {
                "last_name": "Krupka",
                "first_name": "Eyal"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL"
        ],
        "abstract": "  We introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in far-field meeting scenarios, with single-channel and known-geometry multi-channel tracks, and serves as a launch platform for two new datasets: First, a benchmarking dataset of 315 meetings, averaging 6 minutes each, capturing a broad spectrum of real-world acoustic conditions and conversational dynamics. It is recorded across 30 conference rooms, featuring 4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated training dataset, synthesized with enhanced authenticity for real-world generalization, incorporating 15,000 real acoustic transfer functions. The tasks focus on single-device DASR, where multi-channel devices always share the same known geometry. This is aligned with common setups in actual conference rooms, and avoids technical complexities associated with multi-device tasks. It also allows for the development of geometry-specific solutions. The NOTSOFAR-1 Challenge aims to advance research in the field of distant conversational speech recognition, providing key resources to unlock the potential of data-driven methods, which we believe are currently constrained by the absence of comprehensive high-quality training and benchmarking datasets. ",
        "title": "NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant  Meeting Transcription",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08889",
        "abstract_url": "http://arxiv.org/abs/2401.08889",
        "authors": [
            {
                "last_name": "McCallum",
                "first_name": "Matthew C."
            },
            {
                "last_name": "Davies",
                "first_name": "Matthew E. P."
            },
            {
                "last_name": "Henkel",
                "first_name": "Florian"
            },
            {
                "last_name": "Kim",
                "first_name": "Jaehun"
            },
            {
                "last_name": "Sandberg",
                "first_name": "Samuel E."
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "IR",
            "LG",
            "MM"
        ],
        "abstract": "  Audio embeddings are crucial tools in understanding large catalogs of music. Typically embeddings are evaluated on the basis of the performance they provide in a wide range of downstream tasks, however few studies have investigated the local properties of the embedding spaces themselves which are important in nearest neighbor algorithms, commonly used in music search and recommendation. In this work we show that when learning audio representations on music datasets via contrastive learning, musical properties that are typically homogeneous within a track (e.g., key and tempo) are reflected in the locality of neighborhoods in the resulting embedding space. By applying appropriate data augmentation strategies, localisation of such properties can not only be reduced but the localisation of other attributes is increased. For example, locality of features such as pitch and tempo that are less relevant to non-expert listeners, may be mitigated while improving the locality of more salient features such as genre and mood, achieving state-of-the-art performance in nearest neighbor retrieval accuracy. Similarly, we show that the optimal selection of data augmentation strategies for contrastive learning of music audio embeddings is dependent on the downstream task, highlighting this as an important embedding design decision. ",
        "title": "On the Effect of Data-Augmentation on Local Embedding Properties in the  Contrastive Learning of Music Audio Representations",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08890",
        "abstract_url": "http://arxiv.org/abs/2401.08890",
        "authors": [
            {
                "last_name": "Bashir",
                "first_name": "Hafiz Mohsin"
            },
            {
                "last_name": "Faisal",
                "first_name": "Abdullah Bin"
            },
            {
                "last_name": "Dogar",
                "first_name": "Fahad R."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Many cloud systems utilize low-priority flows to achieve various performance objectives (e.g., low latency, high utilization), relying on TCP as their preferred transport protocol. However, the suitability of TCP for such low-priority flows is relatively unexplored. Specifically, how prioritization-induced delays in packet transmission can cause spurious timeouts and low utilization. In this paper, we conduct an empirical study to investigate the performance of TCP for low-priority flows under a wide range of realistic scenarios: use-cases (with accompanying workloads) where the performance of low-priority flows is crucial to the functioning of the overall system as well as various network loads and other network parameters. Our findings yield two key insights: 1) for several popular use-cases (e.g., network scheduling), TCP's performance for low-priority flows is within 2x of a near-optimal scheme, 2) for emerging workloads that exhibit an on-off behavior in the high priority queue (e.g., distributed ML model training), TCP's performance for low-priority flows is poor. Finally, we discuss and conduct preliminary evaluation to show that two simple strategies -- weighted fair queuing (WFQ) and cross-queue congestion notification -- can substantially improve TCP's performance for low-priority flows. ",
        "title": "Characterizing TCP's Performance for Low-Priority Flows Inside a Cloud",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08891",
        "abstract_url": "http://arxiv.org/abs/2401.08891",
        "authors": [
            {
                "last_name": "Henkel",
                "first_name": "Florian"
            },
            {
                "last_name": "Kim",
                "first_name": "Jaehun"
            },
            {
                "last_name": "McCallum",
                "first_name": "Matthew C."
            },
            {
                "last_name": "Sandberg",
                "first_name": "Samuel E."
            },
            {
                "last_name": "Davies",
                "first_name": "Matthew E. P."
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  This paper addresses the problem of global tempo estimation in musical audio. Given that annotating tempo is time-consuming and requires certain musical expertise, few publicly available data sources exist to train machine learning models for this task. Towards alleviating this issue, we propose a fully self-supervised approach that does not rely on any human labeled data. Our method builds on the fact that generic (music) audio embeddings already encode a variety of properties, including information about tempo, making them easily adaptable for downstream tasks. While recent work in self-supervised tempo estimation aimed to learn a tempo specific representation that was subsequently used to train a supervised classifier, we reformulate the task into the binary classification problem of predicting whether a target track has the same or a different tempo compared to a reference. While the former still requires labeled training data for the final classification model, our approach uses arbitrary unlabeled music data in combination with time-stretching for model training as well as a small set of synthetically created reference samples for predicting the final tempo. Evaluation of our approach in comparison with the state-of-the-art reveals highly competitive performance when the constraint of finding the precise tempo octave is relaxed. ",
        "title": "Tempo estimation as fully self-supervised binary classification",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08893",
        "abstract_url": "http://arxiv.org/abs/2401.08893",
        "authors": [
            {
                "last_name": "Ozkara",
                "first_name": "Kaan"
            },
            {
                "last_name": "Karakus",
                "first_name": "Can"
            },
            {
                "last_name": "Raman",
                "first_name": "Parameswaran"
            },
            {
                "last_name": "Hong",
                "first_name": "Mingyi"
            },
            {
                "last_name": "Sabach",
                "first_name": "Shoham"
            },
            {
                "last_name": "Kveton",
                "first_name": "Branislav"
            },
            {
                "last_name": "Cevher",
                "first_name": "Volkan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve their error bounds (up to constants), hinting at an advantage for meta-optimizers. ",
        "title": "MADA: Meta-Adaptive Optimizers through hyper-gradient Descent",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08895",
        "abstract_url": "http://arxiv.org/abs/2401.08895",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Mark"
            },
            {
                "last_name": "Adamiak",
                "first_name": "Emanuel"
            },
            {
                "last_name": "Kozyrakis",
                "first_name": "Christos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC",
            "PF"
        ],
        "abstract": "  The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.   To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Meanwhile, cedar transparently applies a complex and extensible set of optimization techniques (e.g., offloading, caching, prefetching, fusion, and reordering). It then orchestrates processing across a customizable set of local and distributed compute resources in order to maximize processing performance and efficiency, all without user input. On average across six diverse input data pipelines, cedar achieves a 2.49x, 1.87x, 2.18x, and 2.74x higher performance compared to tf.data, tf.data service, Ray Data, and PyTorch's DataLoader, respectively. ",
        "title": "cedar: Composable and Optimized Machine Learning Input Data Pipelines",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08896",
        "abstract_url": "http://arxiv.org/abs/2401.08896",
        "authors": [
            {
                "last_name": "Rafy",
                "first_name": "Md Fazley"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Increasing integration of alternate electricity generation due to declining fossil fuels is becoming an essential option for future generations. As photovoltaic technology brings forth enormous benefits to the alternate solution for future power grid systems, this paper presents a comprehensive real-time system designed to model, control, and monitor a PV array subjected to dynamic loads using the Real-Time Digital Simulator (RTDS) environment. Integration with the Generic Transducer Network (GTNET)- Socket(SKT) module allows for the simulation of various environmental conditions, such as changes in insolation and temperature, and their direct impact on the PV array's performance. Utilizing BeagleBoard technology, the system demonstrates the capability to modify these conditions through real-time data input, subsequently observing the effects on current and voltage output curves. The real-time simulation results are visualized as a SCADA system within the Real-time Simulation for Automated Controller Design (RSCAD) runtime environment, providing insights into the effective management of solar power systems. ",
        "title": "Real-Time Control and Monitoring of Photovoltaic Arrays Using RTDS and  BeagleBoard Technology",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08897",
        "abstract_url": "http://arxiv.org/abs/2401.08897",
        "authors": [
            {
                "last_name": "Jung",
                "first_name": "Hee-Jun"
            },
            {
                "last_name": "Jeong",
                "first_name": "Jaehyoung"
            },
            {
                "last_name": "Kim",
                "first_name": "Kangil"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metric for multi-factor changes in comparison to disentanglement evaluation in VAEs. In quantitative and in-depth qualitative analysis, CFASL demonstrates a significant improvement of disentanglement in single-factor change, and multi-factor change conditions compared to state-of-the-art methods. ",
        "title": "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in  Variational AutoEncoder",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08898",
        "abstract_url": "http://arxiv.org/abs/2401.08898",
        "authors": [
            {
                "last_name": "Ni",
                "first_name": "Tianwei"
            },
            {
                "last_name": "Eysenbach",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Seyedsalehi",
                "first_name": "Erfan"
            },
            {
                "last_name": "Ma",
                "first_name": "Michel"
            },
            {
                "last_name": "Gehring",
                "first_name": "Clement"
            },
            {
                "last_name": "Mahajan",
                "first_name": "Aditya"
            },
            {
                "last_name": "Bacon",
                "first_name": "Pierre-Luc"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with distractors, and POMDPs with sparse rewards. These findings culminate in a set of practical guidelines for RL practitioners. ",
        "title": "Bridging State and History Representations: Understanding  Self-Predictive RL",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08899",
        "abstract_url": "http://arxiv.org/abs/2401.08899",
        "authors": [
            {
                "last_name": "Xian",
                "first_name": "Lu"
            },
            {
                "last_name": "Li",
                "first_name": "Lingyao"
            },
            {
                "last_name": "Xu",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ben Zefeng"
            },
            {
                "last_name": "Hemphill",
                "first_name": "Libby"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Generative AI has exhibited considerable potential to transform various industries and public life. The role of news media coverage of generative AI is pivotal in shaping public perceptions and judgments about this significant technological innovation. This paper provides in-depth analysis and rich insights into the temporal and spatial distribution of topics, sentiment, and substantive themes within global news coverage focusing on the latest emerging technology --generative AI. We collected a comprehensive dataset of news articles (January 2018 to November 2023, N = 24,827). For topic modeling, we employed the BERTopic technique and combined it with qualitative coding to identify semantic themes. Subsequently, sentiment analysis was conducted using the RoBERTa-base model. Analysis of temporal patterns in the data reveals notable variability in coverage across key topics--business, corporate technological development, regulation and security, and education--with spikes in articles coinciding with major AI developments and policy discussions. Sentiment analysis shows a predominantly neutral to positive media stance, with the business-related articles exhibiting more positive sentiment, while regulation and security articles receive a reserved, neutral to negative sentiment. Our study offers a valuable framework to investigate global news discourse and evaluate news attitudes and themes related to emerging technologies. ",
        "title": "Landscape of Generative AI in Global News: Topics, Sentiments, and  Spatiotemporal Analysis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08901",
        "abstract_url": "http://arxiv.org/abs/2401.08901",
        "authors": [
            {
                "last_name": "Sarkar",
                "first_name": "Abhiroop"
            },
            {
                "last_name": "Russo",
                "first_name": "Alejandro"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "PL"
        ],
        "abstract": "  Confidential computing is a security paradigm that enables the protection of confidential code and data in a co-tenanted cloud deployment using specialized hardware isolation units called Trusted Execution Environments (TEEs). By integrating TEEs with a Remote Attestation protocol, confidential computing allows a third party to establish the integrity of an \\textit{enclave} hosted within an untrusted cloud. However, TEE solutions, such as Intel SGX and ARM TrustZone, offer low-level C/C++-based toolchains that are susceptible to inherent memory safety vulnerabilities and lack language constructs to monitor explicit and implicit information-flow leaks. Moreover, the toolchains involve complex multi-project hierarchies and the deployment of hand-written attestation protocols for verifying \\textit{enclave} integrity.   We address the above with HasTEE+, a domain-specific language (DSL) embedded in Haskell that enables programming TEEs in a high-level language with strong type-safety. HasTEE+ assists in multi-tier cloud application development by (1) introducing a \\textit{tierless} programming model for expressing distributed client-server interactions as a single program, (2) integrating a general remote-attestation architecture that removes the necessity to write application-specific cross-cutting attestation code, and (3) employing a dynamic information flow control mechanism to prevent explicit as well as implicit data leaks. We demonstrate the practicality of HasTEE+ through a case study on confidential data analytics, presenting a data-sharing pattern applicable to mutually distrustful participants and providing overall performance metrics. ",
        "title": "HasTEE+ : Confidential Cloud Computing and Analytics with Haskell",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08902",
        "abstract_url": "http://arxiv.org/abs/2401.08902",
        "authors": [
            {
                "last_name": "McCallum",
                "first_name": "Matthew C."
            },
            {
                "last_name": "Henkel",
                "first_name": "Florian"
            },
            {
                "last_name": "Kim",
                "first_name": "Jaehun"
            },
            {
                "last_name": "Sandberg",
                "first_name": "Samuel E."
            },
            {
                "last_name": "Davies",
                "first_name": "Matthew E. P."
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "DL",
            "IR",
            "LG"
        ],
        "abstract": "  Audio embeddings enable large scale comparisons of the similarity of audio files for applications such as search and recommendation. Due to the subjectivity of audio similarity, it can be desirable to design systems that answer not only whether audio is similar, but similar in what way (e.g., wrt. tempo, mood or genre). Previous works have proposed disentangled embedding spaces where subspaces representing specific, yet possibly correlated, attributes can be weighted to emphasize those attributes in downstream tasks. However, no research has been conducted into the independence of these subspaces, nor their manipulation, in order to retrieve tracks that are similar but different in a specific way. Here, we explore the manipulation of tempo in embedding spaces as a case-study towards this goal. We propose tempo translation functions that allow for efficient manipulation of tempo within a pre-existing embedding space whilst maintaining other properties such as genre. As this translation is specific to tempo it enables retrieval of tracks that are similar but have specifically different tempi. We show that such a function can be used as an efficient data augmentation strategy for both training of downstream tempo predictors, and improved nearest neighbor retrieval of properties largely independent of tempo. ",
        "title": "Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for  Tempo Prediction and Search",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08903",
        "abstract_url": "http://arxiv.org/abs/2401.08903",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Fengfan"
            },
            {
                "last_name": "Ling",
                "first_name": "Heifei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experimental results, showcasing its superior performance. ",
        "title": "PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks  on Face Recognition Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08908",
        "abstract_url": "http://arxiv.org/abs/2401.08908",
        "authors": [
            {
                "last_name": "Kamath",
                "first_name": "Aditya K"
            },
            {
                "last_name": "Yadalam",
                "first_name": "Sujay"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS",
            "LG"
        ],
        "abstract": "  Computer systems are becoming increasingly heterogeneous with the emergence of new memory technologies and compute devices. GPUs alongside CPUs have become commonplace and CXL is poised to be a mainstay of cloud systems. The operating system is responsible for managing these hardware resources, requiring modification every time a new device is released. Years of research and development are sunk into tuning the OS for high performance with each new heterogeneous device. With the recent explosion in memory technologies and domain-specific accelerators, it would be beneficial to have an OS that could provide high performance for new devices without significant effort.   We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large Language Models (LLMs) to extract the useful features of new devices from their textual description and uses these features to make operating system decisions at runtime. Adding support to LLaMaS for a new device is as simple as describing the system and new device properties in plaintext.   LLaMaS reduces the burden on system administrators to enable easy integration of new devices into production systems.   Preliminary evaluation using ChatGPT shows that LLMs are capable of extracting device features from text and make correct OS decisions based on those features. ",
        "title": "Herding LLaMaS: Using LLMs as an OS Module",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08909",
        "abstract_url": "http://arxiv.org/abs/2401.08909",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Renchunzi"
            },
            {
                "last_name": "Odonnat",
                "first_name": "Ambroise"
            },
            {
                "last_name": "Feofanov",
                "first_name": "Vasilii"
            },
            {
                "last_name": "Redko",
                "first_name": "Ievgen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "An",
                "first_name": "Bo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach ensuring its empirical success. Extensive experiments conducted on diverse distribution shifts and model structures demonstrate that our method significantly outperforms state-of-the-art algorithms. ",
        "title": "Characterising Gradients for Unsupervised Accuracy Estimation under  Distribution Shift",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08913",
        "abstract_url": "http://arxiv.org/abs/2401.08913",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Chengxu"
            },
            {
                "last_name": "Fan",
                "first_name": "Qinrui"
            },
            {
                "last_name": "Hu",
                "first_name": "Shu"
            },
            {
                "last_name": "Wu",
                "first_name": "Xi"
            },
            {
                "last_name": "Wang",
                "first_name": "Xin"
            },
            {
                "last_name": "Hu",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  An important development direction in the Single-Image Super-Resolution (SISR) algorithms is to improve the efficiency of the algorithms. Recently, efficient Super-Resolution (SR) research focuses on reducing model complexity and improving efficiency through improved deep small kernel convolution, leading to a small receptive field. The large receptive field obtained by large kernel convolution can significantly improve image quality, but the computational cost is too high. To improve the reconstruction details of efficient super-resolution reconstruction, we propose a Symmetric Visual Attention Network (SVAN) by applying large receptive fields. The SVAN decomposes a large kernel convolution into three different combinations of convolution operations and combines them with an attention mechanism to form a Symmetric Large Kernel Attention Block (SLKAB), which forms a symmetric attention block with a bottleneck structure by the size of the receptive field in the convolution combination to extract depth features effectively as the basic component of the SVAN. Our network gets a large receptive field while minimizing the number of parameters and improving the perceptual ability of the model. The experimental results show that the proposed SVAN can obtain high-quality super-resolution reconstruction results using only about 30% of the parameters of existing SOTA methods. ",
        "title": "Efficient Image Super-Resolution via Symmetric Visual Attention Network",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08915",
        "abstract_url": "http://arxiv.org/abs/2401.08915",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Polimetla",
                "first_name": "Tejaswi"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Thomas W."
            },
            {
                "last_name": "Yan",
                "first_name": "Xiang"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Recent years have witnessed an increasing number of artificial intelligence (AI) applications in transportation. As a new and emerging technology, AI's potential to advance transportation goals and the full extent of its impacts on the transportation sector is not yet well understood. As the transportation community explores these topics, it is critical to understand how transportation professionals, the driving force behind AI Transportation applications, perceive AI's potential efficiency and equity impacts. Toward this goal, we surveyed transportation professionals in the United States and collected a total of 354 responses. Based on the survey responses, we conducted both descriptive analysis and latent class cluster analysis (LCCA). The former provides an overview of prevalent attitudes among transportation professionals, while the latter allows the identification of distinct segments based on their latent attitudes toward AI. We find widespread optimism regarding AI's potential to improve many aspects of transportation (e.g., efficiency, cost reduction, and traveler experience); however, responses are mixed regarding AI's potential to advance equity. Moreover, many respondents are concerned that AI ethics are not well understood in the transportation community and that AI use in transportation could exaggerate existing inequalities. Through LCCA, we have identified four latent segments: AI Neutral, AI Optimist, AI Pessimist, and AI Skeptic. The latent class membership is significantly associated with respondents' age, education level, and AI knowledge level. Overall, the study results shed light on the extent to which the transportation community as a whole is ready to leverage AI systems to transform current practices and inform targeted education to improve the understanding of AI among transportation professionals. ",
        "title": "How do transportation professionals perceive the impacts of AI  applications in transportation? A latent class cluster analysis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08916",
        "abstract_url": "http://arxiv.org/abs/2401.08916",
        "authors": [
            {
                "last_name": "Raju",
                "first_name": "Anirudh"
            },
            {
                "last_name": "Khare",
                "first_name": "Aparna"
            },
            {
                "last_name": "He",
                "first_name": "Di"
            },
            {
                "last_name": "Sklyar",
                "first_name": "Ilya"
            },
            {
                "last_name": "Chen",
                "first_name": "Long"
            },
            {
                "last_name": "Alptekin",
                "first_name": "Sam"
            },
            {
                "last_name": "Trinh",
                "first_name": "Viet Anh"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Vaz",
                "first_name": "Colin"
            },
            {
                "last_name": "Ravichandran",
                "first_name": "Venkatesh"
            },
            {
                "last_name": "Maas",
                "first_name": "Roland"
            },
            {
                "last_name": "Rastrow",
                "first_name": "Ariya"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Endpoint (EP) detection is a key component of far-field speech recognition systems that assist the user through voice commands. The endpoint detector has to trade-off between accuracy and latency, since waiting longer reduces the cases of users being cut-off early. We propose a novel two-pass solution for endpointing, where the utterance endpoint detected from a first pass endpointer is verified by a 2nd-pass model termed EP Arbitrator. Our method improves the trade-off between early cut-offs and latency over a baseline endpointer, as tested on datasets including voice-assistant transactional queries, conversational speech, and the public SLURP corpus. We demonstrate that our method shows improvements regardless of the first-pass EP model used. ",
        "title": "Two-pass Endpoint Detection for Speech Recognition",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08919",
        "abstract_url": "http://arxiv.org/abs/2401.08919",
        "authors": [
            {
                "last_name": "ElNokrashy",
                "first_name": "Muhammad"
            },
            {
                "last_name": "AlKhamissi",
                "first_name": "Badr"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial diacritization quality (SR, PDER, HDER, ERE), essential for establishing this as a machine learning task. Lastly, we introduce TD2, a Transformer-variant of an established model which offers a markedly different per formance profile on our proposed indicators compared to all other known systems. ",
        "title": "Partial Diacritization: A Context-Contrastive Inference Approach",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08920",
        "abstract_url": "http://arxiv.org/abs/2401.08920",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Tongda"
            },
            {
                "last_name": "Zhu",
                "first_name": "Ziran"
            },
            {
                "last_name": "He",
                "first_name": "Dailan"
            },
            {
                "last_name": "Li",
                "first_name": "Yanghao"
            },
            {
                "last_name": "Guo",
                "first_name": "Lina"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuanyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Qin",
                "first_name": "Hongwei"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Liu",
                "first_name": "Jingjing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ya-Qin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Idempotence is the stability of image codec to re-compression. At the first glance, it is unrelated to perceptual image compression. However, we find that theoretically: 1) Conditional generative model-based perceptual codec satisfies idempotence; 2) Unconditional generative model with idempotence constraint is equivalent to conditional generative codec. Based on this newfound equivalence, we propose a new paradigm of perceptual image codec by inverting unconditional generative model with idempotence constraints. Our codec is theoretically equivalent to conditional generative codec, and it does not require training new models. Instead, it only requires a pre-trained mean-square-error codec and unconditional generative model. Empirically, we show that our proposed approach outperforms state-of-the-art methods such as HiFiC and ILLM, in terms of Fr\\'echet Inception Distance (FID). The source code is provided in https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression. ",
        "title": "Idempotence and Perceptual Image Compression",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08921",
        "abstract_url": "http://arxiv.org/abs/2401.08921",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Cheng-Xiang"
            },
            {
                "last_name": "Yang",
                "first_name": "Yue"
            },
            {
                "last_name": "Huang",
                "first_name": "Jie"
            },
            {
                "last_name": "Gao",
                "first_name": "Xiqi"
            },
            {
                "last_name": "Cui",
                "first_name": "Tie Jun"
            },
            {
                "last_name": "Hanzo",
                "first_name": "Lajos"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In wireless communications, electromagnetic theory and information theory constitute a pair of fundamental theories, bridged by antenna theory and wireless propagation channel modeling theory. Up to the fifth generation (5G) wireless communication networks, these four theories have been developing relatively independently. However, in sixth generation (6G) space-air-ground-sea wireless communication networks, seamless coverage is expected in the three-dimensional (3D) space, potentially necessitating the acquisition of channel state information (CSI) and channel capacity calculation at anywhere and any time. Additionally, the key 6G technologies such as ultra-massive multiple-input multiple-output (MIMO) and holographic MIMO achieves intricate interaction of the antennas and wireless propagation environments, which necessitates the joint modeling of antennas and wireless propagation channels. To address the challenges in 6G, the integration of the above four theories becomes inevitable, leading to the concept of the so-called electromagnetic information theory (EIT). In this article, a suite of 6G key technologies is highlighted. Then, the concepts and relationships of the four theories are unveiled. Finally, the necessity and benefits of integrating them into the EIT are revealed. ",
        "title": "Electromagnetic Information Theory: Fundamentals and Applications for 6G  Wireless Communication Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08922",
        "abstract_url": "http://arxiv.org/abs/2401.08922",
        "authors": [
            {
                "last_name": "Santos",
                "first_name": "Ronnie de Souza"
            },
            {
                "last_name": "Grillo",
                "first_name": "Willian"
            },
            {
                "last_name": "Cabral",
                "first_name": "Djafran"
            },
            {
                "last_name": "de Castro",
                "first_name": "Catarina"
            },
            {
                "last_name": "Albuquerque",
                "first_name": "Nicole"
            },
            {
                "last_name": "Fran\u00e7a",
                "first_name": "Cesar"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Context. Software professionals learned from their experience during the pandemic that most of their work can be done remotely, and now software companies are expected to adopt hybrid work models to avoid the resignation of talented professionals who require more flexibility and work-life balance. However, hybrid work is a spectrum of flexible work arrangements, and currently, there are no well-established hybrid work configurations to be followed in the post-pandemic period. Goal. We investigated how software engineers are experiencing the post-pandemic hybrid work landscape, aiming to understand the factors that influence their choices between remote and in-office work. Method. We explored a large South American company by collecting quantitative and qualitative data from 545 software professionals who are currently navigating diverse hybrid work arrangements tailored to their individual and team requirements. Findings. Our study revealed an array of factors that significantly impact hybrid work within the software industry, including individual preferences, work-life balance, commute time, social interactions, productivity, and more. Team dynamics, project demands, client expectations, and organizational strategies also play an important role in shaping the complex landscape of hybrid work configurations in software engineering. Conclusions. In summary, the success of hybrid work models depends on balancing individual preferences, team dynamics, and organizational strategies. Our study demonstrated that, at present, there is no one-size-fits-all individual approach to hybrid work in the software industry. ",
        "title": "Post-Pandemic Hybrid Work in Software Companies: Findings from an  Industrial Case Study",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08923",
        "abstract_url": "http://arxiv.org/abs/2401.08923",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Jingtian"
            },
            {
                "last_name": "Liao",
                "first_name": "Kun"
            },
            {
                "last_name": "Dinc",
                "first_name": "Niyazi Ulas"
            },
            {
                "last_name": "Gigli",
                "first_name": "Carlo"
            },
            {
                "last_name": "Bai",
                "first_name": "Bijie"
            },
            {
                "last_name": "Gan",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Li",
                "first_name": "Xurong"
            },
            {
                "last_name": "Chen",
                "first_name": "Hanlong"
            },
            {
                "last_name": "Yang",
                "first_name": "Xilin"
            },
            {
                "last_name": "Li",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Isil",
                "first_name": "Cagatay"
            },
            {
                "last_name": "Rahman",
                "first_name": "Md Sadman Sakib"
            },
            {
                "last_name": "Li",
                "first_name": "Jingxi"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaoyong"
            },
            {
                "last_name": "Jarrahi",
                "first_name": "Mona"
            },
            {
                "last_name": "Psaltis",
                "first_name": "Demetri"
            },
            {
                "last_name": "Ozcan",
                "first_name": "Aydogan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Phase imaging is widely used in biomedical imaging, sensing, and material characterization, among other fields. However, direct imaging of phase objects with subwavelength resolution remains a challenge. Here, we demonstrate subwavelength imaging of phase and amplitude objects based on all-optical diffractive encoding and decoding. To resolve subwavelength features of an object, the diffractive imager uses a thin, high-index solid-immersion layer to transmit high-frequency information of the object to a spatially-optimized diffractive encoder, which converts/encodes high-frequency information of the input into low-frequency spatial modes for transmission through air. The subsequent diffractive decoder layers (in air) are jointly designed with the encoder using deep-learning-based optimization, and communicate with the encoder layer to create magnified images of input objects at its output, revealing subwavelength features that would otherwise be washed away due to diffraction limit. We demonstrate that this all-optical collaboration between a diffractive solid-immersion encoder and the following decoder layers in air can resolve subwavelength phase and amplitude features of input objects in a highly compact design. To experimentally demonstrate its proof-of-concept, we used terahertz radiation and developed a fabrication method for creating monolithic multi-layer diffractive processors. Through these monolithically fabricated diffractive encoder-decoder pairs, we demonstrated phase-to-intensity transformations and all-optically reconstructed subwavelength phase features of input objects by directly transforming them into magnified intensity features at the output. This solid-immersion-based diffractive imager, with its compact and cost-effective design, can find wide-ranging applications in bioimaging, endoscopy, sensing and materials characterization. ",
        "title": "Subwavelength Imaging using a Solid-Immersion Diffractive Optical  Processor",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08925",
        "abstract_url": "http://arxiv.org/abs/2401.08925",
        "authors": [
            {
                "last_name": "Monfared",
                "first_name": "Saleh Khalaj"
            },
            {
                "last_name": "Forte",
                "first_name": "Domenic"
            },
            {
                "last_name": "Tajik",
                "first_name": "Shahin"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Physical side-channel attacks can compromise the security of integrated circuits. Most of the physical side-channel attacks (e.g., power or electromagnetic) exploit the dynamic behavior of a chip, typically manifesting as changes in current consumption or voltage fluctuations where algorithmic countermeasures, such as masking, can effectively mitigate the attacks. However, as demonstrated recently, these mitigation techniques are not entirely effective against backscattered side-channel attacks such as impedance analysis. In the case of an impedance attack, an adversary exploits the data-dependent impedance variations of chip power delivery network (PDN) to extract secret information. In this work, we introduce RandOhm, which exploits moving target defense (MTD) strategy based on partial reconfiguration of mainstream FPGAs, to defend against impedance side-channel attacks. We demonstrate that the information leakage through the PDN impedance could be reduced via run-time reconfiguration of the secret-sensitive parts of the circuitry. Hence, by constantly randomizing the placement and routing of the circuit, one can decorrelate the data-dependent computation from the impedance value. To validate our claims, we present a systematic approach equipped with two different partial reconfiguration strategies on implementations of the AES cipher realized on 28-nm FPGAs. We investigate the overhead of our mitigation in terms of delay and performance and provide security analysis by performing non-profiled and profiled impedance analysis attacks against these implementations to demonstrate the resiliency of our approach. ",
        "title": "RandOhm: Mitigating Impedance Side-channel Attacks using Randomized  Circuit Configurations",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08926",
        "abstract_url": "http://arxiv.org/abs/2401.08926",
        "authors": [
            {
                "last_name": "Fan",
                "first_name": "Songlin"
            },
            {
                "last_name": "Guo",
                "first_name": "Zixuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Wei"
            },
            {
                "last_name": "Li",
                "first_name": "Ge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The evolution of compression and enhancement algorithms necessitates an accurate quality assessment for point clouds. Previous works consistently regard point cloud quality assessment (PCQA) as a MOS regression problem and devise a deterministic mapping, ignoring the stochasticity in generating MOS from subjective tests. Besides, the viewpoint switching of 3D point clouds in subjective tests reinforces the judging stochasticity of different subjects compared with traditional images. This work presents the first probabilistic architecture for no-reference PCQA, motivated by the labeling process of existing datasets. The proposed method can model the quality judging stochasticity of subjects through a tailored conditional variational autoencoder (CVAE) and produces multiple intermediate quality ratings. These intermediate ratings simulate the judgments from different subjects and are then integrated into an accurate quality prediction, mimicking the generation process of a ground truth MOS. Specifically, our method incorporates a Prior Module, a Posterior Module, and a Quality Rating Generator, where the former two modules are introduced to model the judging stochasticity in subjective tests, while the latter is developed to generate diverse quality ratings. Extensive experiments indicate that our approach outperforms previous cutting-edge methods by a large margin and exhibits gratifying cross-dataset robustness. ",
        "title": "Uncertainty-aware No-Reference Point Cloud Quality Assessment",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08930",
        "abstract_url": "http://arxiv.org/abs/2401.08930",
        "authors": [
            {
                "last_name": "Ji",
                "first_name": "Haorui"
            },
            {
                "last_name": "Li",
                "first_name": "Hongdong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diffusion models have demonstrated remarkable success in generative modeling. In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel framework designed to address various challenges in 3D human pose analysis through a unified pipeline. Central to PADS are two distinctive strategies: i) learning a task-agnostic pose prior using a diffusion synthesis process to effectively capture the kinematic constraints in human pose data, and ii) unifying multiple pose analysis tasks like estimation, completion, denoising, etc, as instances of inverse problems. The learned pose prior will be treated as a regularization imposing on task-specific constraints, guiding the optimization process through a series of conditional denoising steps. PADS represents the first diffusion-based framework for tackling general 3D human pose analysis within the inverse problem framework. Its performance has been validated on different benchmarks, signaling the adaptability and robustness of this pipeline. ",
        "title": "3D Human Pose Analysis via Diffusion Synthesis",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08932",
        "abstract_url": "http://arxiv.org/abs/2401.08932",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zili"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Wenyuan"
            },
            {
                "last_name": "Chen",
                "first_name": "Keyan"
            },
            {
                "last_name": "Qi",
                "first_name": "Zipeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Chenyang"
            },
            {
                "last_name": "Zou",
                "first_name": "Zhengxia"
            },
            {
                "last_name": "Shi",
                "first_name": "Zhenwei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Detecting clouds and snow in remote sensing images is an essential preprocessing task for remote sensing imagery. Previous works draw inspiration from semantic segmentation models in computer vision, with most research focusing on improving model architectures to enhance detection performance. However, unlike natural images, the complexity of scenes and the diversity of cloud types in remote sensing images result in many inaccurate labels in cloud and snow detection datasets, introducing unnecessary noises into the training and testing processes. By constructing a new dataset and proposing a novel training strategy with the curriculum learning paradigm, we guide the model in reducing overfitting to noisy labels. Additionally, we design a more appropriate model performance evaluation method, that alleviates the performance assessment bias caused by noisy labels. By conducting experiments on models with UNet and Segformer, we have validated the effectiveness of our proposed method. This paper is the first to consider the impact of label noise on the detection of clouds and snow in remote sensing images. ",
        "title": "Learning to detect cloud and snow in remote sensing images from noisy  labels",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08936",
        "abstract_url": "http://arxiv.org/abs/2401.08936",
        "authors": [
            {
                "last_name": "Afshar",
                "first_name": "Aida"
            },
            {
                "last_name": "Li",
                "first_name": "Wenchao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Reinforcement learning (RL) offers a capable and intuitive structure for the fundamental sequential decision-making problem. Despite impressive breakthroughs, it can still be difficult to employ RL in practice in many simple applications. In this paper, we try to address this issue by introducing a method for designing the components of the RL environment for a given, user-intended application. We provide an initial formalization for the problem of RL component design, that concentrates on designing a good representation for observation and action space. We propose a method named DeLF: Designing Learning Environments with Foundation Models, that employs large language models to design and codify the user's intended learning scenario. By testing our method on four different learning environments, we demonstrate that DeLF can obtain executable environment codes for the corresponding RL problems. ",
        "title": "DeLF: Designing Learning Environments with Foundation Models",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08937",
        "abstract_url": "http://arxiv.org/abs/2401.08937",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weiyao"
            },
            {
                "last_name": "Gleize",
                "first_name": "Pierre"
            },
            {
                "last_name": "Tang",
                "first_name": "Hao"
            },
            {
                "last_name": "Chen",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Liang",
                "first_name": "Kevin J"
            },
            {
                "last_name": "Feiszli",
                "first_name": "Matt"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose. ",
        "title": "ICON: Incremental CONfidence for Joint Pose and Radiance Field  Optimization",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08939",
        "abstract_url": "http://arxiv.org/abs/2401.08939",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yingbing"
            },
            {
                "last_name": "Cheng",
                "first_name": "Jie"
            },
            {
                "last_name": "Wang",
                "first_name": "Sheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Hongji"
            },
            {
                "last_name": "Mei",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Yan",
                "first_name": "Xiaoyang"
            },
            {
                "last_name": "Tang",
                "first_name": "Mingkai"
            },
            {
                "last_name": "Sun",
                "first_name": "Ge"
            },
            {
                "last_name": "Wen",
                "first_name": "Ya"
            },
            {
                "last_name": "Cai",
                "first_name": "Junwei"
            },
            {
                "last_name": "Xie",
                "first_name": "Xupeng"
            },
            {
                "last_name": "Gan",
                "first_name": "Lu"
            },
            {
                "last_name": "Chao",
                "first_name": "Mandan"
            },
            {
                "last_name": "Xin",
                "first_name": "Ren"
            },
            {
                "last_name": "Liu",
                "first_name": "Ming"
            },
            {
                "last_name": "Jiao",
                "first_name": "Jianhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Kangcheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lujia"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The rapid evolution of autonomous vehicles (AVs) has significantly influenced global transportation systems. In this context, we present ``Snow Lion'', an autonomous shuttle meticulously designed to revolutionize on-campus transportation, offering a safer and more efficient mobility solution for students, faculty, and visitors. The primary objective of this research is to enhance campus mobility by providing a reliable, efficient, and eco-friendly transportation solution that seamlessly integrates with existing infrastructure and meets the diverse needs of a university setting. To achieve this goal, we delve into the intricacies of the system design, encompassing sensing, perception, localization, planning, and control aspects. We evaluate the autonomous shuttle's performance in real-world scenarios, involving a 1146-kilometer road haul and the transportation of 442 passengers over a two-month period. These experiments demonstrate the effectiveness of our system and offer valuable insights into the intricate process of integrating an autonomous vehicle within campus shuttle operations. Furthermore, a thorough analysis of the lessons derived from this experience furnishes a valuable real-world case study, accompanied by recommendations for future research and development in the field of autonomous driving. ",
        "title": "Enhancing Campus Mobility: Achievements and Challenges of Autonomous  Shuttle \"Snow Lion''",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08940",
        "abstract_url": "http://arxiv.org/abs/2401.08940",
        "authors": [
            {
                "last_name": "Aslam",
                "first_name": "Saba"
            },
            {
                "last_name": "Rasool",
                "first_name": "Abdur"
            },
            {
                "last_name": "Wu",
                "first_name": "Hongyan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaoli"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Continual learning, the ability of a model to learn over time without forgetting previous knowledge and, therefore, be adaptive to new data, is paramount in dynamic fields such as disease outbreak prediction. Deep neural networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This study introduces a novel CEL model for continual learning by leveraging domain adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate the catastrophic forgetting phenomenon in a domain incremental setting. The Fisher Information Matrix (FIM) is constructed with EWC to develop a regularization term that penalizes changes to important parameters, namely, the important previous knowledge. CEL's performance is evaluated on three distinct diseases, Influenza, Mpox, and Measles, with different metrics. The high R-squared values during evaluation and reevaluation outperform the other state-of-the-art models in several contexts, indicating that CEL adapts to incremental data well. CEL's robustness and reliability are underscored by its minimal 65% forgetting rate and 18% higher memory stability compared to existing benchmark studies. This study highlights CEL's versatility in disease outbreak prediction, addressing evolving data with temporal patterns. It offers a valuable model for proactive disease control with accurate, timely predictions. ",
        "title": "CEL: A Continual Learning Model for Disease Outbreak Prediction by  Leveraging Domain Adaptation via Elastic Weight Consolidation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08943",
        "abstract_url": "http://arxiv.org/abs/2401.08943",
        "authors": [
            {
                "last_name": "Xun",
                "first_name": "Lei"
            },
            {
                "last_name": "Hu",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hengrui"
            },
            {
                "last_name": "Singh",
                "first_name": "Amit Kumar"
            },
            {
                "last_name": "Hare",
                "first_name": "Jonathon"
            },
            {
                "last_name": "Merrett",
                "first_name": "Geoff V."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Distributed inference is a popular approach for efficient DNN inference at the edge. However, traditional Static and Dynamic DNNs are not distribution-friendly, causing system reliability and adaptability issues. In this paper, we introduce Fluid Dynamic DNNs (Fluid DyDNNs), tailored for distributed inference. Distinct from Static and Dynamic DNNs, Fluid DyDNNs utilize a novel nested incremental training algorithm to enable independent and combined operation of its sub-networks, enhancing system reliability and adaptability. Evaluation on embedded Arm CPUs with a DNN model and the MNIST dataset, shows that in scenarios of single device failure, Fluid DyDNNs ensure continued inference, whereas Static and Dynamic DNNs fail. When devices are fully operational, Fluid DyDNNs can operate in either a High-Accuracy mode and achieve comparable accuracy with Static DNNs, or in a High-Throughput mode and achieve 2.5x and 2x throughput compared with Static and Dynamic DNNs, respectively. ",
        "title": "Fluid Dynamic DNNs for Reliable and Adaptive Distributed Inference on  Edge Devices",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08947",
        "abstract_url": "http://arxiv.org/abs/2401.08947",
        "authors": [
            {
                "last_name": "Aslam",
                "first_name": "Saba"
            },
            {
                "last_name": "Aslam",
                "first_name": "Hafsa"
            },
            {
                "last_name": "Manzoor",
                "first_name": "Arslan"
            },
            {
                "last_name": "Hui",
                "first_name": "Chen"
            },
            {
                "last_name": "Rasool",
                "first_name": "Abdur"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  The escalating reliance on revolutionary online web services has introduced heightened security risks, with persistent challenges posed by phishing despite extensive security measures. Traditional phishing systems, reliant on machine learning and manual features, struggle with evolving tactics. Recent advances in deep learning offer promising avenues for tackling novel phishing challenges and malicious URLs. This paper introduces a two-phase stack generalized model named AntiPhishStack, designed to detect phishing sites. The model leverages the learning of URLs and character-level TF-IDF features symmetrically, enhancing its ability to combat emerging phishing threats. In Phase I, features are trained on a base machine learning classifier, employing K-fold cross-validation for robust mean prediction. Phase II employs a two-layered stacked-based LSTM network with five adaptive optimizers for dynamic compilation, ensuring premier prediction on these features. Additionally, the symmetrical predictions from both phases are optimized and integrated to train a meta-XGBoost classifier, contributing to a final robust prediction. The significance of this work lies in advancing phishing detection with AntiPhishStack, operating without prior phishing-specific feature knowledge. Experimental validation on two benchmark datasets, comprising benign and phishing or malicious URLs, demonstrates the model's exceptional performance, achieving a notable 96.04% accuracy compared to existing studies. This research adds value to the ongoing discourse on symmetry and asymmetry in information security and provides a forward-thinking solution for enhancing network security in the face of evolving cyber threats. ",
        "title": "AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized  Phishing URLs Detection",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08948",
        "abstract_url": "http://arxiv.org/abs/2401.08948",
        "authors": [
            {
                "last_name": "Natarajan",
                "first_name": "Ramkumar"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Shohin"
            },
            {
                "last_name": "Choset",
                "first_name": "Howie"
            },
            {
                "last_name": "Likhachev",
                "first_name": "Maxim"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Trajectory optimization is a widely used technique in robot motion planning for letting the dynamics and constraints on the system shape and synthesize complex behaviors. Several previous works have shown its benefits in high-dimensional continuous state spaces and under differential constraints. However, long time horizons and planning around obstacles in non-convex spaces pose challenges in guaranteeing convergence or finding optimal solutions. As a result, discrete graph search planners and sampling-based planers are preferred when facing obstacle-cluttered environments. A recently developed algorithm called INSAT effectively combines graph search in the low-dimensional subspace and trajectory optimization in the full-dimensional space for global kinodynamic planning over long horizons. Although INSAT successfully reasoned about and solved complex planning problems, the numerous expensive calls to an optimizer resulted in large planning times, thereby limiting its practical use. Inspired by the recent work on edge-based parallel graph search, we present PINSAT, which introduces systematic parallelization in INSAT to achieve lower planning times and higher success rates, while maintaining significantly lower costs over relevant baselines. We demonstrate PINSAT by evaluating it on 6 DoF kinodynamic manipulation planning with obstacles. ",
        "title": "PINSAT: Parallelized Interleaving of Graph Search and Trajectory  Optimization for Kinodynamic Motion Planning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08953",
        "abstract_url": "http://arxiv.org/abs/2401.08953",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Tariqul"
            },
            {
                "last_name": "Bappy",
                "first_name": "Faisal Haque"
            },
            {
                "last_name": "Shifat",
                "first_name": "Md Nafis Ul Haque"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Farhan"
            },
            {
                "last_name": "Hasan",
                "first_name": "Kamrul"
            },
            {
                "last_name": "Zaman",
                "first_name": "Tarannum Shaila"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "DS"
        ],
        "abstract": "  An efficient, scalable, and provably secure dynamic auditing scheme is highly desirable in the cloud storage environment for verifying the integrity of the outsourced data. Most of the existing work on remote integrity checking focuses on static archival data and therefore cannot be applied to cases where dynamic data updates are more common. Additionally, existing auditing schemes suffer from performance bottlenecks and scalability issues. To address these issues, in this paper, we present a novel dynamic auditing scheme for centralized cloud environments leveraging an enhanced version of the B-tree. Our proposed scheme achieves the immutable characteristic of a decentralized system (i.e., blockchain technology) while effectively addressing the synchronization and performance challenges of such systems. Unlike other static auditing schemes, our scheme supports dynamic insert, update, and delete operations. Also, by leveraging an enhanced B-tree, our scheme maintains a balanced tree after any alteration to a certain file, improving performance significantly. Experimental results show that our scheme outperforms both traditional Merkle Hash Tree-based centralized auditing and decentralized blockchain-based auditing schemes in terms of block modifications (e.g., insert, delete, update), block retrieval, and data verification time. ",
        "title": "An Efficient and Scalable Auditing Scheme for Cloud Data Storage using  an Enhanced B-tree",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08954",
        "abstract_url": "http://arxiv.org/abs/2401.08954",
        "authors": [
            {
                "last_name": "Christlieb",
                "first_name": "Andrew J."
            },
            {
                "last_name": "Sands",
                "first_name": "William A."
            },
            {
                "last_name": "White",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In a previous paper, we developed a new particle-in-cell method for the Vlasov-Maxwell system in which the electromagnetic fields and the equations of motion for the particles were cast in terms of scalar and vector potentials through a Hamiltonian formulation. This paper extends this new class of methods by focusing on the enforcement the Lorenz gauge condition in both exact and approximate forms using co-located meshes. A time-consistency property of the proposed field solver for the vector potential form of Maxwell's equations is established, which is shown to preserve the equivalence between the semi-discrete Lorenz gauge condition and the analogous semi-discrete continuity equation. Using this property, we present three methods to enforce a semi-discrete gauge condition. The first method introduces an update for the continuity equation that is consistent with the discretization of the Lorenz gauge condition. The second approach we propose enforces a semi-discrete continuity equation using the boundary integral solution to the field equations. The third approach introduces a gauge correcting method that makes direct use of the gauge condition to modify the scalar potential and uses local maps for both the charge and current densities. The vector potential coming from the current density is taken to be exact, and using the Lorenz gauge, we compute a correction to the scalar potential that makes the two potentials satisfy the gauge condition. We demonstrate two of the proposed methods in the context of periodic domains. Problems defined on bounded domains, including those with complex geometric features remain an ongoing effort. However, this work shows that it is possible to design computationally efficient methods that can effectively enforce the Lorenz gauge condition in an non-staggered PIC formulation. ",
        "title": "A Particle-in-cell Method for Plasmas with a Generalized Momentum  Formulation, Part II: Enforcing the Lorenz Gauge Condition",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08956",
        "abstract_url": "http://arxiv.org/abs/2401.08956",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xuyang"
            },
            {
                "last_name": "Yue",
                "first_name": "Xinwei"
            },
            {
                "last_name": "Li",
                "first_name": "Tian"
            },
            {
                "last_name": "Han",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yafei"
            },
            {
                "last_name": "Ding",
                "first_name": "Yong"
            },
            {
                "last_name": "Liu",
                "first_name": "Rongke"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper investigates the application of a unified non-orthogonal multiple access framework in beam hopping (U-NOMA-BH) based satellite communication systems. More specifically, the proposed U-NOMA-BH framework can be applied to code-domain NOMA based BH (CD-NOMA-BH) and power-domain NOMA based BH (PD-NOMA-BH) systems. To satisfy dynamic-uneven traffic demands, we formulate the optimization problem to minimize the square of discrete difference by jointly optimizing power allocation, carrier assignment and beam scheduling. The non-convexity of the objective function and the constraint condition is solved through Dinkelbach's transform and variable relaxation. As a further development, the closed-from and asymptotic expressions of outage probability are derived for CD/PD-NOMA-BH systems. Based on approximated results, the diversity orders of a pair of users are obtained in detail. In addition, the system throughput of U-NOMA-BH is discussed in delay-limited transmission mode. Numerical results verify that: i) The gap between traffic requests of CD/PD-NOMA-BH systems appears to be more closely compared with orthogonal multiple access based BH (OMA-BH); ii) The CD-NOMA-BH system is capable of providing the enhanced traffic request and capacity provision; and iii) The outage behaviors of CD/PD-NOMA-BH are better than that of OMA-BH. ",
        "title": "A Unified NOMA Framework in Beam-Hopping Satellite Communication Systems",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08957",
        "abstract_url": "http://arxiv.org/abs/2401.08957",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Kun"
            },
            {
                "last_name": "Liu",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Qiu",
                "first_name": "Di"
            },
            {
                "last_name": "Li",
                "first_name": "Jinming"
            },
            {
                "last_name": "Che",
                "first_name": "Zhengping"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Qiu",
                "first_name": "Qinru"
            },
            {
                "last_name": "Tang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Imitation learning (IL), aiming to learn optimal control policies from expert demonstrations, has been an effective method for robot manipulation tasks. However, previous IL methods either only use expensive expert demonstrations and omit imperfect demonstrations or rely on interacting with the environment and learning from online experiences. In the context of robotic manipulation, we aim to conquer the above two challenges and propose a novel framework named Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from both expert and imperfect demonstrations without interaction with environments. We reveal that the easy-to-get imperfect demonstrations, such as forward and inverse dynamics, significantly enhance the network by learning fruitful information. To the best of our knowledge, we are the first to attempt to integrate imperfect demonstrations into the offline imitation learning setting for robot manipulation tasks. Extensive experiments on the ManiSkill2 benchmark built on the high-fidelity Sapien simulator and real-world robotic manipulation tasks demonstrated that the proposed method can extract better features and improve the success rates for all tasks. Our code will be released upon acceptance of the paper. ",
        "title": "SWBT: Similarity Weighted Behavior Transformer with the Imperfect  Demonstration for Robotic Manipulation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08959",
        "abstract_url": "http://arxiv.org/abs/2401.08959",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Teng"
            },
            {
                "last_name": "Wang",
                "first_name": "Suhang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods. ",
        "title": "Towards Off-Policy Reinforcement Learning for Ranking Policies with  Human Feedback",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08960",
        "abstract_url": "http://arxiv.org/abs/2401.08960",
        "authors": [
            {
                "last_name": "Nepal",
                "first_name": "Subigya"
            },
            {
                "last_name": "Hernandez",
                "first_name": "Javier"
            },
            {
                "last_name": "Massachi",
                "first_name": "Talie"
            },
            {
                "last_name": "Rowan",
                "first_name": "Kael"
            },
            {
                "last_name": "Amores",
                "first_name": "Judith"
            },
            {
                "last_name": "Suh",
                "first_name": "Jina"
            },
            {
                "last_name": "Ramos",
                "first_name": "Gonzalo"
            },
            {
                "last_name": "Houck",
                "first_name": "Brian"
            },
            {
                "last_name": "Iqbal",
                "first_name": "Shamsi T."
            },
            {
                "last_name": "Czerwinski",
                "first_name": "Mary"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  We present a comprehensive, user-centric approach to understand preferences in AI-based productivity agents and develop personalized solutions tailored to users' needs. Utilizing a two-phase method, we first conducted a survey with 363 participants, exploring various aspects of productivity, communication style, agent approach, personality traits, personalization, and privacy. Drawing on the survey insights, we developed a GPT-4 powered personalized productivity agent that utilizes telemetry data gathered via Viva Insights from information workers to provide tailored assistance. We compared its performance with alternative productivity-assistive tools, such as dashboard and narrative, in a study involving 40 participants. Our findings highlight the importance of user-centric design, adaptability, and the balance between personalization and privacy in AI-assisted productivity tools. By building on the insights distilled from our study, we believe that our work can enable and guide future research to further enhance productivity solutions, ultimately leading to optimized efficiency and user experiences for information workers. ",
        "title": "From User Surveys to Telemetry-Driven Agents: Exploring the Potential of  Personalized Productivity Solutions",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08961",
        "abstract_url": "http://arxiv.org/abs/2401.08961",
        "authors": [
            {
                "last_name": "Du",
                "first_name": "Yihan"
            },
            {
                "last_name": "Srikant",
                "first_name": "R."
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This imposes a huge computational challenge due to the combinatorial action space. To tackle this challenge, we delve into the properties of value functions, and design an oracle BestPerm to efficiently find the optimal item list. Equipped with BestPerm, we develop two algorithms CascadingVI and CascadingBPI, which are both computationally-efficient and sample-efficient, and provide near-optimal regret and sample complexity guarantees. Furthermore, we present experiments to show the improved computational and sample efficiencies of our algorithms compared to straightforward adaptations of existing RL algorithms in practice. ",
        "title": "Cascading Reinforcement Learning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08962",
        "abstract_url": "http://arxiv.org/abs/2401.08962",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Hyunju"
            },
            {
                "last_name": "Kim",
                "first_name": "Geon"
            },
            {
                "last_name": "Lee",
                "first_name": "Taehoon"
            },
            {
                "last_name": "Kim",
                "first_name": "Kisoo"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongman"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG",
            "SD"
        ],
        "abstract": "  With the advancement of IoT technology, recognizing user activities with machine learning methods is a promising way to provide various smart services to users. High-quality data with privacy protection is essential for deploying such services in the real world. Data streams from surrounding ambient sensors are well suited to the requirement. Existing ambient sensor datasets only support constrained private spaces and those for public spaces have yet to be explored despite growing interest in research on them. To meet this need, we build a dataset collected from a meeting room equipped with ambient sensors. The dataset, DOO-RE, includes data streams from various ambient sensor types such as Sound and Projector. Each sensor data stream is segmented into activity units and multiple annotators provide activity labels through a cross-validation annotation process to improve annotation quality. We finally obtain 9 types of activities. To our best knowledge, DOO-RE is the first dataset to support the recognition of both single and group activities in a real meeting room with reliable annotations. ",
        "title": "DOO-RE: A dataset of ambient sensors in a meeting room for activity  recognition",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08964",
        "abstract_url": "http://arxiv.org/abs/2401.08964",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Yixin"
            },
            {
                "last_name": "Lyons",
                "first_name": "Kayley"
            },
            {
                "last_name": "Chen",
                "first_name": "Guanliang"
            },
            {
                "last_name": "Gasevic",
                "first_name": "Dragan"
            },
            {
                "last_name": "Swiecki",
                "first_name": "Zachari"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  We propose a learning analytics-based methodology for assessing the collaborative writing of humans and generative artificial intelligence. Framed by the evidence-centered design, we used elements of knowledge-telling, knowledge transformation, and cognitive presence to identify assessment claims; we used data collected from the CoAuthor writing tool as potential evidence for these claims; and we used epistemic network analysis to make inferences from the data about the claims. Our findings revealed significant differences in the writing processes of different groups of CoAuthor users, suggesting that our method is a plausible approach to assessing human-AI collaborative writing. ",
        "title": "Evidence-centered Assessment for Writing with Generative AI",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08965",
        "abstract_url": "http://arxiv.org/abs/2401.08965",
        "authors": [
            {
                "last_name": "Xun",
                "first_name": "Lei"
            },
            {
                "last_name": "Hare",
                "first_name": "Jonathon"
            },
            {
                "last_name": "Merrett",
                "first_name": "Geoff V."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep neural network (DNN) inference is increasingly being executed on mobile and embedded platforms due to several key advantages in latency, privacy and always-on availability. However, due to limited computing resources, efficient DNN deployment on mobile and embedded platforms is challenging. Although many hardware accelerators and static model compression methods were proposed by previous works, at system runtime, multiple applications are typically executed concurrently and compete for hardware resources. This raises two main challenges: Runtime Hardware Availability and Runtime Application Variability. Previous works have addressed these challenges through either dynamic neural networks that contain sub-networks with different performance trade-offs or runtime hardware resource management. In this thesis, we proposed a combined method, a system was developed for DNN performance trade-off management, combining the runtime trade-off opportunities in both algorithms and hardware to meet dynamically changing application performance targets and hardware constraints in real time. We co-designed novel Dynamic Super-Networks to maximise runtime system-level performance and energy efficiency on heterogeneous hardware platforms. Compared with SOTA, our experimental results using ImageNet on the GPU of Jetson Xavier NX show our model is 2.4x faster for similar ImageNet Top-1 accuracy, or 5.1% higher accuracy at similar latency. We also designed a hierarchical runtime resource manager that tunes both dynamic neural networks and DVFS at runtime. Compared with the Linux DVFS governor schedutil, our runtime approach achieves up to a 19% energy reduction and a 9% latency reduction in single model deployment scenario, and an 89% energy reduction and a 23% latency reduction in a two concurrent model deployment scenario. ",
        "title": "Dynamic DNNs and Runtime Management for Efficient Inference on  Mobile/Embedded Devices",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08967",
        "abstract_url": "http://arxiv.org/abs/2401.08967",
        "authors": [
            {
                "last_name": "Luong",
                "first_name": "Trung Quoc"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xinbo"
            },
            {
                "last_name": "Jie",
                "first_name": "Zhanming"
            },
            {
                "last_name": "Sun",
                "first_name": "Peng"
            },
            {
                "last_name": "Jin",
                "first_name": "Xiaoran"
            },
            {
                "last_name": "Li",
                "first_name": "Hang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT. ",
        "title": "ReFT: Reasoning with Reinforced Fine-Tuning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08968",
        "abstract_url": "http://arxiv.org/abs/2401.08968",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Xiaotian"
            },
            {
                "last_name": "Wang",
                "first_name": "Yiqi"
            },
            {
                "last_name": "Zhai",
                "first_name": "Bohan"
            },
            {
                "last_name": "You",
                "first_name": "Quanzeng"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Multi-modal Large Language Models (MLLMs) are increasingly prominent in the field of artificial intelligence. Visual instruction fine-tuning (IFT) is a vital process for aligning MLLMs' output with user's intentions. High-quality and diversified instruction following data is the key to this fine-tuning process. Recent studies propose to construct visual IFT datasets through a multifaceted approach: transforming existing datasets with rule-based templates, employing GPT-4 for rewriting annotations, and utilizing GPT-4V for visual dataset pseudo-labeling. LLaVA-1.5 adopted similar approach and construct LLaVA-mix-665k, which is one of the simplest, most widely used, yet most effective IFT datasets today. Notably, when properly fine-tuned with this dataset, MLLMs can achieve state-of-the-art performance on several benchmarks. However, we noticed that models trained with this dataset often struggle to follow user instructions properly in multi-round dialog. In addition, tradition caption and VQA evaluation benchmarks, with their closed-form evaluation structure, are not fully equipped to assess the capabilities of modern open-ended generative MLLMs. This problem is not unique to the LLaVA-mix-665k dataset, but may be a potential issue in all IFT datasets constructed from image captioning or VQA sources, though the extent of this issue may vary. We argue that datasets with diverse and high-quality detailed instruction following annotations are essential and adequate for MLLMs IFT. In this work, we establish a new IFT dataset, with images sourced from the COCO dataset along with more diverse instructions. Our experiments show that when fine-tuned with out proposed dataset, MLLMs achieve better performance on open-ended evaluation benchmarks in both single-round and multi-round dialog setting. ",
        "title": "COCO is \"ALL'' You Need for Visual Instruction Fine-tuning",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08972",
        "abstract_url": "http://arxiv.org/abs/2401.08972",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Yufeng"
            },
            {
                "last_name": "Ananthabhotla",
                "first_name": "Ishwarya"
            },
            {
                "last_name": "Ithapu",
                "first_name": "Vamsi Krishna"
            },
            {
                "last_name": "Petridis",
                "first_name": "Stavros"
            },
            {
                "last_name": "Wu",
                "first_name": "Yu-Hsiang"
            },
            {
                "last_name": "Miller",
                "first_name": "Christi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Individuals with impaired hearing experience difficulty in conversations, especially in noisy environments. This difficulty often manifests as a change in behavior and may be captured via facial expressions, such as the expression of discomfort or fatigue. In this work, we build on this idea and introduce the problem of detecting hearing loss from an individual's facial expressions during a conversation. Building machine learning models that can represent hearing-related facial expression changes is a challenge. In addition, models need to disentangle spurious age-related correlations from hearing-driven expressions. To this end, we propose a self-supervised pre-training strategy tailored for the modeling of expression variations. We also use adversarial representation learning to mitigate the age bias. We evaluate our approach on a large-scale egocentric dataset with real-world conversational scenarios involving subjects with hearing loss and show that our method for hearing loss detection achieves superior performance over baselines. ",
        "title": "Hearing Loss Detection from Facial Expressions in One-on-one  Conversations",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08973",
        "abstract_url": "http://arxiv.org/abs/2401.08973",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Aditya"
            },
            {
                "last_name": "Yoffe",
                "first_name": "Luke"
            },
            {
                "last_name": "H\u00f6llerer",
                "first_name": "Tobias"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  One key challenge in Augmented Reality is the placement of virtual content in natural locations. Most existing automated techniques can only work with a closed-vocabulary, fixed set of objects. In this paper, we introduce and evaluate several methods for automatic object placement using recent advances in open-vocabulary vision-language models. Through a multifaceted evaluation, we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark for automatically evaluating the placement of virtual objects in augmented reality, alleviating the need for costly user studies. Through this, in addition to human evaluations, we find that OCTO+ places objects in a valid region over 70% of the time, outperforming other methods on a range of metrics. ",
        "title": "OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed  Reality",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08974",
        "abstract_url": "http://arxiv.org/abs/2401.08974",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Lipeng"
            },
            {
                "last_name": "Ma",
                "first_name": "Wenyan"
            },
            {
                "last_name": "Xiao",
                "first_name": "Zhenyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Rui"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Movable antenna (MA) has emerged as a promising technology to enhance wireless communication performance by enabling the local movement of antennas at the transmitter (Tx) and/or receiver (Rx) for achieving more favorable channel conditions. As the existing studies on MA-aided wireless communications have mainly considered narrow-band transmission in flat fading channels, we investigate in this paper the MA-aided wideband communications employing orthogonal frequency division multiplexing (OFDM) in frequency-selective fading channels. Under the general multi-tap field-response channel model, the wireless channel variations in both space and frequency are characterized with different positions of the MAs. Unlike the narrow-band transmission where the optimal MA position at the Tx/Rx simply maximizes the single-tap channel amplitude, the MA position in the wideband case needs to balance the amplitudes and phases over multiple channel taps in order to maximize the OFDM transmission rate over multiple frequency subcarriers. First, we derive an upper bound on the OFDM achievable rate in closed form when the size of the Tx/Rx region for antenna movement is arbitrarily large. Next, we develop a parallel greedy ascent (PGA) algorithm to obtain locally optimal solutions to the MAs' positions for OFDM rate maximization subject to finite-size Tx/Rx regions. To reduce computational complexity, a simplified PGA algorithm is also provided to optimize the MAs' positions more efficiently. Simulation results demonstrate that the proposed PGA algorithms can approach the OFDM rate upper bound closely with the increase of Tx/Rx region sizes and outperform conventional systems with fixed-position antennas (FPAs) under the wideband channel setup. ",
        "title": "Performance Analysis and Optimization for Movable Antenna Aided Wideband  Communications",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08976",
        "abstract_url": "http://arxiv.org/abs/2401.08976",
        "authors": [
            {
                "last_name": "Qi",
                "first_name": "Chen"
            },
            {
                "last_name": "Jingjing",
                "first_name": "Yang"
            },
            {
                "last_name": "Ming",
                "first_name": "Huang"
            },
            {
                "last_name": "Qiang",
                "first_name": "Zhou"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The radio map, serving as a visual representation of electromagnetic spatial characteristics, plays a pivotal role in assessment of wireless communication networks and radio monitoring coverage. Addressing the issue of low accuracy existing in the current radio map construction, this paper presents a novel radio map construction method based on generative adversarial network (GAN) in which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied to the generator, and we name it as ACT-GAN. It significantly improves the reconstruction accuracy and local texture of the radio maps. The performance of ACT-GAN across three different scenarios is demonstrated. Experiment results reveal that in the scenario without sparse discrete observations, the proposed method reduces the root mean square error (RMSE) by 14.6% in comparison to the state-of-the-art models. In the scenario with sparse discrete observations, the RMSE is diminished by 13.2%. Furthermore, the predictive results of the proposed model show a more lucid representation of electromagnetic spatial field distribution. To verify the universality of this model in radio map construction tasks, the scenario of unknown radio emission source is investigated. The results indicate that the proposed model is robust radio map construction and accurate in predicting the location of the emission source. ",
        "title": "ACT-GAN: Radio map construction based on generative adversarial networks  with ACT blocks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08977",
        "abstract_url": "http://arxiv.org/abs/2401.08977",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Zikai"
            },
            {
                "last_name": "Chen",
                "first_name": "Zihan"
            },
            {
                "last_name": "Liu",
                "first_name": "Liyinglan"
            },
            {
                "last_name": "Feng",
                "first_name": "Yang"
            },
            {
                "last_name": "Wu",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Wanlu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Joey Tianyi"
            },
            {
                "last_name": "Yang",
                "first_name": "Howard Hao"
            },
            {
                "last_name": "Liu",
                "first_name": "Zuozhu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framework for capturing overarching global trends, while concurrently employing individualized classifiers to encapsulate distinct refinements stemming from each client's local features. Building upon this discovery, we establish the Static Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural collapse principles that naturally prune extraneous noisy features and foster the acquisition of potent data representations. Furthermore, leveraging insights from imbalance neural collapse's classifier norm patterns, we develop Global and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global classifier and personalized Euclidean norm transfer to align global features with client preferences. Extensive experimental results on CIFAR-10/100-LT, ImageNet, and iNaturalist demonstrate the advantage of our method over state-of-the-art pFL and Fed-LT approaches. ",
        "title": "FedLoGe: Joint Local and Generic Federated Learning under Long-tailed  Data",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08981",
        "abstract_url": "http://arxiv.org/abs/2401.08981",
        "authors": [
            {
                "last_name": "Dua",
                "first_name": "Zongliang"
            },
            {
                "last_name": "Ma",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Hao",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Liang",
                "first_name": "Yuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Luo",
                "first_name": "Hongzhi"
            },
            {
                "last_name": "Guo",
                "first_name": "Xu"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Compared with traditional design methods, generative design significantly attracts engineers in various disciplines. In thiswork, howto achieve the real-time generative design of optimized structures with various diversities and controllable structural complexities is investigated. To this end, a modified Moving Morphable Component (MMC) method together with novel strategies are adopted to generate high-quality dataset. The complexity level of optimized structures is categorized by the topological invariant. By improving the cost function, the WGAN is trained to produce optimized designs with the input of loading position and complexity level in real time. It is found that, diverse designs with a clear load transmission path and crisp boundary, even not requiring further optimization and different from any reference in the dataset, can be generated by the proposed model. This method holds great potential for future applications of machine learning enhanced intelligent design. ",
        "title": "Real-time generative design of diverse, \"truly\" optimized structures  with controllable structural complexities",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08982",
        "abstract_url": "http://arxiv.org/abs/2401.08982",
        "authors": [
            {
                "last_name": "Tushar",
                "first_name": "Nahid"
            },
            {
                "last_name": "Wu",
                "first_name": "Rencheng"
            },
            {
                "last_name": "She",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Wenchao"
            },
            {
                "last_name": "Shou",
                "first_name": "Wan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  3D printing has enabled various applications using different forms of materials, such as filaments, sheets, and inks. Typically, during 3D printing, feedstocks are transformed into discrete building blocks and placed or deposited in a designated location similar to the manipulation and assembly of discrete objects. However, 3D printing of continuous and flexible tape (with the geometry between filaments and sheets) without breaking or transformation remains underexplored and challenging. Here, we report the design and implementation of a customized end-effector, i.e., tape print module (TPM), to realize robot tape manipulation for 3D printing by leveraging the tension formed on the tape between two endpoints. We showcase the feasibility of manufacturing representative 2D and 3D structures while utilizing conductive copper tape for various electronic applications, such as circuits and sensors. We believe this manipulation strategy could unlock the potential of other tape materials for manufacturing, including packaging tape and carbon fiber prepreg tape, and inspire new mechanisms for robot manipulation, 3D printing, and packaging. ",
        "title": "Robot Tape Manipulation for 3D Printing",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08984",
        "abstract_url": "http://arxiv.org/abs/2401.08984",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xiaolin"
            },
            {
                "last_name": "Zan",
                "first_name": "Daoguang"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Guan",
                "first_name": "Bei"
            },
            {
                "last_name": "Wang",
                "first_name": "Yongji"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  In vertical federated learning (VFL), commercial entities collaboratively train a model while preserving data privacy. However, a malicious participant's poisoning attack may degrade the performance of this collaborative model. The main challenge in achieving the poisoning attack is the absence of access to the server-side top model, leaving the malicious participant without a clear target model. To address this challenge, we introduce an innovative end-to-end poisoning framework P-GAN. Specifically, the malicious participant initially employs semi-supervised learning to train a surrogate target model. Subsequently, this participant employs a GAN-based method to produce adversarial perturbations to degrade the surrogate target model's performance. Finally, the generator is obtained and tailored for VFL poisoning. Besides, we develop an anomaly detection algorithm based on a deep auto-encoder (DAE), offering a robust defense mechanism to VFL scenarios. Through extensive experiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the factors that influence their performance. ",
        "title": "A GAN-based data poisoning framework against anomaly detection in  vertical federated learning",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08986",
        "abstract_url": "http://arxiv.org/abs/2401.08986",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Huang",
                "first_name": "Wenbing"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current state-of-the-art learning-based models such as DiffDock-PP and Multimer particularly for antibody-antigen docking. ",
        "title": "Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid  Interface Prediction",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08987",
        "abstract_url": "http://arxiv.org/abs/2401.08987",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Neha"
            },
            {
                "last_name": "Saxena",
                "first_name": "Vikas"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Quantum cryptography is the study of delivering secret communications across a quantum channel. Recently, Quantum Key Distribution (QKD) has been recognized as the most important breakthrough in quantum cryptography. This process facilitates two distant parties to share secure communications based on physical laws. The BB84 protocol was developed in 1984 and remains the most widely used among BB92, Ekert91, COW, and SARG04 protocols. However the practical security of QKD with imperfect devices have been widely discussed, and there are many ways to guarantee that generated key by QKD still provides unconditional security. This paper proposed a novel method that allows users to communicate while generating the secure keys as well as securing the transmission without any leakage of the data. In this approach sender will never reveal her basis, hence neither the receiver nor the intruder will get knowledge of the fundamental basis.Further to detect Eve, polynomial interpolation is also used as a key verification technique. In order to fully utilize the quantum computing capabilities provided by IBM quantum computers, the protocol is executed using the Qiskit backend for 45 qubits. This article discusses a plot of % error against alpha (strength of eavesdropping). As a result, different types of noise have been included, and the success probability of the desired key bits has been determined. Furthermore, the success probability under depolarizing noise is explained for different qubit counts.Last but not least, even when the applied noise is increased to maximum capacity, a 50% probability of successful key generation is still observed in an experiment. ",
        "title": "The Quantum Cryptography Approach: Unleashing the Potential of Quantum  Key Reconciliation Protocol for Secure Communication",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08988",
        "abstract_url": "http://arxiv.org/abs/2401.08988",
        "authors": [
            {
                "last_name": "Srivastava",
                "first_name": "Varul"
            },
            {
                "last_name": "Gujar",
                "first_name": "Sujit"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Proof-of-Work is a consensus algorithm where miners solve cryptographic puzzles to mine blocks and obtain a reward through some Block Reward Mechanism (BRM). PoW blockchain faces the problem of centralization due to the formation of mining pools, where miners mine blocks as a group and distribute rewards. The rationale is to reduce the risk (variance) in reward while obtaining the same expected block reward. In this work, we address the problem of centralization due to mining pools in PoW blockchain. We propose a two-player game between the new miner joining the system and the PoW blockchain system.   We model the utility for the incoming miner as a combination of (i) expected block reward, (ii) risk, and (iii) cost of switching between different mining pools. With this utility structure, we analyze the equilibrium strategy of the incoming miner for different BRMs: (a) memoryless -- block reward is history independent (e.g., Bitcoin) (b) retentive: block reward is history-dependent (e.g., Fruitchains). For memoryless BRMs, we show that depending on the coefficient of switching cost $c$, the protocol is decentralized when $c = 0$ and centralized when $c > \\underline{c}$. In addition, we show the impossibility of constructing a memoryless BRM where solo mining gives a higher payoff than forming/joining mining pools. While retentive BRM in Fruitchains reduces risk in solo mining, the equilibrium strategy for incoming miners is still to join mining pools, leading to centralization. We then propose our novel retentive BRM -- \\textsf{Decent-BRM}. We show that under \\textsf{Decent-BRM}, incoming miners obtain higher utility in solo mining than joining mining pools. Therefore, no mining pools are formed, and the Pow blockchain using \\textsf{Decent-BRM} is decentralized. ",
        "title": "DECENT-BRM: Decentralization through Block Reward Mechanisms",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08991",
        "abstract_url": "http://arxiv.org/abs/2401.08991",
        "authors": [
            {
                "last_name": "Ajemian",
                "first_name": "Andrew"
            },
            {
                "last_name": "Knight",
                "first_name": "John"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Tommy"
            },
            {
                "last_name": "O'Connor",
                "first_name": "John"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This project introduces a wearable, non-intrusive device for snoring detection and remediation, designed to be placed under or alongside a pillow. The device uses sensors and machine learning algorithms to detect snoring and employs gentle vibrations to prompt positional changes, thereby reducing snoring episodes. The device is capable of connecting via an API to a cloud-based platform for the analysis of snoring sleep patterns and environmental context. The paper details the development from concept to prototype, emphasizing the technical challenges, solutions, and alignment with ubiquitous computing in sleep quality improvement. ",
        "title": "Knight Watch: Ubiquitous Computing Enhancements To Sleep Quality With  Acoustic Analysis",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08992",
        "abstract_url": "http://arxiv.org/abs/2401.08992",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Junwen"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Li",
                "first_name": "Qiujia"
            },
            {
                "last_name": "Sainath",
                "first_name": "Tara N."
            },
            {
                "last_name": "Strohman",
                "first_name": "Trevor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation model and is the only trainable module during the finetuning process with noisy student training. The final model merges the adapter parameters from different checkpoints for different languages. The model performance is validated on a challenging multilingual dictation dataset, which includes 39 tail languages across Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error rate reduction on average and up to 37.5% on a single locale. Furthermore, we show that our parameter-efficient LDA can match the quality of the full model finetuning, thus greatly alleviating the asynchronous peak performance issue. ",
        "title": "Efficient Adapter Finetuning for Tail Languages in Streaming  Multilingual ASR",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08993",
        "abstract_url": "http://arxiv.org/abs/2401.08993",
        "authors": [
            {
                "last_name": "Patel",
                "first_name": "Hrishikesh"
            },
            {
                "last_name": "Chen",
                "first_name": "Tianwa"
            },
            {
                "last_name": "Bongiovanni",
                "first_name": "Ivano"
            },
            {
                "last_name": "Demartini",
                "first_name": "Gianluca"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "IR"
        ],
        "abstract": "  Gender imbalance in Wikipedia content is a known challenge which the editor community is actively addressing. The aim of this paper is to provide the Wikipedia community with instruments to estimate the magnitude of the problem for different entity types (also known as classes) in Wikipedia. To this end, we apply class completeness estimation methods based on the gender attribute. Our results show not only which gender for different sub-classes of Person is more prevalent in Wikipedia, but also an idea of how complete the coverage is for difference genders and sub-classes of Person. ",
        "title": "Estimating Gender Completeness in Wikipedia",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08994",
        "abstract_url": "http://arxiv.org/abs/2401.08994",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiaying"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Prior research on young adults' mental health help-seeking mostly focuses on one particular resource such as a mobile app or digital platform, paying less attention to their lived experiences interacting with the ecosystem of resources. We conducted in-depth interviews with 18 participants about their help-seeking and non-help-seeking experiences. Guided by Social Ecological Theory, we proposed a Socio-technical Ecosystem Framework for mental health care, consisting of four levels of resources, including technological-, interpersonal-, community-, and societal level resources. Using this framework, we identified two types of support systems for help-seeking, single-resource support system and multi-resource support system. These resources support young adults' help-seeking via three mechanisms, \\textit{care-giving}, \\textit{care-mediating}, and \\textit{care-outreaching}, forming various pathways to care. We then pointed out the barriers to resource use at each level and the general challenges in finding a support system. Our findings contributed to a conceptual framework to categorize mental health care. It also serves as a practical framework to identify challenges in the pathways to care and discover design implications. ",
        "title": "Understanding and Facilitating Mental Health Help-Seeking of Young  Adults: A Socio-technical Ecosystem Framework",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08996",
        "abstract_url": "http://arxiv.org/abs/2401.08996",
        "authors": [
            {
                "last_name": "Qiao",
                "first_name": "Ye"
            },
            {
                "last_name": "Xu",
                "first_name": "Haocheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yifan"
            },
            {
                "last_name": "Huang",
                "first_name": "Sitao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural Architecture Search (NAS) effectively discovers new Convolutional Neural Network (CNN) architectures, particularly for accuracy optimization. However, prior approaches often require resource-intensive training on super networks or extensive architecture evaluations, limiting practical applications. To address these challenges, we propose MicroNAS, a hardware-aware zero-shot NAS framework designed for microcontroller units (MCUs) in edge computing. MicroNAS considers target hardware optimality during the search, utilizing specialized performance indicators to identify optimal neural architectures without high computational costs. Compared to previous works, MicroNAS achieves up to 1104x improvement in search efficiency and discovers models with over 3.23x faster MCU inference while maintaining similar accuracy ",
        "title": "MicroNAS: Zero-Shot Neural Architecture Search for MCUs",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08998",
        "abstract_url": "http://arxiv.org/abs/2401.08998",
        "authors": [
            {
                "last_name": "Jung",
                "first_name": "Yoonhwa"
            },
            {
                "last_name": "Cho",
                "first_name": "Ikhyun"
            },
            {
                "last_name": "Hsu",
                "first_name": "Shun-Hsiang"
            },
            {
                "last_name": "Hockenmaier",
                "first_name": "Julia"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "CV"
        ],
        "abstract": "  With growing concerns surrounding privacy and regulatory compliance, the concept of machine unlearning has gained prominence, aiming to selectively forget or erase specific learned information from a trained model. In response to this critical need, we introduce a novel approach called Attack-and-Reset for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial noise to generate a parameter mask, effectively resetting certain parameters and rendering them unlearnable. ARU outperforms current state-of-the-art results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC. In particular, we present the steps involved in attacking and masking that strategically filter and re-initialize network parameters biased towards the forget set. Our work represents a significant advancement in rendering data unexploitable to deep learning models through parameter re-initialization, achieved by harnessing adversarial noise to craft a mask. ",
        "title": "Attack and Reset for Unlearning: Exploiting Adversarial Noise toward  Machine Unlearning through Parameter Re-initialization",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08999",
        "abstract_url": "http://arxiv.org/abs/2401.08999",
        "authors": [
            {
                "last_name": "Laurencon",
                "first_name": "Hugo"
            },
            {
                "last_name": "Bhargava",
                "first_name": "Yesoda"
            },
            {
                "last_name": "Zantye",
                "first_name": "Riddhi"
            },
            {
                "last_name": "S\u00e9gerie",
                "first_name": "Charbel-Rapha\u00ebl"
            },
            {
                "last_name": "Lussange",
                "first_name": "Johann"
            },
            {
                "last_name": "Baths",
                "first_name": "Veeky"
            },
            {
                "last_name": "Gutkin",
                "first_name": "Boris"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Homeostasis is a biological process by which living beings maintain their internal balance. Previous research suggests that homeostasis is a learned behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning (HRRL) framework attempts to explain this learned homeostatic behavior by linking Drive Reduction Theory and Reinforcement Learning. This linkage has been proven in the discrete time-space, but not in the continuous time-space. In this work, we advance the HRRL framework to a continuous time-space environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL) framework. We achieve this by designing a model that mimics the homeostatic mechanisms in a real-world biological agent. This model uses the Hamilton-Jacobian Bellman Equation, and function approximation based on neural networks and Reinforcement Learning. Through a simulation-based experiment we demonstrate the efficacy of this model and uncover the evidence linked to the agent's ability to dynamically choose policies that favor homeostasis in a continuously changing internal-state milieu. Results of our experiments demonstrate that agent learns homeostatic behaviour in a CTCS environment, making CTCS-HRRL a promising framework for modellng animal dynamics and decision-making. ",
        "title": "Continuous Time Continuous Space Homeostatic Reinforcement Learning  (CTCS-HRRL) : Towards Biological Self-Autonomous Agent",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09001",
        "abstract_url": "http://arxiv.org/abs/2401.09001",
        "authors": [
            {
                "last_name": "Gunatilake",
                "first_name": "Hashini"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            },
            {
                "last_name": "Hoda",
                "first_name": "Rashina"
            },
            {
                "last_name": "Mueller",
                "first_name": "Ingo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software engineering (SE) requires developers to collaborate with stakeholders, and understanding their emotions and perspectives is often vital. Empathy is a concept characterising a person's ability to understand and share the feelings of another. However, empathy continues to be an under-researched human aspect in SE. We studied how empathy is practised between developers and end users using a mixed methods case study. We used an empathy test, observations and interviews to collect data, and socio technical grounded theory and descriptive statistics to analyse data. We identified the nature of awareness required to trigger empathy and enablers of empathy. We discovered barriers to empathy and a set of potential strategies to overcome these barriers. We report insights on emerging relationships and present a set of recommendations and potential future works on empathy and SE for software practitioners and SE researchers. ",
        "title": "Enablers and Barriers of Empathy in Software Developer and User  Interaction: A Mixed Methods Case Study",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09002",
        "abstract_url": "http://arxiv.org/abs/2401.09002",
        "authors": [
            {
                "last_name": "shu",
                "first_name": "Dong"
            },
            {
                "last_name": "Jin",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Zhu",
                "first_name": "Suiyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Beichen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zihao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongfeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation aligns with the baseline's trend while offering a more profound and detailed assessment. We believe that by accurately evaluating the effectiveness of attack prompts in the Jailbreak task, our work lays a solid foundation for assessing a wider array of similar or even more complex tasks in the realm of prompt injection, potentially revolutionizing this field. ",
        "title": "AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on  Large Language Models",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09003",
        "abstract_url": "http://arxiv.org/abs/2401.09003",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Haoxiong"
            },
            {
                "last_name": "Yao",
                "first_name": "Andrew Chi-Chih"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\\% accuracy on MATH(arXiv:2103.03874), 5.8\\% higher than the previous (model size $\\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC. ",
        "title": "Augmenting Math Word Problems via Iterative Question Composing",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09006",
        "abstract_url": "http://arxiv.org/abs/2401.09006",
        "authors": [
            {
                "last_name": "Long",
                "first_name": "Xingming"
            },
            {
                "last_name": "Shan",
                "first_name": "Shiguang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous Face Anti-spoofing (FAS) works face the challenge of generalizing in unseen domains. One of the major problems is that most existing FAS datasets are relatively small and lack data diversity. However, we find that there are numerous real faces that can be easily achieved under various conditions, which are neglected by previous FAS works. In this paper, we conduct an Anomalous cue Guided FAS (AG-FAS) method, which leverages real faces for improving model generalization via a De-spoofing Face Generator (DFG). Specifically, the DFG trained only on the real faces gains the knowledge of what a real face should be like and can generate a \"real\" version of the face corresponding to any given input face. The difference between the generated \"real\" face and the input face can provide an anomalous cue for the downstream FAS task. We then propose an Anomalous cue Guided FAS feature extraction Network (AG-Net) to further improve the FAS feature generalization via a cross-attention transformer. Extensive experiments on a total of nine public datasets show our method achieves state-of-the-art results under cross-domain evaluations with unseen scenarios and unknown presentation attacks. ",
        "title": "Generalized Face Liveness Detection via De-spoofing Face Generator",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09008",
        "abstract_url": "http://arxiv.org/abs/2401.09008",
        "authors": [
            {
                "last_name": "Rafif",
                "first_name": "Sulthan"
            },
            {
                "last_name": "Pratama",
                "first_name": "Mochamad Arfan Ravy Wahyu"
            },
            {
                "last_name": "Azhar",
                "first_name": "Mohammad Faris"
            },
            {
                "last_name": "Ibad",
                "first_name": "Ahmad Mustafidul"
            },
            {
                "last_name": "Muflikhah",
                "first_name": "Lailil"
            },
            {
                "last_name": "Yudistira",
                "first_name": "Novanto"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Stride determines the distance between adjacent filter positions as the filter moves across the input. A fixed stride causes important information contained in the image can not be captured, so that important information is not classified. Therefore, in previous research, the DiffStride Method was applied, namely the Strided Convolution Method with which it can learn its own stride value. Severe Quantization and a constraining lower bound on preserved information are arises with Max Pooling Downsampling Method. Spectral Pooling reduce the constraint lower bound on preserved information by cutting off the representation in the frequency domain. In this research a CNN Model is proposed with the Downsampling Learnable Stride Technique performed by Backpropagation combined with the Spectral Pooling Technique. Diffstride and Spectral Pooling techniques are expected to maintain most of the information contained in the image. In this study, we compare the Hybrid Method, which is a combined implementation of Spectral Pooling and DiffStride against the Baseline Method, which is the DiffStride implementation on ResNet 18. The accuracy result of the DiffStride combination with Spectral Pooling improves over DiffStride which is baseline method by 0.0094. This shows that the Hybrid Method can maintain most of the information by cutting of the representation in the frequency domain and determine the stride of the learning result through Backpropagation. ",
        "title": "Hybrid of DiffStride and Spectral Pooling in Convolutional Neural  Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09011",
        "abstract_url": "http://arxiv.org/abs/2401.09011",
        "authors": [
            {
                "last_name": "Habaraduwa",
                "first_name": "Udesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper discusses the limitations of machine learning (ML), particularly deep artificial neural networks (ANNs), which are effective at approximating complex functions but often lack transparency and explanatory power. It highlights the `problem of induction' : the philosophical issue that past observations may not necessarily predict future events, a challenge that ML models face when encountering new, unseen data. The paper argues for the importance of not just making predictions but also providing good explanations, a feature that current models often fail to deliver. It suggests that for AI to progress, we must seek models that offer insights and explanations, not just predictions. ",
        "title": "Inductive Models for Artificial Intelligence Systems are Insufficient  without Good Explanations",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09013",
        "abstract_url": "http://arxiv.org/abs/2401.09013",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongying"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            },
            {
                "last_name": "Li",
                "first_name": "Ruoguang"
            },
            {
                "last_name": "Hou",
                "first_name": "Luyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Lianming"
            },
            {
                "last_name": "Fei",
                "first_name": "Aiguo"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In this paper, we consider an unmanned aerial vehicle (UAV)-enabled emergency communication system, which establishes temporary communication link with users equipment (UEs) in a typical disaster environment with mountainous forest and obstacles. Towards this end, a joint deployment, power allocation, and user association optimization problem is formulated to maximize the total transmission rate, while considering the demand of each UE and the disaster environment characteristics. Then, an alternating optimization algorithm is proposed by integrating coalition game and virtual force approach which captures the impact of the demand priority of UEs and the obstacles to the flight path and consumed power. Simulation results demonstrate that the computation time consumed by our proposed algorithm is only $5.6\\%$ of the traditional heuristic algorithms, which validates its effectiveness in disaster scenarios. ",
        "title": "An Improved Virtual Force Approach for UAV Deployment and Resource  Allocation in Emergency Communications",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09014",
        "abstract_url": "http://arxiv.org/abs/2401.09014",
        "authors": [
            {
                "last_name": "Murata",
                "first_name": "Ryo"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "MA"
        ],
        "abstract": "  Understanding and predicting people flow in urban areas is useful for decision-making in urban planning and marketing strategies. Traditional methods for understanding people flow can be divided into measurement-based approaches and simulation-based approaches. Measurement-based approaches have the advantage of directly capturing actual people flow, but they face the challenge of data imperfection. On the other hand, simulations can obtain complete data on a computer, but they only consider some of the factors determining human behavior, leading to a divergence from actual people flow. Both measurement and simulation methods have unresolved issues, and combining the two can complementarily overcome them. This paper proposes a method that applies data assimilation, a fusion technique of measurement and simulation, to agent-based simulation. Data assimilation combines the advantages of both measurement and simulation, contributing to the creation of an environment that can reflect real people flow while acquiring richer data. The paper verifies the effectiveness of the proposed method in a virtual environment and demonstrates the potential of data assimilation to compensate for the three types of imperfection in people flow measurement techniques. These findings can serve as guidelines for supplementing sparse measurement data in physical environments. ",
        "title": "Data assimilation approach for addressing imperfections in people flow  measurement techniques using particle filter",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09016",
        "abstract_url": "http://arxiv.org/abs/2401.09016",
        "authors": [
            {
                "last_name": "Anari",
                "first_name": "Nima"
            },
            {
                "last_name": "Chewi",
                "first_name": "Sinho"
            },
            {
                "last_name": "Vuong",
                "first_name": "Thuy-Duong"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We show how to sample in parallel from a distribution $\\pi$ over $\\mathbb R^d$ that satisfies a log-Sobolev inequality and has a smooth log-density, by parallelizing the Langevin (resp. underdamped Langevin) algorithms. We show that our algorithm outputs samples from a distribution $\\hat\\pi$ that is close to $\\pi$ in Kullback--Leibler (KL) divergence (resp. total variation (TV) distance), while using only $\\log(d)^{O(1)}$ parallel rounds and $\\widetilde{O}(d)$ (resp. $\\widetilde O(\\sqrt d)$) gradient evaluations in total. This constitutes the first parallel sampling algorithms with TV distance guarantees.   For our main application, we show how to combine the TV distance guarantees of our algorithms with prior works and obtain RNC sampling-to-counting reductions for families of discrete distribution on the hypercube $\\{\\pm 1\\}^n$ that are closed under exponential tilts and have bounded covariance. Consequently, we obtain an RNC sampler for directed Eulerian tours and asymmetric determinantal point processes, resolving open questions raised in prior works. ",
        "title": "Fast parallel sampling under isoperimetry",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09018",
        "abstract_url": "http://arxiv.org/abs/2401.09018",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jianing"
            },
            {
                "last_name": "Papyan",
                "first_name": "Vardan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements reveal a process called Residual Alignment (RA) characterized by four properties:   (RA1) intermediate representations of a given input are equispaced on a line, embedded in high dimensional space, as observed by Gai and Zhang [2021];   (RA2) top left and right singular vectors of Residual Jacobians align with each other and across different depths;   (RA3) Residual Jacobians are at most rank C for fully-connected ResNets, where C is the number of classes; and   (RA4) top singular values of Residual Jacobians scale inversely with depth.   RA consistently occurs in models that generalize well, in both fully-connected and convolutional architectures, across various depths and widths, for varying numbers of classes, on all tested benchmark datasets, but ceases to occur once the skip connections are removed. It also provably occurs in a novel mathematical model we propose. This phenomenon reveals a strong alignment between residual branches of a ResNet (RA2+4), imparting a highly rigid geometric structure to the intermediate representations as they progress linearly through the network (RA1) up to the final layer, where they undergo Neural Collapse. ",
        "title": "Residual Alignment: Uncovering the Mechanisms of Residual Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09019",
        "abstract_url": "http://arxiv.org/abs/2401.09019",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hongruixuan"
            },
            {
                "last_name": "Song",
                "first_name": "Jian"
            },
            {
                "last_name": "Yokoya",
                "first_name": "Naoto"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  Unsupervised multimodal change detection is pivotal for time-sensitive tasks and comprehensive multi-temporal Earth monitoring. In this study, we explore unsupervised multimodal change detection between two key remote sensing data sources: optical high-resolution imagery and OpenStreetMap (OSM) data. Specifically, we propose to utilize the vision foundation model Segmentation Anything Model (SAM), for addressing our task. Leveraging SAM's exceptional zero-shot transfer capability, high-quality segmentation maps of optical images can be obtained. Thus, we can directly compare these two heterogeneous data forms in the so-called segmentation domain. We then introduce two strategies for guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt' methods. The two strategies are designed to detect land-cover changes in general scenarios and to identify new land-cover objects within existing backgrounds, respectively. Experimental results on three datasets indicate that the proposed approach can achieve more competitive results compared to representative unsupervised multimodal change detection methods. ",
        "title": "Change Detection Between Optical Remote Sensing Imagery and Map Data via  Segment Anything Model (SAM)",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09023",
        "abstract_url": "http://arxiv.org/abs/2401.09023",
        "authors": [
            {
                "last_name": "Maity",
                "first_name": "Krishanu"
            },
            {
                "last_name": "Jha",
                "first_name": "Prince"
            },
            {
                "last_name": "Jain",
                "first_name": "Raghav"
            },
            {
                "last_name": "Saha",
                "first_name": "Sriparna"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Pushpak"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Cyberbullying has become a big issue with the popularity of different social media networks and online communication apps. While plenty of research is going on to develop better models for cyberbullying detection in monolingual language, there is very little research on the code-mixed languages and explainability aspect of cyberbullying. Recent laws like \"right to explanations\" of General Data Protection Regulation, have spurred research in developing interpretable models rather than focusing on performance. Motivated by this we develop the first interpretable multi-task model called {\\em mExCB} for automatic cyberbullying detection from code-mixed languages which can simultaneously solve several tasks, cyberbullying detection, explanation/rationale identification, target group detection and sentiment analysis. We have introduced {\\em BullyExplain}, the first benchmark dataset for explainable cyberbullying detection in code-mixed language. Each post in {\\em BullyExplain} dataset is annotated with four labels, i.e., {\\em bully label, sentiment label, target and rationales (explainability)}, i.e., which phrases are being responsible for annotating the post as a bully. The proposed multitask framework (mExCB) based on CNN and GRU with word and sub-sentence (SS) level attention is able to outperform several baselines and state of the art models when applied on {\\em BullyExplain} dataset. ",
        "title": "Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with  Explanation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09025",
        "abstract_url": "http://arxiv.org/abs/2401.09025",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Kyrie Zhixuan"
            },
            {
                "last_name": "Peng",
                "first_name": "Weirui"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Adler",
                "first_name": "Rachel F."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  Sensory substitution or enhancement techniques have been proposed to enable deaf or hard of hearing (DHH) people to listen to and even compose music. However, little is known about how such techniques enhance DHH people's music experience. Since deafness is a spectrum -- as are DHH people's preferences and perceptions of music -- a more situated understanding of their interaction with music is needed. To understand the music experience of this population, we conducted social media analyses, both qualitatively and quantitatively, in the deaf and hard of hearing Reddit communities. Our content analysis revealed that DHH people leveraged sign language and visual/haptic cues to feel the music and preferred familiar, non-lyrical, instrument-heavy, and loud music. In addition, hearing aids were not customized for music, and the visual/haptic techniques developed were not widely adopted by DHH people, leading to their suboptimal music experiences. The DHH community embodied mutual support among music lovers, evidenced by active information sharing and Q&A around music and hearing loss. We reflect on design justice for DHH people's music experience and propose practical design implications to create a more accessible music experience for them. ",
        "title": "Exploring the Diversity of Music Experiences for Deaf and Hard of  Hearing People",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09027",
        "abstract_url": "http://arxiv.org/abs/2401.09027",
        "authors": [
            {
                "last_name": "Su",
                "first_name": "Zheng-Yao"
            },
            {
                "last_name": "Tsai",
                "first_name": "Ming-Chung"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Inspired by the concept of fault tolerance quantum computation, this article proposes a framework dubbed Exact Homomorphic Encryption, EHE, enabling exact computations on encrypted data without the need for pre-decryption. The introduction of quantum gates is a critical step in constructing the message encryption and the computation encryption within the framework. Of significance is that both encryptions are respectively accomplished in a multivariate polynomial set generated by quantum gates. Two fundamental traits of quantum gates, the invertibility and the noncommutativity, establish the success of EHE. The employment of invertible gates allows exact decryptions for both an encrypted message and encrypted computation. The encrypted computation is exact as well because its encryption transformation is conducted with invertible gates. The second trait of noncommutativity among applied quantum gates brings forth the security for the two encryptions. In the message encryption, a plaintext is encoded into a ciphertext via a polynomial set generated by a product of noncommuting gates randomly chosen. Toward the computation encryption, a desired operation is encoded into an encrypted polynomial set generated by another product of noncommuting gates. The encrypted computation is then the evaluation of the encrypted polynomial set on the ciphertext and is referred to as the cryptovaluation. EHE is not only attainable on quantum computers, but also straightforwardly realizable on traditional computing environments. Surpassing the standard security 2^128 of quantum resilience, both the encryptions further reach a security greater than the suggested threshold 2^1024 and are characterized as hyper quantum-resilient. Thanks to the two essential traits of quantum gates, this framework can be regarded as the initial tangible manifestation of the concept noncommutative cryptography. ",
        "title": "Exact Homomorphic Encryption",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09029",
        "abstract_url": "http://arxiv.org/abs/2401.09029",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Dunyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Xi"
            },
            {
                "last_name": "Cai",
                "first_name": "Jinyue"
            },
            {
                "last_name": "Heng",
                "first_name": "Pheng-Ann"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Brain tumor represents one of the most fatal cancers around the world, and is very common in children and the elderly. Accurate identification of the type and grade of tumor in the early stages plays an important role in choosing a precise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of different sequences provide clinicians with important contradictory information to identify tumor regions. However, manual assessment is time-consuming and error-prone due to big amount of data and the diversity of brain tumor types. Hence, there is an unmet need for MRI automated brain tumor diagnosis. We observe that the predictive capability of uni-modality models is limited and their performance varies widely across modalities, and the commonly used modality fusion methods would introduce potential noise, which results in significant performance degradation. To overcome these challenges, we propose a novel cross-modality guidance-aided multi-modal learning with dual attention for addressing the task of MRI brain tumor grading. To balance the tradeoff between model efficiency and efficacy, we employ ResNet Mix Convolution as the backbone network for feature extraction. Besides, dual attention is applied to capture the semantic interdependencies in spatial and slice dimensions respectively. To facilitate information interaction among modalities, we design a cross-modality guidance-aided module where the primary modality guides the other secondary modalities during the process of training, which can effectively leverage the complementary information of different MRI modalities and meanwhile alleviate the impact of the possible noise. ",
        "title": "Cross-modality Guidance-aided Multi-modal Learning with Dual Attention  for MRI Brain Tumor Grading",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09031",
        "abstract_url": "http://arxiv.org/abs/2401.09031",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Bai",
                "first_name": "Andrew"
            },
            {
                "last_name": "Hsieh",
                "first_name": "Cho-Jui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ``black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTrac as a re-normalized adaptation that enables the retrieval of training samples more targeted to the test sample of interest, facilitating a localized measurement of influence and considerably more intuitive visualization. We demonstrate the efficacy of our approach through various evaluation metrics and auxiliary tasks, reducing the amount of generally influential samples to $\\frac{1}{3}$ of its original quantity. ",
        "title": "Data Attribution for Diffusion Models: Timestep-induced Bias in  Influence Estimation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09032",
        "abstract_url": "http://arxiv.org/abs/2401.09032",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Haichao"
            },
            {
                "last_name": "Huang",
                "first_name": "Zhenmin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Li",
                "first_name": "Yulin"
            },
            {
                "last_name": "Shen",
                "first_name": "Shaojie"
            },
            {
                "last_name": "Ma",
                "first_name": "Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA"
        ],
        "abstract": "  This paper investigates a cooperative motion planning problem for large-scale connected autonomous vehicles (CAVs) under limited communications, which addresses the challenges of high communication and computing resource requirements. Our proposed methodology incorporates a parallel optimization algorithm with improved consensus ADMM considering a more realistic locally connected topology network, and time complexity of O(N) is achieved by exploiting the sparsity in the dual update process. To further enhance the computational efficiency, we employ a lightweight evolution strategy for the dynamic connectivity graph of CAVs, and each sub-problem split from the consensus ADMM only requires managing a small group of CAVs. The proposed method implemented with the receding horizon scheme is validated thoroughly, and comparisons with existing numerical solvers and approaches demonstrate the efficiency of our proposed algorithm. Also, simulations on large-scale cooperative driving tasks involving 80 vehicles are performed in the high-fidelity CARLA simulator, which highlights the remarkable computational efficiency, scalability, and effectiveness of our proposed development. Demonstration videos are available at https://henryhcliu.github.io/icadmm_cmp_carla. ",
        "title": "Improved Consensus ADMM for Cooperative Motion Planning of Large-Scale  Connected Autonomous Vehicles with Limited Communication",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09034",
        "abstract_url": "http://arxiv.org/abs/2401.09034",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Changshuo"
            },
            {
                "last_name": "Chen",
                "first_name": "Sirui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Dai",
                "first_name": "Sunhao"
            },
            {
                "last_name": "Yu",
                "first_name": "Weijie"
            },
            {
                "last_name": "Xu",
                "first_name": "Jun"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Reinforcement learning (RL) has gained traction for enhancing user long-term experiences in recommender systems by effectively exploring users' interests. However, modern recommender systems exhibit distinct user behavioral patterns among tens of millions of items, which increases the difficulty of exploration. For example, user behaviors with different activity levels require varying intensity of exploration, while previous studies often overlook this aspect and apply a uniform exploration strategy to all users, which ultimately hurts user experiences in the long run. To address these challenges, we propose User-Oriented Exploration Policy (UOEP), a novel approach facilitating fine-grained exploration among user groups. We first construct a distributional critic which allows policy optimization under varying quantile levels of cumulative reward feedbacks from users, representing user groups with varying activity levels. Guided by this critic, we devise a population of distinct actors aimed at effective and fine-grained exploration within its respective user group. To simultaneously enhance diversity and stability during the exploration process, we further introduce a population-level diversity regularization term and a supervision module. Experimental results on public recommendation datasets demonstrate that our approach outperforms all other baselines in terms of long-term performance, validating its user-oriented exploration effectiveness. Meanwhile, further analyses reveal our approach's benefits of improved performance for low-activity users as well as increased fairness among users. ",
        "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User  Experiences in Recommender Systems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09036",
        "abstract_url": "http://arxiv.org/abs/2401.09036",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Huan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongliang"
            },
            {
                "last_name": "Cai",
                "first_name": "Yi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yunjing"
            },
            {
                "last_name": "Swindlehurst",
                "first_name": "A. Lee"
            },
            {
                "last_name": "Han",
                "first_name": "Zhu"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Illegitimate intelligent reflective surfaces (IRSs) can pose significant physical layer security risks on multi-user multiple-input single-output (MU-MISO) systems. Recently, a DISCO approach has been proposed an illegitimate IRS with random and time-varying reflection coefficients, referred to as a \"disco\" IRS (DIRS). Such DIRS can attack MU-MISO systems without relying on either jamming power or channel state information (CSI), and classical anti-jamming techniques are ineffective for the DIRS-based fully-passive jammers (DIRS-based FPJs). In this paper, we propose an IRS-enhanced anti-jamming precoder against DIRS-based FPJs that requires only statistical rather than instantaneous CSI of the DIRS-jammed channels. Specifically, a legitimate IRS is introduced to reduce the strength of the DIRS-based jamming relative to the transmit signals at a legitimate user (LU). In addition, the active beamforming at the legitimate access point (AP) is designed to maximize the signal-to-jamming-plus-noise ratios (SJNRs). Numerical results are presented to evaluate the effectiveness of the proposed IRS-enhanced anti-jamming precoder against DIRS-based FPJs. ",
        "title": "IRS-Enhanced Anti-Jamming Precoding Against DISCO Physical Layer Jamming  Attacks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09038",
        "abstract_url": "http://arxiv.org/abs/2401.09038",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wanying"
            },
            {
                "last_name": "Li",
                "first_name": "Jinming"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yichen"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Che",
                "first_name": "Zhengping"
            },
            {
                "last_name": "Peng",
                "first_name": "Yaxin"
            },
            {
                "last_name": "Shen",
                "first_name": "Chaomin"
            },
            {
                "last_name": "Liu",
                "first_name": "Dong"
            },
            {
                "last_name": "Feng",
                "first_name": "Feifei"
            },
            {
                "last_name": "Tang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Recent work on visual representation learning has shown to be efficient for robotic manipulation tasks. However, most existing works pretrained the visual backbone solely on 2D images or egocentric videos, ignoring the fact that robots learn to act in 3D space, which is hard to learn from 2D observation. In this paper, we examine the effectiveness of pretraining for vision backbone with public-available large-scale 3D data to improve manipulation policy learning. Our method, namely Depth-aware Pretraining for Robotics (DPR), enables an RGB-only backbone to learn 3D scene representations from self-supervised contrastive learning, where depth information serves as auxiliary knowledge. No 3D information is necessary during manipulation policy learning and inference, making our model enjoy both efficiency and effectiveness in 3D space manipulation. Furthermore, we introduce a new way to inject robots' proprioception into the policy networks that makes the manipulation model robust and generalizable. We demonstrate in experiments that our proposed framework improves performance on unseen objects and visual environments for various robotics tasks on both simulated and real robots. ",
        "title": "Visual Robotic Manipulation with Depth-Aware Pretraining",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09041",
        "abstract_url": "http://arxiv.org/abs/2401.09041",
        "authors": [
            {
                "last_name": "Kuptavanich",
                "first_name": "Kittipitch"
            },
            {
                "last_name": "Reiter",
                "first_name": "Ehud"
            },
            {
                "last_name": "Van Deemter",
                "first_name": "Kees"
            },
            {
                "last_name": "Siddharthan",
                "first_name": "Advaith"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We are developing techniques to generate summary descriptions of sets of objects. In this paper, we present and evaluate a rule-based NLG technique for summarising sets of bibliographical references in academic papers. This extends our previous work on summarising sets of consumer products and shows how our model generalises across these two very different domains. ",
        "title": "Textual Summarisation of Large Sets: Towards a General Approach",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09042",
        "abstract_url": "http://arxiv.org/abs/2401.09042",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhiming"
            },
            {
                "last_name": "Cao",
                "first_name": "Yushi"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiufeng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junzhe"
            },
            {
                "last_name": "Liu",
                "first_name": "Xu"
            },
            {
                "last_name": "Teo",
                "first_name": "Yon Shin"
            },
            {
                "last_name": "Lin",
                "first_name": "Shang-wei"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting. ",
        "title": "LLMs for Relational Reasoning: How Far are We?",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09044",
        "abstract_url": "http://arxiv.org/abs/2401.09044",
        "authors": [
            {
                "last_name": "Habib",
                "first_name": "Hussam"
            },
            {
                "last_name": "Stoldt",
                "first_name": "Ryan"
            },
            {
                "last_name": "High",
                "first_name": "Andrew"
            },
            {
                "last_name": "Ekdale",
                "first_name": "Brian"
            },
            {
                "last_name": "Peterson",
                "first_name": "Ashley"
            },
            {
                "last_name": "Biddle",
                "first_name": "Katy"
            },
            {
                "last_name": "Ssozi",
                "first_name": "Javie"
            },
            {
                "last_name": "Nithyanand",
                "first_name": "Rishab"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC",
            "IR"
        ],
        "abstract": "  The evolution of information-seeking processes, driven by search engines like Google, has transformed the access to information people have. This paper investigates how individuals' preexisting attitudes influence the modern information-seeking process, specifically the results presented by Google Search. Through a comprehensive study involving surveys and information-seeking tasks focusing on the topic of abortion, the paper provides four crucial insights: 1) Individuals with opposing attitudes on abortion receive different search results. 2) Individuals express their beliefs in their choice of vocabulary used in formulating the search queries, shaping the outcome of the search. 3) Additionally, the user's search history contributes to divergent results among those with opposing attitudes. 4) Google Search engine reinforces preexisting beliefs in search results. Overall, this study provides insights into the interplay between human biases and algorithmic processes, highlighting the potential for information polarization in modern information-seeking processes. ",
        "title": "Algorithmic amplification of biases on Google Search",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09047",
        "abstract_url": "http://arxiv.org/abs/2401.09047",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Haoxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yong"
            },
            {
                "last_name": "Cun",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Xia",
                "first_name": "Menghan"
            },
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "Weng",
                "first_name": "Chao"
            },
            {
                "last_name": "Shan",
                "first_name": "Ying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition. ",
        "title": "VideoCrafter2: Overcoming Data Limitations for High-Quality Video  Diffusion Models",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09048",
        "abstract_url": "http://arxiv.org/abs/2401.09048",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Jonghyun"
            },
            {
                "last_name": "Cho",
                "first_name": "Hansam"
            },
            {
                "last_name": "Yoo",
                "first_name": "Youngjoon"
            },
            {
                "last_name": "Kim",
                "first_name": "Seoung Bum"
            },
            {
                "last_name": "Jeong",
                "first_name": "Yonghyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce \\textit{depth disentanglement training} to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce \\textit{soft guidance}, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, \\textsc{Compose and Conquer (CnC)}, unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics. Code: https://github.com/tomtom1103/compose-and-conquer/ ",
        "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image  Synthesis",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09049",
        "abstract_url": "http://arxiv.org/abs/2401.09049",
        "authors": [
            {
                "last_name": "van Kempen",
                "first_name": "Raphael"
            },
            {
                "last_name": "Rehbronn",
                "first_name": "Tim"
            },
            {
                "last_name": "Jose",
                "first_name": "Abin"
            },
            {
                "last_name": "Stegmaier",
                "first_name": "Johannes"
            },
            {
                "last_name": "Lampe",
                "first_name": "Bastian"
            },
            {
                "last_name": "Woopen",
                "first_name": "Timo"
            },
            {
                "last_name": "Eckstein",
                "first_name": "Lutz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Automated vehicles require an accurate perception of their surroundings for safe and efficient driving. Lidar-based object detection is a widely used method for environment perception, but its performance is significantly affected by adverse weather conditions such as rain and fog. In this work, we investigate various strategies for enhancing the robustness of lidar-based object detection by processing sequential data samples generated by lidar sensors. Our approaches leverage temporal information to improve a lidar object detection model, without the need for additional filtering or pre-processing steps. We compare $10$ different neural network architectures that process point cloud sequences including a novel augmentation strategy introducing a temporal offset between frames of a sequence during training and evaluate the effectiveness of all strategies on lidar point clouds under adverse weather conditions through experiments. Our research provides a comprehensive study of effective methods for mitigating the effects of adverse weather on the reliability of lidar-based object detection using sequential data that are evaluated using public datasets such as nuScenes, Dense, and the Canadian Adverse Driving Conditions Dataset. Our findings demonstrate that our novel method, involving temporal offset augmentation through randomized frame skipping in sequences, enhances object detection accuracy compared to both the baseline model (Pillar-based Object Detection) and no augmentation. ",
        "title": "Enhancing Lidar-based Object Detection in Adverse Weather using Offset  Sequences in Time",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09050",
        "abstract_url": "http://arxiv.org/abs/2401.09050",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Zike"
            },
            {
                "last_name": "Zhou",
                "first_name": "Pan"
            },
            {
                "last_name": "Yi",
                "first_name": "Xuanyu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xiaoding"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective \"Consistent3D\" method that explores the ODE deterministic sampling prior for text-to-3D generation. Specifically, at each training iteration, given a rendered image by a 3D model, we first estimate its desired 3D score function by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling. Next, we design a consistency distillation sampling loss which samples along the ODE trajectory to generate two adjacent samples and uses the less noisy sample to guide another more noisy one for distilling the deterministic prior into the 3D model. Experimental results show the efficacy of our Consistent3D in generating high-fidelity and diverse 3D objects and large-scale scenes, as shown in Fig. 1. The codes are available at https://github.com/sail-sg/Consistent3D. ",
        "title": "Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation  with Deterministic Sampling Prior",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09051",
        "abstract_url": "http://arxiv.org/abs/2401.09051",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "K. J. Kevin"
            },
            {
                "last_name": "Liao",
                "first_name": "Q. Vera"
            },
            {
                "last_name": "Xiao",
                "first_name": "Ziang"
            },
            {
                "last_name": "Vaughan",
                "first_name": "Jennifer Wortman"
            },
            {
                "last_name": "Zhang",
                "first_name": "Amy X."
            },
            {
                "last_name": "McDonald",
                "first_name": "David W."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Advancements in large language models (LLMs) are poised to spark a proliferation of LLM-powered user experiences. In product teams, designers are often tasked with crafting user experiences that align with user needs. To involve designers and leverage their user-centered perspectives to create effective and responsible LLM-powered products, we introduce the practice of designerly adaptation for engaging with LLMs as an adaptable design material. We first identify key characteristics of designerly adaptation through a formative study with designers experienced in designing for LLM-powered products (N=12). These characteristics are 1) have a low technical barrier to entry, 2) leverage designers' unique perspectives bridging users and technology, and 3) encourage model tinkering. Based on this characterization, we build Canvil, a Figma widget that operationalizes designerly adaptation. Canvil supports structured authoring of system prompts to adapt LLM behavior, testing of adapted models on diverse user inputs, and integration of model outputs into interface designs. We use Canvil as a technology probe in a group-based design study (6 groups, N=17) to investigate the implications of integrating designerly adaptation into design workflows. We find that designers are able to iteratively tinker with different adaptation approaches and reason about interface affordances to enhance end-user interaction with LLMs. Furthermore, designers identified promising collaborative workflows for designerly adaptation. Our work opens new avenues for collaborative processes and tools that foreground designers' user-centered expertise in the crafting and deployment of LLM-powered user experiences. ",
        "title": "Canvil: Designerly Adaptation for LLM-Powered User Experiences",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09052",
        "abstract_url": "http://arxiv.org/abs/2401.09052",
        "authors": [
            {
                "last_name": "Bl\u00f6cker",
                "first_name": "Christopher"
            },
            {
                "last_name": "Scholtes",
                "first_name": "Ingo"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Networks represent how the entities of a system are connected and can be partitioned differently, prompting ways to compare partitions. Common approaches for comparing network partitions include information-theoretic measures based on mutual information and set-theoretic measures such as the Jaccard index. These measures are often based on computing the agreement in terms of overlap between different partitions of the same set. However, they ignore link patterns which are essential for the organisation of networks. We propose flow divergence, an information-theoretic divergence measure for comparing network partitions, inspired by the ideas behind the Kullback-Leibler divergence and the map equation for community detection. Similar to the Kullback-Leibler divergence, flow divergence adopts a coding perspective and compares two network partitions $\\mathsf{M}_a$ and $\\mathsf{M}_b$ by considering the expected extra number of bits required to describe a random walk on a network using $\\mathsf{M}_b$ relative to reference partition $\\mathsf{M}_a$. Because flow divergence is based on random walks, it can be used to compare partitions with arbitrary and different depths. We show that flow divergence distinguishes between partitions that traditional measures consider to be equally good when compared to a reference partition. Applied to real networks, we use flow divergence to estimate the cost of overfitting in incomplete networks and to visualise the solution landscape of network partitions. ",
        "title": "Flow Divergence: Comparing Maps of Flows with Relative Entropy",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09053",
        "abstract_url": "http://arxiv.org/abs/2401.09053",
        "authors": [
            {
                "last_name": "Normann",
                "first_name": "Dag"
            },
            {
                "last_name": "Sanders",
                "first_name": "Sam"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Open sets are central to mathematics, especially analysis and topology, in ways few notions are. In most, if not all, computational approaches to mathematics, open sets are only studied indirectly via their 'codes' or 'representations'. In this paper, we study how hard it is to compute, given an arbitrary open set of reals, the most common representation, i.e. a countable set of open intervals. We work in Kleene's higher-order computability theory, which was historically based on the S1-S9 schemes and which now has an intuitive lambda calculus formulation due to the authors. We establish many computational equivalences between on one hand the 'structure' functional that converts open sets to the aforementioned representation, and on the other hand functionals arising from mainstream mathematics, like basic properties of semi-continuous functions, the Urysohn lemma, and the Tietze extension theorem. We also compare these functionals to known operations on regulated and bounded variation functions, and the Lebesgue measure restricted to closed sets. We obtain a number of natural computational equivalences for the latter involving theorems from mainstream mathematics. ",
        "title": "On some computational properties of open sets",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09057",
        "abstract_url": "http://arxiv.org/abs/2401.09057",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yunze"
            },
            {
                "last_name": "Chen",
                "first_name": "Changxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zifan"
            },
            {
                "last_name": "Yi",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces a novel approach named CrossVideo, which aims to enhance self-supervised cross-modal contrastive learning in the field of point cloud video understanding. Traditional supervised learning methods encounter limitations due to data scarcity and challenges in label acquisition. To address these issues, we propose a self-supervised learning method that leverages the cross-modal relationship between point cloud videos and image videos to acquire meaningful feature representations. Intra-modal and cross-modal contrastive learning techniques are employed to facilitate effective comprehension of point cloud video. We also propose a multi-level contrastive approach for both modalities. Through extensive experiments, we demonstrate that our method significantly surpasses previous state-of-the-art approaches, and we conduct comprehensive ablation studies to validate the effectiveness of our proposed designs. ",
        "title": "CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point  Cloud Video Understanding",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09059",
        "abstract_url": "http://arxiv.org/abs/2401.09059",
        "authors": [
            {
                "last_name": "Jianu",
                "first_name": "Tudor"
            },
            {
                "last_name": "Huang",
                "first_name": "Baoru"
            },
            {
                "last_name": "Vo",
                "first_name": "Tuan"
            },
            {
                "last_name": "Vu",
                "first_name": "Minh Nhat"
            },
            {
                "last_name": "Kang",
                "first_name": "Jingxuan"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Hoan"
            },
            {
                "last_name": "Omisore",
                "first_name": "Olatunji"
            },
            {
                "last_name": "Berthet-Rayne",
                "first_name": "Pierre"
            },
            {
                "last_name": "Fichera",
                "first_name": "Sebastiano"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Anh"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Endovascular robots have been actively developed in both academia and industry. However, progress toward autonomous catheterization is often hampered by the widespread use of closed-source simulators and physical phantoms. Additionally, the acquisition of large-scale datasets for training machine learning algorithms with endovascular robots is usually infeasible due to expensive medical procedures. In this chapter, we introduce CathSim, the first open-source simulator for endovascular intervention to address these limitations. CathSim emphasizes real-time performance to enable rapid development and testing of learning algorithms. We validate CathSim against the real robot and show that our simulator can successfully mimic the behavior of the real robot. Based on CathSim, we develop a multimodal expert navigation network and demonstrate its effectiveness in downstream endovascular navigation tasks. The intensive experimental results suggest that CathSim has the potential to significantly accelerate research in the autonomous catheterization field. Our project is publicly available at https://github.com/airvlab/cathsim. ",
        "title": "Autonomous Catheterization with Open-source Simulator and Expert  Trajectory",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09060",
        "abstract_url": "http://arxiv.org/abs/2401.09060",
        "authors": [
            {
                "last_name": "Gures",
                "first_name": "Emre"
            },
            {
                "last_name": "Mach",
                "first_name": "Pavel"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The caching paradigm has been introduced to alleviate backhaul traffic load and to reduce latencies due to massive never ending increase in data traffic. To fully exploit the benefits offered by caching, unmanned aerial vehicles (UAVs) and device-to-device (D2D) communication can be further utilized. In contrast to prior works, that strictly limits the content delivery routes up to two hops, we explore a multi-hop communications scenario, where the UAVs, the UEs, or both can relay the content to individual users. In this context, we formulate the problem for joint route selection and power allocation to minimize the overall system content delivery duration. First, motivated by the limitations of existing works, we consider the case where the nodes may transmit content simultaneously rather than sequentially and propose simple yet effective approach to allocate the transmission power. Second, we design a low-complexity greedy algorithm jointly handling route selection and power allocation. The simulation results demonstrate that the proposed greedy algorithm outperforms the benchmark algorithm by up to 56.98% in terms of content delivery duration while it achieves close-to-optimal performance. ",
        "title": "Joint Route Selection and Power Allocation in Multi-hop Cache-enabled  Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09062",
        "abstract_url": "http://arxiv.org/abs/2401.09062",
        "authors": [
            {
                "last_name": "Tassi",
                "first_name": "Andrea"
            },
            {
                "last_name": "Warren",
                "first_name": "Daniel"
            },
            {
                "last_name": "Wang",
                "first_name": "Yue"
            },
            {
                "last_name": "Bhamare",
                "first_name": "Deval"
            },
            {
                "last_name": "Behravesh",
                "first_name": "Rasoul"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Next-generation mobile core networks are required to be scalable and capable of efficiently utilizing heterogeneous bare metal resources that may include edge servers. To this end, microservice-based solutions where control plane procedures are deconstructed in their fundamental building blocks are gaining momentum. This letter proposes an optimization framework delivering the partitioning and mapping of large-scale microservice graphs onto heterogeneous bare metal deployments while minimizing the total network traffic among servers. An efficient heuristic strategy for solving the optimization problem is also provided. Simulation results show that, with the proposed framework, a microservice-based core can consistently support the requested load in heterogeneous bare metal deployments even when alternative architecture fails. Besides, our framework ensures an overall reduction in the control plane-related network traffic if compared to current core architectures. ",
        "title": "On Optimization of Next-Generation Microservice-Based Core Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09064",
        "abstract_url": "http://arxiv.org/abs/2401.09064",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Yanmo"
            },
            {
                "last_name": "Wu",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhang",
                "first_name": "J. Andrew"
            },
            {
                "last_name": "Deng",
                "first_name": "Weibo"
            },
            {
                "last_name": "Guo",
                "first_name": "Y. Jay"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Bi-static sensing is crucial for exploring the potential of networked sensing capabilities in integrated sensing and communications (ISAC). However, it suffers from the challenging clock asynchronism issue. CSI ratio-based sensing is an effective means to address the issue. Its performance bounds, particular for Doppler sensing, have not been fully understood yet. This work endeavors to fill the research gap. Focusing on a single dynamic path in high-SNR scenarios, we derive the closed-form CRB. Then, through analyzing the mutual interference between dynamic and static paths, we simplify the CRB results by deriving close approximations, further unveiling new insights of the impact of numerous physical parameters on Doppler sensing. Moreover, utilizing the new CRB and analyses, we propose novel waveform optimization strategies for noise- and interference-limited sensing scenarios, which are also empowered by closed-form and efficient solutions. Extensive simulation results are provided to validate the preciseness of the derived CRB results and analyses, with the aid of the maximum-likelihood estimator. The results also demonstrate the substantial enhanced Doppler sensing accuracy and the sensing capabilities for low-speed target achieved by the proposed waveform design. ",
        "title": "Performance Bounds and Optimization for CSI-Ratio based Bi-static  Doppler Sensing in ISAC Systems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09067",
        "abstract_url": "http://arxiv.org/abs/2401.09067",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Depeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Tianqi"
            },
            {
                "last_name": "Chen",
                "first_name": "Junwei"
            },
            {
                "last_name": "Ren",
                "first_name": "Qining"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Kenji"
            },
            {
                "last_name": "Zeng",
                "first_name": "Zhigang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptation between old and new tasks with predefined basis vectors. Extensive experiments demonstrate that our method achieves competitive accuracy performance, even with absolute superiority of zero exemplar buffer and 1.02x the base model. ",
        "title": "Towards Continual Learning Desiderata via HSIC-Bottleneck  Orthogonalization and Equiangular Embedding",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09068",
        "abstract_url": "http://arxiv.org/abs/2401.09068",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Lixiang"
            },
            {
                "last_name": "Xiao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Li",
                "first_name": "Zhenjiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection, pre-execution pruning optimizations, runtime acceleration, and post-execution low-cost storage to fill the gap for efficient deployment and execution of pruned models. It can be integrated into commercial ML frameworks for practical deployment, and a prototype system has been developed. Extensive experiments on various models show promising gains compared to state-of-the-art methods. ",
        "title": "DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09070",
        "abstract_url": "http://arxiv.org/abs/2401.09070",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Qinghua"
            },
            {
                "last_name": "Wang",
                "first_name": "Yongzhen"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Knowledge graph (KG) based reasoning has been regarded as an effective means for the analysis of semantic networks and is of great usefulness in areas of information retrieval, recommendation, decision-making, and man-machine interaction. It is widely used in recommendation, decision-making, question-answering, search, and other fields. However, previous studies mainly used low-level knowledge in the KG for reasoning, which may result in insufficient generalization and poor robustness of reasoning. To this end, this paper proposes a new inference approach using a novel knowledge augmentation strategy to improve the generalization capability of KG. This framework extracts high-level pyramidal knowledge from low-level knowledge and applies it to reasoning in a multi-level hierarchical KG, called knowledge pyramid in this paper. We tested some medical data sets using the proposed approach, and the experimental results show that the proposed knowledge pyramid has improved the knowledge inference performance with better generalization. Especially, when there are fewer training samples, the inference accuracy can be significantly improved. ",
        "title": "Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for  Generalized Knowledge Augmentation and Inference",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09071",
        "abstract_url": "http://arxiv.org/abs/2401.09071",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jingwei"
            },
            {
                "last_name": "Huang",
                "first_name": "Kaizhu"
            },
            {
                "last_name": "Yi",
                "first_name": "Xinping"
            },
            {
                "last_name": "Su",
                "first_name": "Zixian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Rui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial domain and inspire us to rethink graph spectral filters beyond the fixed-order polynomials, which neglect global information. Built upon the theoretical findings, we revisit the state-of-the-art spectral GNNs and propose a novel Spatially Adaptive Filtering (SAF) framework, which leverages the adapted new graph by spectral filtering for an auxiliary non-local aggregation. Notably, our proposed SAF comprehensively models both node similarity and dissimilarity from a global perspective, therefore alleviating persistent deficiencies of GNNs related to long-range dependencies and graph heterophily. Extensive experiments over 13 node classification benchmarks demonstrate the superiority of our proposed framework to the state-of-the-art models. ",
        "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive  Filtering",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09072",
        "abstract_url": "http://arxiv.org/abs/2401.09072",
        "authors": [
            {
                "last_name": "Scial\u00f2",
                "first_name": "Stefano"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The present work deals with the numerical resolution of coupled 3D-2D problems arising from the simulation of fluid flow in fractured porous media modeled via the Discrete Fracture and Matrix (DFM) model. According to the DFM model, fractures are represented as planar interfaces immersed in a 3D porous matrix and can behave as preferential flow paths, in the case of conductive fractures, or can actually be a barrier for the flow, when, instead, the permeability in the normal-to-fracture direction is small compared to the permeability of the matrix. Consequently, the pressure solution in a DFM can be discontinuous across a barrier, as a result of the geometrical dimensional reduction operated on the fracture.   The present work is aimed at developing a numerical scheme suitable for the simulation of the flow in a DFM with fractures and barriers, using a mesh for the 3D matrix non conforming to the fractures and that is ready for domain decomposition. This is achieved starting from a PDE-constrained optimization method, currently available in literature only for conductive fractures in a DFM. First, a novel formulation of the optimization problem is defined to account for non permeable fractures. These are described by a filtration-like coupling at the interface with the surrounding porous matrix. Also the extended finite element method with discontinuous enrichment functions is used to reproduce the pressure solution in the matrix around a barrier.   The method is presented here in its simplest form, for clarity of exposition, i.e. considering the case of a single fracture in a 3D domain, also providing a proof of the well posedness of the resulting discrete problem. Four validation examples are proposed to show the viability and the effectiveness of the method. ",
        "title": "A five field formulation for flow simulations in porous media with  fractures and barriers via an optimization based domain decomposition method",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09073",
        "abstract_url": "http://arxiv.org/abs/2401.09073",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhirui"
            },
            {
                "last_name": "Karthik",
                "first_name": "P. N."
            },
            {
                "last_name": "Chee",
                "first_name": "Yeow Meng"
            },
            {
                "last_name": "Tan",
                "first_name": "Vincent Y. F."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT"
        ],
        "abstract": "  We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\\varepsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\\em $\\varepsilon$-differential privacy} ($\\varepsilon$-DP) constraint. We construct a policy satisfying the $\\varepsilon$-DP constraint (called {\\sc DP-BAI}) by proposing the principle of {\\em maximum absolute determinants}, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\\varepsilon$, and (c) the problem complexity that is expressible as the sum of two terms, one characterising the complexity of standard fixed-budget BAI (without privacy constraints), and the other accounting for the $\\varepsilon$-DP constraint. Additionally, we present some auxiliary results that contribute to the derivation of the lower bound on the error probability. These results, we posit, may be of independent interest and could prove instrumental in proving lower bounds on error probabilities in several other bandit problems. Whereas prior works provide results for BAI in the fixed-budget regime without privacy constraints or in the fixed-confidence regime with privacy constraints, our work fills the gap in the literature by providing the results for BAI in the fixed-budget regime under the $\\varepsilon$-DP constraint. ",
        "title": "Fixed-Budget Differentially Private Best Arm Identification",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09074",
        "abstract_url": "http://arxiv.org/abs/2401.09074",
        "authors": [
            {
                "last_name": "La Malfa",
                "first_name": "Emanuele"
            },
            {
                "last_name": "Weinhuber",
                "first_name": "Christoph"
            },
            {
                "last_name": "Torre",
                "first_name": "Orazio"
            },
            {
                "last_name": "Lin",
                "first_name": "Fangru"
            },
            {
                "last_name": "Cohn",
                "first_name": "Anthony"
            },
            {
                "last_name": "Shadbolt",
                "first_name": "Nigel"
            },
            {
                "last_name": "Wooldridge",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "PL"
        ],
        "abstract": "  We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirically, our new Chain of Simulation (CoSm) method improves on the standard Chain of Thought prompting approach by avoiding the pitfalls of memorisation. ",
        "title": "Code Simulation Challenges for Large Language Models",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09075",
        "abstract_url": "http://arxiv.org/abs/2401.09075",
        "authors": [
            {
                "last_name": "Antebi",
                "first_name": "Sagiv"
            },
            {
                "last_name": "Azulay",
                "first_name": "Noam"
            },
            {
                "last_name": "Habler",
                "first_name": "Edan"
            },
            {
                "last_name": "Ganon",
                "first_name": "Ben"
            },
            {
                "last_name": "Shabtai",
                "first_name": "Asaf"
            },
            {
                "last_name": "Elovici",
                "first_name": "Yuval"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  In November 2023, OpenAI introduced a new service allowing users to create custom versions of ChatGPT (GPTs) by using specific instructions and knowledge to guide the model's behavior. We aim to raise awareness of the fact that GPTs can be used maliciously, posing privacy and security risks to their users. ",
        "title": "GPT in Sheep's Clothing: The Risk of Customized GPTs",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09076",
        "abstract_url": "http://arxiv.org/abs/2401.09076",
        "authors": [
            {
                "last_name": "Jamadagni",
                "first_name": "Amit"
            },
            {
                "last_name": "L\u00e4uchli",
                "first_name": "Andreas M."
            },
            {
                "last_name": "Hempel",
                "first_name": "Cornelius"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Rapid advances in quantum computing technology lead to an increasing need for software simulators that enable both algorithm design and the validation of results obtained from quantum hardware. This includes calculations that aim at probing regimes of quantum advantage, where a quantum computer outperforms a classical computer in the same task. High performance computing (HPC) platforms play a crucial role as today's quantum devices already reach beyond the limits of what powerful workstations can model, but a systematic evaluation of the individual performance of the many offered simulation packages is lacking so far. In this Technical Review, we benchmark several software packages capable of simulating quantum dynamics with a special focus on HPC capabilities. We develop a containerized toolchain for benchmarking a large set of simulation packages on a local HPC cluster using different parallelisation capabilities, and compare the performance and system size-scaling for three paradigmatic quantum computing tasks. Our results can help finding the right package for a given simulation task and lay the foundation for a systematic community effort to benchmark and validate upcoming versions of existing and also newly developed simulation packages. ",
        "title": "Benchmarking quantum computer simulation software packages",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09077",
        "abstract_url": "http://arxiv.org/abs/2401.09077",
        "authors": [
            {
                "last_name": "Pascher",
                "first_name": "Max"
            },
            {
                "last_name": "Saad",
                "first_name": "Alia"
            },
            {
                "last_name": "Liebers",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Heger",
                "first_name": "Roman"
            },
            {
                "last_name": "Gerken",
                "first_name": "Jens"
            },
            {
                "last_name": "Schneegass",
                "first_name": "Stefan"
            },
            {
                "last_name": "Gruene",
                "first_name": "Uwe"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  Effective Human-Robot Interaction (HRI) is fundamental to seamlessly integrating robotic systems into our daily lives. However, current communication modes require additional technological interfaces, which can be cumbersome and indirect. This paper presents a novel approach, using direct motion-based communication by moving a robot's end effector. Our strategy enables users to communicate with a robot by using four distinct gestures -- two handshakes ('formal' and 'informal') and two letters ('W' and 'S'). As a proof-of-concept, we conducted a user study with 16 participants, capturing subjective experience ratings and objective data for training machine learning classifiers. Our findings show that the four different gestures performed by moving the robot's end effector can be distinguished with close to 100% accuracy. Our research offers implications for the design of future HRI interfaces, suggesting that motion-based interaction can empower human operators to communicate directly with robots, removing the necessity for additional hardware. ",
        "title": "Hands-On Robotics: Enabling Communication Through Direct Gesture Control",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09080",
        "abstract_url": "http://arxiv.org/abs/2401.09080",
        "authors": [
            {
                "last_name": "Bammer",
                "first_name": "Patrick"
            },
            {
                "last_name": "Banz",
                "first_name": "Lothar"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper a higher-order mixed finite element method for elastoplasticity with linear kinematic hardening is analyzed. Thereby, the non-differentiability of the involved plasticity functional is resolved by a Lagrange multiplier leading to a three field formulation. The finite element discretization is conforming in the displacement field and the plastic strain but potentially non-conforming in the Lagrange multiplier as its Frobenius norm is only constrained in a certain set of Gauss quadrature points. A discrete inf-sup condition with constant 1 and the well posedness of the discrete mixed problem are shown. Moreover, convergence and guaranteed convergence rates are proved with respect to the mesh size and the polynomial degree, which are optimal for the lowest order case. Numerical experiments underline the theoretical results. ",
        "title": "Mixed Finite Elements of Higher-Order in Elastoplasticity",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09082",
        "abstract_url": "http://arxiv.org/abs/2401.09082",
        "authors": [
            {
                "last_name": "Alberts",
                "first_name": "Lize"
            },
            {
                "last_name": "Keeling",
                "first_name": "Geoff"
            },
            {
                "last_name": "McCroskery",
                "first_name": "Amanda"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  With the growing popularity of dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as 'good' social actors and treat people respectfully. ",
        "title": "What makes for a 'good' social actor? Using respect as a lens to  evaluate interactions with language agents",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09083",
        "abstract_url": "http://arxiv.org/abs/2401.09083",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Haonan"
            },
            {
                "last_name": "Su",
                "first_name": "Xin"
            },
            {
                "last_name": "Wu",
                "first_name": "Chen"
            },
            {
                "last_name": "Du",
                "first_name": "Bo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liangpei"
            },
            {
                "last_name": "Li",
                "first_name": "Deren"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, the flourishing large language models(LLM), especially ChatGPT, have shown exceptional performance in language understanding, reasoning, and interaction, attracting users and researchers from multiple fields and domains. Although LLMs have shown great capacity to perform human-like task accomplishment in natural language and natural image, their potential in handling remote sensing interpretation tasks has not yet been fully explored. Moreover, the lack of automation in remote sensing task planning hinders the accessibility of remote sensing interpretation techniques, especially to non-remote sensing experts from multiple research fields. To this end, we present Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to connect various AI-based remote sensing models to solve complicated interpretation tasks. More specifically, given a user request and a remote sensing image, we utilized ChatGPT to understand user requests, perform task planning according to the tasks' functions, execute each subtask iteratively, and generate the final response according to the output of each subtask. Considering that LLM is trained with natural language and is not capable of directly perceiving visual concepts as contained in remote sensing images, we designed visual cues that inject visual information into ChatGPT. With Remote Sensing ChatGPT, users can simply send a remote sensing image with the corresponding request, and get the interpretation results as well as language feedback from Remote Sensing ChatGPT. Experiments and examples show that Remote Sensing ChatGPT can tackle a wide range of remote sensing tasks and can be extended to more tasks with more sophisticated models such as the remote sensing foundation model. The code and demo of Remote Sensing ChatGPT is publicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT . ",
        "title": "Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and  Visual Models",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09084",
        "abstract_url": "http://arxiv.org/abs/2401.09084",
        "authors": [
            {
                "last_name": "Ruan",
                "first_name": "Ludan"
            },
            {
                "last_name": "Tian",
                "first_name": "Lei"
            },
            {
                "last_name": "Huang",
                "first_name": "Chuanwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xinyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Genearation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fr\\'echet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit https://univg-baidu.github.io. ",
        "title": "UniVG: Towards UNIfied-modal Video Generation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09089",
        "abstract_url": "http://arxiv.org/abs/2401.09089",
        "authors": [
            {
                "last_name": "Kislal",
                "first_name": "A. Oguz"
            },
            {
                "last_name": "Rajiv",
                "first_name": "Madhavi"
            },
            {
                "last_name": "Durisi",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Str\u00f6m",
                "first_name": "Erik G."
            },
            {
                "last_name": "Mitra",
                "first_name": "Urbashi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We propose a framework to evaluate the random-coding union bound with parameter $s$ on the achievable error probability in the finite-blocklength regime for a pilot-assisted transmission scheme operating over an imperfectly synchronized and memoryless block-fading waveform channel. Unlike previous results, which disregard the effects of imperfect synchronization, our framework utilizes pilots for both synchronization and channel estimation. Specifically, we provide an algorithm to perform joint synchronization and channel estimation and verify its accuracy by observing its tightness in comparison with the Cramer-Rao bound. Then, we develop an RCUs bound on the error probability, which applies for a receiver that treats the estimates provided by the algorithm as accurate. Additionally, we utilize the saddlepoint approximation to provide a numerically efficient method for evaluating the RCUs bound in this scenario. Our numerical experiments verify the accuracy of the proposed approximation. Moreover, when transmission blocks are received synchronously, numerical results indicate that the number of pilot symbols needed to estimate the fading channel gains to the level of accuracy required in ultra-reliable low-latency communication is also sufficient to acquire sufficiently good synchronization. However, when the blocks are received asynchronously, synchronization becomes the bottleneck for the system performance. ",
        "title": "Is Synchronization a Bottleneck for Pilot-Assisted URLLC Links?",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09090",
        "abstract_url": "http://arxiv.org/abs/2401.09090",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Yunpeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kyrie Zhixuan"
            },
            {
                "last_name": "Liang",
                "first_name": "Yueqing"
            },
            {
                "last_name": "Shu",
                "first_name": "Kai"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Large language models (LLMs) have shown their potential in biomedical fields. However, how the public uses them for healthcare purposes such as medical Q\\&A, self-diagnosis, and daily healthcare information seeking is under-investigated. In this paper, we adopt a mixed-methods approach, including surveys (N=167) and interviews (N=17) to investigate how and why the public uses LLMs for healthcare. LLMs as a healthcare tool have gained popularity, and are often used in combination with other information channels such as search engines and online health communities to optimize information quality. LLMs provide more accurate information and a more convenient interaction/service model compared to traditional channels. LLMs also do a better job of reducing misinformation, especially in daily healthcare questions. Doctors using LLMs for diagnosis is less acceptable than for auxiliary work such as writing medical records. Based on the findings, we reflect on the ethical and effective use of LLMs for healthcare and propose future research directions. ",
        "title": "Understanding the concerns and choices of public when using large  language models for healthcare",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09091",
        "abstract_url": "http://arxiv.org/abs/2401.09091",
        "authors": [
            {
                "last_name": "Karacan",
                "first_name": "Erenay"
            },
            {
                "last_name": "Mendl",
                "first_name": "Christian B."
            },
            {
                "last_name": "Chen",
                "first_name": "Yanbin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  One of the most promising applications of quantum computers is to simulate quantum mechanical systems and deliver an advantage to classical computation by leveraging their inherent quantum behaviour. In this work, we present a new approach to achieve a noise tolerant Hamiltonian simulation algorithm for ground state energy estimation which also surmounts stochastic limitations most of its counterparts face. This algorithm is based on an adaptive set of fuzzy bisection searches to estimate the ground state energy digit by digit that can get to any arbitrary target precision. It builds upon the Quantum Eigenvalue Transformation of Unitary Matrices (QETU) algorithm and it delivers good approximations in simulations with quantum depolarizing probability up to 1e-3, particularly for the Transverse-Field Ising Model (TFIM). We ran simulations with different system Hamiltonians, system sizes and time evolution encoding methods on IBM Qiskit and we demonstrate the key results in this work, as well as compare the performance with other existing methods. ",
        "title": "Noise-Tolerant Quantum Algorithm for Ground State Energy Estimation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09092",
        "abstract_url": "http://arxiv.org/abs/2401.09092",
        "authors": [
            {
                "last_name": "V\u00f6lker",
                "first_name": "Tom"
            },
            {
                "last_name": "Pfister",
                "first_name": "Jan"
            },
            {
                "last_name": "Koopmann",
                "first_name": "Tobias"
            },
            {
                "last_name": "Hotho",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "HC"
        ],
        "abstract": "  The ever-growing corpus of scientific literature presents significant challenges for researchers with respect to discovery, management, and annotation of relevant publications. Traditional platforms like Semantic Scholar, BibSonomy, and Zotero offer tools for literature management, but largely require manual laborious and error-prone input of tags and metadata. Here, we introduce a novel retrieval augmented generation system that leverages chat-based large language models (LLMs) to streamline and enhance the process of publication management. It provides a unified chat-based interface, enabling intuitive interactions with various backends, including Semantic Scholar, BibSonomy, and the Zotero Webscraper. It supports two main use-cases: (1) Explorative Search & Retrieval - leveraging LLMs to search for and retrieve both specific and general scientific publications, while addressing the challenges of content hallucination and data obsolescence; and (2) Cataloguing & Management - aiding in the organization of personal publication libraries, in this case BibSonomy, by automating the addition of metadata and tags, while facilitating manual edits and updates. We compare our system to different LLM models in three different settings, including a user study, and we can show its advantages in different metrics. ",
        "title": "BibSonomy Meets ChatLLMs for Publication Management: From Chat to  Publication Management: Organizing your related work using BibSonomy & LLMs",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09093",
        "abstract_url": "http://arxiv.org/abs/2401.09093",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Haowen"
            },
            {
                "last_name": "Yu",
                "first_name": "F. Richard"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and GRU, have historically held prominence in time series tasks. However, they have recently seen a decline in their dominant position across various time series tasks. As a result, recent advancements in time series forecasting have seen a notable shift away from RNNs towards alternative architectures such as Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs, we design an efficient RNN-based model for time series tasks, named RWKV-TS, with three distinctive features: (i) A novel RNN architecture characterized by $O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture long-term sequence information compared to traditional RNNs. (iii) High computational efficiency coupled with the capacity to scale up effectively. Through extensive experimentation, our proposed RWKV-TS model demonstrates competitive performance when compared to state-of-the-art Transformer-based or CNN-based models. Notably, RWKV-TS exhibits not only comparable performance but also demonstrates reduced latency and memory utilization. The success of RWKV-TS encourages further exploration and innovation in leveraging RNN-based approaches within the domain of Time Series. The combination of competitive performance, low latency, and efficient memory usage positions RWKV-TS as a promising avenue for future research in time series tasks. Code is available at:\\href{https://github.com/howard-hou/RWKV-TS}{ https://github.com/howard-hou/RWKV-TS} ",
        "title": "RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series  Tasks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09097",
        "abstract_url": "http://arxiv.org/abs/2401.09097",
        "authors": [
            {
                "last_name": "Trindade",
                "first_name": "Thomas Trigo"
            },
            {
                "last_name": "Zygalakis",
                "first_name": "Konstantinos C."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider the problem of efficiently simulating stochastic models of chemical kinetics. The Gillespie Stochastic Simulation algorithm (SSA) is often used to simulate these models, however, in many scenarios of interest, the computational cost quickly becomes prohibitive. This is further exasperated in the Bayesian inference context when estimating parameters of chemical models, as the intractability of the likelihood requires multiple simulations of the underlying system. To deal with issues of computational complexity in this paper, we propose a novel hybrid $\\tau$-leap algorithm for simulating well-mixed chemical systems. In particular, the algorithm uses $\\tau$-leap when appropriate (high population densities), and SSA when necessary (low population densities, when discrete effects become non-negligible). In the intermediate regime, a combination of the two methods, which leverages the properties of the underlying Poisson formulation, is employed. As illustrated through a number of numerical experiments the hybrid $\\tau$ offers significant computational savings when compared to SSA without however sacrificing the overall accuracy. This feature is particularly welcomed in the Bayesian inference context, as it allows for parameter estimation of stochastic chemical kinetics at reduced computational cost. ",
        "title": "A hybrid tau-leap for simulating chemical kinetics with applications to  parameter estimationTho",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09101",
        "abstract_url": "http://arxiv.org/abs/2401.09101",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhong",
                "first_name": "Xingguang"
            },
            {
                "last_name": "Wiesmann",
                "first_name": "Louis"
            },
            {
                "last_name": "Posewsky",
                "first_name": "Thorbj\u00f6rn"
            },
            {
                "last_name": "Behley",
                "first_name": "Jens"
            },
            {
                "last_name": "Stachniss",
                "first_name": "Cyrill"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Accurate and robust localization and mapping are essential components for most autonomous robots. In this paper, we propose a SLAM system for building globally consistent maps, called PIN-SLAM, that is based on an elastic and compact point-based implicit neural map representation. Taking range measurements as input, our approach alternates between incremental learning of the local implicit signed distance field and the pose estimation given the current local map using a correspondence-free, point-to-implicit model registration. Our implicit map is based on sparse optimizable neural points, which are inherently elastic and deformable with the global pose adjustment when closing a loop. Loops are also detected using the neural point features. Extensive experiments validate that PIN-SLAM is robust to various environments and versatile to different range sensors such as LiDAR and RGB-D cameras. PIN-SLAM achieves pose estimation accuracy better or on par with the state-of-the-art LiDAR odometry or SLAM systems and outperforms the recent neural implicit SLAM approaches while maintaining a more consistent, and highly compact implicit map that can be reconstructed as accurate and complete meshes. Finally, thanks to the voxel hashing for efficient neural points indexing and the fast implicit map-based registration without closest point association, PIN-SLAM can run at the sensor frame rate on a moderate GPU. Codes will be available at: https://github.com/PRBonn/PIN_SLAM. ",
        "title": "PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation  for Achieving Global Map Consistency",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09102",
        "abstract_url": "http://arxiv.org/abs/2401.09102",
        "authors": [
            {
                "last_name": "Yeung",
                "first_name": "Mason"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In the evolving landscape of Internet technologies, where decentralized systems, especially blockchain-based computation and storage like Ethereum Virtual Machine (EVM), Arweave, and IPFS, are gaining prominence, there remains a stark absence of a holistic decentralized communication framework. This gap underlines the pressing necessity for a protocol that not only enables seamless cross-platform messaging but also allows direct messaging to wallet addresses, fostering interoperability and privacy across diverse platforms. SendingNetwork addresses this need by creating a reliable and secure decentralized communication network, targeting essential challenges like privacy protection, scalability, efficiency, and composability. Central to our approach is the incorporation of edge computing to form an adaptive relay network with the modular libp2p library. We introduce a dynamic group chat encryption mechanism based on the Double Ratchet algorithm for secure communication and propose a Delegation scheme for efficient message processing in large group chats, enhancing both resilience and scalability. Our theoretical analyses affirm the Delegation scheme's superior performance. To bolster system stability and encourage node participation, we integrate two innovative consensus mechanisms: \"Proof of Relay\" for validating message relay workload based on the novel KZG commitment, and \"Proof of Availability\" for ensuring network consistency and managing incentives through Verkle trees. Our whitepaper details the network's key components and architecture, concluding with a roadmap and a preview of future enhancements to SendingNetwork. ",
        "title": "SendingNetwork: Advancing the Future of Decentralized Messaging Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09105",
        "abstract_url": "http://arxiv.org/abs/2401.09105",
        "authors": [
            {
                "last_name": "Bammer",
                "first_name": "Patrick"
            },
            {
                "last_name": "Banz",
                "first_name": "Lothar"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, a reliable a posteriori error estimator for a model problem of elastoplasticity with linear kinematic hardening is derived, which satisfies some (local) efficiency estimates. It is applicable to any discretization that is conforming with respect to the displacement field and the plastic strain. Furthermore, the paper presents $hp$-finite element discretizations relying on a variational inequality as well as on a mixed variational formulation and discusses their equivalence by using biorthogonal basis functions. Numerical experiments demonstrate the applicability of the theoretical findings and underline the potential of $h$- and $hp$-adaptive finite element discretizations for problems of elastoplasticity. ",
        "title": "A Posteriori Error Estimates for $hp$-FE Discretizations in  Elastoplasticity",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09108",
        "abstract_url": "http://arxiv.org/abs/2401.09108",
        "authors": [
            {
                "last_name": "Tamekue",
                "first_name": "Cyprien"
            },
            {
                "last_name": "Prandi",
                "first_name": "Dario"
            },
            {
                "last_name": "Chitour",
                "first_name": "Yacine"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper investigates the replication of experiments by Billock and Tsou [PNAS, 2007] using the controllability of neural fields of Amari-type modelling the cortical activity in the primary visual cortex (V1), focusing on a regular funnel pattern localised in the fovea or the peripheral visual field. The aim is to understand and model the visual phenomena observed in these experiments, emphasising their nonlinear nature. The study involves designing sensory inputs simulating the visual stimuli from Billock and Tsou's experiments. The after-images induced by these inputs are then theoretically and numerically studied to determine their capacity to replicate the experimentally observed visual effects. A key aspect of this research is investigating the effects induced by the nonlinear nature of neural responses. In particular, by highlighting the importance of both excitatory and inhibitory neurons in the emergence of certain visual phenomena, this study suggests that an interplay of both types of neuronal activities plays an essential role in visual processes, challenging the assumption that the latter is mainly driven by excitatory activities alone. ",
        "title": "Reproducibility via neural fields of visual illusions induced by  localized stimuli",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09109",
        "abstract_url": "http://arxiv.org/abs/2401.09109",
        "authors": [
            {
                "last_name": "Theodoridis",
                "first_name": "Johannes"
            },
            {
                "last_name": "Hofmann",
                "first_name": "Jessica"
            },
            {
                "last_name": "Maucher",
                "first_name": "Johannes"
            },
            {
                "last_name": "Schilling",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Do deep learning models for instance segmentation generalize to novel objects in a systematic way? For classification, such behavior has been questioned. In this study, we aim to understand if certain design decisions such as framework, architecture or pre-training contribute to the semantic understanding of instance segmentation. To answer this question, we consider a special case of robustness and compare pre-trained models on a challenging benchmark for object-centric, out-of-distribution texture. We do not introduce another method in this work. Instead, we take a step back and evaluate a broad range of existing literature. This includes Cascade and Mask R-CNN, Swin Transformer, BMask, YOLACT(++), DETR, BCNet, SOTR and SOLOv2. We find that YOLACT++, SOTR and SOLOv2 are significantly more robust to out-of-distribution texture than other frameworks. In addition, we show that deeper and dynamic architectures improve robustness whereas training schedules, data augmentation and pre-training have only a minor impact. In summary we evaluate 68 models on 61 versions of MS COCO for a total of 4148 evaluations. ",
        "title": "Trapped in texture bias? A large scale comparison of deep instance  segmentation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09110",
        "abstract_url": "http://arxiv.org/abs/2401.09110",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Dajiang"
            },
            {
                "last_name": "Hadjicostis",
                "first_name": "Christoforos N."
            },
            {
                "last_name": "Li",
                "first_name": "Zhiwu"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We investigate decentralized state estimation for a discrete event system in a setting where the information received at a coordinator may be corrupted or tampered by a malicious attacker. Specifically, a system is observed by a set of (local) observation sites (OSs) which occasionally send their recorded sequences of observations to the coordinator that is in charge of estimating the system state. The malfunctions and attacks, referred to as errors in this paper, include symbol deletions, insertions and replacements, each of which bears a positive cost. Two types of errors, global errors and local errors, are proposed to describe the impact of errors on decentralized information processing. Global errors occur when all OSs record the same error, while local errors occur when different OSs record different errors. Distinguishing these types of errors is important for a proper design of decentralized information processing (so as to be more resilient and better equipped to handle errors and failures). For each type of error, we propose two methods to efficiently perform state estimation: one based on appropriately modifying the original system and the other based on inferring the matching behavior of the original system. For each method, we adopt an estimation-by-release methodology to design an algorithm for constructing a corresponding synchronizer for state estimation. ",
        "title": "Global and Local Error-Tolerant Decentralized State Estimation under  Partially Ordered Observations",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09112",
        "abstract_url": "http://arxiv.org/abs/2401.09112",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shuo"
            },
            {
                "last_name": "Jia",
                "first_name": "Fan"
            },
            {
                "last_name": "Liu",
                "first_name": "Yingfei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Zehui"
            },
            {
                "last_name": "Wang",
                "first_name": "Tiancai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To enhance perception performance in complex and extensive scenarios within the realm of autonomous driving, there has been a noteworthy focus on temporal modeling, with a particular emphasis on streaming methods. The prevailing trend in streaming models involves the utilization of stream queries for the propagation of temporal information. Despite the prevalence of this approach, the direct application of the streaming paradigm to the construction of vectorized high-definition maps (HD-maps) fails to fully harness the inherent potential of temporal information. This paper introduces the Stream Query Denoising (SQD) strategy as a novel approach for temporal modeling in high-definition map (HD-map) construction. SQD is designed to facilitate the learning of temporal consistency among map elements within the streaming model. The methodology involves denoising the queries that have been perturbed by the addition of noise to the ground-truth information from the preceding frame. This denoising process aims to reconstruct the ground-truth information for the current frame, thereby simulating the prediction process inherent in stream queries. The SQD strategy can be applied to those streaming methods (e.g., StreamMapNet) to enhance the temporal modeling. The proposed SQD-MapNet is the StreamMapNet equipped with SQD. Extensive experiments on nuScenes and Argoverse2 show that our method is remarkably superior to other existing methods across all settings of close range and long range. The code will be available soon. ",
        "title": "Stream Query Denoising for Vectorized HD Map Construction",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09115",
        "abstract_url": "http://arxiv.org/abs/2401.09115",
        "authors": [
            {
                "last_name": "Pulloquinga",
                "first_name": "Jose L."
            },
            {
                "last_name": "Escarabajal",
                "first_name": "Rafael J."
            },
            {
                "last_name": "Valera",
                "first_name": "Angel"
            },
            {
                "last_name": "Valles",
                "first_name": "Marina"
            },
            {
                "last_name": "Mata",
                "first_name": "Vicente"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Parallel robots (PRs) are closed-chain manipulators with diverse applications due to their accuracy and high payload. However, there are configurations within the workspace named Type II singularities where the PRs lose control of the end-effector movements. Type II singularities are a problem for applications where complete control of the end-effector is essential. Trajectory planning produces accurate movements of a PR by avoiding Type II singularities. Generally, singularity avoidance is achieved by optimising a geometrical path with a velocity profile considering singular configurations as obstacles. This research presents an algorithm that avoids Type II singularities by modifying the trajectory of a subset of the actuators. The subset of actuators represents the limbs responsible for a Type II singularity, and they are identified by the angle between two Output Twist Screws. The proposed avoidance algorithm does not require optimisation procedures, which reduces the computational cost for offline trajectory planning and makes it suitable for online trajectory planning. The avoidance algorithm is implemented in offline trajectory planning for a pick and place planar PR and a spatial knee rehabilitation PR ",
        "title": "A Type II Singularity Avoidance Algorithm for Parallel Manipulators  using Output Twist Screws",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09118",
        "abstract_url": "http://arxiv.org/abs/2401.09118",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yu"
            },
            {
                "last_name": "Cheng",
                "first_name": "Jin"
            },
            {
                "last_name": "Li",
                "first_name": "Tingyue"
            },
            {
                "last_name": "Miao",
                "first_name": "Yun"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  High-frequency issues have been remarkably challenges in numerical methods for partial differential equations. In this paper, a learning based numerical method (LbNM) is proposed for Helmholtz equation with high frequency. The main novelty is using Tikhonov regularization method to stably learn the solution operator by utilizing relevant information especially the fundamental solutions. Then applying the solution operator to a new boundary input could quickly update the solution. Based on the method of fundamental solutions and the quantitative Runge approximation, we give the error estimate. This indicates interpretability and generalizability of the present method. Numerical results validates the error analysis and demonstrates the high-precision and high-efficiency features. ",
        "title": "Learning based numerical methods for Helmholtz equation with high  frequency",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09124",
        "abstract_url": "http://arxiv.org/abs/2401.09124",
        "authors": [
            {
                "last_name": "Khatun",
                "first_name": "Mirza Akhi"
            },
            {
                "last_name": "Memon",
                "first_name": "Sanober Farheen"
            },
            {
                "last_name": "Eising",
                "first_name": "Ciar\u00e1n"
            },
            {
                "last_name": "Dhirani",
                "first_name": "Lubna Luxmi"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The Healthcare Internet-of-Things (H-IoT), commonly known as Digital Healthcare, is a data-driven infrastructure that highly relies on smart sensing devices (i.e., blood pressure monitors, temperature sensors, etc.) for faster response time, treatments, and diagnosis. However, with the evolving cyber threat landscape, IoT devices have become more vulnerable to the broader risk surface (e.g., risks associated with generative AI, 5G-IoT, etc.), which, if exploited, may lead to data breaches, unauthorized access, and lack of command and control and potential harm. This paper reviews the fundamentals of healthcare IoT, its privacy, and data security challenges associated with machine learning and H-IoT devices. The paper further emphasizes the importance of monitoring healthcare IoT layers such as perception, network, cloud, and application. Detecting and responding to anomalies involves various cyber-attacks and protocols such as Wi-Fi 6, Narrowband Internet of Things (NB-IoT), Bluetooth, ZigBee, LoRa, and 5G New Radio (5G NR). A robust authentication mechanism based on machine learning and deep learning techniques is required to protect and mitigate H-IoT devices from increasing cybersecurity vulnerabilities. Hence, in this review paper, security and privacy challenges and risk mitigation strategies for building resilience in H-IoT are explored and reported. ",
        "title": "Machine Learning for Healthcare-IoT Security: A Review and Risk  Mitigation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09125",
        "abstract_url": "http://arxiv.org/abs/2401.09125",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Junfu"
            },
            {
                "last_name": "Guo",
                "first_name": "Yuanfang"
            },
            {
                "last_name": "Yang",
                "first_name": "Liang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yunhong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\\sqrt{\\mathbb{E}\\left[\\operatorname{deg}\\right]}$, where $\\mathbb{E}\\left[\\operatorname{deg}\\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimental impact on separability, which is equivalent to degrading $\\mathbb{E}\\left[\\operatorname{deg}\\right]$. Finally, when applying multiple GC operations, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions. It indicates that the nodes still possess separability as $l$ goes to infinity in a wide range of regimes. Extensive experiments on both synthetic and real-world data verify the effectiveness of our theory. ",
        "title": "Understanding Heterophily for Graph Neural Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09126",
        "abstract_url": "http://arxiv.org/abs/2401.09126",
        "authors": [
            {
                "last_name": "Ummenhofer",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Agrawal",
                "first_name": "Sanskar"
            },
            {
                "last_name": "Sepulveda",
                "first_name": "Rene"
            },
            {
                "last_name": "Lao",
                "first_name": "Yixing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kai"
            },
            {
                "last_name": "Cheng",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Richter",
                "first_name": "Stephan"
            },
            {
                "last_name": "Wang",
                "first_name": "Shenlong"
            },
            {
                "last_name": "Ros",
                "first_name": "German"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Reconstructing an object from photos and placing it virtually in a new environment goes beyond the standard novel view synthesis task as the appearance of the object has to not only adapt to the novel viewpoint but also to the new lighting conditions and yet evaluations of inverse rendering methods rely on novel view synthesis data or simplistic synthetic datasets for quantitative analysis. This work presents a real-world dataset for measuring the reconstruction and rendering of objects for relighting. To this end, we capture the environment lighting and ground truth images of the same objects in multiple environments allowing to reconstruct the objects from images taken in one environment and quantify the quality of the rendered views for the unseen lighting environments. Further, we introduce a simple baseline composed of off-the-shelf methods and test several state-of-the-art methods on the relighting task and show that novel view synthesis is not a reliable proxy to measure performance. Code and dataset are available at https://github.com/isl-org/objects-with-lighting . ",
        "title": "Objects With Lighting: A Real-World Dataset for Evaluating  Reconstruction and Rendering for Object Relighting",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09127",
        "abstract_url": "http://arxiv.org/abs/2401.09127",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yifei"
            },
            {
                "last_name": "Gao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jingjing"
            },
            {
                "last_name": "He",
                "first_name": "Ziming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yunsheng"
            },
            {
                "last_name": "Lu",
                "first_name": "Chen"
            },
            {
                "last_name": "Xiao",
                "first_name": "Pei"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Motivated by the need for increased spectral efficiency and the proliferation of intelligent applications, the sixth-generation (6G) mobile network is anticipated to integrate the dual-functions of communication and sensing (C&S). Although the millimeter wave (mmWave) communication and mmWave radar share similar multiple-input multiple-output (MIMO) architecture for integration, the full potential of dual-function synergy remains to be exploited. In this paper, we commence by overviewing state-of-the-art schemes from the aspects of waveform design and signal processing. Nevertheless, these approaches face the dilemma of mutual compromise between C&S performance. To this end, we reveal and exploit the synergy between C&S. In the proposed framework, we introduce a two-stage frame structure and resort artificial intelligence (AI) to achieve the synergistic gain by designing a joint C&S channel semantic extraction and reconstruction network (JCASCasterNet). With just a cost-effective and energy-efficient single sensing antenna, the proposed scheme achieves enhanced overall performance while requiring only limited pilot and feedback signaling overhead. In the end, we outline the challenges that lie ahead in the future development of integrated sensing and communication networks, along with promising directions for further research. ",
        "title": "AI Empowered Channel Semantic Acquisition for 6G Integrated Sensing and  Communication Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09129",
        "abstract_url": "http://arxiv.org/abs/2401.09129",
        "authors": [
            {
                "last_name": "Praprotnik",
                "first_name": "Ada \u0160adl"
            },
            {
                "last_name": "Vavpeti\u010d",
                "first_name": "Ale\u0161"
            },
            {
                "last_name": "\u017dagar",
                "first_name": "Emil"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The optimal one-sided parametric polynomial approximants of a circular arc are considered. More precisely, the approximant must be entirely in or out of the underlying circle of an arc. The natural restriction to an arc's approximants interpolating boundary points is assumed. However, the study of approximants, which additionally interpolate corresponding tangent directions and curvatures at the boundary of an arc, is also considered. Several low-degree polynomial approximants are studied in detail. When several solutions fulfilling the interpolation conditions exist, the optimal one is characterized, and a numerical algorithm for its construction is suggested. Theoretical results are demonstrated with several numerical examples and a comparison with general (i.e. non-one-sided) approximants are provided. ",
        "title": "Optimal one-sided approximants of circular arc",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09132",
        "abstract_url": "http://arxiv.org/abs/2401.09132",
        "authors": [
            {
                "last_name": "Pulloquinga",
                "first_name": "Jose L."
            },
            {
                "last_name": "Escarabajal",
                "first_name": "Rafael J."
            },
            {
                "last_name": "Valles",
                "first_name": "Marina"
            },
            {
                "last_name": "Diaz-Rodriguez",
                "first_name": "Miguel"
            },
            {
                "last_name": "Mata",
                "first_name": "Vicente"
            },
            {
                "last_name": "Valera",
                "first_name": "Angel"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Rehabilitation tasks demand robust and accurate trajectory-tracking performance, mainly achieved with parallel robots. In this field, limiting the value of the force exerted on the patient is crucial, especially when an injured limb is involved. In human-robot interaction studies, the admittance controller modifies the location of the robot according to the user efforts driving the end-effector to an arbitrary location within the workspace. However, a parallel robot has singularities within the workspace, making implementing a conventional admittance controller unsafe. Thus, this study proposes an admittance controller that overcomes the limitations of singular configurations by using a real-time singularity avoidance algorithm. The singularity avoidance algorithm modifies the original trajectory based on the actual location of the parallel robot. The complemented admittance controller is applied to a 4 degrees of freedom parallel robot for knee rehabilitation. In this case, the actual location is measured by a 3D tracking system because the location calculated by the forward kinematics is inaccurate in the vicinity of a singularity. The experimental results verify the effectiveness of the proposed admittance controller for safe knee rehabilitation exercises ",
        "title": "Admittance Controller Complemented with Real-time Singularity Avoidance  for Rehabilitation Parallel Robots",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09133",
        "abstract_url": "http://arxiv.org/abs/2401.09133",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haowen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhao"
            },
            {
                "last_name": "Che",
                "first_name": "Zhengping"
            },
            {
                "last_name": "Qiao",
                "first_name": "Liang"
            },
            {
                "last_name": "Huang",
                "first_name": "Yakun"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Qiao",
                "first_name": "Xiuquan"
            },
            {
                "last_name": "Tang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Reconstructing real-world objects and estimating their movable joint structures are pivotal technologies within the field of robotics. Previous research has predominantly focused on supervised approaches, relying on extensively annotated datasets to model articulated objects within limited categories. However, this approach falls short of effectively addressing the diversity present in the real world. To tackle this issue, we propose a self-supervised interaction perception method, referred to as SM$^3$, which leverages multi-view RGB images captured before and after interaction to model articulated objects, identify the movable parts, and infer the parameters of their rotating joints. By constructing 3D geometries and textures from the captured 2D images, SM$^3$ achieves integrated optimization of movable part and joint parameters during the reconstruction process, obviating the need for annotations. Furthermore, we introduce the MMArt dataset, an extension of PartNet-Mobility, encompassing multi-view and multi-modal data of articulated objects spanning diverse categories. Evaluations demonstrate that SM$^3$ surpasses existing benchmarks across various categories and objects, while its adaptability in real-world scenarios has been thoroughly validated. ",
        "title": "SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images  for Articulated Objects",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09135",
        "abstract_url": "http://arxiv.org/abs/2401.09135",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Bo"
            },
            {
                "last_name": "Chhaparia",
                "first_name": "Rachita"
            },
            {
                "last_name": "Douillard",
                "first_name": "Arthur"
            },
            {
                "last_name": "Kale",
                "first_name": "Satyen"
            },
            {
                "last_name": "Rusu",
                "first_name": "Andrei A."
            },
            {
                "last_name": "Shen",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Szlam",
                "first_name": "Arthur"
            },
            {
                "last_name": "Ranzato",
                "first_name": "Marc'Aurelio"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time. ",
        "title": "Asynchronous Local-SGD Training for Language Modeling",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09140",
        "abstract_url": "http://arxiv.org/abs/2401.09140",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Min"
            },
            {
                "last_name": "Yang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Multi-perspective cameras with potentially non-overlapping fields of view have become an important exteroceptive sensing modality in a number of applications such as intelligent vehicles, drones, and mixed reality headsets. In this work, we challenge one of the basic assumptions made in these scenarios, which is that the multi-camera rig is rigid. More specifically, we are considering the problem of estimating the relative pose between a static non-rigid rig in different spatial orientations while taking into account the effect of gravity onto the system. The deformable physical connections between each camera and the body center are approximated by a simple cantilever model, and inserted into the generalized epipolar constraint. Our results lead us to the important insight that the latent parameters of the deformation model, meaning the gravity vector in both views, become observable. We present a concise analysis of the observability of all variables based on noise, outliers, and rig rigidity for two different algorithms. The first one is a vision-only alternative, while the second one makes use of additional gravity measurements. To conclude, we demonstrate the ability to sense gravity in a real-world example, and discuss practical implications. ",
        "title": "Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09145",
        "abstract_url": "http://arxiv.org/abs/2401.09145",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Ivan"
            },
            {
                "last_name": "Liu",
                "first_name": "Fangyuan"
            },
            {
                "last_name": "Zhong",
                "first_name": "Qi"
            },
            {
                "last_name": "Ma",
                "first_name": "Fei"
            },
            {
                "last_name": "Ni",
                "first_name": "Shiguang"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Multimodal emotion recognition techniques are increasingly essential for assessing mental states. Image-based methods, however, tend to focus predominantly on overt visual cues and often overlook subtler mental state changes. Psychophysiological research has demonstrated that HR and skin temperature are effective in detecting ANS activities, thereby revealing these subtle changes. However, traditional HR tools are generally more costly and less portable, while skin temperature analysis usually necessitates extensive manual processing. Advances in remote-PPG and automatic thermal ROI detection algorithms have been developed to address these issues, yet their accuracy in practical applications remains limited. This study aims to bridge this gap by integrating r-PPG with thermal imaging to enhance prediction performance. Ninety participants completed a 20-minute questionnaire to induce cognitive stress, followed by watching a film aimed at eliciting moral elevation. The results demonstrate that the combination of r-PPG and thermal imaging effectively detects emotional shifts. Using r-PPG alone, the prediction accuracy was 77% for cognitive stress and 61% for moral elevation, as determined by SVM. Thermal imaging alone achieved 79% accuracy for cognitive stress and 78% for moral elevation, utilizing a RF algorithm. An early fusion strategy of these modalities significantly improved accuracies, achieving 87% for cognitive stress and 83% for moral elevation using RF. Further analysis, which utilized statistical metrics and explainable machine learning methods including SHAP, highlighted key features and clarified the relationship between cardiac responses and facial temperature variations. Notably, it was observed that cardiovascular features derived from r-PPG models had a more pronounced influence in data fusion, despite thermal imaging's higher predictive accuracy in unimodal analysis. ",
        "title": "Your blush gives you away: detecting hidden mental states with remote  photoplethysmography and thermal imaging",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09146",
        "abstract_url": "http://arxiv.org/abs/2401.09146",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Hexiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Fengqi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Qianyu"
            },
            {
                "last_name": "Yi",
                "first_name": "Ran"
            },
            {
                "last_name": "Tan",
                "first_name": "Xin"
            },
            {
                "last_name": "Ma",
                "first_name": "Lizhuang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image animation aims to bring static images to life according to driving videos and create engaging visual content that can be used for various purposes such as animation, entertainment, and education. Recent unsupervised methods utilize affine and thin-plate spline transformations based on keypoints to transfer the motion in driving frames to the source image. However, limited by the expressive power of the transformations used, these methods always produce poor results when the gap between the motion in the driving frame and the source image is large. To address this issue, we propose to model motion from the source image to the driving frame in highly-expressive diffeomorphism spaces. Firstly, we introduce Continuous Piecewise-Affine based (CPAB) transformation to model the motion and present a well-designed inference algorithm to generate CPAB transformation from control keypoints. Secondly, we propose a SAM-guided keypoint semantic loss to further constrain the keypoint extraction process and improve the semantic consistency between the corresponding keypoints on the source and driving images. Finally, we design a structure alignment loss to align the structure-related features extracted from driving and generated images, thus helping the generator generate results that are more consistent with the driving action. Extensive experiments on four datasets demonstrate the effectiveness of our method against state-of-the-art competitors quantitatively and qualitatively. Code will be publicly available at: https://github.com/DevilPG/AAAI2024-CPABMM. ",
        "title": "Continuous Piecewise-Affine Based Motion Model for Image Animation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09149",
        "abstract_url": "http://arxiv.org/abs/2401.09149",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Qiaoling"
            },
            {
                "last_name": "Gu",
                "first_name": "Diandian"
            },
            {
                "last_name": "Wang",
                "first_name": "Guoteng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xun"
            },
            {
                "last_name": "Xiong",
                "first_name": "YingTong"
            },
            {
                "last_name": "Huang",
                "first_name": "Ting"
            },
            {
                "last_name": "Hu",
                "first_name": "Qinghao"
            },
            {
                "last_name": "Jin",
                "first_name": "Xin"
            },
            {
                "last_name": "Wen",
                "first_name": "Yonggang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianwei"
            },
            {
                "last_name": "Sun",
                "first_name": "Peng"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Large language models (LLMs) with long sequences begin to power more and more fundamentally new applications we use every day. Existing methods for long-sequence LLM training are neither efficient nor compatible with commonly-used training algorithms such as FlashAttention. We design Buff to address these issues. Buff decouples all of the sharding dimensions into a new hierarchical space, and systematically analyzes the memory and communication cost of LLM training. Then, it generates an effective hybrid parallelism strategy. We design a new selective overlap mechanism to mitigate the communication overhead introduced by the hybrid parallelism. We also implement memory management techniques to reduce GPU memory fragmentation. Evaluation results show that Buff generates parallelization strategies that match or outperform existing methods in model FLOPs utilization. ",
        "title": "InternEvo: Efficient Long-sequence Large Language Model Training via  Hybrid Parallelism and Redundant Sharding",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09150",
        "abstract_url": "http://arxiv.org/abs/2401.09150",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Feng"
            },
            {
                "last_name": "Wang",
                "first_name": "Kuang"
            },
            {
                "last_name": "Li",
                "first_name": "Haizhou"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the contemporary information era, significantly accelerated by the advent of Large-scale Language Models, the proliferation of scientific literature is reaching unprecedented levels. Researchers urgently require efficient tools for reading and summarizing academic papers, uncovering significant scientific literature, and employing diverse interpretative methodologies. To address this burgeoning demand, the role of automated scientific literature interpretation systems has become paramount. However, prevailing models, both commercial and open-source, confront notable challenges: they often overlook multimodal data, grapple with summarizing over-length texts, and lack diverse user interfaces. In response, we introduce an open-source multi-modal automated academic paper interpretation system (MMAPIS) with three-step process stages, incorporating LLMs to augment its functionality. Our system first employs the hybrid modality preprocessing and alignment module to extract plain text, and tables or figures from documents separately. It then aligns this information based on the section names they belong to, ensuring that data with identical section names are categorized under the same section. Following this, we introduce a hierarchical discourse-aware summarization method. It utilizes the extracted section names to divide the article into shorter text segments, facilitating specific summarizations both within and between sections via LLMs with specific prompts. Finally, we have designed four types of diversified user interfaces, including paper recommendation, multimodal Q\\&A, audio broadcasting, and interpretation blog, which can be widely applied across various scenarios. Our qualitative and quantitative evaluations underscore the system's superiority, especially in scientific summarization, where it outperforms solutions relying solely on GPT-4. ",
        "title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers  Interpretation System",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09160",
        "abstract_url": "http://arxiv.org/abs/2401.09160",
        "authors": [
            {
                "last_name": "Qu",
                "first_name": "Hao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lilian"
            },
            {
                "last_name": "Mao",
                "first_name": "Jun"
            },
            {
                "last_name": "Tie",
                "first_name": "Junbo"
            },
            {
                "last_name": "He",
                "first_name": "Xiaofeng"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaoping"
            },
            {
                "last_name": "Shi",
                "first_name": "Yifei"
            },
            {
                "last_name": "Chen",
                "first_name": "Changhao"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Unreliable feature extraction and matching in handcrafted features undermine the performance of visual SLAM in complex real-world scenarios. While learned local features, leveraging CNNs, demonstrate proficiency in capturing high-level information and excel in matching benchmarks, they encounter challenges in continuous motion scenes, resulting in poor generalization and impacting loop detection accuracy. To address these issues, we present DK-SLAM, a monocular visual SLAM system with adaptive deep local features. MAML optimizes the training of these features, and we introduce a coarse-to-fine feature tracking approach. Initially, a direct method approximates the relative pose between consecutive frames, followed by a feature matching method for refined pose estimation. To counter cumulative positioning errors, a novel online learning binary feature-based online loop closure module identifies loop nodes within a sequence. Experimental results underscore DK-SLAM's efficacy, outperforms representative SLAM solutions, such as ORB-SLAM3 on publicly available datasets. ",
        "title": "DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning,  Tracking and Loop-Closing",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09162",
        "abstract_url": "http://arxiv.org/abs/2401.09162",
        "authors": [
            {
                "last_name": "Mendes",
                "first_name": "Paulo"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Ubiquitous extended reality environments such as the Metaverse will have a significant impact on the Internet, which will evolve to interconnect a large number of mixed reality spaces. Currently, Metaverse development is related to the creation of mixed reality environments, not tackling the required networking functionalities. This article analyzes suitable networking design choices to support the Metaverse, proposing a new service-centric networking approach capable of incorporating low-latency data fetching, distributed computing, and fusion of heterogeneous data types over the Cloud-to-Thing continuum. ",
        "title": "Named Service Networking as a primer for the Metaverse",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09168",
        "abstract_url": "http://arxiv.org/abs/2401.09168",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Diefenbach",
                "first_name": "Dennis"
            },
            {
                "last_name": "Gourru",
                "first_name": "Antoine"
            },
            {
                "last_name": "Gravier",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The progress introduced by pre-trained language models and their fine-tuning has resulted in significant improvements in most downstream NLP tasks. The unsupervised training of a language model combined with further target task fine-tuning has become the standard QA fine-tuning procedure. In this work, we demonstrate that this strategy is sub-optimal for fine-tuning QA models, especially under a low QA annotation budget, which is a usual setting in practice due to the extractive QA labeling cost. We draw our conclusions by conducting an exhaustive analysis of the performance of the alternatives of the sequential fine-tuning strategy on different QA datasets. Based on the experiments performed, we observed that the best strategy to fine-tune the QA model in low-budget settings is taking a pre-trained language model (PLM) and then fine-tuning PLM with a dataset composed of the target dataset and SQuAD dataset. With zero extra annotation effort, the best strategy outperforms the standard strategy by 2.28% to 6.48%. Our experiments provide one of the first investigations on how to best fine-tune a QA system under a low budget and are therefore of the utmost practical interest to the QA practitioners. ",
        "title": "Fine-tuning Strategies for Domain Specific Question Answering under Low  Annotation Budget Constraints",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09175",
        "abstract_url": "http://arxiv.org/abs/2401.09175",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Defretiere",
                "first_name": "Clement"
            },
            {
                "last_name": "Diefenbach",
                "first_name": "Dennis"
            },
            {
                "last_name": "Gravier",
                "first_name": "Christophe"
            },
            {
                "last_name": "Gourru",
                "first_name": "Antoine"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Question Answering (QA) is increasingly used by search engines to provide results to their end-users, yet very few websites currently use QA technologies for their search functionality. To illustrate the potential of QA technologies for the website search practitioner, we demonstrate web searches that combine QA over knowledge graphs and QA over free text -- each being usually tackled separately. We also discuss the different benefits and drawbacks of both approaches for web site searches. We use the case studies made of websites hosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently from a search engine (e.g. Google, Bing, etc), the data are indexed integrally, i.e. we do not index only a subset, and they are indexed exclusively, i.e. we index only data available on the corresponding website. ",
        "title": "QAnswer: Towards Question Answering Search over Websites",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09176",
        "abstract_url": "http://arxiv.org/abs/2401.09176",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Liye"
            },
            {
                "last_name": "Li",
                "first_name": "Biaoshun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yihao"
            },
            {
                "last_name": "Lin",
                "first_name": "Mujie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shipeng"
            },
            {
                "last_name": "Li",
                "first_name": "Chenxin"
            },
            {
                "last_name": "Pang",
                "first_name": "Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Ling"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Antibody-drug conjugate (ADC) has revolutionized the field of cancer treatment in the era of precision medicine due to their ability to precisely target cancer cells and release highly effective drug. Nevertheless, the realization of rational design of ADC is very difficult because the relationship between their structures and activities is difficult to understand. In the present study, we introduce a unified deep learning framework called ADCNet to help design potential ADCs. The ADCNet highly integrates the protein representation learning language model ESM-2 and small-molecule representation learning language model FG-BERT models to achieve activity prediction through learning meaningful features from antigen and antibody protein sequences of ADC, SMILES strings of linker and payload, and drug-antibody ratio (DAR) value. Based on a carefully designed and manually tailored ADC data set, extensive evaluation results reveal that ADCNet performs best on the test set compared to baseline machine learning models across all evaluation metrics. For example, it achieves an average prediction accuracy of 87.12%, a balanced accuracy of 0.8689, and an area under receiver operating characteristic curve of 0.9293 on the test set. In addition, cross-validation, ablation experiments, and external independent testing results further prove the stability, advancement, and robustness of the ADCNet architecture. For the convenience of the community, we develop the first online platform (https://ADCNet.idruglab.cn) for the prediction of ADCs activity based on the optimal ADCNet model, and the source code is publicly available at https://github.com/idrugLab/ADCNet. ",
        "title": "ADCNet: a unified framework for predicting the activity of antibody-drug  conjugates",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09180",
        "abstract_url": "http://arxiv.org/abs/2401.09180",
        "authors": [
            {
                "last_name": "Antonio",
                "first_name": "Almud\u00e9var"
            },
            {
                "last_name": "Th\u00e9o",
                "first_name": "Mariotte"
            },
            {
                "last_name": "Alfonso",
                "first_name": "Ortega"
            },
            {
                "last_name": "Marie",
                "first_name": "Tahon"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the other one hardly contains any domain information. ",
        "title": "Unsupervised Multiple Domain Translation through Controlled  Disentanglement in Variational Autoencoder",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09181",
        "abstract_url": "http://arxiv.org/abs/2401.09181",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Junhao"
            },
            {
                "last_name": "Ma",
                "first_name": "Qianli"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhen"
            },
            {
                "last_name": "Wu",
                "first_name": "Binquan"
            },
            {
                "last_name": "Feng",
                "first_name": "Huawen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. Our experiments demonstrate that Fwd-Prompt achieves state-of-the-art performance while updating fewer parameters and requiring no old samples. Our research sheds light on the potential of continuously adapting MLLMs to new tasks under the instruction tuning paradigm and encourages future studies to explore MCIT. The code will soon be publicly available. ",
        "title": "Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with  Positive Forward Transfer",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09184",
        "abstract_url": "http://arxiv.org/abs/2401.09184",
        "authors": [
            {
                "last_name": "Datres",
                "first_name": "Massimiliano"
            },
            {
                "last_name": "Leonardi",
                "first_name": "Gian Paolo"
            },
            {
                "last_name": "Figalli",
                "first_name": "Alessio"
            },
            {
                "last_name": "Sutter",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets. ",
        "title": "A Two-Scale Complexity Measure for Deep Learning Models",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09185",
        "abstract_url": "http://arxiv.org/abs/2401.09185",
        "authors": [
            {
                "last_name": "Schulz-Rosengarten",
                "first_name": "Alexander"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Akash"
            },
            {
                "last_name": "Clement",
                "first_name": "Malte"
            },
            {
                "last_name": "von Hanxleden",
                "first_name": "Reinhard"
            },
            {
                "last_name": "Asch",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Lohstroh",
                "first_name": "Marten"
            },
            {
                "last_name": "Lee",
                "first_name": "Edward A."
            },
            {
                "last_name": "Araya",
                "first_name": "Gustavo Quiros"
            },
            {
                "last_name": "Shukla",
                "first_name": "Ankit"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  Behavior Trees (BTs) provide a lean set of control flow elements that are easily composable in a modular tree structure. They are well established for modeling the high-level behavior of non-player characters in computer games and recently gained popularity in other areas such as industrial automation. While BTs nicely express control, data handling aspects so far must be provided separately, e. g. in the form of blackboards. This may hamper reusability and can be a source of nondeterminism. We here present a dataflow extension to BTs that explicitly models data relations and communication. We provide a combined textual/graphical approach in line with modern, productivity-enhancing pragmatics-aware modeling techniques. We realized and validated that approach in the recently introduced polyglot coordination language Lingua Franca (LF). ",
        "title": "Behavior Trees with Dataflow: Coordinating Reactive Tasks in Lingua  Franca",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09186",
        "abstract_url": "http://arxiv.org/abs/2401.09186",
        "authors": [
            {
                "last_name": "Dougrez-Lewis",
                "first_name": "John D."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "SI"
        ],
        "abstract": "  Computer applications seeking to persist files remotely across the Internet are faced with a bewildering choice of mechanisms which tend to boil down to monolithic proprietary closed-source Vendor solutions. We introduce The Mikado Filesystem (mikfs), which provides an open simple lightweight interoperable portable extensible remote filesystem that is open source. mikfs consists of client applications accessing remote servers via RPC running over TCP/IP connections. mikfs is defined as a concrete set of API method calls over gRPC expressed in Google's Protocol Buffers' IDL. gRPC supports a wide variety of programming languages & platforms. For a given language + platform, the gRPC toolset can generate client- & server-side stubs from the IDL callable from client & server code in the selected languages, e.g., a client written in C# or java running on a Windows PC can access a server written in C++ running on Linux. mikfs consists of a virtual hierarchical tree of files & directories. This logical filesystem is not constrained to the limits and file naming conventions of the host's own physical native filesystem. API methods are provided for authentication; for atomic file-level operations on files & directories; for clients to register to receive notifications of file & directory changes on a server. The public API allows developers to write their own new servers and clients; allowing migration of hosted files between different implementations; extension with new methods & features; is Open Source code available for inspection and adaptation. gRPC provides secure authenticated connection & communication over HTTP/2; End-to-End Privacy & Security against eavesdropping of data in transit; support for multiple alternate user login mechanisms. mikfs is provided as source code, 'The Bootstrap Distribution', consisting of an ecosystem of clients, servers, tools and utilities. ",
        "title": "The Mikado Filesystem: An experimental RPC filesystem running over gRPC",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09190",
        "abstract_url": "http://arxiv.org/abs/2401.09190",
        "authors": [
            {
                "last_name": "Brahmi",
                "first_name": "Walid"
            },
            {
                "last_name": "Jdey",
                "first_name": "Imen"
            },
            {
                "last_name": "Drira",
                "first_name": "Fadoua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  In the field of dentistry, there is a growing demand for increased precision in diagnostic tools, with a specific focus on advanced imaging techniques such as computed tomography, cone beam computed tomography, magnetic resonance imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep learning has emerged as a pivotal tool in this context, enabling the implementation of automated segmentation techniques crucial for extracting essential diagnostic data. This integration of cutting-edge technology addresses the urgent need for effective management of dental conditions, which, if left undetected, can have a significant impact on human health. The impressive track record of deep learning across various domains, including dentistry, underscores its potential to revolutionize early detection and treatment of oral health issues. Objective: Having demonstrated significant results in diagnosis and prediction, deep convolutional neural networks (CNNs) represent an emerging field of multidisciplinary research. The goals of this study were to provide a concise overview of the state of the art, standardize the current debate, and establish baselines for future research. Method: In this study, a systematic literature review is employed as a methodology to identify and select relevant studies that specifically investigate the deep learning technique for dental imaging analysis. This study elucidates the methodological approach, including the systematic collection of data, statistical analysis, and subsequent dissemination of outcomes. Conclusion: This work demonstrates how Convolutional Neural Networks (CNNs) can be employed to analyze images, serving as effective tools for detecting dental pathologies. Although this research acknowledged some limitations, CNNs utilized for segmenting and categorizing teeth exhibited their highest level of performance overall. ",
        "title": "Exploring the Role of Convolutional Neural Networks (CNN) in Dental  Radiography Segmentation: A Comprehensive Systematic Literature Review",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09191",
        "abstract_url": "http://arxiv.org/abs/2401.09191",
        "authors": [
            {
                "last_name": "Trillos",
                "first_name": "Nicolas Garcia"
            },
            {
                "last_name": "Jacobs",
                "first_name": "Matt"
            },
            {
                "last_name": "Kim",
                "first_name": "Jakwang"
            },
            {
                "last_name": "Werenski",
                "first_name": "Matthew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Despite the success of deep learning-based algorithms, it is widely known that neural networks may fail to be robust. A popular paradigm to enforce robustness is adversarial training (AT), however, this introduces many computational and theoretical difficulties. Recent works have developed a connection between AT in the multiclass classification setting and multimarginal optimal transport (MOT), unlocking a new set of tools to study this problem. In this paper, we leverage the MOT connection to propose computationally tractable numerical algorithms for computing universal lower bounds on the optimal adversarial risk and identifying optimal classifiers. We propose two main algorithms based on linear programming (LP) and entropic regularization (Sinkhorn). Our key insight is that one can harmlessly truncate the higher order interactions between classes, preventing the combinatorial run times typically encountered in MOT problems. We validate these results with experiments on MNIST and CIFAR-$10$, which demonstrate the tractability of our approach. ",
        "title": "An Optimal Transport Approach for Computing Adversarial Training Lower  Bounds in Multiclass Classification",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09192",
        "abstract_url": "http://arxiv.org/abs/2401.09192",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Yu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ye"
            },
            {
                "last_name": "Yin",
                "first_name": "Yichun"
            },
            {
                "last_name": "Shi",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Xu",
                "first_name": "Zenglin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ming"
            },
            {
                "last_name": "Shang",
                "first_name": "Lifeng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Qun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\\textbf{a}res lessons for ex\\textbf{p}anding \\textbf{o}perations by \\textbf{l}earning high-\\textbf{l}ayer functi\\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even rivaling methods using pretrained models, making it a universal and efficient solution for training deep models while reducing time, financial, and environmental costs. ",
        "title": "Preparing Lessons for Progressive Training on Language Models",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09193",
        "abstract_url": "http://arxiv.org/abs/2401.09193",
        "authors": [
            {
                "last_name": "Bicciato",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Cosmo",
                "first_name": "Luca"
            },
            {
                "last_name": "Minello",
                "first_name": "Giorgia"
            },
            {
                "last_name": "Rossi",
                "first_name": "Luca"
            },
            {
                "last_name": "Torsello",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph neural networks are increasingly becoming the framework of choice for graph-based machine learning. In this paper, we propose a new graph neural network architecture that substitutes classical message passing with an analysis of the local distribution of node features. To this end, we extract the distribution of features in the egonet for each local neighbourhood and compare them against a set of learned label distributions by taking the histogram intersection kernel. The similarity information is then propagated to other nodes in the network, effectively creating a message passing-like mechanism where the message is determined by the ensemble of the features. We perform an ablation study to evaluate the network's performance under different choices of its hyper-parameters. Finally, we test our model on standard graph classification and regression benchmarks, and we find that it outperforms widely used alternative approaches, including both graph kernels and graph neural networks. ",
        "title": "GNN-LoFI: a Novel Graph Neural Network through Localized Feature-based  Histogram Intersection",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09195",
        "abstract_url": "http://arxiv.org/abs/2401.09195",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Su",
                "first_name": "Sitong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Junchen"
            },
            {
                "last_name": "Gao",
                "first_name": "Lianli"
            },
            {
                "last_name": "Song",
                "first_name": "Jingkuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The video composition task aims to integrate specified foregrounds and backgrounds from different videos into a harmonious composite. Current approaches, predominantly trained on videos with adjusted foreground color and lighting, struggle to address deep semantic disparities beyond superficial adjustments, such as domain gaps. Therefore, we propose a training-free pipeline employing a pre-trained diffusion model imbued with semantic prior knowledge, which can process composite videos with broader semantic disparities. Specifically, we process the video frames in a cascading manner and handle each frame in two processes with the diffusion model. In the inversion process, we propose Balanced Partial Inversion to obtain generation initial points that balance reversibility and modifiability. Then, in the generation process, we further propose Inter-Frame Augmented attention to augment foreground continuity across frames. Experimental results reveal that our pipeline successfully ensures the visual harmony and inter-frame coherence of the outputs, demonstrating efficacy in managing broader semantic disparities. ",
        "title": "Training-Free Semantic Video Composition via Pre-trained Diffusion Model",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09198",
        "abstract_url": "http://arxiv.org/abs/2401.09198",
        "authors": [
            {
                "last_name": "Steeven",
                "first_name": "Janny"
            },
            {
                "last_name": "Madiha",
                "first_name": "Nadri"
            },
            {
                "last_name": "Julie",
                "first_name": "Digne"
            },
            {
                "last_name": "Christian",
                "first_name": "Wolf"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial condition. Our practical implementation involves recurrent GNNs and a spatio-temporal attention observer capable of interpolating the solution at arbitrary locations. Our model not only generalizes to new initial conditions (as standard auto-regressive models do) but also performs evaluation at arbitrary space and time locations. We evaluate on three standard datasets in fluid dynamics and compare to strong baselines, which are outperformed both in classical settings and in the extended new task requiring continuous predictions. ",
        "title": "Space and Time Continuous Physics Simulation From Partial Observations",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09199",
        "abstract_url": "http://arxiv.org/abs/2401.09199",
        "authors": [
            {
                "last_name": "Ramadan",
                "first_name": "Qusai"
            },
            {
                "last_name": "Boukhers",
                "first_name": "Zeyd"
            },
            {
                "last_name": "AlShaikh",
                "first_name": "Muath"
            },
            {
                "last_name": "Lange",
                "first_name": "Christoph"
            },
            {
                "last_name": "J\u00fcrjens",
                "first_name": "Jan"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "DB"
        ],
        "abstract": "  Traditional data monetization approaches face challenges related to data protection and logistics. In response, digital data marketplaces have emerged as intermediaries simplifying data transactions. Despite the growing establishment and acceptance of digital data marketplaces, significant challenges hinder efficient data trading. As a result, few companies can derive tangible value from their data, leading to missed opportunities in understanding customers, pricing decisions, and fraud prevention. In this paper, we explore both technical and organizational challenges affecting data monetization. Moreover, we identify areas in need of further research, aiming to expand the boundaries of current knowledge by emphasizing where research is currently limited or lacking. ",
        "title": "Data Trading and Monetization: Challenges and Open Research Directions",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09200",
        "abstract_url": "http://arxiv.org/abs/2401.09200",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Jiyun"
            },
            {
                "last_name": "Yong",
                "first_name": "Sangeon"
            },
            {
                "last_name": "Kwon",
                "first_name": "Taegyun"
            },
            {
                "last_name": "Nam",
                "first_name": "Juhan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  The goal of real-time lyrics alignment is to take live singing audio as input and to pinpoint the exact position within given lyrics on the fly. The task can benefit real-world applications such as the automatic subtitling of live concerts or operas. However, designing a real-time model poses a great challenge due to the constraints of only using past input and operating within a minimal latency. Furthermore, due to the lack of datasets for real-time models for lyrics alignment, previous studies have mostly evaluated with private in-house datasets, resulting in a lack of standard evaluation methods. This paper presents a real-time lyrics alignment system for classical vocal performances with two contributions. First, we improve the lyrics alignment algorithm by finding an optimal combination of chromagram and phonetic posteriorgram (PPG) that capture melodic and phonetics features of the singing voice, respectively. Second, we recast the Schubert Winterreise Dataset (SWD) which contains multiple performance renditions of the same pieces as an evaluation set for the real-time lyrics alignment. ",
        "title": "A Real-Time Lyrics Alignment System Using Chroma And Phonetic Features  For Classical Vocal Performance",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09201",
        "abstract_url": "http://arxiv.org/abs/2401.09201",
        "authors": [
            {
                "last_name": "Krivulin",
                "first_name": "N."
            },
            {
                "last_name": "Garg",
                "first_name": "A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider a queueing model of a battery swapping and charging station (BSCS) for electric vehicles. We describe the dynamics of the model by a system of recurrence equations that involve random variables, and define a performance measure for the model as the mean operation cycle time of the station. Furthermore, the system of equations is represented in terms of tropical algebra in vector form as an implicit linear state dynamic equation. The performance measure takes the form of the mean growth rate of the state vector (the Lyapunov exponent). By applying methods and techniques of tropical algebra, we first transform the implicit equation into an explicit one with a state transition matrix with random entries, and then obtain the Lyapunov exponent as the limit of the expected value of norm of matrix products. We discuss application of the obtained result to evaluate the performance of one BSCS, and to find optimal distribution of batteries between stations in a network of BSCSs. ",
        "title": "Tropical modeling of battery swapping and charging station",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09202",
        "abstract_url": "http://arxiv.org/abs/2401.09202",
        "authors": [
            {
                "last_name": "H\u00f6rsch",
                "first_name": "Florian"
            },
            {
                "last_name": "Picasarri-Arrieta",
                "first_name": "Lucas"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  We consider two decomposition problems in directed graphs. We say that a digraph is $k$-bounded for some $k \\in \\mathbb{Z}_{\\geq 1}$ if each of its connected components contains at most $k$ arcs.   For the first problem, a directed linear forest is a collection of vertex-disjoint directed paths and we consider the problem of decomposing a given digraph into a $k$-bounded and an $\\ell$-bounded directed linear forest for some fixed $k,\\ell \\in \\mathbb{Z}_{\\geq 1}\\cup \\{\\infty\\}$. We give a full dichotomy for this problem by showing that it can be solved in polynomial time if $k+\\ell \\leq 3$ and is NP-complete otherwise. This answers a question of Campbell, H\\\"orsch, and Moore.   For the second problem, we say that an out-galaxy is a vertex-disjoint collection of out-stars. Again, we give a full dichotomy of when a given digraph can be edge-decomposed into a $k$-bounded and an $\\ell$-bounded out-galaxy for fixed $k,\\ell \\in \\mathbb{Z}_{\\geq 1}\\cup \\{\\infty\\}$. More precisely, we show that the problem can be solved in polynomial time if $\\min\\{k,\\ell\\}\\in \\{1,\\infty\\}$ and is NP-complete otherwise. ",
        "title": "Complexity results on the decomposition of a digraph into directed  linear forests and out-stars",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09204",
        "abstract_url": "http://arxiv.org/abs/2401.09204",
        "authors": [
            {
                "last_name": "Xavier",
                "first_name": "Bruno Missi"
            },
            {
                "last_name": "Dzaferagic",
                "first_name": "Merim"
            },
            {
                "last_name": "Vil\u00e0",
                "first_name": "Irene"
            },
            {
                "last_name": "Martinello",
                "first_name": "Magnos"
            },
            {
                "last_name": "Ruffini",
                "first_name": "Marco"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  Only the chairs can edit In the fight against cyber attacks, Network Softwarization (NS) is a flexible and adaptable shield, using advanced software to spot malicious activity in regular network traffic. However, the availability of comprehensive datasets for mobile networks, which are fundamental for the development of Machine Learning (ML) solutions for attack detection near their source, is still limited. Cross-Domain Artificial Intelligence (AI) can be the key to address this, although its application in Open Radio Access Network (O-RAN) is still at its infancy. To address these challenges, we deployed an end-to-end O-RAN network, that was used to collect data from the RAN and the transport network. These datasets allow us to combine the knowledge from an in-network ML traffic classifier for attack detection to bolster the training of an ML-based traffic classifier specifically tailored for the RAN. Our results demonstrate the potential of the proposed approach, achieving an accuracy rate of 93%. This approach not only bridges critical gaps in mobile network security but also showcases the potential of cross-domain AI in enhancing the efficacy of network security measures. ",
        "title": "Cross-Domain AI for Early Attack Detection and Defense Against Malicious  Flows in O-RAN",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09207",
        "abstract_url": "http://arxiv.org/abs/2401.09207",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Yihan"
            },
            {
                "last_name": "Wheeldon",
                "first_name": "Adrian"
            },
            {
                "last_name": "Mughal",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Agwa",
                "first_name": "Shady"
            },
            {
                "last_name": "Prodromakis",
                "first_name": "Themis"
            },
            {
                "last_name": "Serb",
                "first_name": "Alexantrou"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Content addressable memory is popular in the field of intelligent computing systems with its searching nature. Emerging CAMs show a promising increase in pixel density and a decrease in power consumption than pure CMOS solutions. This article introduced an energy-efficient 3T1R1C TCAM cooperating with capacitor dividers and RRAM devices. The RRAM as a storage element also acts as a switch to the capacitor divider while searching for content. CAM cells benefit from working parallel in an array structure. We implemented a 64 x 64 array and digital controllers to perform with an internal built-in clock frequency of 875MHz. Both data searches and reads take 3x clock cycles. Its worst average energy for data match is reported to be 1.71 fJ/bit-search and the worst average energy for data miss is found with 4.69 fJ/bit-search. The prototype is simulated and fabricated in 0.18 um technology with in-lab RRAM post-processing. Such memory explores the charge domain searching mechanism and can be applied to data centers that are power-hungry. ",
        "title": "An Energy-efficient Capacitive-Memristive Content Addressable Memory",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09209",
        "abstract_url": "http://arxiv.org/abs/2401.09209",
        "authors": [
            {
                "last_name": "Lepipas",
                "first_name": "Anastasios"
            },
            {
                "last_name": "Borovykh",
                "first_name": "Anastasia"
            },
            {
                "last_name": "Demetriou",
                "first_name": "Soteris"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SI"
        ],
        "abstract": "  Adversaries have been targeting unique identifiers to launch typo-squatting, mobile app squatting and even voice squatting attacks. Anecdotal evidence suggest that online social networks (OSNs) are also plagued with accounts that use similar usernames. This can be confusing to users but can also be exploited by adversaries. However, to date no study characterizes this problem on OSNs. In this work, we define the username squatting problem and design the first multi-faceted measurement study to characterize it on X. We develop a username generation tool (UsernameCrazy) to help us analyze hundreds of thousands of username variants derived from celebrity accounts. Our study reveals that thousands of squatted usernames have been suspended by X, while tens of thousands that still exist on the network are likely bots. Out of these, a large number share similar profile pictures and profile names to the original account signalling impersonation attempts. We found that squatted accounts are being mentioned by mistake in tweets hundreds of thousands of times and are even being prioritized in searches by the network's search recommendation algorithm exacerbating the negative impact squatted accounts can have in OSNs. We use our insights and take the first step to address this issue by designing a framework (SQUAD) that combines UsernameCrazy with a new classifier to efficiently detect suspicious squatted accounts. Our evaluation of SQUAD's prototype implementation shows that it can achieve 94% F1-score when trained on a small dataset. ",
        "title": "Username Squatting on Online Social Networks: A Study on X",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09210",
        "abstract_url": "http://arxiv.org/abs/2401.09210",
        "authors": [
            {
                "last_name": "Pera",
                "first_name": "Arianna"
            },
            {
                "last_name": "Aiello",
                "first_name": "Luca Maria"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Narratives can be powerful tools for inspiring action on pressing societal issues such as climate change. While social science theories offer frameworks for understanding the narratives that arise within collective movements, these are rarely applied to the vast data available from social media platforms, which play a significant role in shaping public opinion and mobilizing collective action. This gap in the empirical evaluation of online narratives limits our understanding of their relationship with public response. In this study, we focus on plant-based diets as a form of pro-environmental action and employ natural language processing to operationalize a theoretical framework of moral narratives specific to the vegan movement. We apply this framework to narratives found in YouTube videos promoting environmental initiatives such as Veganuary, Meatless March, and No Meat May. Our analysis reveals that several narrative types, as defined by the theory, are empirically present in the data. To identify narratives with the potential to elicit positive public engagement, we used text processing to estimate the proportion of comments supporting collective action across narrative types. Video narratives advocating social fight, whether through protest or through efforts to convert others to the cause, are associated with a stronger sense of collective action in the respective comments. These narrative types also demonstrate increased semantic coherence and alignment between the message and public response, markers typically associated with successful collective action. Our work offers new insights into the complex factors that influence the emergence of collective action, thereby informing the development of effective communication strategies within social movements. ",
        "title": "Narratives of Collective Action in YouTube's Discourse on Veganism",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09216",
        "abstract_url": "http://arxiv.org/abs/2401.09216",
        "authors": [
            {
                "last_name": "Krivulin",
                "first_name": "N."
            },
            {
                "last_name": "Gubanov",
                "first_name": "S."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  New solutions for problems in optimal scheduling of activities in a project under temporal constraints are developed in the framework of tropical algebra, which deals with the theory and application of algebraic systems with idempotent operations. We start with a constrained tropical optimization problem that has an objective function represented as a vector form given by an arbitrary matrix, and that can be solved analytically in a closed but somewhat complicated form. We examine a special case of the problem when the objective function is given by a matrix of unit rank, and show that the solution can be sufficiently refined in this case, which results in an essentially simplified analytical form and reduced computational complexity of the solution. We exploit the obtained result to find complete solutions of project scheduling problems to minimize the project makespan and the maximum absolute deviation of start times of activities under temporal constraints. The constraint under consideration include ``start-start'', ``start-finish'' and ``finish-start'' precedence relations, release times, release deadlines and completion deadlines for activities. As an application, we consider optimal scheduling problems of a vaccination project in a medical centre. ",
        "title": "Algebraic solution of project scheduling problems with temporal  constraints",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09217",
        "abstract_url": "http://arxiv.org/abs/2401.09217",
        "authors": [
            {
                "last_name": "Plabst",
                "first_name": "Daniel"
            },
            {
                "last_name": "Prinz",
                "first_name": "Tobias"
            },
            {
                "last_name": "Diedolo",
                "first_name": "Francesca"
            },
            {
                "last_name": "Wiegart",
                "first_name": "Thomas"
            },
            {
                "last_name": "B\u00f6cherer",
                "first_name": "Georg"
            },
            {
                "last_name": "Hanik",
                "first_name": "Norbert"
            },
            {
                "last_name": "Kramer",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Neural networks (NNs) inspired by the forward-backward algorithm (FBA) are used as equalizers for bandlimited channels with a memoryless nonlinearity. The NN-equalizers are combined with successive interference cancellation (SIC) to approach the information rates of joint detection and decoding (JDD) with considerably less complexity than JDD and other existing equalizers. Simulations for short-haul optical fiber links with square-law detection illustrate the gains of NNs as compared to the complexity-limited FBA and Gibbs sampling. ",
        "title": "Neural Network Equalizers and Successive Interference Cancellation for  Bandlimited Channels with a Nonlinearity",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09218",
        "abstract_url": "http://arxiv.org/abs/2401.09218",
        "authors": [
            {
                "last_name": "Shpilrain",
                "first_name": "Vladimir"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  In this survey, we address the worst-case, average-case, and generic-case time complexity of the word problem and some other algorithmic problems in several classes of groups and show that it is often the case that the average-case complexity of the word problem is linear with respect to the length of an input word, which is as good as it gets if one considers groups given by generators and defining relations. At the same time, there are other natural algorithmic problems, for instance, the geodesic (decision) problem or Whitehead's automorphism problem, where the average-case time complexity can be sublinear, even constant. ",
        "title": "Complexity of some algorithmic problems in groups: a survey",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09220",
        "abstract_url": "http://arxiv.org/abs/2401.09220",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Kai"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Lin",
                "first_name": "Weihong"
            },
            {
                "last_name": "Zhong",
                "first_name": "Zhuoyao"
            },
            {
                "last_name": "Sun",
                "first_name": "Lei"
            },
            {
                "last_name": "Huo",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Existing methods for Visual Information Extraction (VIE) from form-like documents typically fragment the process into separate subtasks, such as key information extraction, key-value pair extraction, and choice group extraction. However, these approaches often overlook the hierarchical structure of form documents, including hierarchical key-value pairs and hierarchical choice groups. To address these limitations, we present a new perspective, reframing VIE as a relation prediction problem and unifying labels of different tasks into a single label space. This unified approach allows for the definition of various relation types and effectively tackles hierarchical relationships in form-like documents. In line with this perspective, we present UniVIE, a unified model that addresses the VIE problem comprehensively. UniVIE functions using a coarse-to-fine strategy. It initially generates tree proposals through a tree proposal network, which are subsequently refined into hierarchical trees by a relation decoder module. To enhance the relation prediction capabilities of UniVIE, we incorporate two novel tree constraints into the relation decoder: a tree attention mask and a tree level embedding. Extensive experimental evaluations on both our in-house dataset HierForms and a publicly available dataset SIBR, substantiate that our method achieves state-of-the-art results, underscoring the effectiveness and potential of our unified approach in advancing the field of VIE. ",
        "title": "UniVIE: A Unified Label Space Approach to Visual Information Extraction  from Form-like Documents",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09221",
        "abstract_url": "http://arxiv.org/abs/2401.09221",
        "authors": [
            {
                "last_name": "Zadok",
                "first_name": "Yair"
            },
            {
                "last_name": "Voloch",
                "first_name": "Nadav"
            },
            {
                "last_name": "Voloch-Bloch",
                "first_name": "Noa"
            },
            {
                "last_name": "Hajaj",
                "first_name": "Maor Meir"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Using well-known mathematical problems for encryption is a widely used technique because they are computationally hard and provide security against potential attacks on the encryption method. The subset sum problem (SSP) can be defined as finding a subset of integers from a given set, whose sum is equal to a specified integer. The classic SSP has various variants, one of which is the multiple-subset problem (MSSP). In the MSSP, the goal is to select items from a given set and distribute them among multiple bins, en-suring that the capacity of each bin is not exceeded while maximizing the total weight of the selected items. This approach addresses a related problem with a different perspective. Here a related different kind of problem is approached: given a set of sets A={A1, A2..., An}, find an integer s for which every subset of the given sets is summed up to, if such an integer exists. The problem is NP-complete when considering it as a variant of SSP. However, there exists an algorithm that is relatively efficient for known pri-vate keys. This algorithm is based on dispensing non-relevant values of the potential sums. In this paper we present the encryption scheme based on MSSP and present its novel usage and implementation in communication. ",
        "title": "Multiple Subset Problem as an encryption scheme for communication",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09229",
        "abstract_url": "http://arxiv.org/abs/2401.09229",
        "authors": [
            {
                "last_name": "Richter",
                "first_name": "Hendrik"
            },
            {
                "last_name": "Thomson",
                "first_name": "Sarah L."
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  We propose a new way of looking at local optima networks (LONs). LONs represent fitness landscapes; the nodes are local optima, and the edges are search transitions between them. Many metrics computed on LONs have been proposed and shown to be linked to metaheuristic search difficulty. These have typically considered LONs as describing static structures. In contrast to this, Laplacian dynamics (LD) is an approach to consider the information flow across a network as a dynamical process. We adapt and apply LD to the context of LONs. As a testbed, we consider instances from the quadratic assignment problem (QAP) library. Metrics related to LD are proposed and these are compared with existing LON metrics. The results show that certain LD metrics are strong predictors of metaheuristic performance for iterated local search and tabu search. ",
        "title": "Information flow and Laplacian dynamics on local optima networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09231",
        "abstract_url": "http://arxiv.org/abs/2401.09231",
        "authors": [
            {
                "last_name": "Neto",
                "first_name": "Augusto"
            },
            {
                "last_name": "Cerqueira",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Curado",
                "first_name": "Marilia"
            },
            {
                "last_name": "Monteiro",
                "first_name": "Edmundo"
            },
            {
                "last_name": "Mendes",
                "first_name": "Paulo"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The great demand for real-time multimedia sessions encompassing groups of users (multi-user), associated with the limitations of the current Internet in providing quality assurance, has raised challenges for defining the best mechanisms to deploy the Next Generation of Networks (NGN). There is a consensus that an efficient and scalable provisioning of network resources is crucial for the success of the NGN, mainly in what concerns access networks. Previous solutions for the control of multi-user sessions rely mostly on uncoordinated actions to allocate per-flow bandwidth and multicast trees. This paper introduces a Multiuser Aggregated Resource Allocation mechanism (MARA) that coordinates the control of class-based bandwidth and multicast resources in a scalable manner. In comparison with previous work, MARA significantly reduces signaling, state and processing overhead. The performance benefits of MARA are analyzed though simulations, which successfully demonstrated the significant optimization in the network performance. ",
        "title": "Scalable Resource Provisioning for Multi-user Communications in Next  Generation Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09232",
        "abstract_url": "http://arxiv.org/abs/2401.09232",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shunchi"
            },
            {
                "last_name": "Hu",
                "first_name": "Kai"
            },
            {
                "last_name": "Ma",
                "first_name": "Chixiang"
            },
            {
                "last_name": "Zhong",
                "first_name": "Zhuoyao"
            },
            {
                "last_name": "Sun",
                "first_name": "Lei"
            },
            {
                "last_name": "Huo",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Contextual Text Block Detection (CTBD) is the task of identifying coherent text blocks within the complexity of natural scenes. Previous methodologies have treated CTBD as either a visual relation extraction challenge within computer vision or as a sequence modeling problem from the perspective of natural language processing. We introduce a new framework that frames CTBD as a graph generation problem. This methodology consists of two essential procedures: identifying individual text units as graph nodes and discerning the sequential reading order relationships among these units as graph edges. Leveraging the cutting-edge capabilities of DQ-DETR for node detection, our framework innovates further by integrating a novel mechanism, a Dynamic Relation Transformer (DRFormer), dedicated to edge generation. DRFormer incorporates a dual interactive transformer decoder that deftly manages a dynamic graph structure refinement process. Through this iterative process, the model systematically enhances the graph's fidelity, ultimately resulting in improved precision in detecting contextual text blocks. Comprehensive experimental evaluations conducted on both SCUT-CTW-Context and ReCTS-Context datasets substantiate that our method achieves state-of-the-art results, underscoring the effectiveness and potential of our graph generation framework in advancing the field of CTBD. ",
        "title": "Dynamic Relation Transformer for Contextual Text Block Detection",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09234",
        "abstract_url": "http://arxiv.org/abs/2401.09234",
        "authors": [
            {
                "last_name": "Sarriguren",
                "first_name": "Alfredo Go\u00f1i"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  SARRIGUREN, a new complete algorithm for SAT based on counting clauses (which is valid also for Unique-SAT and #SAT) is described, analyzed and tested. Although existing complete algorithms for SAT perform slower with clauses with many literals, that is an advantage for SARRIGUREN, because the more literals are in the clauses the bigger is the probability of overlapping among clauses, a property that makes the clause counting process more efficient. Actually, it provides a $O(m^2 \\times n/k)$ time complexity for random $k$-SAT instances of $n$ variables and $m$ relatively dense clauses, where that density level is relative to the number of variables $n$, that is, clauses are relatively dense when $k\\geq7\\sqrt{n}$. Although theoretically there could be worst-cases with exponential complexity, the probability of those cases to happen in random $k$-SAT with relatively dense clauses is practically zero. The algorithm has been empirically tested and that polynomial time complexity maintains also for $k$-SAT instances with less dense clauses ($k\\geq5\\sqrt{n}$). That density could, for example, be of only 0.049 working with $n=20000$ variables and $k=989$ literals. In addition, they are presented two more complementary algorithms that provide the solutions to $k$-SAT instances and valuable information about number of solutions for each literal. Although this algorithm does not solve the NP=P problem (it is not a polynomial algorithm for 3-SAT), it broads the knowledge about that subject, because $k$-SAT with $k>3$ and dense clauses is not harder than 3-SAT. Moreover, the Python implementation of the algorithms, and all the input datasets and obtained results in the experiments are made available. ",
        "title": "SARRIGUREN: a polynomial-time complete algorithm for random $k$-SAT with  relatively dense clauses",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09235",
        "abstract_url": "http://arxiv.org/abs/2401.09235",
        "authors": [
            {
                "last_name": "Pacini",
                "first_name": "Marco"
            },
            {
                "last_name": "Dong",
                "first_name": "Xiaowen"
            },
            {
                "last_name": "Lepri",
                "first_name": "Bruno"
            },
            {
                "last_name": "Santin",
                "first_name": "Gabriele"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Equivariant neural networks have shown improved performance, expressiveness and sample complexity on symmetrical domains. But for some specific symmetries, representations, and choice of coordinates, the most common point-wise activations, such as ReLU, are not equivariant, hence they cannot be employed in the design of equivariant neural networks. The theorem we present in this paper describes all possible combinations of finite-dimensional representations, choice of coordinates and point-wise activations to obtain an exactly equivariant layer, generalizing and strengthening existing characterizations. Notable cases of practical relevance are discussed as corollaries. Indeed, we prove that rotation-equivariant networks can only be invariant, as it happens for any network which is equivariant with respect to connected compact groups. Then, we discuss implications of our findings when applied to important instances of exactly equivariant networks. First, we completely characterize permutation equivariant networks such as Invariant Graph Networks with point-wise nonlinearities and their geometric counterparts, highlighting a plethora of models whose expressive power and performance are still unknown. Second, we show that feature spaces of disentangled steerable convolutional neural networks are trivial representations. ",
        "title": "A Characterization Theorem for Equivariant Networks with Point-wise  Activations",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09237",
        "abstract_url": "http://arxiv.org/abs/2401.09237",
        "authors": [
            {
                "last_name": "Rathjens",
                "first_name": "Jan"
            },
            {
                "last_name": "Wiskott",
                "first_name": "Laurenz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Predictive coding-inspired deep networks for visual computing integrate classification and reconstruction processes in shared intermediate layers. Although synergy between these processes is commonly assumed, it has yet to be convincingly demonstrated. In this study, we take a critical look at how classifying and reconstructing interact in deep learning architectures. Our approach utilizes a purposefully designed family of model architectures reminiscent of autoencoders, each equipped with an encoder, a decoder, and a classification head featuring varying modules and complexities. We meticulously analyze the extent to which classification- and reconstruction-driven information can seamlessly coexist within the shared latent layer of the model architectures. Our findings underscore a significant challenge: Classification-driven information diminishes reconstruction-driven information in intermediate layers' shared representations and vice versa. While expanding the shared representation's dimensions or increasing the network's complexity can alleviate this trade-off effect, our results challenge prevailing assumptions in predictive coding and offer guidance for future iterations of predictive coding concepts in deep networks. ",
        "title": "Classification and Reconstruction Processes in Deep Predictive Coding  Networks: Antagonists or Allies?",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09239",
        "abstract_url": "http://arxiv.org/abs/2401.09239",
        "authors": [
            {
                "last_name": "Reyzabal",
                "first_name": "Mikel De Iturrate"
            },
            {
                "last_name": "Chen",
                "first_name": "Mingcong"
            },
            {
                "last_name": "Huang",
                "first_name": "Wei"
            },
            {
                "last_name": "Ourselin",
                "first_name": "Sebastien"
            },
            {
                "last_name": "Liu",
                "first_name": "Hongbin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Precisely determining the contact force during safe interaction in Minimally Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired by post-operative qualitative analysis from surgical videos, the use of cross-modality data driven deep neural network models has been one of the newest approaches to predict sensorless force trends. However, these methods required for large and variable datasets which are not currently available. In this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft environments for the training of deep neural models. In order to reduce the bias from a single dataset, we present a pipeline to generalize different vision and state data inputs for mixed dataset training, using a previously validated dataset with different setup. Finally, we present a variable encoder-decoder architecture to predict the forces done by the laparoscopic tool using single input or sequence of inputs. For input sequence, we use a recurrent decoder, named with the prefix R, and a new temporal sampling to represent the acceleration of the tool. During our training, we demonstrate that single dataset training tends to overfit to the training data domain, but has difficulties on translating the results across new domains. However, dataset mixing presents a good translation with a mean relative estimated force error of 5% and 12% for the recurrent and non-recurrent models respectively. Our method, also marginally increase the effectiveness of transformers for force estimation up to a maximum of ~15%, as the volume of available data is increase by 150%. In conclusion, we demonstrate that mixing experimental set ups for vision-state force estimation in MIRS is a possible approach towards the general solution of the problem. ",
        "title": "DaFoEs: Mixing Datasets towards the generalization of vision-state  deep-learning Force Estimation in Minimally Invasive Robotic Surgery",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09240",
        "abstract_url": "http://arxiv.org/abs/2401.09240",
        "authors": [
            {
                "last_name": "Ramahlosi",
                "first_name": "MN"
            },
            {
                "last_name": "Madani",
                "first_name": "Y"
            },
            {
                "last_name": "Akanbi",
                "first_name": "A"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "DC",
            "NI"
        ],
        "abstract": "  In our digital world, access to personal and public data has become an item of concern, with challenging security and privacy aspects. Modern information systems are heterogeneous in nature and have an inherent security vulnerability, which is susceptible to data interception and data modification due to unsecured communication data pipelines between connected endpoints. This re-search article presents a blockchain-based model for securing data pipelines in a heterogeneous information system using an integrated multi-hazard early warning system (MHEWS) as a case study. The proposed model utilizes the inherent security features of blockchain technology to address the security and privacy concerns that arise in data pipelines. The model is designed to ensure data integrity, confidentiality, and authenticity in a decentralized manner. The model is evaluated in a hybrid environment using a prototype implementation and simulation experiments with outcomes that demonstrate advantages over traditional approaches for a tamper-proof and immutable data pipeline for data authenticity and integrity using a confidential ledger. ",
        "title": "A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous  Information System",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09241",
        "abstract_url": "http://arxiv.org/abs/2401.09241",
        "authors": [
            {
                "last_name": "Trevisan",
                "first_name": "Elia"
            },
            {
                "last_name": "Alonso-Mora",
                "first_name": "Javier"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Motion planning for autonomous robots in human-populated environments poses numerous challenges due to uncertainties in the robot's dynamics, environment, and interaction with other agents. Sampling-based MPC approaches, such as Model Predictive Path Integral (MPPI) control, have shown promise in addressing these complex motion planning problems. However, the performance of MPPI heavily relies on the choice of the sampling distribution. Existing literature often uses the previously computed input sequence as the mean of a Gaussian distribution for sampling, leading to potential failures and local minima. In this paper, we propose novel derivations of the MPPI method to enhance its efficiency, robustness, and convergence. Our approach includes a mathematical formulation allowing for arbitrary sampling distributions, addressing numerical issues, and alleviating the problem of local minima. We present an efficient importance sampling scheme that combines classical and learning-based ancillary controllers simultaneously, resulting in more informative sampling and control fusion. We demonstrate our proposed scheme's superior efficiency and robustness through experiments by handling model uncertainties, rapid environmental changes and reducing susceptibility to local minima. ",
        "title": "Biased-MPPI: Informing Sampling-Based Model Predictive Control by Fusing  Ancillary Controllers",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09242",
        "abstract_url": "http://arxiv.org/abs/2401.09242",
        "authors": [
            {
                "last_name": "Haller",
                "first_name": "Elena"
            },
            {
                "last_name": "Sidorenko",
                "first_name": "Galina"
            },
            {
                "last_name": "Amador",
                "first_name": "Oscar"
            },
            {
                "last_name": "Nilsson",
                "first_name": "Emil"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  V2X communications are nowadays performed at 5.9\\,GHz spectrum, either using WiFi-based or Cellular technology. The channel capacity is limited, and congestion control regulates the number of messages that can enter the medium. With user rate growing, overloading becomes a factor that might affect road safety and traffic efficiency. The present paper evaluates the potential of using Radar-Based Communication (RadCom) for offloading the V2X spectrum. We consider a heavy-duty vehicle (HDV) platooning scenario as a case of maneuver coordination where local messages are transmitted by means of RadCom at different penetration rates. Simulations show significant improvements in channel occupation and network reliability. As a result, RadCom allows for shorter safe and energy efficient inter-vehicle distances. ",
        "title": "Offloading platooning applications from 5.9\\,GHz V2X to Radar  Communications: effects on safety and efficiency",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09243",
        "abstract_url": "http://arxiv.org/abs/2401.09243",
        "authors": [
            {
                "last_name": "Mani",
                "first_name": "Sabariswaran"
            },
            {
                "last_name": "Chandra",
                "first_name": "Abhranil"
            },
            {
                "last_name": "Venkataraman",
                "first_name": "Sreyas"
            },
            {
                "last_name": "Rizvi",
                "first_name": "Adyan"
            },
            {
                "last_name": "Sirvi",
                "first_name": "Yash"
            },
            {
                "last_name": "Bhattacharya",
                "first_name": "Soumojit"
            },
            {
                "last_name": "Hazra",
                "first_name": "Aritra"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned representations. Goal state conditioning and mapping to transitions resulted in a minute increase in the success rate and mean-reward. As for the agent policy, we developed DiffClone, a behaviour cloning agent improved using conditional diffusion. ",
        "title": "DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven  Policy Learning",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09244",
        "abstract_url": "http://arxiv.org/abs/2401.09244",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Aiqi"
            },
            {
                "last_name": "Zubiaga",
                "first_name": "Arkaitz"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The growing prevalence and rapid evolution of offensive language in social media amplify the complexities of detection, particularly highlighting the challenges in identifying such content across diverse languages. This survey presents a systematic and comprehensive exploration of Cross-Lingual Transfer Learning (CLTL) techniques in offensive language detection in social media. Our study stands as the first holistic overview to focus exclusively on the cross-lingual scenario in this domain. We analyse 67 relevant papers and categorise these studies across various dimensions, including the characteristics of multilingual datasets used, the cross-lingual resources employed, and the specific CLTL strategies implemented. According to \"what to transfer\", we also summarise three main CLTL transfer approaches: instance, feature, and parameter transfer. Additionally, we shed light on the current challenges and future research opportunities in this field. Furthermore, we have made our survey resources available online, including two comprehensive tables that provide accessible references to the multilingual datasets and CLTL methods used in the reviewed literature. ",
        "title": "Cross-lingual Offensive Language Detection: A Systematic Review of  Datasets, Transfer Approaches and Challenges",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09245",
        "abstract_url": "http://arxiv.org/abs/2401.09245",
        "authors": [
            {
                "last_name": "K\u00fcchler",
                "first_name": "Jan"
            },
            {
                "last_name": "Kr\u00f6ll",
                "first_name": "Daniel"
            },
            {
                "last_name": "Schoenen",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Witte",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep neural network models for image segmentation can be a powerful tool for the automation of motor claims handling processes in the insurance industry. A crucial aspect is the reliability of the model outputs when facing adverse conditions, such as low quality photos taken by claimants to document damages. We explore the use of a meta-classification model to assess the precision of segments predicted by a model trained for the semantic segmentation of car body parts. Different sets of features correlated with the quality of a segment are compared, and an AUROC score of 0.915 is achieved for distinguishing between high- and low-quality segments. By removing low-quality segments, the average mIoU of the segmentation output is improved by 16 percentage points and the number of wrongly predicted segments is reduced by 77%. ",
        "title": "Uncertainty estimates for semantic segmentation: providing enhanced  reliability for automated motor claims handling",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09248",
        "abstract_url": "http://arxiv.org/abs/2401.09248",
        "authors": [
            {
                "last_name": "Petrak",
                "first_name": "Dominic"
            },
            {
                "last_name": "Tran",
                "first_name": "Thy Thy"
            },
            {
                "last_name": "Gurevych",
                "first_name": "Iryna"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  The success of task-oriented and document-grounded dialogue systems depends on users accepting and enjoying using them. To achieve this, recently published work in the field of Human-Computer Interaction suggests that the combination of considering demographic information, user emotions and learning from the implicit feedback in their utterances, is particularly important. However, these findings have not yet been transferred to the field of Natural Language Processing, where these data are primarily studied separately. Accordingly, no sufficiently annotated dataset is available. To address this gap, we introduce FEDI, the first English dialogue dataset for task-oriented document-grounded dialogues annotated with demographic information, user emotions and implicit feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data have the potential to improve task completion and the factual consistency of the generated responses and user acceptance. ",
        "title": "Learning from Emotions, Demographic Information and Implicit User  Feedback in Task-Oriented Document-Grounded Dialogues",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09251",
        "abstract_url": "http://arxiv.org/abs/2401.09251",
        "authors": [
            {
                "last_name": "Mualem",
                "first_name": "Loay"
            },
            {
                "last_name": "Tukan",
                "first_name": "Murad"
            },
            {
                "last_name": "Fledman",
                "first_name": "Moran"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Optimization of DR-submodular functions has experienced a notable surge in significance in recent times, marking a pivotal development within the domain of non-convex optimization. Motivated by real-world scenarios, some recent works have delved into the maximization of non-monotone DR-submodular functions over general (not necessarily down-closed) convex set constraints. Up to this point, these works have all used the minimum $\\ell_\\infty$ norm of any feasible solution as a parameter. Unfortunately, a recent hardness result due to Mualem \\& Feldman~\\cite{mualem2023resolving} shows that this approach cannot yield a smooth interpolation between down-closed and non-down-closed constraints. In this work, we suggest novel offline and online algorithms that provably provide such an interpolation based on a natural decomposition of the convex body constraint into two distinct convex bodies: a down-closed convex body and a general convex body. We also empirically demonstrate the superiority of our proposed algorithms across three offline and two online applications. ",
        "title": "Bridging the Gap Between General and Down-Closed Convex Sets in  Submodular Maximization",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09252",
        "abstract_url": "http://arxiv.org/abs/2401.09252",
        "authors": [
            {
                "last_name": "da Silveira",
                "first_name": "Thiago Lopes Trugillo"
            },
            {
                "last_name": "Pinto",
                "first_name": "Paulo Gamarra Lessa"
            },
            {
                "last_name": "Llerena",
                "first_name": "Jeffri Erwin Murrugarra"
            },
            {
                "last_name": "Jung",
                "first_name": "Claudio Rosito"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "LG"
        ],
        "abstract": "  This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and discuss commonly adopted datasets and figures of merit indicated for each purpose and list recent results for completeness. We conclude this paper by pointing out current and future trends. ",
        "title": "3D Scene Geometry Estimation from 360$^\\circ$ Imagery: A Survey",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09256",
        "abstract_url": "http://arxiv.org/abs/2401.09256",
        "authors": [
            {
                "last_name": "Steinberg",
                "first_name": "Yossef"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The relay channel with unreliable helper is introduced and studied. The model is that of a classical relay channel where the input from the relay to the channel has an extra primitive link whose presence is not assured a priori. The extra link represents a helper who may decide not to cooperate in transmission. The goal is to devise robust coding schemes that exploit all the relay links when they are present, but can also operate, possibly at reduced rates, when the extra primitive link (helper) is absent. The capacity region of this class of problems is defined, and fully characterized for degraded relay channels. The degraded Gaussian relay channel with unreliable relay link is solved. ",
        "title": "Relay Channels with Unreliable Helpers",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09257",
        "abstract_url": "http://arxiv.org/abs/2401.09257",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Feiyang"
            },
            {
                "last_name": "Lin",
                "first_name": "Baijiong"
            },
            {
                "last_name": "Cao",
                "first_name": "Xiaofeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Tsang",
                "first_name": "Ivor"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO) problem, where the upper-level subproblem is a multi-objective optimization problem and the lower-level subproblem is for scalar optimization. Existing gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the computational inefficient problem. To address this, we propose an efficient first-order multi-gradient method for MOBLO, called FORUM. Specifically, we reformulate MOBLO problems as a constrained multi-objective optimization (MOO) problem via the value-function approach. Then we propose a novel multi-gradient aggregation method to solve the challenging constrained MOO problem. Theoretically, we provide the complexity analysis to show the efficiency of the proposed method and a non-asymptotic convergence result. Empirically, extensive experiments demonstrate the effectiveness and efficiency of the proposed FORUM method in different learning problems. In particular, it achieves state-of-the-art performance on three multi-task learning benchmark datasets. ",
        "title": "A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level  Optimization",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09258",
        "abstract_url": "http://arxiv.org/abs/2401.09258",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yinuo"
            },
            {
                "last_name": "Wu",
                "first_name": "Kun"
            },
            {
                "last_name": "Yi",
                "first_name": "Tianjiao"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Ju",
                "first_name": "Xiaozhu"
            },
            {
                "last_name": "Che",
                "first_name": "Zhengping"
            },
            {
                "last_name": "Qiu",
                "first_name": "Qinru"
            },
            {
                "last_name": "Liu",
                "first_name": "Chi Harold"
            },
            {
                "last_name": "Tang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Visuomotor policies, which learn control mechanisms directly from high-dimensional visual observations, confront challenges in adapting to new environments with intricate visual variations. Data augmentation emerges as a promising method for bridging these generalization gaps by enriching data variety. However, straightforwardly augmenting the entire observation shall impose excessive burdens on policy learning and may even result in performance degradation. In this paper, we propose to improve the generalization ability of visuomotor policies as well as preserve training stability from two aspects: 1) We learn a control-aware mask through a self-supervised reconstruction task with three auxiliary losses and then apply strong augmentation only to those control-irrelevant regions based on the mask to reduce the generalization gaps. 2) To address training instability issues prevalent in visual reinforcement learning (RL), we distill the knowledge from a pretrained RL expert processing low-level environment states, to the student visuomotor policy. The policy is subsequently deployed to unseen environments without any further finetuning. We conducted comparison and ablation studies across various benchmarks: the DMControl Generalization Benchmark (DMC-GB), the enhanced Robot Manipulation Distraction Benchmark (RMDB), and a specialized long-horizontal drawer-opening robotic task. The extensive experimental results well demonstrate the effectiveness of our method, e.g., showing a 17\\% improvement over previous methods in the video-hard setting of DMC-GB. ",
        "title": "An Efficient Generalizable Framework for Visuomotor Policies via  Control-aware Augmentation and Privilege-guided Distillation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09259",
        "abstract_url": "http://arxiv.org/abs/2401.09259",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Jiaxi"
            },
            {
                "last_name": "Li",
                "first_name": "Qianxiao"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We study the problem of distribution shift generally arising in machine-learning augmented hybrid simulation, where parts of simulation algorithms are replaced by data-driven surrogates. We first establish a mathematical framework to understand the structure of machine-learning augmented hybrid simulation problems, and the cause and effect of the associated distribution shift. We show correlations between distribution shift and simulation error both numerically and theoretically. Then, we propose a simple methodology based on tangent-space regularized estimator to control the distribution shift, thereby improving the long-term accuracy of the simulation results. In the linear dynamics case, we provide a thorough theoretical analysis to quantify the effectiveness of the proposed method. Moreover, we conduct several numerical experiments, including simulating a partially known reaction-diffusion equation and solving Navier-Stokes equations using the projection method with a data-driven pressure solver. In all cases, we observe marked improvements in simulation accuracy under the proposed method, especially for systems with high degrees of distribution shift, such as those with relatively strong non-linear reaction mechanisms, or flows at large Reynolds numbers. ",
        "title": "Mitigating distribution shift in machine learning-augmented hybrid  simulation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09261",
        "abstract_url": "http://arxiv.org/abs/2401.09261",
        "authors": [
            {
                "last_name": "Shang",
                "first_name": "Zongjiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Ling"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Demystifying interactions between temporal patterns of different scales is fundamental to precise long-range time series forecasting. However, previous works lack the ability to model high-order interactions. To promote more comprehensive pattern interaction modeling for long-range time series forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper) framework. Specifically, a multi-scale hypergraph is introduced to provide foundations for modeling high-order pattern interactions. Then by treating hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph modeling. In addition, a tri-stage message passing mechanism is introduced to aggregate pattern information and learn the interaction strength between temporal patterns of different scales. Extensive experiments on five real-world datasets demonstrate that MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 8.73% and 7.15% over the best baseline in MSE and MAE, respectively. ",
        "title": "MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series  Forecasting",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09266",
        "abstract_url": "http://arxiv.org/abs/2401.09266",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Chuyu"
            },
            {
                "last_name": "Ren",
                "first_name": "Hui"
            },
            {
                "last_name": "He",
                "first_name": "Xuming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods. In this paper, we first introduce a more practical problem setting named deep imbalanced clustering, where the underlying classes exhibit an imbalance distribution. To tackle this problem, we propose a novel pseudo-labeling-based learning framework. Our framework formulates pseudo-label generation as a progressive partial optimal transport problem, which progressively transports each sample to imbalanced clusters under prior distribution constraints, thus generating imbalance-aware pseudo-labels and learning from high-confident samples. In addition, we transform the initial formulation into an unbalanced optimal transport problem with augmented constraints, which can be solved efficiently by a fast matrix scaling algorithm. Experiments on various datasets, including a human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority of our method. ",
        "title": "P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced  Clustering",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09267",
        "abstract_url": "http://arxiv.org/abs/2401.09267",
        "authors": [
            {
                "last_name": "Ads",
                "first_name": "Mohamed"
            },
            {
                "last_name": "ElSawy",
                "first_name": "Hesham"
            },
            {
                "last_name": "Hassanein",
                "first_name": "Hossam S."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Wireless Federated Learning (FL) is an emerging distributed machine learning paradigm, particularly gaining momentum in domains with confidential and private data on mobile clients. However, the location-dependent performance, in terms of transmission rates and susceptibility to transmission errors, poses major challenges for wireless FL's convergence speed and accuracy. The challenge is more acute for hostile environments without a metric that authenticates the data quality and security profile of the clients. In this context, this paper proposes a novel risk-aware accelerated FL framework that accounts for the clients heterogeneity in the amount of possessed data, transmission rates, transmission errors, and trustworthiness. Classifying clients according to their location-dependent performance and trustworthiness profiles, we propose a dynamic risk-aware global model aggregation scheme that allows clients to participate in descending order of their transmission rates and an ascending trustworthiness constraint. In particular, the transmission rate is the dominant participation criterion for initial rounds to accelerate the convergence speed. Our model then progressively relaxes the transmission rate restriction to explore more training data at cell-edge clients. The aggregation rounds incorporate a debiasing factor that accounts for transmission errors. Risk-awareness is enabled by a validation set, where the base station eliminates non-trustworthy clients at the fine-tuning stage. The proposed scheme is benchmarked against a conservative scheme (i.e., only allowing trustworthy devices) and an aggressive scheme (i.e., oblivious to the trust metric). The numerical results highlight the superiority of the proposed scheme in terms of accuracy and convergence speed when compared to both benchmarks. ",
        "title": "Risk-Aware Accelerated Wireless Federated Learning with Heterogeneous  Clients",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09268",
        "abstract_url": "http://arxiv.org/abs/2401.09268",
        "authors": [
            {
                "last_name": "Schleich",
                "first_name": "Philipp"
            },
            {
                "last_name": "Kristensen",
                "first_name": "Lasse Bj\u00f8rn"
            },
            {
                "last_name": "Avagliano",
                "first_name": "Davide"
            },
            {
                "last_name": "Bagherimehrab",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Aldossary",
                "first_name": "Abdulrahman"
            },
            {
                "last_name": "Gorgulla",
                "first_name": "Christoph"
            },
            {
                "last_name": "Fitzsimons",
                "first_name": "Joe"
            },
            {
                "last_name": "Aspuru-Guzik",
                "first_name": "Al\u00e1n"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  Simulating chemical systems is highly sought after and computationally challenging, as the simulation cost exponentially increases with the system size. Quantum computers have been proposed as a computational means to overcome this bottleneck. Most efforts recently have been spent on determining the ground states of chemical systems. Hardness results and the lack of efficient heuristics for initial-state generation sheds doubt on the feasibility. Here we propose an inherently efficient approach for solving chemical simulation problems, meaning it requires quantum circuits of size scaling polynomially in relevant system parameters. If a set of assumptions can be satisfied, our approach finds good initial states by assembling initial states for dynamical simulation in a scattering tree. We discuss a variety of quantities of chemical interest that can be measured based on quantum simulation, e.g. of a reaction, succeeding the initial state preparation. ",
        "title": "Chemically Motivated Simulation Problems are Efficiently Solvable by a  Quantum Computer",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09270",
        "abstract_url": "http://arxiv.org/abs/2401.09270",
        "authors": [
            {
                "last_name": "Ambridge",
                "first_name": "Todd Waugh"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  The real numbers are important in both mathematics and computation theory. Computationally, real numbers can be represented in several ways; most commonly using inexact floating-point data-types, but also using exact arbitrary-precision data-types which satisfy the expected mathematical properties of the reals. This thesis is concerned with formalising properties of certain types for exact real arithmetic, as well as utilising them computationally for the purposes of search, optimisation and regression.   We develop, in a constructive and univalent type-theoretic foundation of mathematics, a formalised framework for performing search, optimisation and regression on a wide class of types. This framework utilises Mart\\'in Escard\\'o's prior work on searchable types, along with a convenient version of ultrametric spaces -- which we call closeness spaces -- in order to consistently search certain infinite types using the functional programming language and proof assistant Agda.   We formally define and prove the convergence properties of type-theoretic variants of global optimisation and parametric regression, problems related to search from the literature of analysis. As we work in a constructive setting, these convergence theorems yield computational algorithms for correct optimisation and regression on the types of our framework.   Importantly, we can instantiate our framework on data-types from the literature of exact real arithmetic, allowing us to perform our variants of search, optimisation and regression on ternary signed-digit encodings of the real numbers, as well as a simplified version of Hans-J. Boehm's functional encodings of real numbers. Furthermore, we contribute to the extensive work on ternary signed-digits by formally verifying the definition of certain exact real arithmetic operations using the Escard\\'o-Simpson interval object specification of compact intervals. ",
        "title": "Exact Real Search: Formalised Optimisation and Regression in  Constructive Univalent Mathematics",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09271",
        "abstract_url": "http://arxiv.org/abs/2401.09271",
        "authors": [
            {
                "last_name": "Heidler",
                "first_name": "Konrad"
            },
            {
                "last_name": "Nitze",
                "first_name": "Ingmar"
            },
            {
                "last_name": "Grosse",
                "first_name": "Guido"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiao Xiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Arctic Permafrost is facing significant changes due to global climate change. As these regions are largely inaccessible, remote sensing plays a crucial rule in better understanding the underlying processes not just on a local scale, but across the Arctic. In this study, we focus on the remote detection of retrogressive thaw slumps (RTS), a permafrost disturbance comparable to landslides induced by thawing. For such analyses from space, deep learning has become an indispensable tool, but limited labelled training data remains a challenge for training accurate models. To improve model generalization across the Arctic without the need for additional labelled data, we present a semi-supervised learning approach to train semantic segmentation models to detect RTS. Our framework called PixelDINO is trained in parallel on labelled data as well as unlabelled data. For the unlabelled data, the model segments the imagery into self-taught pseudo-classes and the training procedure ensures consistency of these pseudo-classes across strong augmentations of the input data. Our experimental results demonstrate that PixelDINO can improve model performance both over supervised baseline methods as well as existing semi-supervised semantic segmentation approaches, highlighting its potential for training robust models that generalize well to regions that were not included in the training data. The project page containing code and other materials for this study can be found at \\url{https://khdlr.github.io/PixelDINO/}. ",
        "title": "PixelDINO: Semi-Supervised Semantic Segmentation for Detecting  Permafrost Disturbances",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09273",
        "abstract_url": "http://arxiv.org/abs/2401.09273",
        "authors": [
            {
                "last_name": "Moroni",
                "first_name": "Mart\u00edn Santiago"
            },
            {
                "last_name": "Terraf",
                "first_name": "Pedro S\u00e1nchez"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We provide a fine classification of bisimilarities between states of possibly different labelled Markov processes (LMP). We show that a bisimilarity relation proposed by Panangaden that uses direct sums coincides with \"event bisimilarity\" from his joint work with Danos, Desharnais, and Laviolette. We also extend Giorgio Bacci's notions of bisimilarity between two different processes to the case of nondeterministic LMP and generalize the game characterization of state bisimilarity by Clerc et al. for the latter. ",
        "title": "A classification of bisimilarities for general Markov decision processes",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09274",
        "abstract_url": "http://arxiv.org/abs/2401.09274",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Luwei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a strict saddle property for $\\ell_p$ regularized functions, and propose an iterative reweighted $\\ell_1$ algorithm to solve the $\\ell_p$ regularized problems. The algorithm is guaranteed to converge only to local minimizers when randomly initialized. The strict saddle property is shown generic on these sparse optimization problems. Those analyses as well as the proposed algorithm can be easily extended to general nonconvex regularized problems. ",
        "title": "Avoiding strict saddle points of nonconvex regularized problems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09275",
        "abstract_url": "http://arxiv.org/abs/2401.09275",
        "authors": [
            {
                "last_name": "Hanna",
                "first_name": "Carol"
            },
            {
                "last_name": "Clark",
                "first_name": "David"
            },
            {
                "last_name": "Sarro",
                "first_name": "Federica"
            },
            {
                "last_name": "Petke",
                "first_name": "Justyna"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  A hot fix is an improvement to a specific time-critical issue deployed to a software system in production. While hot fixing is an essential and common activity in software maintenance, it has never been surveyed as a research activity. Thus, such a review is long overdue. In this paper, we conduct a comprehensive literature review of work on hot fixing. We highlight the fields where this topic has been addressed, inconsistencies we identified in the terminology, gaps in the literature, and directions for future work. Our search concluded with 91 papers on the topic between the year 2000 and 2022. The papers found encompass many different research areas such as log analysis, runtime patching (also known as hot patching), and automated repair, as well as various application domains such as security, mobile, and video games. We find that there are many directions that can take hot fix research forward such as unifying existing terminology, establishing a benchmark set of hot fixes, researching costs and frequency of hot fixes, and researching the possibility of end-to-end automation of detection, mitigation, and propagation. We discuss these avenues in detail to inspire the community to systematize hot fixing as a software engineering activity. We hope that this paper streamlines the existing body of work and drives research in the area forward. ",
        "title": "Hot Fixing Software: A Comprehensive Review of Terminology, Techniques,  and Applications",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09278",
        "abstract_url": "http://arxiv.org/abs/2401.09278",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Zhou"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiuyi"
            },
            {
                "last_name": "Chen",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fred"
            },
            {
                "last_name": "Woodruff",
                "first_name": "David"
            },
            {
                "last_name": "Hazan",
                "first_name": "Elad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Fast changing states or volatile environments pose a significant challenge to online optimization, which needs to perform rapid adaptation under limited observation. In this paper, we give query and regret optimal bandit algorithms under the strict notion of strongly adaptive regret, which measures the maximum regret over any contiguous interval $I$. Due to its worst-case nature, there is an almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one query per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just two queries per round, we give Strongly Adaptive Bandit Learner (StABL) that achieves $\\tilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with $n$ arms. The bound is tight and cannot be improved in general. Our algorithm leverages a multiplicative update scheme of varying stepsizes and a carefully chosen observation distribution to control the variance. Furthermore, we extend our results and provide optimal algorithms in the bandit convex optimization setting. Finally, we empirically demonstrate the superior performance of our algorithms under volatile environments and for downstream tasks, such as algorithm selection for hyperparameter optimization. ",
        "title": "Adaptive Regret for Bandits Made Possible: Two Queries Suffice",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09281",
        "abstract_url": "http://arxiv.org/abs/2401.09281",
        "authors": [
            {
                "last_name": "Lopes",
                "first_name": "Andr\u00e9"
            },
            {
                "last_name": "Castro",
                "first_name": "Daniel"
            },
            {
                "last_name": "Romano",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Processing-In-Memory (PIM) is a novel approach that augments existing DRAM memory chips with lightweight logic. By allowing to offload computations to the PIM system, this architecture allows for circumventing the data-bottleneck problem that affects many modern workloads. This work tackles the problem of how to build efficient software implementations of the Transactional Memory (TM) abstraction by introducing PIM-STM, a library that provides a range of diverse TM implementations for UPMEM, the first commercial PIM system. Via an extensive study we assess the efficiency of alternative choices in the design space of TM algorithms on this emerging architecture. We further quantify the impact of using different memory tiers of the UPMEM system (having different trade-offs for what concerns latency vs capacity) to store the metadata used by different TM implementations. Finally, we assess the gains achievable in terms of performance and memory efficiency when using PIM-STM to accelerate TM applications originally conceived for conventional CPU-based systems. ",
        "title": "PIM-STM: Software Transactional Memory for Processing-In-Memory Systems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09283",
        "abstract_url": "http://arxiv.org/abs/2401.09283",
        "authors": [
            {
                "last_name": "Thies",
                "first_name": "Mareike"
            },
            {
                "last_name": "Wagner",
                "first_name": "Fabian"
            },
            {
                "last_name": "Maul",
                "first_name": "Noah"
            },
            {
                "last_name": "Yu",
                "first_name": "Haijun"
            },
            {
                "last_name": "Meier",
                "first_name": "Manuela"
            },
            {
                "last_name": "Schneider",
                "first_name": "Linda-Sophie"
            },
            {
                "last_name": "Gu",
                "first_name": "Mingxuan"
            },
            {
                "last_name": "Mei",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Folle",
                "first_name": "Lukas"
            },
            {
                "last_name": "Maier",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cone-beam computed tomography (CBCT) systems, with their portability, present a promising avenue for direct point-of-care medical imaging, particularly in critical scenarios such as acute stroke assessment. However, the integration of CBCT into clinical workflows faces challenges, primarily linked to long scan duration resulting in patient motion during scanning and leading to image quality degradation in the reconstructed volumes. This paper introduces a novel approach to CBCT motion estimation using a gradient-based optimization algorithm, which leverages generalized derivatives of the backprojection operator for cone-beam CT geometries. Building on that, a fully differentiable target function is formulated which grades the quality of the current motion estimate in reconstruction space. We drastically accelerate motion estimation yielding a 19-fold speed-up compared to existing methods. Additionally, we investigate the architecture of networks used for quality metric regression and propose predicting voxel-wise quality maps, favoring autoencoder-like architectures over contracting ones. This modification improves gradient flow, leading to more accurate motion estimation. The presented method is evaluated through realistic experiments on head anatomy. It achieves a reduction in reprojection error from an initial average of 3mm to 0.61mm after motion compensation and consistently demonstrates superior performance compared to existing approaches. The analytic Jacobian for the backprojection operation, which is at the core of the proposed method, is made publicly available. In summary, this paper contributes to the advancement of CBCT integration into clinical workflows by proposing a robust motion estimation approach that enhances efficiency and accuracy, addressing critical challenges in time-sensitive scenarios. ",
        "title": "A gradient-based approach to fast and accurate head motion compensation  in cone-beam CT",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09284",
        "abstract_url": "http://arxiv.org/abs/2401.09284",
        "authors": [
            {
                "last_name": "Takano",
                "first_name": "Ryousei"
            },
            {
                "last_name": "Ishii",
                "first_name": "Kiyo"
            },
            {
                "last_name": "Shimizu",
                "first_name": "Toshiyuki"
            },
            {
                "last_name": "Okazaki",
                "first_name": "Fumihiro"
            },
            {
                "last_name": "Namiki",
                "first_name": "Shu"
            },
            {
                "last_name": "Sato",
                "first_name": "Ken-ichi"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  We experimentally verify a fast control plane with 100 microseconds of configuration time that can support more than 1000 racks, leveraged by a software-defined network controller and an industrial real-time Ethernet standard EtherCAT. ",
        "title": "A Fast Control Plane for a Large-Scale and High-Speed Optical Circuit  Switch System",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09285",
        "abstract_url": "http://arxiv.org/abs/2401.09285",
        "authors": [
            {
                "last_name": "Pizzinini",
                "first_name": "Clemens"
            },
            {
                "last_name": "Rosner",
                "first_name": "Philipp"
            },
            {
                "last_name": "Ziegler",
                "first_name": "David"
            },
            {
                "last_name": "Lienkamp",
                "first_name": "Markus"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Transportation is a constitutional part of most supply and value chains in modern economies. Smallholder farmers in rural Ethiopia face severe challenges along their supply and value chains. In particular, suitable, affordable, and available transport services are in high demand. To develop context-specific technical solutions, a problem-to-solution methodology based on the interaction with technology is developed. With this approach, we fill the gap between proven transportation assessment frameworks and general user-centered techniques. Central to our approach is an electric test vehicle that is implemented in rural supply and value chains for research, development, and testing. Based on our objective and the derived methodological requirements, a set of existing methods is selected. Local partners are integrated in an organizational framework that executes major parts of this research endeavour in Arsi Zone, Oromia Region, Ethiopia. ",
        "title": "Enhancing Rural Agricultural Value Chains through Electric Mobility  Services in Ethiopia",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09286",
        "abstract_url": "http://arxiv.org/abs/2401.09286",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Dong"
            },
            {
                "last_name": "Beltrame",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Deploying controllers trained with Reinforcement Learning (RL) on real robots can be challenging: RL relies on agents' policies being modeled as Markov Decision Processes (MDPs), which assume an inherently discrete passage of time. The use of MDPs results in that nearly all RL-based control systems employ a fixed-rate control strategy with a period (or time step) typically chosen based on the developer's experience or specific characteristics of the application environment. Unfortunately, the system should be controlled at the highest, worst-case frequency to ensure stability, which can demand significant computational and energy resources and hinder the deployability of the controller on onboard hardware. Adhering to the principles of reactive programming, we surmise that applying control actions only when necessary enables the use of simpler hardware and helps reduce energy consumption. We challenge the fixed frequency assumption by proposing a variant of RL with variable control rate. In this approach, the policy decides the action the agent should take as well as the duration of the time step associated with that action. In our new setting, we expand Soft Actor-Critic (SAC) to compute the optimal policy with a variable control rate, introducing the Soft Elastic Actor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a proof-of-concept simulation driving an agent with Newtonian kinematics. Our experiments show higher average returns, shorter task completion times, and reduced computational resources when compared to fixed rate policies. ",
        "title": "Deployable Reinforcement Learning with Variable Control Rate",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09289",
        "abstract_url": "http://arxiv.org/abs/2401.09289",
        "authors": [
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            },
            {
                "last_name": "van Weelden",
                "first_name": "Lisanne"
            },
            {
                "last_name": "Waytz",
                "first_name": "Adam"
            },
            {
                "last_name": "Franconeri",
                "first_name": "Steven"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  People routinely rely on data to make decisions, but the process can be riddled with biases. We show that patterns in data might be noticed first or more strongly, depending on how the data is visually represented or what the viewer finds salient. We also demonstrate that viewer interpretation of data is similar to that of 'ambiguous figures' such that two people looking at the same data can come to different decisions. In our studies, participants read visualizations depicting competitions between two entities, where one has a historical lead (A) but the other has been gaining momentum (B) and predicted a winner, across two chart types and three annotation approaches. They either saw the historical lead as salient and predicted that A would win, or saw the increasing momentum as salient and predicted B to win. These results suggest that decisions can be influenced by both how data are presented and what patterns people find visually salient. ",
        "title": "Same Data, Diverging Perspectives: The Power of Visualizations to Elicit  Competing Interpretations",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09290",
        "abstract_url": "http://arxiv.org/abs/2401.09290",
        "authors": [
            {
                "last_name": "Pavlidakis",
                "first_name": "Manos"
            },
            {
                "last_name": "Vasiliadis",
                "first_name": "Giorgos"
            },
            {
                "last_name": "Mavridis",
                "first_name": "Stelios"
            },
            {
                "last_name": "Argyros",
                "first_name": "Anargyros"
            },
            {
                "last_name": "Chazapis",
                "first_name": "Antony"
            },
            {
                "last_name": "Bilas",
                "first_name": "Angelos"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Modern GPU applications, such as machine learning (ML) frameworks, can only partially utilize beefy GPUs, leading to GPU underutilization in cloud environments. Sharing GPUs across multiple applications from different users can improve resource utilization and consequently cost, energy, and power efficiency. However, GPU sharing creates memory safety concerns because kernels must share a single GPU address space (GPU context). Previous GPU memory protection approaches have limited deployability because they require specialized hardware extensions or access to source code. This is often unavailable in GPU-accelerated libraries heavily utilized by ML frameworks. In this paper, we present G-Safe, a PTX-level bounds checking approach for GPUs that limits GPU kernels of each application to stay within the memory partition allocated to them. G-Safe relies on three mechanisms: (1) It divides the common GPU address space into separate partitions for different applications. (2) It intercepts and checks data transfers, fencing erroneous operations. (3) It instruments all GPU kernels at the PTX level (available in closed GPU libraries) fencing all kernel memory accesses outside application memory bounds. We implement G-Safe as an external, dynamically linked library that can be pre-loaded at application startup time. G-Safe's approach is transparent to applications and can support real-life, complex frameworks, such as Caffe and PyTorch, that issue billions of GPU kernels. Our evaluation shows that the overhead of G-Safe compared to native (unprotected) for such frameworks is between 4\\% - 12\\% and on average 9\\%. ",
        "title": "G-Safe: Safe GPU Sharing in Multi-Tenant Environments",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09292",
        "abstract_url": "http://arxiv.org/abs/2401.09292",
        "authors": [
            {
                "last_name": "Thomasian",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF"
        ],
        "abstract": "  We review studies based on analytic and simulation methods for hierarchical performance analysis of Queueing Network - QN models, which result in an order of magnitude reduction in performance evaluation cost with respect to simulation. The computational cost at the lower level is reduced when the computer system is modeled as a product-form QN. A Continuous Time Markov Chain - CTMC or discrete-event simulation can then be used at the higher level. We first consider a multiprogrammed transaction - txn processing system with Poisson arrivals and predeclared locks requests. Txn throughputs obtained by the analysis of multiprogrammed computer systems serve as the transition rates in a higher level CTMC to determine txn response times. We next analyze a task system where task precedence relationships are specified by a directed acyclic graph to determine its makespan. Task service demands are specified on the devices of a computer system. The composition of tasks in execution determines txn throughputs, which serve as transition rates among the states of the higher level CTMC model. As a third example we consider the hierarchical simulation of a timesharing system with two user classes. Txn throughputs in processing various combinations of requests are obtained by analyzing a closed product-form QN model. A discrete event simulator is provided. More detailed QN modeling parameters, such as the distribution of the number of cycles in central server model - CSM affects the performance of a fork/join queueing system. This detail can be taken into account in Schwetman's hybrid simulation method, which counts remaining cycles in CSM. We propose an extension to hybrid simulation to adjust job service demands according to elapsed time, rather than counting cycles. An example where Equilibrium Point Analysis to reduce computaional cost is privided. ",
        "title": "Hierarchical Analyses Applied to Computer System Performance: Review and  Call for Further Studies",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09294",
        "abstract_url": "http://arxiv.org/abs/2401.09294",
        "authors": [
            {
                "last_name": "Chung",
                "first_name": "Yoonjin"
            },
            {
                "last_name": "Lee",
                "first_name": "Junwon"
            },
            {
                "last_name": "Nam",
                "first_name": "Juhan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  Foley sound, audio content inserted synchronously with videos, plays a critical role in the user experience of multimedia content. Recently, there has been active research in Foley sound synthesis, leveraging the advancements in deep generative models. However, such works mainly focus on replicating a single sound class or a textual sound description, neglecting temporal information, which is crucial in the practical applications of Foley sound. We present T-Foley, a Temporal-event-guided waveform generation model for Foley sound synthesis. T-Foley generates high-quality audio using two conditions: the sound class and temporal event feature. For temporal conditioning, we devise a temporal event feature and a novel conditioning technique named Block-FiLM. T-Foley achieves superior performance in both objective and subjective evaluation metrics and generates Foley sound well-synchronized with the temporal events. Additionally, we showcase T-Foley's practical applications, particularly in scenarios involving vocal mimicry for temporal event control. We show the demo on our companion website. ",
        "title": "T-FOLEY: A Controllable Waveform-Domain Diffusion Model for  Temporal-Event-Guided Foley Sound Synthesis",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09296",
        "abstract_url": "http://arxiv.org/abs/2401.09296",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Wanting"
            },
            {
                "last_name": "Peng",
                "first_name": "Xin"
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Traditional visual-inertial state estimation targets absolute camera poses and spatial landmark locations while first-order kinematics are typically resolved as an implicitly estimated sub-state. However, this poses a risk in velocity-based control scenarios, as the quality of the estimation of kinematics depends on the stability of absolute camera and landmark coordinates estimation. To address this issue, we propose a novel solution to tight visual-inertial fusion directly at the level of first-order kinematics by employing a dynamic vision sensor instead of a normal camera. More specifically, we leverage trifocal tensor geometry to establish an incidence relation that directly depends on events and camera velocity, and demonstrate how velocity estimates in highly dynamic situations can be obtained over short time intervals. Noise and outliers are dealt with using a nested two-layer RANSAC scheme. Additionally, smooth velocity signals are obtained from a tight fusion with pre-integrated inertial signals using a sliding window optimizer. Experiments on both simulated and real data demonstrate that the proposed tight event-inertial fusion leads to continuous and reliable velocity estimation in highly dynamic scenarios independently of absolute coordinates. Furthermore, in extreme cases, it achieves more stable and more accurate estimation of kinematics than traditional, point-position-based visual-inertial odometry. ",
        "title": "Tight Fusion of Events and Inertial Measurements for Direct Velocity  Estimation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09308",
        "abstract_url": "http://arxiv.org/abs/2401.09308",
        "authors": [
            {
                "last_name": "Damiano",
                "first_name": "Stefano"
            },
            {
                "last_name": "Bondi",
                "first_name": "Luca"
            },
            {
                "last_name": "Ghaffarzadegan",
                "first_name": "Shabnam"
            },
            {
                "last_name": "Guntoro",
                "first_name": "Andre"
            },
            {
                "last_name": "van Waterschoot",
                "first_name": "Toon"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  In the design of traffic monitoring solutions for optimizing the urban mobility infrastructure, acoustic vehicle counting models have received attention due to their cost effectiveness and energy efficiency. Although deep learning has proven effective for visual traffic monitoring, its use has not been thoroughly investigated in the audio domain, likely due to real-world data scarcity. In this work, we propose a novel approach to acoustic vehicle counting by developing: i) a traffic noise simulation framework to synthesize realistic vehicle pass-by events; ii) a strategy to mix synthetic and real data to train a deep-learning model for traffic counting. The proposed system is capable of simultaneously counting cars and commercial vehicles driving on a two-lane road, and identifying their direction of travel under moderate traffic density conditions. With only 24 hours of labeled real-world traffic noise, we are able to improve counting accuracy on real-world data from $63\\%$ to $88\\%$ for cars and from $86\\%$ to $94\\%$ for commercial vehicles. ",
        "title": "Can Synthetic Data Boost the Training of Deep Acoustic Vehicle Counting  Networks?",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09317",
        "abstract_url": "http://arxiv.org/abs/2401.09317",
        "authors": [
            {
                "last_name": "Shao",
                "first_name": "Shuai"
            },
            {
                "last_name": "Ye",
                "first_name": "Xiaowei"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We derive the strong spatial mixing property for the general 2-spin system from zero-free regions of its partition function. We view the partition function of the 2-spin system as a multivariate function over three complex parameters $(\\beta, \\gamma, \\lambda)$, and we allow the zero-free regions of $\\beta, \\gamma$ or $\\lambda$ to be of arbitrary shapes. As long as the zero-free region contains a positive point and it is a complex neighborhood of $\\lambda=0$ when fixing $\\beta, \\gamma \\in \\mathbb{C}$, or a complex neighborhood of $\\beta\\gamma=1$ when fixing $\\beta, \\lambda\\in \\mathbb{C}$ or $\\gamma, \\lambda\\in \\mathbb{C}$ respectively, we are able to show that the corresponding 2-spin system exhibits strong spatial mixing on such a region. The underlying graphs of the 2-spin system are not necessarily of bounded degree, while are required to include graphs with pinned vertices. We prove this result by establishing a Christoffel-Darboux type identity for the 2-spin system on trees and using certain tools from complex analysis.   To our best knowledge, our result is general enough to turn all currently known zero-free regions of the partition function of the 2-spin system where pinned vertices are allowed into the strong spatial mixing property. Moreover, we extend our result to obtain strong spatial mixing for the ferromagnetic Ising model (even with non-uniform external fields) from the celebrated Lee-Yang circle theorem. ",
        "title": "From Zero-Freeness to Strong Spatial Mixing via a Christoffel-Darboux  Type Identity",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09321",
        "abstract_url": "http://arxiv.org/abs/2401.09321",
        "authors": [
            {
                "last_name": "Fernandez-Cortizas",
                "first_name": "Miguel"
            },
            {
                "last_name": "Perez-Saura",
                "first_name": "David"
            },
            {
                "last_name": "Sanz",
                "first_name": "Ricardo"
            },
            {
                "last_name": "Molina",
                "first_name": "Martin"
            },
            {
                "last_name": "Campoy",
                "first_name": "Pascual"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The development of collective-aware multi-robot systems is crucial for enhancing the efficiency and robustness of robotic applications in multiple fields. These systems enable collaboration, coordination, and resource sharing among robots, leading to improved scalability, adaptability to dynamic environments, and increased overall system robustness. In this work, we want to provide a brief overview of this research topic and identify open challenges. ",
        "title": "The landscape of Collective Awareness in multi-robot systems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09322",
        "abstract_url": "http://arxiv.org/abs/2401.09322",
        "authors": [
            {
                "last_name": "Saravanan",
                "first_name": "Suchetan"
            },
            {
                "last_name": "Chauffaut",
                "first_name": "Corentin"
            },
            {
                "last_name": "Chanel",
                "first_name": "Caroline"
            },
            {
                "last_name": "Vivet",
                "first_name": "Damien"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Active visual SLAM finds a wide array of applications in GNSS-Denied sub-terrain environments and outdoor environments for ground robots. To achieve robust localization and mapping accuracy, it is imperative to incorporate the perception considerations in the goal selection and path planning towards the goal during an exploration mission. Through this work, we propose FIT-SLAM (Fisher Information and Traversability estimation-based Active SLAM), a new exploration method tailored for unmanned ground vehicles (UGVs) to explore 3D environments. This approach is devised with the dual objectives of sustaining an efficient exploration rate while optimizing SLAM accuracy. Initially, an estimation of a global traversability map is conducted, which accounts for the environmental constraints pertaining to traversability. Subsequently, we propose a goal candidate selection approach along with a path planning method towards this goal that takes into account the information provided by the landmarks used by the SLAM backend to achieve robust localization and successful path execution . The entire algorithm is tested and evaluated first in a simulated 3D world, followed by a real-world environment and is compared to pre-existing exploration methods. The results obtained during this evaluation demonstrate a significant increase in the exploration rate while effectively minimizing the localization covariance. ",
        "title": "FIT-SLAM -- Fisher Information and Traversability estimation-based  Active SLAM for exploration in 3D environments",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09323",
        "abstract_url": "http://arxiv.org/abs/2401.09323",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haixin"
            },
            {
                "last_name": "Li",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Dwivedi",
                "first_name": "Anubhav"
            },
            {
                "last_name": "Hara",
                "first_name": "Kentaro"
            },
            {
                "last_name": "Wu",
                "first_name": "Tailin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green's function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96\\%. Our source code can be found https://github.com/AI4Science-WestlakeU/beno.git. ",
        "title": "BENO: Boundary-embedded Neural Operators for Elliptic PDEs",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09324",
        "abstract_url": "http://arxiv.org/abs/2401.09324",
        "authors": [
            {
                "last_name": "Maquil",
                "first_name": "Val\u00e9rie"
            },
            {
                "last_name": "Anastasiou",
                "first_name": "Dimitra"
            },
            {
                "last_name": "Afkari",
                "first_name": "Hoorieh"
            },
            {
                "last_name": "Coppens",
                "first_name": "Adrien"
            },
            {
                "last_name": "Hermen",
                "first_name": "Johannes"
            },
            {
                "last_name": "Schwartz",
                "first_name": "Lou"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Sharing a physical environment, such as that of a wall-display, facilitates gaining awareness of others' actions and intentions, thereby bringing benefits for collaboration. Previous studies have provided first insights on awareness in the context of tabletops or smaller vertical displays. This paper seeks to advance the current understanding on how users share awareness information in wall-display environments and focusses on mid-air pointing gestures as a foundational part of communication. We present a scenario dealing with the organization of medical supply chains in crisis situations, and report on the results of a user study with 24 users, split into 6 groups of 4, performing several tasks. We investigate pointing gestures and identify three subtypes used as awareness cues during face-to-face collaboration: narrative pointing, loose pointing, and sharp pointing. Our observations show that reliance on gesture subtypes varies across participants and groups, and that sometimes vague pointing is sufficient to support verbal negotiations. ",
        "title": "Establishing Awareness through Pointing Gestures during Collaborative  Decision-Making in a Wall-Display Environment",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09325",
        "abstract_url": "http://arxiv.org/abs/2401.09325",
        "authors": [
            {
                "last_name": "Jia",
                "first_name": "Jia"
            },
            {
                "last_name": "Lee",
                "first_name": "Geunho"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhibo"
            },
            {
                "last_name": "Zhi",
                "first_name": "Lyu"
            },
            {
                "last_name": "He",
                "first_name": "Yuchu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, the application of deep learning to change detection (CD) has significantly progressed in remote sensing images. In recent years, CD tasks have mostly used architectures such as CNN and Transformer to identify these changes. However, these architectures have shortcomings in representing boundary details and are prone to false alarms and missed detections under complex lighting and weather conditions. For that, we propose a new network, Siamese Meets Diffusion Network (SMDNet). This network combines the Siam-U2Net Feature Differential Encoder (SU-FDE) and the denoising diffusion implicit model to improve the accuracy of image edge change detection and enhance the model's robustness under environmental changes. First, we propose an innovative SU-FDE module that utilizes shared weight features to capture differences between time series images and identify similarities between features to enhance edge detail detection. Furthermore, we add an attention mechanism to identify key coarse features to improve the model's sensitivity and accuracy. Finally, the diffusion model of progressive sampling is used to fuse key coarse features, and the noise reduction ability of the diffusion model and the advantages of capturing the probability distribution of image data are used to enhance the adaptability of the model in different environments. Our method's combination of feature extraction and diffusion models demonstrates effectiveness in change detection in remote sensing images. The performance evaluation of SMDNet on LEVIR-CD, DSIFN-CD, and CDD datasets yields validated F1 scores of 90.99%, 88.40%, and 88.47%, respectively. This substantiates the advanced capabilities of our model in accurately identifying variations and intricate details. ",
        "title": "Siamese Meets Diffusion Network: SMDNet for Enhanced Change Detection in  High-Resolution RS Imagery",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09328",
        "abstract_url": "http://arxiv.org/abs/2401.09328",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Wanting"
            },
            {
                "last_name": "Hu",
                "first_name": "Lan"
            },
            {
                "last_name": "Tsakiris",
                "first_name": "Manolis C."
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Over the past decade, the Gr\\\"obner basis theory and automatic solver generation have lead to a large number of solutions to geometric vision problems. In practically all cases, the derived solvers apply a fixed elimination template to calculate the Gr\\\"obner basis and thereby identify the zero-dimensional variety of the original polynomial constraints. However, it is clear that different variable or monomial orderings lead to different elimination templates, and we show that they may present a large variability in accuracy for a certain instance of a problem. The present paper has two contributions. We first show that for a common class of problems in geometric vision, variable reordering simply translates into a permutation of the columns of the initial coefficient matrix, and that -- as a result -- one and the same elimination template can be reused in different ways, each one leading to potentially different accuracy. We then prove that the original set of coefficients may contain sufficient information to train a classifier for online selection of a good solver, most notably at the cost of only a small computational overhead. We demonstrate wide applicability at the hand of generic dense polynomial problem solvers, as well as a concrete solver from geometric vision. ",
        "title": "Online Stability Improvement of Groebner Basis Solvers using Deep  Learning",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09329",
        "abstract_url": "http://arxiv.org/abs/2401.09329",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Siqi"
            },
            {
                "last_name": "Resnick",
                "first_name": "Paul"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In computational social science, researchers often use a pre-trained, black box classifier to estimate the frequency of each class in unlabeled datasets. A variety of prevalence estimation techniques have been developed in the literature, each yielding an unbiased estimate if certain stability assumption holds. This work introduces a framework to rethink the prevalence estimation process as calibrating the classifier outputs against ground truth labels to obtain the joint distribution of a base dataset and then extrapolating to the joint distribution of a target dataset. We call this framework \"Calibrate-Extrapolate\". Visualizing the joint distribution makes the stability assumption needed for a prevalence estimation technique clear and easy to understand. In the calibration phase, the techniques assume only a stable calibration curve between a calibration dataset and the full base dataset. This allows for the classifier outputs to be used for purposive sampling, thus improving the efficiency of calibration. In the extrapolation phase, some techniques assume a stable calibration curve while some assume stable class-conditional densities. We discuss the stability assumptions from a causal perspective. By specifying base and target joint distributions, we can generate simulated datasets, as a way to build intuitions about the impacts of assumption violations. This also leads to a better understanding of how the classifier predictive power affects the accuracy of prevalence estimates: the greater the predictive power, the lower the sensitivity to violations of stability assumptions in the extrapolation phase. We illustrate the framework with an application that estimates the prevalence of toxic news comments over time on Reddit, Twitter, and YouTube, using Jigsaw's Perspective API as a black box classifier. ",
        "title": "Calibrate-Extrapolate: Rethinking Prevalence Estimation with Black Box  Classifiers",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09331",
        "abstract_url": "http://arxiv.org/abs/2401.09331",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Wanting"
            },
            {
                "last_name": "Zhang",
                "first_name": "Si'ao"
            },
            {
                "last_name": "Cui",
                "first_name": "Li"
            },
            {
                "last_name": "Peng",
                "first_name": "Xin"
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Despite the promise of superior performance under challenging conditions, event-based motion estimation remains a hard problem owing to the difficulty of extracting and tracking stable features from event streams. In order to robustify the estimation, it is generally believed that fusion with other sensors is a requirement. In this work, we demonstrate reliable, purely event-based visual odometry on planar ground vehicles by employing the constrained non-holonomic motion model of Ackermann steering platforms. We extend single feature n-linearities for regular frame-based cameras to the case of quasi time-continuous event-tracks, and achieve a polynomial form via variable degree Taylor expansions. Robust averaging over multiple event tracks is simply achieved via histogram voting. As demonstrated on both simulated and real data, our algorithm achieves accurate and robust estimates of the vehicle's instantaneous rotational velocity, and thus results that are comparable to the delta rotations obtained by frame-based sensors under normal conditions. We furthermore significantly outperform the more traditional alternatives in challenging illumination scenarios. The code is available at \\url{https://github.com/gowanting/NHEVO}. ",
        "title": "Event-Based Visual Odometry on Non-Holonomic Ground Vehicles",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09332",
        "abstract_url": "http://arxiv.org/abs/2401.09332",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Li",
                "first_name": "Jianwen"
            },
            {
                "last_name": "Mahmoudian",
                "first_name": "Nina"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Vision-driven autonomous flight and obstacle avoidance of Unmanned Aerial Vehicles (UAVs) along complex riverine environments for tasks like rescue and surveillance requires a robust control policy, which is yet difficult to obtain due to the shortage of trainable river environment simulators and reward sparsity in such environments. To easily verify the navigation controller performance for the river following task before real-world deployment, we developed a trainable photo-realistic dynamics-free riverine simulation environment using Unity. Successful river following trajectories in the environment are manually collected and Behavior Clone (BC) is used to train an Imitation Learning (IL) agent to mimic expert behavior and generate expert guidance. Finally, a framework is proposed to train a Deep Reinforcement Learning (DRL) agent using BC expert guidance and improve the expert policy online by sampling good demonstrations produced by the DRL to increase convergence rate and policy performance. This framework is able to solve the along-river autonomous navigation task and outperform baseline RL and IL methods. The code and trainable environments are available. ",
        "title": "Vision-driven Autonomous Flight of UAV Along River Using Deep  Reinforcement Learning with Dynamic Expert Guidance",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09333",
        "abstract_url": "http://arxiv.org/abs/2401.09333",
        "authors": [
            {
                "last_name": "Gordillo",
                "first_name": "Diana Davila"
            },
            {
                "last_name": "Timoneda",
                "first_name": "Joan"
            },
            {
                "last_name": "Vera",
                "first_name": "Sebastian Vallejo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Current methods to identify and classify racist language in text rely on small-n qualitative approaches or large-n approaches focusing exclusively on overt forms of racist discourse. This article provides a step-by-step generalizable guideline to identify and classify different forms of racist discourse in large corpora. In our approach, we start by conceptualizing racism and its different manifestations. We then contextualize these racist manifestations to the time and place of interest, which allows researchers to identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a cross-lingual model for supervised text classification with a cutting-edge contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our pretrained model, outperform other state-of-the-art approaches in classifying racism in large corpora. We illustrate our approach using a corpus of tweets relating to the Ecuadorian ind\\'igena community between 2018 and 2021. ",
        "title": "Machines Do See Color: A Guideline to Classify Different Forms of Racist  Discourse in Large Corpora",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09334",
        "abstract_url": "http://arxiv.org/abs/2401.09334",
        "authors": [
            {
                "last_name": "Fang",
                "first_name": "Meng"
            },
            {
                "last_name": "Deng",
                "first_name": "Shilong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yudi"
            },
            {
                "last_name": "Shi",
                "first_name": "Zijing"
            },
            {
                "last_name": "Chen",
                "first_name": "Ling"
            },
            {
                "last_name": "Pechenizkiy",
                "first_name": "Mykola"
            },
            {
                "last_name": "Wang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks. ",
        "title": "Large Language Models Are Neurosymbolic Reasoners",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09336",
        "abstract_url": "http://arxiv.org/abs/2401.09336",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Luyi"
            },
            {
                "last_name": "Tan",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Gao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Xin"
            },
            {
                "last_name": "Longo",
                "first_name": "Valentina"
            },
            {
                "last_name": "Ventura-D\u00edaz",
                "first_name": "Sof\u00eda"
            },
            {
                "last_name": "D'Angelo",
                "first_name": "Anna"
            },
            {
                "last_name": "Teuwen",
                "first_name": "Jonas"
            },
            {
                "last_name": "Mann",
                "first_name": "Ritse"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Clinicians compare breast DCE-MRI after neoadjuvant chemotherapy (NAC) with pre-treatment scans to evaluate the response to NAC. Clinical evidence supports that accurate longitudinal deformable registration without deforming treated tumor regions is key to quantifying tumor changes. We propose a conditional pyramid registration network based on unsupervised keypoint detection and selective volume-preserving to quantify changes over time. In this approach, we extract the structural and the abnormal keypoints from DCE-MRI, apply the structural keypoints for the registration algorithm to restrict large deformation, and employ volume-preserving loss based on abnormal keypoints to keep the volume of the tumor unchanged after registration. We use a clinical dataset with 1630 MRI scans from 314 patients treated with NAC. The results demonstrate that our method registers with better performance and better volume preservation of the tumors. Furthermore, a local-global-combining biomarker based on the proposed method achieves high accuracy in pathological complete response (pCR) prediction, indicating that predictive information exists outside tumor regions. The biomarkers could potentially be used to avoid unnecessary surgeries for certain patients. It may be valuable for clinicians and/or computer systems to conduct follow-up tumor segmentation and response prediction on images registered by our method. Our code is available on \\url{https://github.com/fiy2W/Treatment-aware-Longitudinal-Registration}. ",
        "title": "To deform or not: treatment-aware longitudinal registration for breast  DCE-MRI during neoadjuvant chemotherapy via unsupervised keypoints detection",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09339",
        "abstract_url": "http://arxiv.org/abs/2401.09339",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Jie"
            },
            {
                "last_name": "Doshi",
                "first_name": "Vishwaraj"
            },
            {
                "last_name": "Eun",
                "first_name": "Do Young"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Two-timescale stochastic approximation (TTSA) is among the most general frameworks for iterative stochastic algorithms. This includes well-known stochastic optimization methods such as SGD variants and those designed for bilevel or minimax problems, as well as reinforcement learning like the family of gradient-based temporal difference (GTD) algorithms. In this paper, we conduct an in-depth asymptotic analysis of TTSA under controlled Markovian noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA influenced by the underlying Markov chain, which has not been addressed by previous CLT results of TTSA only with Martingale difference noise. Building upon our CLT, we expand its application horizon of efficient sampling strategies from vanilla SGD to a wider TTSA context in distributed learning, thus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT result to deduce the statistical properties of GTD algorithms with nonlinear function approximation using Markovian samples and show their identical asymptotic performance, a perspective not evident from current finite-time bounds. ",
        "title": "Central Limit Theorem for Two-Timescale Stochastic Approximation with  Markovian Noise: Theory and Applications",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09340",
        "abstract_url": "http://arxiv.org/abs/2401.09340",
        "authors": [
            {
                "last_name": "Jia",
                "first_name": "Baoxiong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yixin"
            },
            {
                "last_name": "Yu",
                "first_name": "Huangyue"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Niu",
                "first_name": "Xuesong"
            },
            {
                "last_name": "Liu",
                "first_name": "Tengyu"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Huang",
                "first_name": "Siyuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "LG",
            "RO"
        ],
        "abstract": "  3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: https://scene-verse.github.io . ",
        "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene  Understanding",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09343",
        "abstract_url": "http://arxiv.org/abs/2401.09343",
        "authors": [
            {
                "last_name": "Vlasov",
                "first_name": "Vladimir"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Slot labelling is an essential component of any dialogue system, aiming to find important arguments in every user turn. Common approaches involve large pre-trained language models (PLMs) like BERT or RoBERTa, but they face challenges such as high computational requirements and dependence on pre-training data. In this work, we propose a lightweight method which performs on par or better than the state-of-the-art PLM-based methods, while having almost 10x less trainable parameters. This makes it especially applicable for real-life industry scenarios. ",
        "title": "Efficient slot labelling",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09346",
        "abstract_url": "http://arxiv.org/abs/2401.09346",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wanrong"
            },
            {
                "last_name": "Lou",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Wei",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Wu",
                "first_name": "Wei Biao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Uncertainty quantification for estimation through stochastic optimization solutions in an online setting has gained popularity recently. This paper introduces a novel inference method focused on constructing confidence intervals with efficient computation and fast convergence to the nominal level. Specifically, we propose to use a small number of independent multi-runs to acquire distribution information and construct a t-based confidence interval. Our method requires minimal additional computation and memory beyond the standard updating of estimates, making the inference process almost cost-free. We provide a rigorous theoretical guarantee for the confidence interval, demonstrating that the coverage is approximately exact with an explicit convergence rate and allowing for high confidence level inference. In particular, a new Gaussian approximation result is developed for the online estimators to characterize the coverage properties of our confidence intervals in terms of relative errors. Additionally, our method also allows for leveraging parallel computing to further accelerate calculations using multiple cores. It is easy to implement and can be integrated with existing stochastic algorithms without the need for complicated modifications. ",
        "title": "High Confidence Level Inference is Almost Free using Parallel Stochastic  Optimization",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09348",
        "abstract_url": "http://arxiv.org/abs/2401.09348",
        "authors": [
            {
                "last_name": "Brugnoli",
                "first_name": "Andrea"
            },
            {
                "last_name": "Mehrmann",
                "first_name": "Volker"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  It is well known that the Lagrangian and Hamiltonian descriptions of field theories are equivalent at the discrete time level when variational integrators are used. Besides the symplectic Hamiltonian structure, many physical systems exhibit a Hamiltonian structure when written in mixed form. In this contribution, the discrete equivalence of Lagrangian, symplectic Hamiltonian and mixed formulations is investigated for linear wave propagation phenomena. Under compatibility conditions between the finite elements, the Lagrangian and mixed formulations are indeed equivalent. For the time discretization the leapfrog scheme and the implicit midpoint rule are considered. In mixed methods applied to wave problems the primal variable (e.g. the displacement in mechanics or the magnetic potential in electromagnetism) is not an unknown of the problem and is reconstructed a posteriori from its time derivative. When this reconstruction is performed via the trapezoidal rule, then these time-discretization methods lead to equivalent formulations. ",
        "title": "On the discrete equivalence of Lagrangian, Hamiltonian and mixed finite  element formulations for linear wave phenomena",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09350",
        "abstract_url": "http://arxiv.org/abs/2401.09350",
        "authors": [
            {
                "last_name": "Bruch",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "IR"
        ],
        "abstract": "  Vectors are universal mathematical objects that can represent text, images, speech, or a mix of these data modalities. That happens regardless of whether data is represented by hand-crafted features or learnt embeddings. Collect a large enough quantity of such vectors and the question of retrieval becomes urgently relevant: Finding vectors that are more similar to a query vector. This monograph is concerned with the question above and covers fundamental concepts along with advanced data structures and algorithms for vector retrieval. In doing so, it recaps this fascinating topic and lowers barriers of entry into this rich area of research. ",
        "title": "Foundations of Vector Retrieval",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09352",
        "abstract_url": "http://arxiv.org/abs/2401.09352",
        "authors": [
            {
                "last_name": "Beik-Mohammadi",
                "first_name": "Hadi"
            },
            {
                "last_name": "Hauberg",
                "first_name": "S\u00f8ren"
            },
            {
                "last_name": "Arvanitidis",
                "first_name": "Georgios"
            },
            {
                "last_name": "Figueroa",
                "first_name": "Nadia"
            },
            {
                "last_name": "Neumann",
                "first_name": "Gerhard"
            },
            {
                "last_name": "Rozo",
                "first_name": "Leonel"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Stability guarantees are crucial when ensuring a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural networks. We propose a novel methodology to learn neural contractive dynamical systems, where our neural architecture ensures contraction, and hence, global stability. To efficiently scale the method to high-dimensional dynamical systems, we develop a variant of the variational autoencoder that learns dynamics in a low-dimensional latent representation space while retaining contractive stability after decoding. We further extend our approach to learning contractive systems on the Lie group of rotations to account for full-pose end-effector dynamic motions. The result is the first highly flexible learning architecture that provides contractive stability guarantees with capability to perform obstacle avoidance. Empirically, we demonstrate that our approach encodes the desired dynamics more accurately than the current state-of-the-art, which provides less strong stability guarantees. ",
        "title": "Neural Contractive Dynamical Systems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09354",
        "abstract_url": "http://arxiv.org/abs/2401.09354",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Hania"
            },
            {
                "last_name": "Khalid",
                "first_name": "Aleena Fatima"
            },
            {
                "last_name": "Hassan",
                "first_name": "Zaryab"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  This research investigates the transferability of Automatic Speech Recognition (ASR)-robust Natural Language Understanding (NLU) models from controlled experimental conditions to practical, real-world applications. Focused on smart home automation commands in Urdu, the study assesses model performance under diverse noise profiles, linguistic variations, and ASR error scenarios. Leveraging the UrduBERT model, the research employs a systematic methodology involving real-world data collection, cross-validation, transfer learning, noise variation studies, and domain adaptation. Evaluation metrics encompass task-specific accuracy, latency, user satisfaction, and robustness to ASR errors. The findings contribute insights into the challenges and adaptability of ASR-robust NLU models in transcending controlled environments. ",
        "title": "Transcending Controlled Environments Assessing the Transferability of  ASRRobust NLU Models to Real-World Applications",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09356",
        "abstract_url": "http://arxiv.org/abs/2401.09356",
        "authors": [
            {
                "last_name": "De Sensi",
                "first_name": "Daniele"
            },
            {
                "last_name": "Bonato",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Saam",
                "first_name": "David"
            },
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG",
            "NI",
            "PF"
        ],
        "abstract": "  The allreduce collective operation accounts for a significant fraction of the runtime of workloads running on distributed systems. One factor determining its performance is the distance between communicating nodes, especially on networks like torus, where a higher distance implies multiple messages being forwarded on the same link, thus reducing the allreduce bandwidth. Torus networks are widely used on systems optimized for machine learning workloads (e.g., Google TPUs and Amazon Trainium devices), as well as on some of the Top500 supercomputers. To improve allreduce performance on torus networks we introduce Swing, a new algorithm that keeps a low distance between communicating nodes by swinging between torus directions. Our analysis and experimental evaluation show that Swing outperforms by up to 3x existing allreduce algorithms for vectors ranging from 32B to 128MiB, on different types of torus and torus-like topologies, regardless of their shape and size. ",
        "title": "Swing: Short-cutting Rings for Higher Bandwidth Allreduce",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09358",
        "abstract_url": "http://arxiv.org/abs/2401.09358",
        "authors": [
            {
                "last_name": "Tamayo",
                "first_name": "Jaime"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Lorena Isabel Barona"
            },
            {
                "last_name": "Caraguay",
                "first_name": "\u00c1ngel Leonardo Valdivieso"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Recent years witnessed a surge in network traffic due to the emergence of new online services, causing periodic saturation and complexity problems. Additionally, the growing number of IoT devices further compounds the problem. Software Defined Network (SDN) is a new architecture which offers innovative advantages that help to reduce saturation problems. Despite its benefits, SDNs not only can be affected by traditional attacks but also introduce new security challenges. In this context, Distributed Denial of Service (DDoS) is one of the most important attacks that can damage an SDN network's normal operation. Furthermore, if these attacks are executed using botnets, they can use thousands of compromised devices to disrupt critical online services. This paper proposes a framework for detecting DDoS attacks generated by a group of botnets in an SDN network. The framework is implemented using open-source tools such as Mininet and OpenDaylight and tested in a centralized network topology using BYOB and SNORT. The results demonstrate real-time attack identification by implementing an intrusion detection mechanism in the victim client. Our proposed solution offers quick and effective detection of DDoS attacks in SDN networks. The framework can successfully differentiate the type of attack with high accuracy in a short time ",
        "title": "Detection of Distributed Denial of Service Attacks Carried Out by  Botnets in Software-Defined Networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09359",
        "abstract_url": "http://arxiv.org/abs/2401.09359",
        "authors": [
            {
                "last_name": "Riedel",
                "first_name": "Samuel"
            },
            {
                "last_name": "Gantenbein",
                "first_name": "Marc"
            },
            {
                "last_name": "Ottaviano",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Extensive polling in shared-memory manycore systems can lead to contention, decreased throughput, and poor energy efficiency. Both lock implementations and the general-purpose atomic operation, load-reserved/store-conditional (LRSC), cause polling due to serialization and retries. To alleviate this overhead, we propose LRwait and SCwait, a synchronization pair that eliminates polling by allowing contending cores to sleep while waiting for previous cores to finish their atomic access. As a scalable implementation of LRwait, we present Colibri, a distributed and scalable approach to managing LRwait reservations. Through extensive benchmarking on an open-source RISC-V platform with 256 cores, we demonstrate that Colibri outperforms current synchronization approaches for various concurrent algorithms with high and low contention regarding throughput, fairness, and energy efficiency. With an area overhead of only 6%, Colibri outperforms LRSC-based implementations by a factor of 6.5x in terms of throughput and 7.1x in terms of energy efficiency. ",
        "title": "LRSCwait: Enabling Scalable and Efficient Synchronization in Manycore  Systems through Polling-Free and Retry-Free Operation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09366",
        "abstract_url": "http://arxiv.org/abs/2401.09366",
        "authors": [
            {
                "last_name": "Lamiaux",
                "first_name": "Thomas"
            },
            {
                "last_name": "Ahrens",
                "first_name": "Benedikt"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL"
        ],
        "abstract": "  Characterizing programming languages with variable binding as initial objects, was first achieved by Fiore, Plotkin, and Turi in their seminal paper published at LICS'99. To do so, in particular to prove initiality theorems, they developed a framework based on monoidal categories, functors with strengths, and $\\Sigma$-monoids. An alternative approach using modules over monads was later introduced by Hirschowitz and Maggesi, for endofunctor categories, that is, for particular monoidal categories. This approach has the advantage of providing a more general and abstract definition of signatures and models; however, no general initiality result is known for this notion of signature. Furthermore, Matthes and Uustalu provided a categorical formalism for constructing (initial) monads via Mendler-style recursion, that can also be used for initial semantics. The different approaches have been developed further in several articles. However, in practice, the literature is difficult to access, and links between the different strands of work remain underexplored.   In the present work, we give an introduction to initial semantics that encompasses the three different strands. We develop a suitable \"pushout\" of Hirschowitz and Maggesi's framework with Fiore's, and rely on Matthes and Uustalu's formalism to provide modular proofs. For this purpose, we generalize both Hirschowitz and Maggesi's framework, and Matthes and Uustalu's formalism to the general setting of monoidal categories studied by Fiore and collaborators. Moreover, we provide fully worked out presentation of some basic instances of the literature, and an extensive discussion of related work explaining the links between the different approaches. ",
        "title": "An Introduction to Different Approaches to Initial Semantics",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09368",
        "abstract_url": "http://arxiv.org/abs/2401.09368",
        "authors": [
            {
                "last_name": "Jankowski",
                "first_name": "Robert"
            },
            {
                "last_name": "Hozhabrierdi",
                "first_name": "Pegah"
            },
            {
                "last_name": "Bogu\u00f1\u00e1",
                "first_name": "Mari\u00e1n"
            },
            {
                "last_name": "Serrano",
                "first_name": "M. \u00c1ngeles"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In existing models and embedding methods of networked systems, node features describing their qualities are usually overlooked in favor of focusing solely on node connectivity. This study introduces $FiD$-Mercator, a model-based ultra-low dimensional reduction technique that integrates node features with network structure to create $D$-dimensional maps of complex networks in a hyperbolic space. This embedding method efficiently uses features as an initial condition, guiding the search of nodes' coordinates towards an optimal solution. The research reveals that downstream task performance improves with the correlation between network connectivity and features, emphasizing the importance of such correlation for enhancing the description and predictability of real networks. Simultaneously, hyperbolic embedding's ability to reproduce local network properties remains unaffected by the inclusion of features. The findings highlight the necessity for developing novel network embedding techniques capable of exploiting such correlations to optimize both network structure and feature association jointly in the future. ",
        "title": "Feature-aware ultra-low dimensional reduction of real networks",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09372",
        "abstract_url": "http://arxiv.org/abs/2401.09372",
        "authors": [
            {
                "last_name": "Edelmann",
                "first_name": "Dominik"
            },
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Bal\u00e1zs"
            },
            {
                "last_name": "Lubich",
                "first_name": "Christian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper studies an evolving bulk--surface finite element method for a model of tissue growth, which is a modification of the model of Eyles, King and Styles (2019). The model couples a Poisson equation on the domain with a forced mean curvature flow of the free boundary, with nontrivial bulk--surface coupling in both the velocity law of the evolving surface and the boundary condition of the Poisson equation. The numerical method discretizes evolution equations for the mean curvature and the outer normal and it uses a harmonic extension of the surface velocity into the bulk. The discretization admits a convergence analysis in the case of continuous finite elements of polynomial degree at least two. The stability of the discretized bulk--surface coupling is a major concern. The error analysis combines stability estimates and consistency estimates to yield optimal-order $H^1$-norm error bounds for the computed tissue pressure and for the surface position, velocity, normal vector and mean curvature. Numerical experiments illustrate and complement the theoretical results. ",
        "title": "Numerical analysis of an evolving bulk--surface model of tumour growth",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09375",
        "abstract_url": "http://arxiv.org/abs/2401.09375",
        "authors": [
            {
                "last_name": "J",
                "first_name": "Veejay Karthik"
            },
            {
                "last_name": "Vachhani",
                "first_name": "Leena"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Self-navigation in non-coordinating crowded environments is formidably challenging within multi-agent systems consisting of non-holonomic robots operating through local sensing. Our primary objective is the development of a novel, rapid, sensor-driven, self-navigation controller that directly computes control commands to enable safe maneuvering while coexisting with other agents. We propose an input-constrained feedback controller meticulously crafted for non-holonomic mobile robots and the characterization of associated invariant sets. The invariant sets are the key to maintaining stability and safety amidst the non-cooperating agents. We then propose a planning strategy that strategically guides the generation of invariant sets toward the agent's intended target. This enables the agents to directly compute theoretically safe control inputs without explicitly requiring pre-planned paths/trajectories to reliably navigate through crowded multi-agent environments. The practicality of our technique is demonstrated through hardware experiments, and the ability to parallelize computations to shorten computational durations for synthesizing safe control commands. The proposed approach finds potential applications in crowded multi-agent scenarios that require rapid control computations based on perceived safety bounds during run-time. ",
        "title": "Self-navigation in crowds: An invariant set-based approach",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09376",
        "abstract_url": "http://arxiv.org/abs/2401.09376",
        "authors": [
            {
                "last_name": "Slote",
                "first_name": "Kevin"
            },
            {
                "last_name": "Lee",
                "first_name": "Elaine"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In the realm of machine learning and statistical modeling, practitioners often work under the assumption of accessible, static, labeled data for evaluation and training. However, this assumption often deviates from reality where data may be private, encrypted, difficult- to-measure, or unlabeled. In this paper, we bridge this gap by adapting the Hui-Walter paradigm, a method traditionally applied in epidemiology and medicine, to the field of machine learning. This approach enables us to estimate key performance metrics such as false positive rate, false negative rate, and priors in scenarios where no ground truth is available. We further extend this paradigm for handling online data, opening up new possibilities for dynamic data environments. Our methodology involves partitioning data into latent classes to simulate multiple data populations (if natural populations are unavailable) and independently training models to replicate multiple tests. By cross-tabulating binary outcomes across ensemble categorizers and multiple populations, we are able to estimate unknown parameters through Gibbs sampling, eliminating the need for ground-truth or labeled data. This paper showcases the potential of our methodology to transform machine learning practices by allowing for accurate model assessment under dynamic and uncertain data conditions. ",
        "title": "Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter  Paradigm for Performance Estimation in Online and Static Settings",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09382",
        "abstract_url": "http://arxiv.org/abs/2401.09382",
        "authors": [
            {
                "last_name": "Yoo",
                "first_name": "Uksang"
            },
            {
                "last_name": "Lopez",
                "first_name": "Ziven"
            },
            {
                "last_name": "Ichnowski",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Oh",
                "first_name": "Jean"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Soft robotic shape estimation and proprioception are challenging because of soft robot's complex deformation behaviors and infinite degrees of freedom. A soft robot's continuously deforming body makes it difficult to integrate rigid sensors and to reliably estimate its shape. In this work, we present Proprioceptive Omnidirectional End-effector (POE), which has six embedded microphones across the tendon-driven soft robot's surface. We first introduce novel applications of previously proposed 3D reconstruction methods to acoustic signals from the microphones for soft robot shape proprioception. To improve the proprioception pipeline's training efficiency and model prediction consistency, we present POE-M. POE-M first predicts key point positions from the acoustic signal observations with the embedded microphone array. Then we utilize an energy-minimization method to reconstruct a physically admissible high-resolution mesh of POE given the estimated key points. We evaluate the mesh reconstruction module with simulated data and the full POE-M pipeline with real-world experiments. We demonstrate that POE-M's explicit guidance of the key points during the mesh reconstruction process provides robustness and stability to the pipeline with ablation studies. POE-M reduced the maximum Chamfer distance error by 23.10 % compared to the state-of-the-art end-to-end soft robot proprioception models and achieved 4.91 mm average Chamfer distance error during evaluation. ",
        "title": "POE: Acoustic Soft Robotic Proprioception for Omnidirectional  End-effectors",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09383",
        "abstract_url": "http://arxiv.org/abs/2401.09383",
        "authors": [
            {
                "last_name": "Mohr",
                "first_name": "Gideon"
            },
            {
                "last_name": "Guarnieri",
                "first_name": "Marco"
            },
            {
                "last_name": "Reineke",
                "first_name": "Jan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Microarchitectural attacks compromise security by exploiting software-visible artifacts of microarchitectural optimizations such as caches and speculative execution. Defending against such attacks at the software level requires an appropriate abstraction at the instruction set architecture (ISA) level that captures microarchitectural leakage. Hardware-software leakage contracts have recently been proposed as such an abstraction. In this paper, we propose a semi-automatic methodology for synthesizing hardware-software leakage contracts for open-source microarchitectures. For a given ISA, our approach relies on human experts to (a) capture the space of possible contracts in the form of contract templates and (b) devise a test-case generation strategy to explore a microarchitecture's potential leakage. For a given implementation of an ISA, these two ingredients are then used to automatically synthesize the most precise leakage contract that is satisfied by the microarchitecture. We have instantiated this methodology for the RISC-V ISA and applied it to the Ibex and CVA6 open-source processors. Our experiments demonstrate the practical applicability of the methodology and uncover subtle and unexpected leaks. ",
        "title": "Synthesizing Hardware-Software Leakage Contracts for RISC-V Open-Source  Processors",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09384",
        "abstract_url": "http://arxiv.org/abs/2401.09384",
        "authors": [
            {
                "last_name": "Guan",
                "first_name": "Yanran"
            },
            {
                "last_name": "van Kaick",
                "first_name": "Oliver"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CV",
            "LG"
        ],
        "abstract": "  Methods that use neural networks for synthesizing 3D shapes in the form of a part-based representation have been introduced over the last few years. These methods represent shapes as a graph or hierarchy of parts and enable a variety of applications such as shape sampling and reconstruction. However, current methods do not allow easily regenerating individual shape parts according to user preferences. In this paper, we investigate techniques that allow the user to generate multiple, diverse suggestions for individual parts. Specifically, we experiment with multimodal deep generative models that allow sampling diverse suggestions for shape parts and focus on models which have not been considered in previous work on shape synthesis. To provide a comparative study of these techniques, we introduce a method for synthesizing 3D shapes in a part-based representation and evaluate all the part suggestion techniques within this synthesis method. In our method, which is inspired by previous work, shapes are represented as a set of parts in the form of implicit functions which are then positioned in space to form the final shape. Synthesis in this representation is enabled by a neural network architecture based on an implicit decoder and a spatial transformer. We compare the various multimodal generative models by evaluating their performance in generating part suggestions. Our contribution is to show with qualitative and quantitative evaluations which of the new techniques for multimodal part generation perform the best and that a synthesis method based on the top-performing techniques allows the user to more finely control the parts that are generated in the 3D shapes while maintaining high shape fidelity when reconstructing shapes. ",
        "title": "Diverse Part Synthesis for 3D Shape Creation",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09386",
        "abstract_url": "http://arxiv.org/abs/2401.09386",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Luchuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Pinxin"
            },
            {
                "last_name": "Chen",
                "first_name": "Lele"
            },
            {
                "last_name": "Liu",
                "first_name": "Celong"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenliang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent years have witnessed considerable achievements in facial avatar reconstruction with neural volume rendering. Despite notable advancements, the reconstruction of complex and dynamic head movements from monocular videos still suffers from capturing and restoring fine-grained details. In this work, we propose a novel approach, named Tri$^2$-plane, for monocular photo-realistic volumetric head avatar reconstructions. Distinct from the existing works that rely on a single tri-plane deformation field for dynamic facial modeling, the proposed Tri$^2$-plane leverages the principle of feature pyramids and three top-to-down lateral connections tri-planes for details improvement. It samples and renders facial details at multiple scales, transitioning from the entire face to specific local regions and then to even more refined sub-regions. Moreover, we incorporate a camera-based geometry-aware sliding window method as an augmentation in training, which improves the robustness beyond the canonical space, with a particular improvement in cross-identity generation capabilities. Experimental outcomes indicate that the Tri$^2$-plane not only surpasses existing methodologies but also achieves superior performance across both quantitative metrics and qualitative assessments through experiments. ",
        "title": "Tri$^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09387",
        "abstract_url": "http://arxiv.org/abs/2401.09387",
        "authors": [
            {
                "last_name": "Hallyburton",
                "first_name": "R. Spencer"
            },
            {
                "last_name": "Hunt",
                "first_name": "David"
            },
            {
                "last_name": "Luo",
                "first_name": "Shaocheng"
            },
            {
                "last_name": "Pajic",
                "first_name": "Miroslav"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The performance and safety of autonomous vehicles (AVs) deteriorates under adverse environments and adversarial actors. The investment in multi-sensor, multi-agent (MSMA) AVs is meant to promote improved efficiency of travel and mitigate safety risks. Unfortunately, minimal investment has been made to develop security-aware MSMA sensor fusion pipelines leaving them vulnerable to adversaries. To advance security analysis of AVs, we develop the Multi-Agent Security Testbed, MAST, in the Robot Operating System (ROS2). Our framework is scalable for general AV scenarios and is integrated with recent multi-agent datasets. We construct the first bridge between AVstack and ROS and develop automated AV pipeline builds to enable rapid AV prototyping. We tackle the challenge of deploying variable numbers of agent/adversary nodes at launch-time with dynamic topic remapping. Using this testbed, we motivate the need for security-aware AV architectures by exposing the vulnerability of centralized multi-agent fusion pipelines to (un)coordinated adversary models in case studies and Monte Carlo analysis. ",
        "title": "A Multi-Agent Security Testbed for the Analysis of Attacks and Defenses  in Collaborative Sensor Fusion",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09388",
        "abstract_url": "http://arxiv.org/abs/2401.09388",
        "authors": [
            {
                "last_name": "Lykov",
                "first_name": "Artem"
            },
            {
                "last_name": "Litvinov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Konenkov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Prochii",
                "first_name": "Rinat"
            },
            {
                "last_name": "Burtsev",
                "first_name": "Nikita"
            },
            {
                "last_name": "Abdulkarim",
                "first_name": "Ali Alridha"
            },
            {
                "last_name": "Bazhenov",
                "first_name": "Artem"
            },
            {
                "last_name": "Berman",
                "first_name": "Vladimir"
            },
            {
                "last_name": "Tsetserukou",
                "first_name": "Dzmitry"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper introduces CognitiveDog, a pioneering development of quadruped robot with Large Multi-modal Model (LMM) that is capable of not only communicating with humans verbally but also physically interacting with the environment through object manipulation. The system was realized on Unitree Go1 robot-dog equipped with a custom gripper and demonstrated autonomous decision-making capabilities, independently determining the most appropriate actions and interactions with various objects to fulfill user-defined tasks. These tasks do not necessarily include direct instructions, challenging the robot to comprehend and execute them based on natural language input and environmental cues. The paper delves into the intricacies of this system, dataset characteristics, and the software architecture. Key to this development is the robot's proficiency in navigating space using Visual-SLAM, effectively manipulating and transporting objects, and providing insightful natural language commentary during task execution. Experimental results highlight the robot's advanced task comprehension and adaptability, underscoring its potential in real-world applications. The dataset used to fine-tune the robot-dog behavior generation model is provided at the following link: huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset ",
        "title": "CognitiveDog: Large Multimodal Model Based System to Translate Vision  and Language into Action of Quadruped Robot",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09393",
        "abstract_url": "http://arxiv.org/abs/2401.09393",
        "authors": [
            {
                "last_name": "Anagolum",
                "first_name": "Sashwat"
            },
            {
                "last_name": "Alavisamani",
                "first_name": "Narges"
            },
            {
                "last_name": "Das",
                "first_name": "Poulami"
            },
            {
                "last_name": "Qureshi",
                "first_name": "Moinuddin"
            },
            {
                "last_name": "Kessler",
                "first_name": "Eric"
            },
            {
                "last_name": "Shi",
                "first_name": "Yunong"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  Designing performant and noise-robust circuits for Quantum Machine Learning (QML) is challenging -- the design space scales exponentially with circuit size, and there are few well-supported guiding principles for QML circuit design. Although recent Quantum Circuit Search (QCS) methods attempt to search for performant QML circuits that are also robust to hardware noise, they directly adopt designs from classical Neural Architecture Search (NAS) that are misaligned with the unique constraints of quantum hardware, resulting in high search overheads and severe performance bottlenecks.   We present \\'Eliv\\'agar, a novel resource-efficient, noise-guided QCS framework. \\'Eliv\\'agar innovates in all three major aspects of QCS -- search space, search algorithm and candidate evaluation strategy -- to address the design flaws in current classically-inspired QCS methods. \\'Eliv\\'agar achieves hardware-efficiency and avoids an expensive circuit-mapping co-search via noise- and device topology-aware candidate generation. By introducing two cheap-to-compute predictors, Clifford noise resilience and Representational capacity, \\'Eliv\\'agar decouples the evaluation of noise robustness and performance, enabling early rejection of low-fidelity circuits and reducing circuit evaluation costs. Due to its resource-efficiency, \\'Eliv\\'agar can further search for data embeddings, significantly improving performance.   Based on a comprehensive evaluation of \\'Eliv\\'agar on 12 real quantum devices and 9 QML applications, \\'Eliv\\'agar achieves 5.3% higher accuracy and a 271$\\times$ speedup compared to state-of-the-art QCS methods. ",
        "title": "\\'Eliv\\'agar: Efficient Quantum Circuit Search for Classification",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09395",
        "abstract_url": "http://arxiv.org/abs/2401.09395",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Ghosal",
                "first_name": "Deepanway"
            },
            {
                "last_name": "Majumder",
                "first_name": "Navonil"
            },
            {
                "last_name": "Aditya",
                "first_name": "Somak"
            },
            {
                "last_name": "Mihalcea",
                "first_name": "Rada"
            },
            {
                "last_name": "Poria",
                "first_name": "Soujanya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness, in mathematical reasoning tasks, remains an open question. In response, we develop (i) an ontology of perturbations of maths questions, (ii) a semi-automatic method of perturbation, and (iii) a dataset of perturbed maths questions to probe the limits of LLM capabilities in mathematical reasoning tasks. These controlled perturbations span across multiple fine dimensions of the structural and representational aspects of maths questions. Using GPT-4, we generated the MORE dataset by perturbing randomly selected five seed questions from GSM8K. This process was guided by our ontology and involved a thorough automatic and manual filtering process, yielding a set of 216 maths problems. We conducted comprehensive evaluation of both closed-source and open-source LLMs on MORE. The results show a significant performance drop across all the models against the perturbed questions. This strongly suggests that current LLMs lack robust mathematical skills and deep reasoning abilities. This research not only identifies multiple gaps in the capabilities of current models, but also highlights multiple potential directions for future development. Our dataset will be made publicly available at https://huggingface.co/datasets/declare-lab/GSM8k_MORE. ",
        "title": "Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating  LLMs' Mathematical Competency through Ontology-guided Perturbations",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09407",
        "abstract_url": "http://arxiv.org/abs/2401.09407",
        "authors": [
            {
                "last_name": "Bethany",
                "first_name": "Mazal"
            },
            {
                "last_name": "Wherry",
                "first_name": "Brandon"
            },
            {
                "last_name": "Bethany",
                "first_name": "Emet"
            },
            {
                "last_name": "Vishwamitra",
                "first_name": "Nishant"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore, t-SNE visualizations of the embeddings from a pretrained LLM's encoder show that they cannot reliably distinguish between human and machine-generated text. Based on our findings, we introduce a novel system, T5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder combined with LLM embedding sub-clustering to address the text produced by diverse generators and domains in the real world. We evaluate our approach across 9 machine-generated text systems and 9 domains and find that our approach provides state-of-the-art generalization ability, with an average increase in F1 score on machine-generated text of 19.6\\% on unseen generators and domains compared to the top performing existing approaches and correctly attributes the generator of text with an accuracy of 93.6\\%. ",
        "title": "Deciphering Textual Authenticity: A Generalized Strategy through the  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated  Text",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09410",
        "abstract_url": "http://arxiv.org/abs/2401.09410",
        "authors": [
            {
                "last_name": "Corti\u00f1as-Lorenzo",
                "first_name": "Karina"
            },
            {
                "last_name": "Lindley",
                "first_name": "Si\u00e2n"
            },
            {
                "last_name": "Larsen-Ledet",
                "first_name": "Ida"
            },
            {
                "last_name": "Mitra",
                "first_name": "Bhaskar"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC"
        ],
        "abstract": "  Knowledge can't be disentangled from people. As AI knowledge systems mine vast volumes of work-related data, the knowledge that's being extracted and surfaced is intrinsically linked to the people who create and use it. When these systems get embedded in organizational settings, the information that is brought to the foreground and the information that's pushed to the periphery can influence how individuals see each other and how they see themselves at work. In this paper, we present the looking-glass metaphor and use it to conceptualize AI knowledge systems as systems that reflect and distort, expanding our view on transparency requirements, implications and challenges. We formulate transparency as a key mediator in shaping different ways of seeing, including seeing into the system, which unveils its capabilities, limitations and behavior, and seeing through the system, which shapes workers' perceptions of their own contributions and others within the organization. Recognizing the sociotechnical nature of these systems, we identify three transparency dimensions necessary to realize the value of AI knowledge systems, namely system transparency, procedural transparency and transparency of outcomes. We discuss key challenges hindering the implementation of these forms of transparency, bringing to light the wider sociotechnical gap and highlighting directions for future Computer-supported Cooperative Work (CSCW) research. ",
        "title": "Through the Looking-Glass: Transparency Implications and Challenges in  Enterprise AI Knowledge Systems",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09412",
        "abstract_url": "http://arxiv.org/abs/2401.09412",
        "authors": [
            {
                "last_name": "Orvedal",
                "first_name": "Asbj\u00f8rn O."
            },
            {
                "last_name": "Lin",
                "first_name": "Hsuan-Yin"
            },
            {
                "last_name": "Rosnes",
                "first_name": "Eirik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "CR"
        ],
        "abstract": "  We consider the problem of weakly-private information retrieval (WPIR) when data is encoded by a maximum distance separable code and stored across multiple servers. In WPIR, a user wishes to retrieve a piece of data from a set of servers without leaking too much information about which piece of data she is interested in. We study and provide the first WPIR protocols for this scenario and present results on their optimal trade-off between download rate and information leakage using the maximal leakage privacy metric. ",
        "title": "Weakly-Private Information Retrieval From MDS-Coded Distributed Storage",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09413",
        "abstract_url": "http://arxiv.org/abs/2401.09413",
        "authors": [
            {
                "last_name": "Vobecky",
                "first_name": "Antonin"
            },
            {
                "last_name": "Sim\u00e9oni",
                "first_name": "Oriane"
            },
            {
                "last_name": "Hurych",
                "first_name": "David"
            },
            {
                "last_name": "Gidaris",
                "first_name": "Spyros"
            },
            {
                "last_name": "Bursuc",
                "first_name": "Andrei"
            },
            {
                "last_name": "P\u00e9rez",
                "first_name": "Patrick"
            },
            {
                "last_name": "Sivic",
                "first_name": "Josef"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We describe an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images with the objective of enabling 3D grounding, segmentation and retrieval of free-form language queries. This is a challenging problem because of the 2D-3D ambiguity and the open-vocabulary nature of the target tasks, where obtaining annotated training data in 3D is difficult. The contributions of this work are three-fold. First, we design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Second, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language and (iii) LiDAR point clouds, and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations. Finally, we demonstrate quantitatively the strengths of the proposed model on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes. You can find the project page here https://vobecant.github.io/POP3D. ",
        "title": "POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09414",
        "abstract_url": "http://arxiv.org/abs/2401.09414",
        "authors": [
            {
                "last_name": "Zhuang",
                "first_name": "Shaobin"
            },
            {
                "last_name": "Li",
                "first_name": "Kunchang"
            },
            {
                "last_name": "Chen",
                "first_name": "Xinyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaohui"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yali"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "MM"
        ],
        "abstract": "  In this work, we present Vlogger, a generic AI system for generating a minute-level video blog (i.e., vlog) of user descriptions. Different from short videos with a few seconds, vlog often contains a complex storyline with diversified scenes, which is challenging for most existing video generation approaches. To break through this bottleneck, our Vlogger smartly leverages Large Language Model (LLM) as Director and decomposes a long video generation task of vlog into four key stages, where we invoke various foundation models to play the critical roles of vlog professionals, including (1) Script, (2) Actor, (3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings, our Vlogger can generate vlogs through explainable cooperation of top-down planning and bottom-up shooting. Moreover, we introduce a novel video diffusion model, ShowMaker, which serves as a videographer in our Vlogger for generating the video snippet of each shooting scene. By incorporating Script and Actor attentively as textual and visual prompts, it can effectively enhance spatial-temporal coherence in the snippet. Besides, we design a concise mixed training paradigm for ShowMaker, boosting its capacity for both T2V generation and prediction. Finally, the extensive experiments show that our method achieves state-of-the-art performance on zero-shot T2V generation and prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs from open-world descriptions, without loss of video coherence on script and actor. The code and model is all available at https://github.com/zhuangshaobin/Vlogger. ",
        "title": "Vlogger: Make Your Dream A Vlog",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09415",
        "abstract_url": "http://arxiv.org/abs/2401.09415",
        "authors": [
            {
                "last_name": "Alderman",
                "first_name": "Seth J."
            },
            {
                "last_name": "Luikart",
                "first_name": "Roan W."
            },
            {
                "last_name": "Marshall",
                "first_name": "Nicholas F."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper studies the effect of adding geometrically smoothed momentum to the randomized Kaczmarz algorithm, which is an instance of stochastic gradient descent on a linear least squares loss function. We prove a result about the expected error in the direction of singular vectors of the matrix defining the least squares loss. We present several numerical examples illustrating the utility of our result and pose several questions. ",
        "title": "Randomized Kaczmarz with geometrically smoothed momentum",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09416",
        "abstract_url": "http://arxiv.org/abs/2401.09416",
        "authors": [
            {
                "last_name": "Yeh",
                "first_name": "Yu-Ying"
            },
            {
                "last_name": "Huang",
                "first_name": "Jia-Bin"
            },
            {
                "last_name": "Kim",
                "first_name": "Changil"
            },
            {
                "last_name": "Xiao",
                "first_name": "Lei"
            },
            {
                "last_name": "Nguyen-Phuoc",
                "first_name": "Thu"
            },
            {
                "last_name": "Khan",
                "first_name": "Numair"
            },
            {
                "last_name": "Zhang",
                "first_name": "Cheng"
            },
            {
                "last_name": "Chandraker",
                "first_name": "Manmohan"
            },
            {
                "last_name": "Marshall",
                "first_name": "Carl S"
            },
            {
                "last_name": "Dong",
                "first_name": "Zhao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengqin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We present TextureDreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a pivotal challenge in vision and graphics. Industrial companies hire experienced artists to manually craft textures for 3D assets. Classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. Our core idea, personalized geometry-aware score distillation (PGSD), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. Our integration and several essential modifications substantially improve the texture quality. Experiments on real images spanning different categories show that TextureDreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art. ",
        "title": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware  Diffusion",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09417",
        "abstract_url": "http://arxiv.org/abs/2401.09417",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Lianghui"
            },
            {
                "last_name": "Liao",
                "first_name": "Bencheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qian"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinggang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., Mamba, have shown great potential for long sequence modeling. Building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance of visual representation learning on self-attention is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim. ",
        "title": "Vision Mamba: Efficient Visual Representation Learning with  Bidirectional State Space Model",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09419",
        "abstract_url": "http://arxiv.org/abs/2401.09419",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Chung Min"
            },
            {
                "last_name": "Wu",
                "first_name": "Mingxuan"
            },
            {
                "last_name": "Kerr",
                "first_name": "Justin"
            },
            {
                "last_name": "Goldberg",
                "first_name": "Ken"
            },
            {
                "last_name": "Tancik",
                "first_name": "Matthew"
            },
            {
                "last_name": "Kanazawa",
                "first_name": "Angjoo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at https://www.garfield.studio/ ",
        "title": "GARField: Group Anything with Radiance Fields",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09420",
        "abstract_url": "http://arxiv.org/abs/2401.09420",
        "authors": [
            {
                "last_name": "Lammie",
                "first_name": "Corey"
            },
            {
                "last_name": "Ponzina",
                "first_name": "Flavio"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Klein",
                "first_name": "Joshua"
            },
            {
                "last_name": "Zapater",
                "first_name": "Marina"
            },
            {
                "last_name": "Boybat",
                "first_name": "Irem"
            },
            {
                "last_name": "Sebastian",
                "first_name": "Abu"
            },
            {
                "last_name": "Ansaloni",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Atienza",
                "first_name": "David"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  When arranged in a crossbar configuration, resistive memory devices can be used to execute MVM, the most dominant operation of many ML algorithms, in constant time complexity. Nonetheless, when performing computations in the analog domain, novel challenges are introduced in terms of arithmetic precision and stochasticity, due to non-ideal circuit and device behaviour. Moreover, these non-idealities have a temporal dimension, resulting in a degrading application accuracy over time. Facing these challenges, we propose a novel framework, named LionHeart, to obtain hybrid analog-digital mappings to execute DL inference workloads using heterogeneous accelerators. The accuracy-constrained mappings derived by LionHeart showcase, across different DNNs and datasets, high accuracy and potential for speedup. The results of the full system simulations highlight run-time reductions and energy efficiency gains that exceed 6X, with a user-defined accuracy threshold with respect to a fully digital floating point implementation. ",
        "title": "LionHeart: A Layer-based Mapping Framework for Heterogeneous Systems  with Analog In-Memory Computing Tiles",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:quant-ph/0309033",
        "abstract_url": "http://arxiv.org/abs/quant-ph/0309033",
        "authors": [
            {
                "last_name": "La Mura",
                "first_name": "Pierfrancesco"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Correlated equilibria are sometimes more efficient than the Nash equilibria of a game without signals. We investigate whether the availability of quantum signals in the context of a classical strategic game may allow the players to achieve even better efficiency than in any correlated equilibrium with classical signals, and find the answer to be positive. ",
        "title": "Correlated Equilibria of Classical Strategic Games with Quantum Signals",
        "date": "2003-09-02",
        "group": "cs"
    }
]
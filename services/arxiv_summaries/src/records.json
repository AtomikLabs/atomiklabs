[
    {
        "identifier": "oai:arXiv.org:1204.1134",
        "abstract_url": "http://arxiv.org/abs/1204.1134",
        "authors": [
            {
                "last_name": "Carlucci",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Zdanowski",
                "first_name": "Konrad"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LO",
            "",
            ""
        ],
        "abstract": "  We characterize the computational content and the proof-theoretic strength of\na Ramsey-type theorem for bi-colorings of so-called {\\em exactly large} sets.\nAn {\\it exactly large} set is a set $X\\subset\\Nat$ such that\n$\\card(X)=\\min(X)+1$. The theorem we analyze is as follows. For every infinite\nsubset $M$ of $\\Nat$, for every coloring $C$ of the exactly large subsets of\n$M$ in two colors, there exists and infinite subset $L$ of $M$ such that $C$ is\nconstant on all exactly large subsets of $L$. This theorem is essentially due\nto Pudl\\`ak and R\\\"odl and independently to Farmaki. We prove that --- over\nComputable Mathematics --- this theorem is equivalent to closure under the\n$\\omega$ Turing jump (i.e., under arithmetical truth). Natural combinatorial\ntheorems at this level of complexity are rare. Our results give a complete\ncharacterization of the theorem from the point of view of Computable\nMathematics and of the Proof Theory of Arithmetic. This nicely extends the\ncurrent knowledge about the strength of Ramsey Theorem. We also show that\nanalogous results hold for a related principle based on the Regressive Ramsey\nTheorem. In addition we give a further characterization in terms of truth\npredicates over Peano Arithmetic. We conjecture that analogous results hold for\nlarger ordinals.\n",
        "title": "The strength of Ramsey Theorem for coloring relatively large sets",
        "date": "2012-04-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.00118",
        "abstract_url": "http://arxiv.org/abs/2104.00118",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Peipei"
            },
            {
                "last_name": "Rupp",
                "first_name": "Andreas"
            },
            {
                "last_name": "Kanschat",
                "first_name": "Guido"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Uniform convergence of the geometric multigrid V-cycle is proven for HDG\nmethods with a new set of assumptions on the injection operators from coarser\nto finer meshes. The scheme involves standard smoothers and local solvers which\nare bounded, convergent, and consistent. Elliptic regularity is used in the\nproofs. The new assumptions admit injection operators local to a single coarse\ngrid cell. Examples for admissible injection operators are given. The analysis\napplies to the hybridized local discontinuous Galerkin method, hybridized\nRaviart-Thomas, and hybridized Brezzi-Douglas-Marini mixed element methods.\nNumerical experiments are provided to confirm the theoretical results.\n",
        "title": "Analysis of injection operators in multigrid solvers for hybridized\n  discontinuous Galerkin methods",
        "date": "2021-03-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.09029",
        "abstract_url": "http://arxiv.org/abs/2104.09029",
        "authors": [
            {
                "last_name": "Layeghy",
                "first_name": "Siamak"
            },
            {
                "last_name": "Gallagher",
                "first_name": "Marcus"
            },
            {
                "last_name": "Portmann",
                "first_name": "Marius"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CR",
            "LG"
        ],
        "abstract": "  Network Intrusion Detection Systems (NIDSs) are an increasingly important\ntool for the prevention and mitigation of cyber attacks. A number of labelled\nsynthetic datasets generated have been generated and made publicly available by\nresearchers, and they have become the benchmarks via which new ML-based NIDS\nclassifiers are being evaluated. Recently published results show excellent\nclassification performance with these datasets, increasingly approaching 100\npercent performance across key evaluation metrics such as accuracy, F1 score,\netc. Unfortunately, we have not yet seen these excellent academic research\nresults translated into practical NIDS systems with such near-perfect\nperformance. This motivated our research presented in this paper, where we\nanalyse the statistical properties of the benign traffic in three of the more\nrecent and relevant NIDS datasets, (CIC, UNSW, ...). As a comparison, we\nconsider two datasets obtained from real-world production networks, one from a\nuniversity network and one from a medium size Internet Service Provider (ISP).\nOur results show that the two real-world datasets are quite similar among\nthemselves in regards to most of the considered statistical features. Equally,\nthe three synthetic datasets are also relatively similar within their group.\nHowever, and most importantly, our results show a distinct difference of most\nof the considered statistical features between the three synthetic datasets and\nthe two real-world datasets. Since ML relies on the basic assumption of\ntraining and test datasets being sampled from the same distribution, this\nraises the question of how well the performance results of ML-classifiers\ntrained on the considered synthetic datasets can translate and generalise to\nreal-world networks. We believe this is an interesting and relevant question\nwhich provides motivation for further research in this space.\n",
        "title": "Benchmarking the Benchmark -- Analysis of Synthetic NIDS Datasets",
        "date": "2021-04-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.11448",
        "abstract_url": "http://arxiv.org/abs/2104.11448",
        "authors": [
            {
                "last_name": "Vizca\u00edno",
                "first_name": "Aurora"
            },
            {
                "last_name": "de Guzm\u00e1n",
                "first_name": "Ignacio Garc\u00eda-Rodr\u00edguez"
            },
            {
                "last_name": "Manjavacas",
                "first_name": "Antonio"
            },
            {
                "last_name": "Garc\u00eda",
                "first_name": "F\u00e9lix"
            },
            {
                "last_name": "Cruz-Lemus",
                "first_name": "Jos\u00e9 A."
            },
            {
                "last_name": "Serrano",
                "first_name": "Manuel \u00c1ngel"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Technology has changed both our way of life and the way in which we learn.\nStudents now attend lectures with laptops and mobile phones, and this situation\nis accentuated in the case of students on Computer Science degrees, since they\nrequire their computers in order to participate in both theoretical and\npractical lessons. Problems, however, arise when the students' social networks\nare opened on their computers and they receive notifications that interrupt\ntheir work. We set up a workshop regarding time, thoughts and attention\nmanagement with the objective of teaching our students techniques that would\nallow them to manage interruptions, concentrate better and definitively make\nbetter use of their time. Those who took part in the workshop were then\nevaluated to discover its effects. The results obtained are quite optimistic\nand are described in this paper with the objective of encouraging other\nuniversities to perform similar initiatives.\n",
        "title": "How to help university students to manage their interruptions and\n  improve their attention and time management",
        "date": "2021-04-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2107.12312",
        "abstract_url": "http://arxiv.org/abs/2107.12312",
        "authors": [
            {
                "last_name": "Pecho-Ninapaytan",
                "first_name": "Andrea"
            },
            {
                "last_name": "Zambrano-Zuta",
                "first_name": "Stefany"
            },
            {
                "last_name": "Vargas-Bianchi",
                "first_name": "Lizardo"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The spread of fake news has been increasing, which gives rise to a special\ninterest in the development of identification and coping skills among news\nconsumers so that they can filter out misleading information. Studies suggest\nthat older people share more fake news from social media. There is scarce\nliterature that analyse how baby boomers behave in the face of fake news. The\npurpose of this study is to examine how female baby boomers deal with fake news\non Facebook and their available resources to learn how to identify and handle\ndubious information. A qualitative study and thematic analysis were conducted\nusing information obtained from interviewing female baby boomers. Four themes\nemerge from the analysis, revealing that participants recognise that they can\nidentify fake news but may not always be able to do so due to limitations in\ntheir understanding of an issue or uncertainty about its source. Participants\nshow participants empirically develop critical identification and filtering\nskills with the assistance from close family members.\n",
        "title": "'No, auntie, that's false': Female baby boomers develop critical skills\n  to confront fake news with guidance from relatives",
        "date": "2021-07-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2107.12879",
        "abstract_url": "http://arxiv.org/abs/2107.12879",
        "authors": [
            {
                "last_name": "Vargas-Bianchi",
                "first_name": "Lizardo"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Much research has been conducted on how consumption is related to human\nrelations, for example, consumer communities organized around specific brands,\nor the way people use products to define their own identity and transmit a\ndesired image. However, only a scarcity of research has examined the\nconsumption behaviour when the fundamental intention is to leverage group\nbelonging. The literature comprises a single theoretical framework that\ndescribes this behaviour, a nascent proposition that has not been tested. This\nstudy examines the transferability of that theoretical framework in a different\ncontext than the one used for its proposal and its extent on the phenomenon of\nconsuming to leverage belonging. A qualitative deductive case study and a\npattern matching analysis technique were employed, followed by a structural\ncoding analysis of interview data. The findings revealed the model is\ntransferable, however its conceptual extent on the phenomenon it addresses\nfaces limitations. These findings allow the proposal of an alternative\nframework, the Belonging-Oriented Consumption Model. This model provides a\ntheoretical basis for future research on consumer belonging behaviour.\n",
        "title": "Consumer belonging behaviour: Qualitative testing of a theoretical\n  framework and proposal of an alternative model",
        "date": "2021-07-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2108.04186",
        "abstract_url": "http://arxiv.org/abs/2108.04186",
        "authors": [
            {
                "last_name": "Thilakarathne",
                "first_name": "Haritha"
            },
            {
                "last_name": "Nibali",
                "first_name": "Aiden"
            },
            {
                "last_name": "He",
                "first_name": "Zhen"
            },
            {
                "last_name": "Morgan",
                "first_name": "Stuart"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a novel deep learning based group activity recognition approach\ncalled the Pose Only Group Activity Recognition System (POGARS), designed to\nuse only tracked poses of people to predict the performed group activity. In\ncontrast to existing approaches for group activity recognition, POGARS uses 1D\nCNNs to learn spatiotemporal dynamics of individuals involved in a group\nactivity and forgo learning features from pixel data. The proposed model uses a\nspatial and temporal attention mechanism to infer person-wise importance and\nmulti-task learning for simultaneously performing group and individual action\nclassification. Experimental results confirm that POGARS achieves highly\ncompetitive results compared to state-of-the-art methods on a widely used\npublic volleyball dataset despite only using tracked pose as input. Further our\nexperiments show by using pose only as input, POGARS has better generalization\ncapabilities compared to methods that use RGB as input.\n",
        "title": "Pose is all you need: The pose only group activity recognition system\n  (POGARS)",
        "date": "2021-08-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.06068",
        "abstract_url": "http://arxiv.org/abs/2201.06068",
        "authors": [
            {
                "last_name": "Kepner",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Bernays",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Buckley",
                "first_name": "Stephen"
            },
            {
                "last_name": "Cho",
                "first_name": "Kenjiro"
            },
            {
                "last_name": "Conrad",
                "first_name": "Cary"
            },
            {
                "last_name": "Daigle",
                "first_name": "Leslie"
            },
            {
                "last_name": "Erhardt",
                "first_name": "Keeley"
            },
            {
                "last_name": "Gadepally",
                "first_name": "Vijay"
            },
            {
                "last_name": "Greene",
                "first_name": "Barry"
            },
            {
                "last_name": "Jones",
                "first_name": "Michael"
            },
            {
                "last_name": "Knake",
                "first_name": "Robert"
            },
            {
                "last_name": "Maggs",
                "first_name": "Bruce"
            },
            {
                "last_name": "Michaleas",
                "first_name": "Peter"
            },
            {
                "last_name": "Meiners",
                "first_name": "Chad"
            },
            {
                "last_name": "Morris",
                "first_name": "Andrew"
            },
            {
                "last_name": "Pentland",
                "first_name": "Alex"
            },
            {
                "last_name": "Pisharody",
                "first_name": "Sandeep"
            },
            {
                "last_name": "Powazek",
                "first_name": "Sarah"
            },
            {
                "last_name": "Prout",
                "first_name": "Andrew"
            },
            {
                "last_name": "Reiner",
                "first_name": "Philip"
            },
            {
                "last_name": "Suzuki",
                "first_name": "Koichi"
            },
            {
                "last_name": "Takahashi",
                "first_name": "Kenji"
            },
            {
                "last_name": "Tauber",
                "first_name": "Tony"
            },
            {
                "last_name": "Walker",
                "first_name": "Leah"
            },
            {
                "last_name": "Stetson",
                "first_name": "Douglas"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CY",
            "NI",
            "SI"
        ],
        "abstract": "  Adversarial Internet robots (botnets) represent a growing threat to the safe\nuse and stability of the Internet. Botnets can play a role in launching\nadversary reconnaissance (scanning and phishing), influence operations\n(upvoting), and financing operations (ransomware, market manipulation, denial\nof service, spamming, and ad click fraud) while obfuscating tailored tactical\noperations. Reducing the presence of botnets on the Internet, with the\naspirational target of zero, is a powerful vision for galvanizing policy\naction. Setting a global goal, encouraging international cooperation, creating\nincentives for improving networks, and supporting entities for botnet takedowns\nare among several policies that could advance this goal. These policies raise\nsignificant questions regarding proper authorities/access that cannot be\nanswered in the abstract. Systems analysis has been widely used in other\ndomains to achieve sufficient detail to enable these questions to be dealt with\nin concrete terms. Defeating botnets using an observe-pursue-counter\narchitecture is analyzed, the technical feasibility is affirmed, and the\nauthorities/access questions are significantly narrowed. Recommended next steps\ninclude: supporting the international botnet takedown community, expanding\nnetwork observatories, enhancing the underlying network science at scale,\nconducting detailed systems analysis, and developing appropriate policy\nframeworks.\n",
        "title": "Zero Botnets: An Observe-Pursue-Counter Approach",
        "date": "2022-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.12673",
        "abstract_url": "http://arxiv.org/abs/2201.12673",
        "authors": [
            {
                "last_name": "Rasetto",
                "first_name": "Marco"
            },
            {
                "last_name": "Wan",
                "first_name": "Qingzhou"
            },
            {
                "last_name": "Akolkar",
                "first_name": "Himanshu"
            },
            {
                "last_name": "Shi",
                "first_name": "Bertram"
            },
            {
                "last_name": "Xiong",
                "first_name": "Feng"
            },
            {
                "last_name": "Benosman",
                "first_name": "Ryad"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Neuromorphic engineering has led to the necessary process of rethinking of\nhow we process and integrate information, analyze data, and use the resulting\ninsights to improve computation and avoid the current high power and latency of\nArtificial Intelligence (AI) hardware. Current neuromorphic processors are,\nhowever, limited by digital technologies, which cannot reproduce the abilities\nof biological neural computation in terms of power, latency and area cost. In\nthis paper, we show that the combined use of the dynamic properties of\nmemristors to implement a model of synaptic integration and the determination\nof the correct level of abstraction of biological neural networks has the\npotential to open a new range of capabilities for neuromorphic processors. We\ntest this approach using a novel three-terminal LixWO3 electrochemical\nmemristor, by deriving its conductance model and using it to emulate synaptic\ntemporal kernel computation in the context of a pattern recognition task. We\nshow that these devices allow for robust results with no loss in precision\nwhile opening the path for an energy efficient approach to build novel\nbio-inspired processing units in silicon.\n",
        "title": "The Challenges Ahead for Bio-inspired Neuromorphic Event Processors: How\n  Memristors Dynamic Properties Could Revolutionize Machine Learning",
        "date": "2022-01-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2202.12438",
        "abstract_url": "http://arxiv.org/abs/2202.12438",
        "authors": [
            {
                "last_name": "Dvo\u0159\u00e1k",
                "first_name": "Pavel"
            },
            {
                "last_name": "Krawczyk",
                "first_name": "Monika"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Novotn\u00e1",
                "first_name": "Jana"
            },
            {
                "last_name": "Rz\u0105\u017cewski",
                "first_name": "Pawe\u0142"
            },
            {
                "last_name": "\u017buk",
                "first_name": "Aneta"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  A locally surjective homomorphism from a graph $G$ to a graph $H$ is an\nedge-preserving mapping from $V(G)$ to $V(H)$ that is surjective in the\nneighborhood of each vertex in $G$. In the list locally surjective homomorphism\nproblem, denoted by LLSHom($H$), the graph $H$ is fixed and the instance\nconsists of a graph $G$ whose every vertex is equipped with a subset of $V(H)$,\ncalled list. We ask for the existence of a locally surjective homomorphism from\n$G$ to $H$, where every vertex of $G$ is mapped to a vertex from its list. In\nthis paper, we study the complexity of the LLSHom($H$) problem in $F$-free\ngraphs, i.e., graphs that exclude a fixed graph $F$ as an induced subgraph. We\naim to understand for which pairs $(H,F)$ the problem can be solved in\nsubexponential time.\n  We show that for all graphs $H$, for which the problem is NP-hard in general\ngraphs, it cannot be solved in subexponential time in $F$-free graphs unless\n$F$ is a bounded-degree forest or the ETH fails. The initial study reveals that\na natural subfamily of bounded-degree forests $F$ that might lead to some\ntractability results is the family $\\mathcal S$ consisting of forests whose\nevery component has at most three leaves. In this case, we exhibit the\nfollowing dichotomy theorem: besides the cases that are polynomial-time\nsolvable in general graphs, the graphs $H \\in \\{P_3,C_4\\}$ are the only\nconnected ones that allow for a subexponential-time algorithm in $F$-free\ngraphs for every $F \\in \\mathcal S$ (unless the ETH fails).\n",
        "title": "List Locally Surjective Homomorphisms in Hereditary Graph Classes",
        "date": "2022-02-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2203.05103",
        "abstract_url": "http://arxiv.org/abs/2203.05103",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Wei",
                "first_name": "Shikui"
            },
            {
                "last_name": "Lu",
                "first_name": "Qiming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Neural Ordinary Differential Equations (Neural ODEs) construct the continuous\ndynamics of hidden units using ordinary differential equations specified by a\nneural network, demonstrating promising results on many tasks. However, Neural\nODEs still do not perform well on image recognition tasks. The possible reason\nis that the one-hot encoding vector commonly used in Neural ODEs can not\nprovide enough supervised information. We propose a new training based on\nknowledge distillation to construct more powerful and robust Neural ODEs\nfitting image recognition tasks. Specially, we model the training of Neural\nODEs into a teacher-student learning process, in which we propose ResNets as\nthe teacher model to provide richer supervised information. The experimental\nresults show that the new training manner can improve the classification\naccuracy of Neural ODEs by 24% on CIFAR10 and 5% on SVHN. In addition, we also\nquantitatively discuss the effect of both knowledge distillation and time\nhorizon in Neural ODEs on robustness against adversarial examples. The\nexperimental analysis concludes that introducing the knowledge distillation and\nincreasing the time horizon can improve the robustness of Neural ODEs against\nadversarial examples.\n",
        "title": "Improving Neural ODEs via Knowledge Distillation",
        "date": "2022-03-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2204.07319",
        "abstract_url": "http://arxiv.org/abs/2204.07319",
        "authors": [
            {
                "last_name": "Hung",
                "first_name": "Nguyen"
            },
            {
                "last_name": "Rego",
                "first_name": "Francisco"
            },
            {
                "last_name": "Quintas",
                "first_name": "Joao"
            },
            {
                "last_name": "Cruz",
                "first_name": "Joao"
            },
            {
                "last_name": "Jacinto",
                "first_name": "Marcelo"
            },
            {
                "last_name": "Souto",
                "first_name": "David"
            },
            {
                "last_name": "Potes",
                "first_name": "Andre"
            },
            {
                "last_name": "Sebastiao",
                "first_name": "Luis"
            },
            {
                "last_name": "Pascoal",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  This article presents an in-depth review of the topic of path following for\nautonomous robotic vehicles, with a specific focus on vehicle motion in two\ndimensional space (2D). From a control system standpoint, path following can be\nformulated as the problem of stabilizing a path following error system that\ndescribes the dynamics of position and possibly orientation errors of a vehicle\nwith respect to a path, with the errors defined in an appropriate reference\nframe. In spite of the large variety of path following methods described in the\nliterature we show that, in principle, most of them can be categorized in two\ngroups: stabilization of the path following error system expressed either in\nthe vehicle's body frame or in a frame attached to a \"reference point\" moving\nalong the path, such as a Frenet-Serret (F-S) frame or a Parallel Transport\n(P-T) frame. With this observation, we provide a unified formulation that is\nsimple but general enough to cover many methods available in the literature. We\nthen discuss the advantages and disadvantages of each method, comparing them\nfrom the design and implementation standpoint. We further show experimental\nresults of the path following methods obtained from field trials testing with\nunder-actuated and fully-actuated autonomous marine vehicles. In addition, we\nintroduce open-source Matlab and Gazebo/ROS simulation toolboxes that are\nhelpful in testing path following methods prior to their integration in the\ncombined guidance, navigation, and control systems of autonomous vehicles.\n",
        "title": "A review of path following control strategies for autonomous robotic\n  vehicles: theory, simulations, and experiments",
        "date": "2022-04-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2205.02065",
        "abstract_url": "http://arxiv.org/abs/2205.02065",
        "authors": [
            {
                "last_name": "Posso",
                "first_name": "Julien"
            },
            {
                "last_name": "Bois",
                "first_name": "Guy"
            },
            {
                "last_name": "Savaria",
                "first_name": "Yvon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Spacecraft pose estimation is an essential computer vision application that\ncan improve the autonomy of in-orbit operations. An ESA/Stanford competition\nbrought out solutions that seem hardly compatible with the constraints imposed\non spacecraft onboard computers. URSONet is among the best in the competition\nfor its generalization capabilities but at the cost of a tremendous number of\nparameters and high computational complexity. In this paper, we propose\nMobile-URSONet: a spacecraft pose estimation convolutional neural network with\n178 times fewer parameters while degrading accuracy by no more than four times\ncompared to URSONet.\n",
        "title": "Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose\n  Estimation",
        "date": "2022-05-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2205.08143",
        "abstract_url": "http://arxiv.org/abs/2205.08143",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhu",
                "first_name": "Binbin"
            },
            {
                "last_name": "Kong",
                "first_name": "Lingsi"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianlin"
            },
            {
                "last_name": "Gao",
                "first_name": "Bin"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianhua"
            },
            {
                "last_name": "Tian",
                "first_name": "Dingcheng"
            },
            {
                "last_name": "Yao",
                "first_name": "Yudong"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Ultrasound-guided nerve block anesthesia (UGNB) is a high-tech visual nerve\nblock anesthesia method that can observe the target nerve and its surrounding\nstructures, the puncture needle's advancement, and local anesthetics spread in\nreal-time. The key in UGNB is nerve identification. With the help of deep\nlearning methods, the automatic identification or segmentation of nerves can be\nrealized, assisting doctors in completing nerve block anesthesia accurately and\nefficiently. Here, we establish a public dataset containing 320 ultrasound\nimages of brachial plexus (BP). Three experienced doctors jointly produce the\nBP segmentation ground truth and label brachial plexus trunks. We design a\nbrachial plexus segmentation system (BPSegSys) based on deep learning. BPSegSys\nachieves experienced-doctor-level nerve identification performance in various\nexperiments. We evaluate BPSegSys' performance in terms of\nintersection-over-union (IoU), a commonly used performance measure for\nsegmentation experiments. Considering three dataset groups in our established\npublic dataset, the IoU of BPSegSys are 0.5238, 0.4715, and 0.5029,\nrespectively, which exceed the IoU 0.5205, 0.4704, and 0.4979 of experienced\ndoctors. In addition, we show that BPSegSys can help doctors identify brachial\nplexus trunks more accurately, with IoU improvement up to 27%, which has\nsignificant clinical application value.\n",
        "title": "Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A\n  Comparative Study with Doctors' Manual Segmentation",
        "date": "2022-05-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.10943",
        "abstract_url": "http://arxiv.org/abs/2206.10943",
        "authors": [
            {
                "last_name": "Linders",
                "first_name": "Viktor"
            },
            {
                "last_name": "Birken",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Conservation and consistency are fundamental properties of discretizations of\nsystems of hyperbolic conservation laws. Here, these concepts are extended to\nthe realm of iterative methods by formally defining locally conservative and\nflux consistent iterations. These concepts are of both theoretical and\npractical importance: Based on recent work by the authors, it is shown that\npseudo-time iterations using explicit Runge-Kutta methods are locally\nconservative but not necessarily flux consistent. An extension of the\nLax-Wendroff theorem is presented, revealing convergence towards weak solutions\nof a temporally retarded system of conservation laws. Each equation is modified\nin the same way, namely by a particular scalar factor multiplying the spatial\nflux terms. A technique for enforcing flux consistency, and thereby recovering\nconvergence, is presented. Further, local conservation is established for all\nKrylov subspace methods, with and without restarts, and for Newton's method\nunder certain assumptions on the discretization. Thus it is shown that\nNewton-Krylov methods are locally conservative, although not necessarily flux\nconsistent. Numerical experiments with the 2D compressible Euler equations\ncorroborate the theoretical results. Further numerical investigations of the\nimpact of flux consistency on Newton-Krylov methods indicate that its effect is\ncase dependent, and diminishes as the number of iterations grow.\n",
        "title": "Locally conservative and flux consistent iterative methods",
        "date": "2022-06-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.15464",
        "abstract_url": "http://arxiv.org/abs/2206.15464",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Andi"
            },
            {
                "last_name": "Cincio",
                "first_name": "Lukasz"
            },
            {
                "last_name": "Coles",
                "first_name": "Patrick J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We study the problem of learning the parameters for the Hamiltonian of a\nquantum many-body system, given limited access to the system. In this work, we\nbuild upon recent approaches to Hamiltonian learning via derivative estimation.\nWe propose a protocol that improves the scaling dependence of prior works,\nparticularly with respect to parameters relating to the structure of the\nHamiltonian (e.g., its locality $k$). Furthermore, by deriving exact bounds on\nthe performance of our protocol, we are able to provide a precise numerical\nprescription for theoretically optimal settings of hyperparameters in our\nlearning protocol, such as the maximum evolution time (when learning with\nunitary dynamics) or minimum temperature (when learning with Gibbs states).\nThanks to these improvements, our protocol is practical for large problems: we\ndemonstrate this with a numerical simulation of our protocol on an 80-qubit\nsystem.\n",
        "title": "Practical Black Box Hamiltonian Learning",
        "date": "2022-06-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.07425",
        "abstract_url": "http://arxiv.org/abs/2207.07425",
        "authors": [
            {
                "last_name": "Hatzel",
                "first_name": "Meike"
            },
            {
                "last_name": "Jaffke",
                "first_name": "Lars"
            },
            {
                "last_name": "Lima",
                "first_name": "Paloma T."
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Sharma",
                "first_name": "Roohani"
            },
            {
                "last_name": "Sorge",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We show fixed-parameter tractability of the Directed Multicut problem with\nthree terminal pairs (with a randomized algorithm). This problem, given a\ndirected graph $G$, pairs of vertices (called terminals) $(s_1,t_1)$,\n$(s_2,t_2)$, and $(s_3,t_3)$, and an integer $k$, asks to find a set of at most\n$k$ non-terminal vertices in $G$ that intersect all $s_1t_1$-paths, all\n$s_2t_2$-paths, and all $s_3t_3$-paths. The parameterized complexity of this\ncase has been open since Chitnis, Cygan, Hajiaghayi, and Marx proved\nfixed-parameter tractability of the 2-terminal-pairs case at SODA 2012, and\nPilipczuk and Wahlstr\\\"{o}m proved the W[1]-hardness of the 4-terminal-pairs\ncase at SODA 2016.\n  On the technical side, we use two recent developments in parameterized\nalgorithms. Using the technique of directed flow-augmentation [Kim, Kratsch,\nPilipczuk, Wahlstr\\\"{o}m, STOC 2022] we cast the problem as a CSP problem with\nfew variables and constraints over a large ordered domain.We observe that this\nproblem can be in turn encoded as an FO model-checking task over a structure\nconsisting of a few 0-1 matrices. We look at this problem through the lenses of\ntwin-width, a recently introduced structural parameter [Bonnet, Kim,\nThomass\\'{e}, Watrigant, FOCS 2020]: By a recent characterization [Bonnet,\nGiocanti, Ossona de Mendes, Simon, Thomass\\'{e}, Toru\\'{n}czyk, STOC 2022] the\nsaid FO model-checking task can be done in FPT time if the said matrices have\nbounded grid rank. To complete the proof, we show an irrelevant vertex rule: If\nany of the matrices in the said encoding has a large grid minor, a vertex\ncorresponding to the ``middle'' box in the grid minor can be proclaimed\nirrelevant -- not contained in the sought solution -- and thus reduced.\n",
        "title": "Fixed-parameter tractability of Directed Multicut with three terminal\n  pairs parameterized by the size of the cutset: twin-width meets\n  flow-augmentation",
        "date": "2022-07-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.07426",
        "abstract_url": "http://arxiv.org/abs/2207.07426",
        "authors": [
            {
                "last_name": "Jaffke",
                "first_name": "Lars"
            },
            {
                "last_name": "Lima",
                "first_name": "Paloma T."
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Souza",
                "first_name": "Ueverton S."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  We study a generalization of the classic Global Min-Cut problem, called\nGlobal Label Min-Cut (or sometimes Global Hedge Min-Cut): the edges of the\ninput (multi)graph are labeled (or partitioned into color classes or hedges),\nand removing all edges of the same label (color or from the same hedge) costs\none. The problem asks to disconnect the graph at minimum cost.\n  While the $st$-cut version of the problem is known to be NP-hard, the above\nglobal cut version is known to admit a quasi-polynomial randomized $n^{O(\\log\n\\mathrm{OPT})}$-time algorithm due to Ghaffari, Karger, and Panigrahi [SODA\n2017]. They consider this as ``strong evidence that this problem is in P''. We\nshow that this is actually not the case. We complete the study of the\ncomplexity of the Global Label Min-Cut problem by showing that the\nquasi-polynomial running time is probably optimal: We show that the existence\nof an algorithm with running time $(np)^{o(\\log n/ (\\log \\log n)^2)}$ would\ncontradict the Exponential Time Hypothesis, where $n$ is the number of\nvertices, and $p$ is the number of labels in the input. The key step for the\nlower bound is a proof that Global Label Min-Cut is W[1]-hard when\nparameterized by the number of uncut labels. In other words, the problem is\ndifficult in the regime where almost all labels need to be cut to disconnect\nthe graph. To turn this lower bound into a quasi-polynomial-time lower bound,\nwe also needed to revisit the framework due to Marx [Theory Comput. 2010] of\nproving lower bounds assuming Exponential Time Hypothesis through the Subgraph\nIsomorphism problem parameterized by the number of edges of the pattern. Here,\nwe provide an alternative simplified proof of the hardness of this problem that\nis more versatile with respect to the choice of the regimes of the parameters.\n",
        "title": "A tight quasi-polynomial bound for Global Label Min-Cut",
        "date": "2022-07-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.13643",
        "abstract_url": "http://arxiv.org/abs/2208.13643",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lei"
            },
            {
                "last_name": "Liu",
                "first_name": "Xin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  Decentralized optimization with orthogonality constraints is found widely in\nscientific computing and data science. Since the orthogonality constraints are\nnonconvex, it is quite challenging to design efficient algorithms. Existing\napproaches leverage the geometric tools from Riemannian optimization to solve\nthis problem at the cost of high sample and communication complexities. To\nrelieve this difficulty, based on two novel techniques that can waive the\northogonality constraints, we propose a variance-reduced stochastic gradient\ntracking (VRSGT) algorithm with the convergence rate of $O(1 / k)$ to a\nstationary point. To the best of our knowledge, VRSGT is the first algorithm\nfor decentralized optimization with orthogonality constraints that reduces both\nsampling and communication complexities simultaneously. In the numerical\nexperiments, VRSGT has a promising performance in a real-world autonomous\ndriving application.\n",
        "title": "A Variance-Reduced Stochastic Gradient Tracking Algorithm for\n  Decentralized Optimization with Orthogonality Constraints",
        "date": "2022-08-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.14403",
        "abstract_url": "http://arxiv.org/abs/2208.14403",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Ayoosh"
            },
            {
                "last_name": "Kim",
                "first_name": "Hunmin"
            },
            {
                "last_name": "Yu",
                "first_name": "Simon"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Hovakimyan",
                "first_name": "Naira"
            },
            {
                "last_name": "Caccamo",
                "first_name": "Marco"
            },
            {
                "last_name": "Sha",
                "first_name": "Lui"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Perception of obstacles remains a critical safety concern for autonomous\nvehicles. Real-world collisions have shown that the autonomy faults leading to\nfatal collisions originate from obstacle existence detection. Open source\nautonomous driving implementations show a perception pipeline with complex\ninterdependent Deep Neural Networks. These networks are not fully verifiable,\nmaking them unsuitable for safety-critical tasks.\n  In this work, we present a safety verification of an existing LiDAR based\nclassical obstacle detection algorithm. We establish strict bounds on the\ncapabilities of this obstacle detection algorithm. Given safety standards, such\nbounds allow for determining LiDAR sensor properties that would reliably\nsatisfy the standards. Such analysis has as yet been unattainable for neural\nnetwork based perception systems. We provide a rigorous analysis of the\nobstacle detection system with empirical results based on real-world sensor\ndata.\n",
        "title": "Verifiable Obstacle Detection",
        "date": "2022-08-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2209.06171",
        "abstract_url": "http://arxiv.org/abs/2209.06171",
        "authors": [
            {
                "last_name": "Cook",
                "first_name": "Linda"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Reinald",
                "first_name": "Amadeus"
            },
            {
                "last_name": "Souza",
                "first_name": "U\u00e9verton S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            ""
        ],
        "abstract": "  An oriented graph is a digraph that does not contain a directed cycle of\nlength two. An (oriented) graph $D$ is $H$-free if $D$ does not contain $H$ as\nan induced sub(di)graph. The Gy\\'arf\\'as-Sumner conjecture is a widely-open\nconjecture on simple graphs, which states that for any forest $F$, there is\nsome function $f$ such that every $F$-free graph $G$ with clique number\n$\\omega(G)$ has chromatic number at most $f(\\omega(G))$. Aboulker, Charbit, and\nNaserasr [Extension of Gy\\'arf\\'as-Sumner Conjecture to Digraphs; E-JC 2021]\nproposed an analogue of this conjecture to the dichromatic number of oriented\ngraphs. The dichromatic number of a digraph $D$ is the minimum number of colors\nrequired to color the vertex set of $D$ so that no directed cycle in $D$ is\nmonochromatic.\n  Aboulker, Charbit, and Naserasr's $\\overrightarrow{\\chi}$-boundedness\nconjecture states that for every oriented forest $F$, there is some function\n$f$ such that every $F$-free oriented graph $D$ has dichromatic number at most\n$f(\\omega(D))$, where $\\omega(D)$ is the size of a maximum clique in the graph\nunderlying $D$. In this paper, we perform the first step towards proving\nAboulker, Charbit, and Naserasr's $\\overrightarrow{\\chi}$-boundedness\nconjecture by showing that it holds when $F$ is any orientation of a path on\nfour vertices.\n",
        "title": "Proving a directed analogue of the Gy\\'arf\\'as-Sumner conjecture for\n  orientations of $P_4$",
        "date": "2022-09-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2210.05405",
        "abstract_url": "http://arxiv.org/abs/2210.05405",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Ruolin"
            },
            {
                "last_name": "Ma",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ao"
            },
            {
                "last_name": "Dustdar",
                "first_name": "Schahram"
            },
            {
                "last_name": "Wang",
                "first_name": "Shangguang"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Recent developments in the aerospace industry have led to a dramatic\nreduction in the manufacturing and launch costs of low Earth orbit satellites.\nThe new trend enables the paradigm shift of satellite-terrestrial integrated\nnetworks with global coverage. In particular, the integration of 5G\ncommunication systems and satellites has the potential to restructure\nnext-generation mobile networks. By leveraging the network function\nvirtualization and network slicing, the orbital 5G core networks will\nfacilitate the coordination and management of network functions in\nsatellite-terrestrial integrated networks. We are the first to deploy a\nlightweight 5G core network on a real-world satellite to investigate its\nfeasibility. We conducted experiments to validate the onboard 5G core network\nfunctions. The validated procedures include registration and session setup\nprocedures. The results show that the 5G core network can function normally and\ngenerate correct signaling.\n",
        "title": "From Earth to Space: A First Deployment of 5G Core Network on Satellite",
        "date": "2022-10-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2210.11481",
        "abstract_url": "http://arxiv.org/abs/2210.11481",
        "authors": [
            {
                "last_name": "Clainche",
                "first_name": "Soledad Le"
            },
            {
                "last_name": "Ferrer",
                "first_name": "Esteban"
            },
            {
                "last_name": "Gibson",
                "first_name": "Sam"
            },
            {
                "last_name": "Cross",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Parente",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Vinuesa",
                "first_name": "Ricardo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  This review covers the new developments in machine learning (ML) that are\nimpacting the multi-disciplinary area of aerospace engineering, including\nfundamental fluid dynamics (experimental and numerical), aerodynamics,\nacoustics, combustion and structural health monitoring. We review the state of\nthe art, gathering the advantages and challenges of ML methods across different\naerospace disciplines and provide our view on future opportunities. The basic\nconcepts and the most relevant strategies for ML are presented together with\nthe most relevant applications in aerospace engineering, revealing that ML is\nimproving aircraft performance and that these techniques will have a large\nimpact in the near future.\n",
        "title": "Improving aircraft performance using machine learning: a review",
        "date": "2022-10-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2211.12049",
        "abstract_url": "http://arxiv.org/abs/2211.12049",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Ming"
            },
            {
                "last_name": "Xia",
                "first_name": "Zeke"
            },
            {
                "last_name": "Yue",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Huang",
                "first_name": "Yihao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Chen",
                "first_name": "Mingsong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  As a promising distributed machine learning paradigm that enables\ncollaborative training without compromising data privacy, Federated Learning\n(FL) has been increasingly used in AIoT (Artificial Intelligence of Things)\ndesign. However, due to the lack of efficient management of straggling devices,\nexisting FL methods greatly suffer from the problems of low inference accuracy\nand long training time. Things become even worse when taking various uncertain\nfactors (e.g., network delays, performance variances caused by process\nvariation) existing in AIoT scenarios into account. To address this issue, this\npaper proposes a novel asynchronous FL framework named GitFL, whose\nimplementation is inspired by the famous version control system Git. Unlike\ntraditional FL, the cloud server of GitFL maintains a master model (i.e., the\nglobal model) together with a set of branch models indicating the trained local\nmodels committed by selected devices, where the master model is updated based\non both all the pushed branch models and their version information, and only\nthe branch models after the pull operation are dispatched to devices. By using\nour proposed Reinforcement Learning (RL)-based device selection mechanism, a\npulled branch model with an older version will be more likely to be dispatched\nto a faster and less frequently selected device for the next round of local\ntraining. In this way, GitFL enables both effective control of model staleness\nand adaptive load balance of versioned models among straggling devices, thus\navoiding the performance deterioration. Comprehensive experimental results on\nwell-known models and datasets show that, compared with state-of-the-art\nasynchronous FL methods, GitFL can achieve up to 2.64X training acceleration\nand 7.88% inference accuracy improvements in various uncertain scenarios.\n",
        "title": "GitFL: Adaptive Asynchronous Federated Learning using Version Control",
        "date": "2022-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2211.14944",
        "abstract_url": "http://arxiv.org/abs/2211.14944",
        "authors": [
            {
                "last_name": "Valente",
                "first_name": "Luca"
            },
            {
                "last_name": "Tortorella",
                "first_name": "Yvan"
            },
            {
                "last_name": "Sinigaglia",
                "first_name": "Mattia"
            },
            {
                "last_name": "Tagliavini",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Capotondi",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            },
            {
                "last_name": "Rossi",
                "first_name": "Davide"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  IoT applications span a wide range in performance and memory footprint, under\ntight cost and power constraints. High-end applications rely on power-hungry\nSystems-on-Chip (SoCs) featuring powerful processors, large LPDDR/DDR3/4/5\nmemories, and supporting full-fledged Operating Systems (OS). On the contrary,\nlow-end applications typically rely on Ultra-Low-Power ucontrollers with a\n\"close to metal\" software environment and simple micro-kernel-based runtimes.\nEmerging applications and trends of IoT require the \"best of both worlds\":\ncheap and low-power SoC systems with a well-known and agile software\nenvironment based on full-fledged OS (e.g., Linux), coupled with extreme energy\nefficiency and parallel digital signal processing capabilities. We present\nHULK-V: an open-source Heterogeneous Linux-capable RISC-V-based SoC coupling a\n64-bit RISC-V processor with an 8-core Programmable Multi-Core Accelerator\n(PMCA), delivering up to 13.8 GOps, up to 157 GOps/W and accelerating the\nexecution of complex DSP and ML tasks by up to 112x over the host processor.\nHULK-V leverages a lightweight, fully digital memory hierarchy based on\nHyperRAM IoT DRAM that exposes up to 512 MB of DRAM memory to the host CPU.\nFeaturing HyperRAMs, HULK-V doubles the energy efficiency without significant\nperformance loss compared to featuring power-hungry LPDDR memories, requiring\nexpensive and large mixed-signal PHYs. HULK-V, implemented in Global Foundries\n22nm FDX technology, is a fully digital ultra-low-cost SoC running a 64-bit\nLinux software stack with OpenMP host-to-PMCA offload within a power envelope\nof just 250 mW.\n",
        "title": "HULK-V: a Heterogeneous Ultra-low-power Linux capable RISC-V SoC",
        "date": "2022-11-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.09560",
        "abstract_url": "http://arxiv.org/abs/2212.09560",
        "authors": [
            {
                "last_name": "Llorente",
                "first_name": "Victor J."
            },
            {
                "last_name": "Kou",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Valero",
                "first_name": "Eusebio"
            },
            {
                "last_name": "Ferrer",
                "first_name": "Esteban"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The Immersed Boundary Method (IBM) is a popular numerical approach to impose\nboundary conditions without relying on body-fitted grids, thus reducing the\ncostly effort of mesh generation. To obtain enhanced accuracy, IBM can be\ncombined with high-order methods (e.g., discontinuous Galerkin). For this\ncombination to be effective, an analysis of the numerical errors is essential.\nIn this work, we apply, for the first time, a modified equation analysis to the\ncombination of IBM (based on volume penalization) and high-order methods (based\non nodal discontinuous Galerkin methods) to analyze a priori numerical errors\nand obtain practical guidelines on the selection of IBM parameters. The\nanalysis is performed on a linear advection-diffusion equation with Dirichlet\nboundary conditions. Three ways to penalize the immerse boundary are\nconsidered, the first penalizes the solution inside the IBM region (classic\napproach), whilst the second and third penalize the first and second\nderivatives of the solution. We find optimal combinations of the penalization\nparameters, including the first and second penalizing derivatives, resulting in\nminimum errors. We validate the theoretical analysis with numerical experiments\nfor one- and two-dimensional advection-diffusion equations.\n",
        "title": "A modified equation analysis for immersed boundary methods based on\n  volume penalization: applications to linear advection-diffusion and\n  high-order discontinuous Galerkin schemes",
        "date": "2022-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.14344",
        "abstract_url": "http://arxiv.org/abs/2212.14344",
        "authors": [
            {
                "last_name": "Grimm",
                "first_name": "Volker"
            },
            {
                "last_name": "Kliesch",
                "first_name": "Tobias"
            },
            {
                "last_name": "Quispel",
                "first_name": "G. R. W."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Discrete gradients (DG) or more exactly discrete gradient methods are time\nintegration schemes that are custom-built to preserve first integrals or\nLyapunov functions of a given ordinary differential equation (ODE). In\nconservative molecular dynamics (MD) simulations, the energy of the system is\nconstant and therefore a first integral of motion. Hence, discrete gradient\nmethods seem to be a natural choice as an integration scheme in conservative\nmolecular dynamics simulations.\n",
        "title": "Discrete gradients in short-range molecular dynamics simulations",
        "date": "2022-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.02464",
        "abstract_url": "http://arxiv.org/abs/2302.02464",
        "authors": [
            {
                "last_name": "Bijalwan",
                "first_name": "Ashutosh"
            },
            {
                "last_name": "Mu\u00f1oz",
                "first_name": "Jose J"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Optimal Control Problems consist on the optimisation of an objective\nfunctional subjected to a set of Ordinary Differential Equations. In this work,\nwe consider the effects on the stability of the numerical solution when this\noptimisation is discretised in time. In particular, we analyse a OCP with a\nquadratic functional and linear ODE, discretised with Mid-point and implicit\nEuler. We show that the numerical stability and the presence of numerical\noscillations depends not only on the time-step size, but also on the parameters\nof the objective functional, which measures the amount of control input.\nFinally, we also show with an illustrative example that these results also\ncarry over non-linear optimal control problems\n",
        "title": "On the numerical stability of discretised Optimal Control Problems",
        "date": "2023-02-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.04607",
        "abstract_url": "http://arxiv.org/abs/2302.04607",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiabei"
            },
            {
                "last_name": "Pang",
                "first_name": "Yanwei"
            },
            {
                "last_name": "Cao",
                "first_name": "Jiale"
            },
            {
                "last_name": "Sun",
                "first_name": "Hanqing"
            },
            {
                "last_name": "Shao",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Li",
                "first_name": "Xuelong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Weakly supervised person search aims to perform joint pedestrian detection\nand re-identification (re-id) with only person bounding-box annotations.\nRecently, the idea of contrastive learning is initially applied to weakly\nsupervised person search, where two common contrast strategies are memory-based\ncontrast and intra-image contrast. We argue that current intra-image contrast\nis shallow, which suffers from spatial-level and occlusion-level variance. In\nthis paper, we present a novel deep intra-image contrastive learning using a\nSiamese network. Two key modules are spatial-invariant contrast (SIC) and\nocclusion-invariant contrast (OIC). SIC performs many-to-one contrasts between\ntwo branches of Siamese network and dense prediction contrasts in one branch of\nSiamese network. With these many-to-one and dense contrasts, SIC tends to learn\ndiscriminative scale-invariant and location-invariant features to solve\nspatial-level variance. OIC enhances feature consistency with the masking\nstrategy to learn occlusion-invariant features. Extensive experiments are\nperformed on two person search datasets CUHK-SYSU and PRW, respectively. Our\nmethod achieves a state-of-the-art performance among weakly supervised one-step\nperson search approaches. We hope that our simple intra-image contrastive\nlearning can provide more paradigms on weakly supervised person search. The\nsource code is available at \\url{https://github.com/jiabeiwangTJU/DICL}.\n",
        "title": "Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step\n  Person Search",
        "date": "2023-02-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.07623",
        "abstract_url": "http://arxiv.org/abs/2302.07623",
        "authors": [
            {
                "last_name": "Nikolopoulos",
                "first_name": "Georgios M."
            },
            {
                "last_name": "Fischlin",
                "first_name": "Marc"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CR"
        ],
        "abstract": "  Quantum key-distribution protocols allow two honest distant parties to\nestablish a common truly random secret key in the presence of powerful\nadversaries, provided that the two users share beforehand a short secret key.\nThis pre-shared secret key is used mainly for authentication purposes in the\npost-processing of classical data that have been obtained during the quantum\ncommunication stage, and it prevents a man-in-the-middle attack. The necessity\nof a pre-shared key is usually considered as the main drawback of quantum\nkey-distribution protocols, which becomes even stronger for large networks\ninvolving more that two users. Here we discuss the conditions under which\nphysical unclonable function can be integrated in currently available quantum\nkey-distribution systems, in order to facilitate the generation and the\ndistribution of the necessary pre-shared key, with the smallest possible cost\nin the security of the systems. Moreover, the integration of physical\nunclonable functions in quantum key-distribution networks allows for real-time\nauthentication of the devices that are connected to the network.\n",
        "title": "Quantum key distribution with post-processing driven by physical\n  unclonable functions",
        "date": "2023-02-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.05432",
        "abstract_url": "http://arxiv.org/abs/2303.05432",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Anwesha"
            },
            {
                "last_name": "Upadhyay",
                "first_name": "Shashankaditya"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Indranil"
            },
            {
                "last_name": "Panigrahi",
                "first_name": "Prasanta K."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SI",
            ""
        ],
        "abstract": "  Market competition has a role which is directly or indirectly associated with\ninfluential effects of individual sectors on other sectors of the economy. The\npresent work studies the relative position of a product in the market through\nthe identification of influential spreaders and its corresponding effect on the\nother sectors of the market using complex network analysis during the pre-,\nin-, and post-crisis induced lockdown periods using daily data of NSE from\nDecember, 2019 to June, 2021. The existing approaches using different\ncentrality measures failed to distinguish between the positive and negative\ninfluences of the different sectors in the market which act as spreaders. To\nobviate this problem, this paper presents an effective measure called LIEST\n(Local Influential Effects for Specific Target) that can examine the positive\nand negative influences separately with respect to any crisis period. LIEST\nconsiders the combined impact of all possible nodes which are at most three\nsteps away from the specific targets for the networks. The essence of\nnon-linearity in the network dynamics without considering single node effect\nbecomes visible particularly in the proposed network.\n",
        "title": "Describing the effect of influential spreaders on the different sectors\n  of Indian market: a complex networks perspective",
        "date": "2022-10-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10502",
        "abstract_url": "http://arxiv.org/abs/2303.10502",
        "authors": [
            {
                "last_name": "Azumah",
                "first_name": "Sylvia W"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            },
            {
                "last_name": "Ozer",
                "first_name": "Murat"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Technological advancements have resulted in an exponential increase in the\nuse of online social networks (OSNs) worldwide. While online social networks\nprovide a great communication medium, they also increase the user's exposure to\nlife-threatening situations such as suicide, eating disorder, cybercrime,\ncompulsive behavior, anxiety, and depression. To tackle the issue of\ncyberbullying, most existing literature focuses on developing approaches to\nidentifying factors and understanding the textual factors associated with\ncyberbullying. While most of these approaches have brought great success in\ncyberbullying research, data availability needed to develop model detection\nremains a challenge in the research space. This paper conducts a comprehensive\nliterature review to provide an understanding of cyberbullying detection.\n",
        "title": "Cyberbullying in Text Content Detection: An Analytical Review",
        "date": "2023-03-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.14970",
        "abstract_url": "http://arxiv.org/abs/2303.14970",
        "authors": [
            {
                "last_name": "Dujmovi\u0107",
                "first_name": "Vida"
            },
            {
                "last_name": "Hickingbotham",
                "first_name": "Robert"
            },
            {
                "last_name": "Joret",
                "first_name": "Gwena\u00ebl"
            },
            {
                "last_name": "Micek",
                "first_name": "Piotr"
            },
            {
                "last_name": "Morin",
                "first_name": "Pat"
            },
            {
                "last_name": "Wood",
                "first_name": "David R."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM"
        ],
        "abstract": "  We prove that for every tree $T$ of radius $h$, there is an integer $c$ such\nthat every $T$-minor-free graph is contained in $H\\boxtimes K_c$ for some graph\n$H$ with pathwidth at most $2h-1$. This is a qualitative strengthening of the\nExcluded Tree Minor Theorem of Robertson and Seymour (GM I). We show that\nradius is the right parameter to consider in this setting, and $2h-1$ is the\nbest possible bound.\n",
        "title": "The Excluded Tree Minor Theorem Revisited",
        "date": "2023-03-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.11072",
        "abstract_url": "http://arxiv.org/abs/2304.11072",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Nafis Tanveer"
            },
            {
                "last_name": "Parra",
                "first_name": "Gonzalo De La Torre"
            },
            {
                "last_name": "Manuel",
                "first_name": "Dylan"
            },
            {
                "last_name": "Bou-Harb",
                "first_name": "Elias"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG"
        ],
        "abstract": "  Over the years, open-source software systems have become prey to threat\nactors. Even as open-source communities act quickly to patch the breach, code\nvulnerability screening should be an integral part of agile software\ndevelopment from the beginning. Unfortunately, current vulnerability screening\ntechniques are ineffective at identifying novel vulnerabilities or providing\ndevelopers with code vulnerability and classification. Furthermore, the\ndatasets used for vulnerability learning often exhibit distribution shifts from\nthe real-world testing distribution due to novel attack strategies deployed by\nadversaries and as a result, the machine learning model's performance may be\nhindered or biased. To address these issues, we propose a joint interpolated\nmultitasked unbiased vulnerability classifier comprising a transformer\n\"RoBERTa\" and graph convolution neural network (GCN). We present a training\nprocess utilizing a semantic vulnerability graph (SVG) representation from\nsource code, created by integrating edges from a sequential flow, control flow,\nand data flow, as well as a novel flow dubbed Poacher Flow (PF). Poacher flow\nedges reduce the gap between dynamic and static program analysis and handle\ncomplex long-range dependencies. Moreover, our approach reduces biases of\nclassifiers regarding unbalanced datasets by integrating Focal Loss objective\nfunction along with SVG. Remarkably, experimental results show that our\nclassifier outperforms state-of-the-art results on vulnerability detection with\nfewer false negatives and false positives. After testing our model across\nmultiple datasets, it shows an improvement of at least 2.41% and 18.75% in the\nbest-case scenario. Evaluations using N-day program samples demonstrate that\nour proposed approach achieves a 93% accuracy and was able to detect 4,\nzero-day vulnerabilities from popular GitHub repositories.\n",
        "title": "An Unbiased Transformer Source Code Learning with Semantic Vulnerability\n  Graph",
        "date": "2023-04-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.14853",
        "abstract_url": "http://arxiv.org/abs/2304.14853",
        "authors": [
            {
                "last_name": "Manjunath",
                "first_name": "Shashank"
            },
            {
                "last_name": "Perea",
                "first_name": "Jose A."
            },
            {
                "last_name": "Sathyanarayana",
                "first_name": "Aarti"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CG"
        ],
        "abstract": "  Topological data analysis (TDA) is an emerging technique for biological\nsignal processing. TDA leverages the invariant topological features of signals\nin a metric space for robust analysis of signals even in the presence of noise.\nIn this paper, we leverage TDA on brain connectivity networks derived from\nelectroencephalogram (EEG) signals to identify statistical differences between\npediatric patients with obstructive sleep apnea (OSA) and pediatric patients\nwithout OSA. We leverage a large corpus of data, and show that TDA enables us\nto see a statistical difference between the brain dynamics of the two groups.\n",
        "title": "Topological Data Analysis of Electroencephalogram Signals for Pediatric\n  Obstructive Sleep Apnea",
        "date": "2023-04-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.10945",
        "abstract_url": "http://arxiv.org/abs/2305.10945",
        "authors": [
            {
                "last_name": "Heisler",
                "first_name": "Marcel"
            },
            {
                "last_name": "Becker-Asano",
                "first_name": "Christian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper describes, how current Machine Learning (ML) techniques combined\nwith simple rule-based animation routines make an android robot head an\nembodied conversational agent with ChatGPT as its core component. The android\nrobot head is described, technical details are given of how lip-sync animation\nis being achieved, and general software design decisions are presented. A\npublic presentation of the system revealed improvement opportunities that are\nreported and that lead our iterative implementation approach.\n",
        "title": "An Android Robot Head as Embodied Conversational Agent",
        "date": "2023-05-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11728",
        "abstract_url": "http://arxiv.org/abs/2305.11728",
        "authors": [
            {
                "last_name": "Tabatabaei",
                "first_name": "Zahra"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adrian"
            },
            {
                "last_name": "Moll",
                "first_name": "Javier Oliver"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Digital pathology has revolutionized cancer diagnosis by leveraging\nContent-Based Medical Image Retrieval (CBMIR) for analyzing histopathological\nWhole Slide Images (WSIs). CBMIR enables searching for similar content,\nenhancing diagnostic reliability and accuracy. In 2020, breast and prostate\ncancer constituted 11.7% and 14.1% of cases, respectively, as reported by the\nGlobal Cancer Observatory (GCO). The proposed Unsupervised CBMIR (UCBMIR)\nreplicates the traditional cancer diagnosis workflow, offering a dependable\nmethod to support pathologists in WSI-based diagnostic conclusions. This\napproach alleviates pathologists' workload, potentially enhancing diagnostic\nefficiency. To address the challenge of the lack of labeled histopathological\nimages in CBMIR, a customized unsupervised Convolutional Auto Encoder (CAE) was\ndeveloped, extracting 200 features per image for the search engine component.\nUCBMIR was evaluated using widely-used numerical techniques in CBMIR, alongside\nvisual evaluation and comparison with a classifier. The validation involved\nthree distinct datasets, with an external evaluation demonstrating its\neffectiveness. UCBMIR outperformed previous studies, achieving a top 5 recall\nof 99% and 80% on BreaKHis and SICAPv2, respectively, using the first\nevaluation technique. Precision rates of 91% and 70% were achieved for BreaKHis\nand SICAPv2, respectively, using the second evaluation technique. Furthermore,\nUCBMIR demonstrated the capability to identify various patterns in patches,\nachieving an 81% accuracy in the top 5 when tested on an external image from\nArvaniti.\n",
        "title": "Towards More Transparent and Accurate Cancer Diagnosis with an\n  Unsupervised CAE Approach",
        "date": "2023-05-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.12567",
        "abstract_url": "http://arxiv.org/abs/2305.12567",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Linyuan"
            },
            {
                "last_name": "Xiong",
                "first_name": "Chenyan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Bajaj",
                "first_name": "Payal"
            },
            {
                "last_name": "Xie",
                "first_name": "Yiqing"
            },
            {
                "last_name": "Cheung",
                "first_name": "Alvin"
            },
            {
                "last_name": "Gao",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Song",
                "first_name": "Xia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper explores the effectiveness of model-generated signals in improving\nzero-shot generalization of text-to-text Transformers such as T5. We study\nvarious designs to pretrain T5 using an auxiliary model to construct more\nchallenging token replacements for the main model to denoise. Key aspects under\nstudy include the decoding target, the location of the RTD head, and the\nmasking pattern. Based on these studies, we develop a new model, METRO-T0,\nwhich is pretrained using the redesigned ELECTRA-Style pretraining strategies\nand then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all\nsimilar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU,\nand rivals the state-of-the-art T0-11B model with only 8% of its parameters.\nOur analysis on model's neural activation and parameter sensitivity reveals\nthat the effectiveness of METRO-T0 stems from more balanced contribution of\nparameters and better utilization of their capacity. The code and model\ncheckpoints are available at https://github.com/gonglinyuan/metro_t0.\n",
        "title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of\n  Text-to-Text Transformers",
        "date": "2023-05-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.12722",
        "abstract_url": "http://arxiv.org/abs/2305.12722",
        "authors": [
            {
                "last_name": "Nilsson",
                "first_name": "Gustav"
            },
            {
                "last_name": "Aquino",
                "first_name": "Alejandro D. Owen"
            },
            {
                "last_name": "Coogan",
                "first_name": "Samuel"
            },
            {
                "last_name": "Molzahn",
                "first_name": "Daniel K."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The ongoing electrification of the transportation fleet will increase the\nload on the electric power grid. Since both the transportation network and the\npower grid already experience periods of significant stress, joint analyses of\nboth infrastructures will most likely be necessary to ensure acceptable\noperation in the future. To enable such analyses, this paper presents an\nopen-source testbed that jointly simulates high-fidelity models of both the\nelectric distribution system and the transportation network. The testbed\nutilizes two open-source simulators, OpenDSS to simulate the electric\ndistribution system and the microscopic traffic simulator SUMO to simulate the\ntraffic dynamics. Electric vehicle charging links the electric distribution\nsystem and the transportation network models at vehicle locations determined\nusing publicly available parcel data. Leveraging high-fidelity synthetic\nelectric distribution system data from the SMART-DS project and transportation\nsystem data from OpenStreetMap, this testbed models the city of Greensboro, NC\ndown to the household level. Moreover, the methodology and the supporting\nscripts released with the testbed allow adaption to other areas where\nhigh-fidelity geolocated OpenDSS datasets are available. After describing the\ncomponents and usage of the testbed, we exemplify applications enabled by the\ntestbed via two scenarios modeling the extreme stresses encountered during\nevacuations.\n",
        "title": "GreenEVT: Greensboro Electric Vehicle Testbed",
        "date": "2023-05-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.14361",
        "abstract_url": "http://arxiv.org/abs/2305.14361",
        "authors": [
            {
                "last_name": "Scheper",
                "first_name": "Tjeerd V. olde"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The representation of arbitrary data in a biological system is one of the\nmost elusive elements of biological information processing. The often\nlogarithmic nature of information in amplitude and frequency presented to\nbiosystems prevents simple encapsulation of the information contained in the\ninput. Criticality Analysis (CA) is a bio-inspired method of information\nrepresentation within a controlled self-organised critical system that allows\nscale-free representation. This is based on the concept of a reservoir of\ndynamic behaviour in which self-similar data will create dynamic nonlinear\nrepresentations. This unique projection of data preserves the similarity of\ndata within a multidimensional neighbourhood. The input can be reduced\ndimensionally to a projection output that retains the features of the overall\ndata, yet has much simpler dynamic response. The method depends only on the\nrate control of chaos applied to the underlying controlled models, that allows\nthe encoding of arbitrary data, and promises optimal encoding of data given\nbiological relevant networks of oscillators. The CA method allows for a\nbiologically relevant encoding mechanism of arbitrary input to biosystems,\ncreating a suitable model for information processing in varying complexity of\norganisms and scale-free data representation for machine learning.\n",
        "title": "Criticality Analysis: Bio-inspired Nonlinear Data Representation",
        "date": "2023-05-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.14141",
        "abstract_url": "http://arxiv.org/abs/2306.14141",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Linhui"
            },
            {
                "last_name": "Liu",
                "first_name": "Hong"
            },
            {
                "last_name": "Song",
                "first_name": "Pinhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengyuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Underwater object detection (UOD) plays a significant role in aquaculture and\nmarine environmental protection. Considering the challenges posed by low\ncontrast and low-light conditions in underwater environments, several\nunderwater image enhancement (UIE) methods have been proposed to improve the\nquality of underwater images. However, only using the enhanced images does not\nimprove the performance of UOD, since it may unavoidably remove or alter\ncritical patterns and details of underwater objects. In contrast, we believe\nthat exploring the complementary information from the two domains is beneficial\nfor UOD. The raw image preserves the natural characteristics of the scene and\ntexture information of the objects, while the enhanced image improves the\nvisibility of underwater objects. Based on this perspective, we propose a Gated\nCross-domain Collaborative Network (GCC-Net) to address the challenges of poor\nvisibility and low contrast in underwater environments, which comprises three\ndedicated components. Firstly, a real-time UIE method is employed to generate\nenhanced images, which can improve the visibility of objects in low-contrast\nareas. Secondly, a cross-domain feature interaction module is introduced to\nfacilitate the interaction and mine complementary information between raw and\nenhanced image features. Thirdly, to prevent the contamination of unreliable\ngenerated results, a gated feature fusion module is proposed to adaptively\ncontrol the fusion ratio of cross-domain information. Our method presents a new\nUOD paradigm from the perspective of cross-domain information interaction and\nfusion. Experimental results demonstrate that the proposed GCC-Net achieves\nstate-of-the-art performance on four underwater datasets.\n",
        "title": "A Gated Cross-domain Collaborative Network for Underwater Object\n  Detection",
        "date": "2023-06-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.15412",
        "abstract_url": "http://arxiv.org/abs/2306.15412",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Haojie"
            },
            {
                "last_name": "Cao",
                "first_name": "Xueke"
            },
            {
                "last_name": "Dan",
                "first_name": "Tangpeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Yueguo"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  Vocal pitch is an important high-level feature in music audio processing.\nHowever, extracting vocal pitch in polyphonic music is more challenging due to\nthe presence of accompaniment. To eliminate the influence of the accompaniment,\nmost previous methods adopt music source separation models to obtain clean\nvocals from polyphonic music before predicting vocal pitches. As a result, the\nperformance of vocal pitch estimation is affected by the music source\nseparation models. To address this issue and directly extract vocal pitches\nfrom polyphonic music, we propose a robust model named RMVPE. This model can\nextract effective hidden features and accurately predict vocal pitches from\npolyphonic music. The experimental results demonstrate the superiority of RMVPE\nin terms of raw pitch accuracy (RPA) and raw chroma accuracy (RCA).\nAdditionally, experiments conducted with different types of noise show that\nRMVPE is robust across all signal-to-noise ratio (SNR) levels. The code of\nRMVPE is available at https://github.com/Dream-High/RMVPE.\n",
        "title": "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music",
        "date": "2023-06-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.00265",
        "abstract_url": "http://arxiv.org/abs/2307.00265",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Ying"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingqing"
            },
            {
                "last_name": "Chen",
                "first_name": "Wen"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Li",
                "first_name": "Ming"
            },
            {
                "last_name": "da Costa",
                "first_name": "Daniel Benevides"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper studies an intelligent reflecting surface (IRS)-aided\nmulti-antenna simultaneous wireless information and power transfer (SWIPT)\nsystem where an $M$-antenna access point (AP) serves $K$ single-antenna\ninformation users (IUs) and $J$ single-antenna energy users (EUs) with the aid\nof an IRS with phase errors. We explicitly concentrate on overloaded scenarios\nwhere $K + J > M$ and $K \\geq M$. Our goal is to maximize the minimum\nthroughput among all the IUs by optimizing the allocation of resources\n(including time, transmit beamforming at the AP, and reflect beamforming at the\nIRS), while guaranteeing the minimum amount of harvested energy at each EU.\nTowards this goal, we propose two user grouping (UG) schemes, namely, the\nnon-overlapping UG scheme and the overlapping UG scheme, where the difference\nlies in whether identical IUs can exist in multiple groups. Different IU groups\nare served in orthogonal time dimensions, while the IUs in the same group are\nserved simultaneously with all the EUs via spatial multiplexing. The two\nproblems corresponding to the two UG schemes are mixed-integer non-convex\noptimization problems and difficult to solve optimally. We propose efficient\nalgorithms for these two problems based on the big-M formulation, the penalty\nmethod, the block coordinate descent, and the successive convex approximation.\nSimulation results show that: 1) the non-robust counterparts of the proposed\nrobust designs are unsuitable for practical IRS-aided SWIPT systems with phase\nerrors since the energy harvesting constraints cannot be satisfied; 2) the\nproposed UG strategies can significantly improve the max-min throughput over\nthe benchmark schemes without UG or adopting random UG; 3) the overlapping UG\nscheme performs much better than its non-overlapping counterpart when the\nabsolute difference between $K$ and $M$ is small and the EH constraints are not\nstringent.\n",
        "title": "IRS-Aided Overloaded Multi-Antenna Systems: Joint User Grouping and\n  Resource Allocation",
        "date": "2023-07-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.07469",
        "abstract_url": "http://arxiv.org/abs/2307.07469",
        "authors": [
            {
                "last_name": "Wen",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Tang",
                "first_name": "Zixuan"
            },
            {
                "last_name": "Pang",
                "first_name": "Yunsheng"
            },
            {
                "last_name": "Ding",
                "first_name": "Beichen"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengyuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "RO"
        ],
        "abstract": "  Recognizing interactive action plays an important role in human-robot\ninteraction and collaboration. Previous methods use late fusion and\nco-attention mechanism to capture interactive relations, which have limited\nlearning capability or inefficiency to adapt to more interacting entities. With\nassumption that priors of each entity are already known, they also lack\nevaluations on a more general setting addressing the diversity of subjects. To\naddress these problems, we propose an Interactive Spatiotemporal Token\nAttention Network (ISTA-Net), which simultaneously model spatial, temporal, and\ninteractive relations. Specifically, our network contains a tokenizer to\npartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to\nrepresent motions of multiple diverse entities. By extending the entity\ndimension, ISTs provide better interactive representations. To jointly learn\nalong three dimensions in ISTs, multi-head self-attention blocks integrated\nwith 3D convolutions are designed to capture inter-token correlations. When\nmodeling correlations, a strict entity ordering is usually irrelevant for\nrecognizing interactive actions. To this end, Entity Rearrangement is proposed\nto eliminate the orderliness in ISTs for interchangeable entities. Extensive\nexperiments on four datasets verify the effectiveness of ISTA-Net by\noutperforming state-of-the-art methods. Our code is publicly available at\nhttps://github.com/Necolizer/ISTA-Net\n",
        "title": "Interactive Spatiotemporal Token Attention Network for Skeleton-based\n  General Interactive Action Recognition",
        "date": "2023-07-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.02541",
        "abstract_url": "http://arxiv.org/abs/2308.02541",
        "authors": [
            {
                "last_name": "Brandizzi",
                "first_name": "Nicolo'"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC",
            "MA",
            "",
            "",
            ""
        ],
        "abstract": "  In the recent shift towards human-centric AI, the need for machines to\naccurately use natural language has become increasingly important. While a\ncommon approach to achieve this is to train large language models, this method\npresents a form of learning misalignment where the model may not capture the\nunderlying structure and reasoning humans employ in using natural language,\npotentially leading to unexpected or unreliable behavior. Emergent\ncommunication (Emecom) is a field of research that has seen a growing number of\npublications in recent years, aiming to develop artificial agents capable of\nusing natural language in a way that goes beyond simple discriminative tasks\nand can effectively communicate and learn new concepts. In this review, we\npresent Emecom under two aspects. Firstly, we delineate all the common\nproprieties we find across the literature and how they relate to human\ninteractions. Secondly, we identify two subcategories and highlight their\ncharacteristics and open challenges. We encourage researchers to work together\nby demonstrating that different methods can be viewed as diverse solutions to a\ncommon problem and emphasize the importance of including diverse perspectives\nand expertise in the field. We believe a deeper understanding of human\ncommunication is crucial to developing machines that can accurately use natural\nlanguage in human-machine interactions.\n",
        "title": "Towards More Human-like AI Communication: A Review of Emergent\n  Communication Research",
        "date": "2023-08-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.04769",
        "abstract_url": "http://arxiv.org/abs/2308.04769",
        "authors": [
            {
                "last_name": "Hidaka",
                "first_name": "Ryo"
            },
            {
                "last_name": "Hamakawa",
                "first_name": "Yohei"
            },
            {
                "last_name": "Nakayama",
                "first_name": "Jun"
            },
            {
                "last_name": "Tatsumura",
                "first_name": "Kosuke"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "CE",
            ""
        ],
        "abstract": "  Correlation-diversified portfolios can be constructed by finding the maximum\nindependent sets (MISs) in market graphs with edges corresponding to\ncorrelations between two stocks. The computational complexity to find the MIS\nincreases exponentially as the size of the market graph increases, making the\nMIS selection in a large-scale market graph difficult. Here we construct a\ndiversified portfolio by solving the MIS problem for a large-scale market graph\nwith a combinatorial optimization solver (an Ising machine) based on a\nquantum-inspired algorithm called simulated bifurcation (SB) and investigate\nthe investment performance of the constructed portfolio using long-term\nhistorical market data. Comparisons using stock universes of various sizes\n[TOPIX 100, Nikkei 225, TOPIX 1000, and TOPIX (including approximately 2,000\nconstituents)] show that the SB-based solver outperforms conventional MIS\nsolvers in terms of computation-time and solution-accuracy. By using the\nSB-based solver, we optimized the parameters of a MIS portfolio strategy\nthrough iteration of the backcast simulation that calculates the performance of\nthe MIS portfolio strategy based on a large-scale universe covering more than\n1,700 Japanese stocks for a long period of 10 years. It has been found that the\nbest MIS portfolio strategy (Sharpe ratio = 1.16, annualized return/risk =\n16.3%/14.0%) outperforms the major indices such as TOPIX (0.66, 10.0%/15.2%)\nand MSCI Japan Minimum Volatility Index (0.64, 7.7%/12.1%) for the period from\n2013 to 2023.\n",
        "title": "Correlation-diversified portfolio construction by finding maximum\n  independent set in large-scale market graph",
        "date": "2023-08-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.12116",
        "abstract_url": "http://arxiv.org/abs/2308.12116",
        "authors": [
            {
                "last_name": "Reich",
                "first_name": "Christoph"
            },
            {
                "last_name": "Prangemeier",
                "first_name": "Tim"
            },
            {
                "last_name": "Koeppl",
                "first_name": "Heinz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Segmenting cells and tracking their motion over time is a common task in\nbiomedical applications. However, predicting accurate instance-wise\nsegmentation and cell motions from microscopy imagery remains a challenging\ntask. Using microstructured environments for analyzing single cells in a\nconstant flow of media adds additional complexity. While large-scale labeled\nmicroscopy datasets are available, we are not aware of any large-scale dataset,\nincluding both cells and microstructures. In this paper, we introduce the\ntrapped yeast cell (TYC) dataset, a novel dataset for understanding\ninstance-level semantics and motions of cells in microstructures. We release\n$105$ dense annotated high-resolution brightfield microscopy images, including\nabout $19$k instance masks. We also release $261$ curated video clips composed\nof $1293$ high-resolution microscopy images to facilitate unsupervised\nunderstanding of cell motions and morphology. TYC offers ten times more\ninstance annotations than the previously largest dataset, including cells and\nmicrostructures. Our effort also exceeds previous attempts in terms of\nmicrostructure variability, resolution, complexity, and capturing device\n(microscopy) variability. We facilitate a unified comparison on our novel\ndataset by introducing a standardized evaluation strategy. TYC and evaluation\ncode are publicly available under CC BY 4.0 license.\n",
        "title": "The TYC Dataset for Understanding Instance-Level Semantics and Motions\n  of Cells in Microstructures",
        "date": "2023-08-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.13694",
        "abstract_url": "http://arxiv.org/abs/2308.13694",
        "authors": [
            {
                "last_name": "McDermott",
                "first_name": "Matthew"
            },
            {
                "last_name": "Rife",
                "first_name": "Jason"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Because scanning-LIDAR sensors require finite time to create a point cloud,\nsensor motion during a scan warps the resulting image, a phenomenon known as\nmotion distortion or rolling shutter. Motion-distortion correction methods\nexist, but they rely on external measurements or Bayesian filtering over\nmultiple LIDAR scans. In this paper we propose a novel algorithm that performs\nsnapshot processing to obtain a motion-distortion correction. Snapshot\nprocessing, which registers a current LIDAR scan to a reference image without\nusing external sensors or Bayesian filtering, is particularly relevant for\nlocalization to a high-definition (HD) map. Our approach, which we call\nVelocity-corrected Iterative Compact Ellipsoidal Transformation (VICET),\nextends the well-known Normal Distributions Transform (NDT) algorithm to solve\njointly for both a 6 Degree-of-Freedom (DOF) rigid transform between two LIDAR\nscans and a set of 6DOF motion states that describe distortion within the\ncurrent LIDAR scan. Using experiments, we show that VICET achieves\nsignificantly higher accuracy than NDT or Iterative Closest Point (ICP)\nalgorithms when localizing a distorted raw LIDAR scan against an undistorted HD\nMap. We recommend the reader explore our open-source code and visualizations at\nhttps://github.com/mcdermatt/VICET, which supplements this manuscript.\n",
        "title": "Correcting Motion Distortion for LIDAR HD-Map Localization",
        "date": "2023-08-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.09676",
        "abstract_url": "http://arxiv.org/abs/2309.09676",
        "authors": [
            {
                "last_name": "Bogdoll",
                "first_name": "Daniel"
            },
            {
                "last_name": "Pavlitska",
                "first_name": "Svetlana"
            },
            {
                "last_name": "Klaus",
                "first_name": "Simon"
            },
            {
                "last_name": "Z\u00f6llner",
                "first_name": "J. Marius"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "RO"
        ],
        "abstract": "  Anomalies in the domain of autonomous driving are a major hindrance to the\nlarge-scale deployment of autonomous vehicles. In this work, we focus on\nhigh-resolution camera data from urban scenes that include anomalies of various\ntypes and sizes. Based on a Variational Autoencoder, we condition its latent\nspace to classify samples as either normal data or anomalies. In order to\nemphasize especially small anomalies, we perform experiments where we provide\nthe VAE with a discrepancy map as an additional input, evaluating its impact on\nthe detection performance. Our method separates normal data and anomalies into\nisolated clusters while still reconstructing high-quality images, leading to\nmeaningful latent representations.\n",
        "title": "Conditioning Latent-Space Clusters for Real-World Anomaly Classification",
        "date": "2023-09-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.11285",
        "abstract_url": "http://arxiv.org/abs/2309.11285",
        "authors": [
            {
                "last_name": "Sarvazyan",
                "first_name": "Areg Mikael"
            },
            {
                "last_name": "Gonz\u00e1lez",
                "first_name": "Jos\u00e9 \u00c1ngel"
            },
            {
                "last_name": "Franco-Salvador",
                "first_name": "Marc"
            },
            {
                "last_name": "Rangel",
                "first_name": "Francisco"
            },
            {
                "last_name": "Chulvi",
                "first_name": "Berta"
            },
            {
                "last_name": "Rosso",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  This paper presents the overview of the AuTexTification shared task as part\nof the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the\nframework of the SEPLN 2023 conference. AuTexTification consists of two\nsubtasks: for Subtask 1, participants had to determine whether a text is\nhuman-authored or has been generated by a large language model. For Subtask 2,\nparticipants had to attribute a machine-generated text to one of six different\ntext generation models. Our AuTexTification 2023 dataset contains more than\n160.000 texts across two languages (English and Spanish) and five domains\n(tweets, reviews, news, legal, and how-to articles). A total of 114 teams\nsigned up to participate, of which 36 sent 175 runs, and 20 of them sent their\nworking notes. In this overview, we present the AuTexTification dataset and\ntask, the submitted participating systems, and the results.\n",
        "title": "Overview of AuTexTification at IberLEF 2023: Detection and Attribution\n  of Machine-Generated Text in Multiple Domains",
        "date": "2023-09-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.00658",
        "abstract_url": "http://arxiv.org/abs/2310.00658",
        "authors": [
            {
                "last_name": "Prather",
                "first_name": "James"
            },
            {
                "last_name": "Denny",
                "first_name": "Paul"
            },
            {
                "last_name": "Leinonen",
                "first_name": "Juho"
            },
            {
                "last_name": "Becker",
                "first_name": "Brett A."
            },
            {
                "last_name": "Albluwi",
                "first_name": "Ibrahim"
            },
            {
                "last_name": "Craig",
                "first_name": "Michelle"
            },
            {
                "last_name": "Keuning",
                "first_name": "Hieke"
            },
            {
                "last_name": "Kiesler",
                "first_name": "Natalie"
            },
            {
                "last_name": "Kohn",
                "first_name": "Tobias"
            },
            {
                "last_name": "Luxton-Reilly",
                "first_name": "Andrew"
            },
            {
                "last_name": "MacNeil",
                "first_name": "Stephen"
            },
            {
                "last_name": "Peterson",
                "first_name": "Andrew"
            },
            {
                "last_name": "Pettit",
                "first_name": "Raymond"
            },
            {
                "last_name": "Reeves",
                "first_name": "Brent N."
            },
            {
                "last_name": "Savelka",
                "first_name": "Jaromir"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "",
            "HC"
        ],
        "abstract": "  Recent advancements in artificial intelligence (AI) are fundamentally\nreshaping computing, with large language models (LLMs) now effectively being\nable to generate and interpret source code and natural language instructions.\nThese emergent capabilities have sparked urgent questions in the computing\neducation community around how educators should adapt their pedagogy to address\nthe challenges and to leverage the opportunities presented by this new\ntechnology. In this working group report, we undertake a comprehensive\nexploration of LLMs in the context of computing education and make five\nsignificant contributions. First, we provide a detailed review of the\nliterature on LLMs in computing education and synthesise findings from 71\nprimary articles. Second, we report the findings of a survey of computing\nstudents and instructors from across 20 countries, capturing prevailing\nattitudes towards LLMs and their use in computing education contexts. Third, to\nunderstand how pedagogy is already changing, we offer insights collected from\nin-depth interviews with 22 computing educators from five continents who have\nalready adapted their curricula and assessments. Fourth, we use the ACM Code of\nEthics to frame a discussion of ethical issues raised by the use of large\nlanguage models in computing education, and we provide concrete advice for\npolicy makers, educators, and students. Finally, we benchmark the performance\nof LLMs on various computing education datasets, and highlight the extent to\nwhich the capabilities of current models are rapidly improving. Our aim is that\nthis report will serve as a focal point for both researchers and practitioners\nwho are exploring, adapting, using, and evaluating LLMs and LLM-based tools in\ncomputing classrooms.\n",
        "title": "The Robots are Here: Navigating the Generative AI Revolution in\n  Computing Education",
        "date": "2023-10-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.00804",
        "abstract_url": "http://arxiv.org/abs/2310.00804",
        "authors": [
            {
                "last_name": "Marykovskiy",
                "first_name": "Yuriy"
            },
            {
                "last_name": "Clark",
                "first_name": "Thomas"
            },
            {
                "last_name": "Day",
                "first_name": "Justin"
            },
            {
                "last_name": "Wiens",
                "first_name": "Marcus"
            },
            {
                "last_name": "Henderson",
                "first_name": "Charles"
            },
            {
                "last_name": "Quick",
                "first_name": "Julian"
            },
            {
                "last_name": "Abdallah",
                "first_name": "Imad"
            },
            {
                "last_name": "Sempreviva",
                "first_name": "Anna Maria"
            },
            {
                "last_name": "Calbimonte",
                "first_name": "Jean-Paul"
            },
            {
                "last_name": "Chatzi",
                "first_name": "Eleni"
            },
            {
                "last_name": "Barber",
                "first_name": "Sarah"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  With the rapid evolution of the wind energy sector, there is an\never-increasing need to create value from the vast amounts of data made\navailable both from within the domain, as well as from other sectors. This\narticle addresses the challenges faced by wind energy domain experts in\nconverting data into domain knowledge, connecting and integrating it with other\nsources of knowledge, and making it available for use in next generation\nartificially intelligent systems. To this end, this article highlights the role\nthat knowledge engineering can play in the process of digital transformation of\nthe wind energy sector. It presents the main concepts underpinning\nKnowledge-Based Systems and summarises previous work in the areas of knowledge\nengineering and knowledge representation in a manner that is relevant and\naccessible to domain experts. A systematic analysis of the current\nstate-of-the-art on knowledge engineering in the wind energy domain is\nperformed, with available tools put into perspective by establishing the main\ndomain actors and their needs and identifying key problematic areas. Finally,\nguidelines for further development and improvement are provided.\n",
        "title": "Knowledge Engineering for Wind Energy",
        "date": "2023-10-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.03038",
        "abstract_url": "http://arxiv.org/abs/2310.03038",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Wang",
                "first_name": "Lu"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingshan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "ET",
            ""
        ],
        "abstract": "  The moving target segmentation (MTS) aims to segment out moving targets in\nthe video, however, the classical algorithm faces the huge challenge of\nreal-time processing in the current video era. Some scholars have successfully\ndemonstrated the quantum advantages in some video processing tasks, but not\nconcerning moving target segmentation. In this paper, a quantum moving target\nsegmentation algorithm for grayscale video is proposed, which can use quantum\nmechanism to simultaneously calculate the difference of all pixels in all\nadjacent frames and then quickly segment out the moving target. In addition, a\nfeasible quantum comparator is designed to distinguish the grayscale values\nwith the threshold. Then several quantum circuit units, including three-frame\ndifference, binarization and AND operation, are designed in detail, and then\nare combined together to construct the complete quantum circuits for segmenting\nthe moving target. For a quantum video with $2^m$ frames (every frame is a\n$2^n\\times 2^n$ image with $q$ grayscale levels), the complexity of our\nalgorithm can be reduced to O$(n^2 + q)$. Compared with the classic\ncounterpart, it is an exponential speedup, while its complexity is also\nsuperior to the existing quantum algorithms. Finally, the experiment is\nconducted on IBM Q to show the feasibility of our algorithm in the noisy\nintermediate-scale quantum (NISQ) era.\n",
        "title": "A quantum moving target segmentation algorithm for grayscale video",
        "date": "2023-10-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.08261",
        "abstract_url": "http://arxiv.org/abs/2310.08261",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Wei",
                "first_name": "Haiyue"
            },
            {
                "last_name": "Bai",
                "first_name": "Lin"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  LiDAR and cameras are complementary sensors for 3D object detection in\nautonomous driving. However, it is challenging to explore the unnatural\ninteraction between point clouds and images, and the critical factor is how to\nconduct feature alignment of heterogeneous modalities. Currently, many methods\nachieve feature alignment by projection calibration only, without considering\nthe problem of coordinate conversion accuracy errors between sensors, leading\nto sub-optimal performance. In this paper, we present GraphAlign, a more\naccurate feature alignment strategy for 3D object detection by graph matching.\nSpecifically, we fuse image features from a semantic segmentation encoder in\nthe image branch and point cloud features from a 3D Sparse CNN in the LiDAR\nbranch. To save computation, we construct the nearest neighbor relationship by\ncalculating Euclidean distance within the subspaces that are divided into the\npoint cloud features. Through the projection calibration between the image and\npoint cloud, we project the nearest neighbors of point cloud features onto the\nimage features. Then by matching the nearest neighbors with a single point\ncloud to multiple images, we search for a more appropriate feature alignment.\nIn addition, we provide a self-attention module to enhance the weights of\nsignificant relations to fine-tune the feature alignment between heterogeneous\nmodalities. Extensive experiments on nuScenes benchmark demonstrate the\neffectiveness and efficiency of our GraphAlign.\n",
        "title": "GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for\n  Multi-Modal 3D Object Detection",
        "date": "2023-10-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.12572",
        "abstract_url": "http://arxiv.org/abs/2310.12572",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Mengce"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  We point out critical deficiencies in lattice-based cryptanalysis of common\nprime RSA presented in ``Remarks on the cryptanalysis of common prime RSA for\nIoT constrained low power devices'' [Information Sciences, 538 (2020) 54--68].\nTo rectify these flaws, we carefully scrutinize the relevant parameters\ninvolved in the analysis during solving a specific trivariate integer\npolynomial equation. Additionally, we offer a synthesized attack illustration\nof small private key attacks on common prime RSA.\n",
        "title": "Notes on Small Private Key Attacks on Common Prime RSA",
        "date": "2023-10-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.16984",
        "abstract_url": "http://arxiv.org/abs/2310.16984",
        "authors": [
            {
                "last_name": "Sheese",
                "first_name": "Brad"
            },
            {
                "last_name": "Liffiton",
                "first_name": "Mark"
            },
            {
                "last_name": "Savelka",
                "first_name": "Jaromir"
            },
            {
                "last_name": "Denny",
                "first_name": "Paul"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Providing personalized assistance at scale is a long-standing challenge for\ncomputing educators, but a new generation of tools powered by large language\nmodels (LLMs) offers immense promise. Such tools can, in theory, provide\non-demand help in large class settings and be configured with appropriate\nguardrails to prevent misuse and mitigate common concerns around learner\nover-reliance. However, the deployment of LLM-powered tools in authentic\nclassroom settings is still rare, and very little is currently known about how\nstudents will use them in practice and what type of help they will seek. To\naddress this, we examine students' use of an innovative LLM-powered tool that\nprovides on-demand programming assistance without revealing solutions directly.\nWe deployed the tool for 12 weeks in an introductory computer and data science\ncourse ($n = 52$), collecting more than 2,500 queries submitted by students\nthroughout the term. We manually categorized all student queries based on the\ntype of assistance sought, and we automatically analyzed several additional\nquery characteristics. We found that most queries requested immediate help with\nprogramming assignments, whereas fewer requests asked for help on related\nconcepts or for deepening conceptual understanding. Furthermore, students often\nprovided minimal information to the tool, suggesting this is an area in which\ntargeted instruction would be beneficial. We also found that students who\nachieved more success in the course tended to have used the tool more\nfrequently overall. Lessons from this research can be leveraged by programming\neducators and institutions who plan to augment their teaching with emerging\nLLM-powered tools.\n",
        "title": "Patterns of Student Help-Seeking When Using a Large Language\n  Model-Powered Programming Assistant",
        "date": "2023-10-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.18010",
        "abstract_url": "http://arxiv.org/abs/2310.18010",
        "authors": [
            {
                "last_name": "Gharibian",
                "first_name": "Sevag"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CC"
        ],
        "abstract": "  When it comes to NP, its natural definition, its wide applicability across\nscientific disciplines, and its timeless relevance, the writing is on the wall:\nThere can be only one. Quantum NP, on the other hand, is clearly the apple that\nfell far from the tree of NP. Two decades since the first definitions of\nquantum NP started rolling in, quantum complexity theorists face a stark\nreality: There's QMA, QCMA, QMA1, QMA(2), StoqMA, and NQP. In this article\naimed at a general theoretical computer science audience, I survey these\nvarious definitions of quantum NP, their strengths and weaknesses, and why most\nof them, for better or worse, actually appear to fit naturally into the\ncomplexity zoo.\n",
        "title": "The 7 faces of quantum NP",
        "date": "2023-10-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.00253",
        "abstract_url": "http://arxiv.org/abs/2311.00253",
        "authors": [
            {
                "last_name": "Pashazad",
                "first_name": "Hossein"
            },
            {
                "last_name": "Song",
                "first_name": "Xiaoyu"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Dynamic shearing banding and fracturing in unsaturated porous media is a\nsignificant problem in engineering and science. This article proposes a\nmultiphase micro-periporomechanics (uPPM) paradigm for modeling dynamic shear\nbanding and fracturing in unsaturated porous media. Periporomechanics (PPM) is\na nonlocal reformulation of classical poromechanics to model continuous and\ndiscontinuous deformation/fracture and fluid flow in porous media through a\nsingle framework. In PPM, a multiphase porous material is postulated as a\ncollection of a finite number of mixed material points. The length scale in PPM\nthat dictates the nonlocal interaction between material points is a\nmathematical object that lacks a direct physical meaning. As a novelty, in the\ncoupled uPPM, a microstructure-based material length scale is incorporated by\nconsidering micro-rotations of the solid skeleton following the Cosserat\ncontinuum theory for solids. As a new contribution, we reformulate the\nsecond-order work for detecting material instability and the energy-based crack\ncriterion and J-integral for modeling fracturing in the uPPM paradigm. The\nstabilized Cosserat PPM correspondence principle that mitigates the multiphase\nzero-energy mode instability is augmented to include unsaturated fluid flow. We\nhave numerically implemented the novel uPPM paradigm through a dual-way\nfractional-step algorithm in time and a hybrid Lagrangian-Eulerian meshfree\nmethod in space. Numerical examples are presented to demonstrate the robustness\nand efficacy of the proposed uPPM paradigm for modeling shear banding and\nfracturing in unsaturated porous media.\n",
        "title": "Computational multiphase micro-periporomechanics for dynamic shear\n  banding and fracturing of unsaturated porous media",
        "date": "2023-10-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.04060",
        "abstract_url": "http://arxiv.org/abs/2311.04060",
        "authors": [
            {
                "last_name": "R\u00f6stel",
                "first_name": "Lennart"
            },
            {
                "last_name": "Pitz",
                "first_name": "Johannes"
            },
            {
                "last_name": "Sievers",
                "first_name": "Leon"
            },
            {
                "last_name": "B\u00e4uml",
                "first_name": "Berthold"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  This paper identifies and addresses the problems with naively combining\n(reinforcement) learning-based controllers and state estimators for robotic\nin-hand manipulation. Specifically, we tackle the challenging task of purely\ntactile, goal-conditioned, dextrous in-hand reorientation with the hand\npointing downwards. Due to the limited sensing available, many control\nstrategies that are feasible in simulation when having full knowledge of the\nobject's state do not allow for accurate state estimation. Hence, separately\ntraining the controller and the estimator and combining the two at test time\nleads to poor performance. We solve this problem by coupling the control policy\nto the state estimator already during training in simulation. This approach\nleads to more robust state estimation and overall higher performance on the\ntask while maintaining an interpretability advantage over end-to-end policy\nlearning. With our GPU-accelerated implementation, learning from scratch takes\na median training time of only 6.5 hours on a single, low-cost GPU. In\nsimulation experiments with the DLR-Hand II and for four significantly\ndifferent object shapes, we provide an in-depth analysis of the performance of\nour approach. We demonstrate the successful sim2real transfer by rotating the\nfour objects to all 24 orientations in the $\\pi/2$ discretization of SO(3),\nwhich has never been achieved for such a diverse set of shapes. Finally, our\nmethod can reorient a cube consecutively to nine goals (median), which was\nbeyond the reach of previous methods in this challenging setting.\n",
        "title": "Estimator-Coupled Reinforcement Learning for Robust Purely Tactile\n  In-Hand Manipulation",
        "date": "2023-11-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.06067",
        "abstract_url": "http://arxiv.org/abs/2311.06067",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Xin"
            },
            {
                "last_name": "Chen",
                "first_name": "Shikun"
            },
            {
                "last_name": "Cao",
                "first_name": "Yichao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xin"
            },
            {
                "last_name": "Lu",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "CV"
        ],
        "abstract": "  In recent years, hashing methods have been popular in the large-scale media\nsearch for low storage and strong representation capabilities. To describe\nobjects with similar overall appearance but subtle differences, more and more\nstudies focus on hashing-based fine-grained image retrieval. Existing hashing\nnetworks usually generate both local and global features through attention\nguidance on the same deep activation tensor, which limits the diversity of\nfeature representations. To handle this limitation, we substitute convolutional\ndescriptors for attention-guided features and propose an Attributes Grouping\nand Mining Hashing (AGMH), which groups and embeds the category-specific visual\nattributes in multiple descriptors to generate a comprehensive feature\nrepresentation for efficient fine-grained image retrieval. Specifically, an\nAttention Dispersion Loss (ADL) is designed to force the descriptors to attend\nto various local regions and capture diverse subtle details. Moreover, we\npropose a Stepwise Interactive External Attention (SIEA) to mine critical\nattributes in each descriptor and construct correlations between fine-grained\nattributes and objects. The attention mechanism is dedicated to learning\ndiscrete attributes, which will not cost additional computations in hash codes\ngeneration. Finally, the compact binary codes are learned by preserving\npairwise similarities. Experimental results demonstrate that AGMH consistently\nyields the best performance against state-of-the-art methods on fine-grained\nbenchmark datasets.\n",
        "title": "Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval",
        "date": "2023-11-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.06826",
        "abstract_url": "http://arxiv.org/abs/2311.06826",
        "authors": [
            {
                "last_name": "Meding",
                "first_name": "Kristof"
            },
            {
                "last_name": "Hagendorff",
                "first_name": "Thilo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CY"
        ],
        "abstract": "  Fairness in machine learning (ML) is an ever-growing field of research due to\nthe manifold potential for harm from algorithmic discrimination. To prevent\nsuch harm, a large body of literature develops new approaches to quantify\nfairness. Here, we investigate how one can divert the quantification of\nfairness by describing a practice we call \"fairness hacking\" for the purpose of\nshrouding unfairness in algorithms. This impacts end-users who rely on learning\nalgorithms, as well as the broader community interested in fair AI practices.\nWe introduce two different categories of fairness hacking in reference to the\nestablished concept of p-hacking. The first category, intra-metric fairness\nhacking, describes the misuse of a particular metric by adding or removing\nsensitive attributes from the analysis. In this context, countermeasures that\nhave been developed to prevent or reduce p-hacking can be applied to similarly\nprevent or reduce fairness hacking. The second category of fairness hacking is\ninter-metric fairness hacking. Inter-metric fairness hacking is the search for\na specific fair metric with given attributes. We argue that countermeasures to\nprevent or reduce inter-metric fairness hacking are still in their infancy.\nFinally, we demonstrate both types of fairness hacking using real datasets. Our\npaper intends to serve as a guidance for discussions within the fair ML\ncommunity to prevent or reduce the misuse of fairness metrics, and thus reduce\noverall harm from ML applications.\n",
        "title": "Fairness Hacking: The Malicious Practice of Shrouding Unfairness in\n  Algorithms",
        "date": "2023-11-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.09061",
        "abstract_url": "http://arxiv.org/abs/2311.09061",
        "authors": [
            {
                "last_name": "Karlsson",
                "first_name": "T."
            },
            {
                "last_name": "\u00c5blad",
                "first_name": "E."
            },
            {
                "last_name": "Hermansson",
                "first_name": "T."
            },
            {
                "last_name": "Carlson",
                "first_name": "J. S."
            },
            {
                "last_name": "Tenf\u00e4lt",
                "first_name": "G."
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "",
            "",
            ""
        ],
        "abstract": "  Designing cable harnesses can be time-consuming and complex due to many\ndesign and manufacturing aspects and rules. Automating the design process can\nhelp to fulfil these rules, speed up the process, and optimize the design. To\naccommodate this, we formulate a harness routing optimization problem to\nminimize cable lengths, maximize bundling by rewarding shared paths, and\noptimize the cables' spatial location with respect to case-specific information\nof the routing environment, e.g., zones to avoid. A deterministic and\ncomputationally effective cable harness routing algorithm has been developed to\nsolve the routing problem and is used to generate a set of cable harness\ntopology candidates and approximate the Pareto front. Our approach was tested\nagainst a stochastic and an exact solver and our routing algorithm generated\nobjective function values better than the stochastic approach and close to the\nexact solver. Our algorithm was able to find solutions, some of them being\nproven to be near-optimal, for three industrial-sized 3D cases within\nreasonable time (in magnitude of seconds to minutes) and the computation times\nwere comparable to those of the stochastic approach.\n",
        "title": "Automatic cable harness layout routing in a customizable 3D environment",
        "date": "2023-11-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10653",
        "abstract_url": "http://arxiv.org/abs/2311.10653",
        "authors": [
            {
                "last_name": "Keyvanian",
                "first_name": "Shafagh"
            },
            {
                "last_name": "Johnson",
                "first_name": "Michelle J."
            },
            {
                "last_name": "Figueroa",
                "first_name": "Nadia"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  A realistic human kinematic model that satisfies anatomical constraints is\nessential for human-robot interaction, biomechanics and robot-assisted\nrehabilitation. Modeling realistic joint constraints, however, is challenging\nas human arm motion is constrained by joint limits, inter- and intra-joint\ndependencies, self-collisions, individual capabilities and muscular or\nneurological constraints which are difficult to represent. Hence, physicians\nand researchers have relied on simple box-constraints, ignoring important\nanatomical factors. In this paper, we propose a data-driven method to learn\nrealistic anatomically constrained upper-limb range of motion (RoM) boundaries\nfrom motion capture data. This is achieved by fitting a one-class support\nvector machine to a dataset of upper-limb joint space exploration motions with\nan efficient hyper-parameter tuning scheme. Our approach outperforms similar\nworks focused on valid RoM learning. Further, we propose an impairment index\n(II) metric that offers a quantitative assessment of capability/impairment when\ncomparing healthy and impaired arms. We validate the metric on healthy subjects\nphysically constrained to emulate hemiplegia and different disability levels as\nstroke patients.\n",
        "title": "Learning Realistic Joint Space Boundaries for Range of Motion Analysis\n  of Healthy and Impaired Human Arms",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10765",
        "abstract_url": "http://arxiv.org/abs/2311.10765",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yufeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The challenge of improving translation accuracy in GPT-4 is being addressed\nby harnessing a method known as in-context learning. This paper introduces a\nstrategic approach to utilize in-context learning specifically for machine\ntranslation, aiming to significantly boost accuracy. The crux of this method\nlies in the judicious selection of demonstrations that are most effective for\nin-context learning. By selecting these examples carefully, GPT-4 can utilize\nthem to achieve remarkably accurate machine translations, eliminating the need\nfor task-specific fine-tuning. This technique is anchored in the semantic\nsimilarities between the user's prompt and the chosen dataset. Sentences from\nthis dataset, carefully picked for their relevance and clarity, serve as potent\ndemonstrations for in-context learning. This approach not only enhances\ntranslation accuracy but also enriches the understanding of nuanced linguistic\nstructures. It represents a significant step forward in machine learning,\nleveraging the inherent capabilities of GPT-4 to provide translations that are\nnot only accurate but also contextually rich and linguistically sophisticated.\nThis method demonstrates the potential of in-context learning in overcoming\nlanguage barriers, opening new avenues for cross-cultural communication and\nglobal collaboration.\n",
        "title": "Enhancing Machine Translation through Advanced In-Context Learning: A\n  Methodological Strategy for GPT-4 Improvement",
        "date": "2023-11-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.16065",
        "abstract_url": "http://arxiv.org/abs/2311.16065",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Xianghua"
            },
            {
                "last_name": "Hu",
                "first_name": "Chen"
            },
            {
                "last_name": "Ren",
                "first_name": "Hanchi"
            },
            {
                "last_name": "Deng",
                "first_name": "Jingjing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  This review paper takes a comprehensive look at malicious attacks against FL,\ncategorizing them from new perspectives on attack origins and targets, and\nproviding insights into their methodology and impact. In this survey, we focus\non threat models targeting the learning process of FL systems. Based on the\nsource and target of the attack, we categorize existing threat models into four\ntypes, Data to Model (D2M), Model to Data (M2D), Model to Model (M2M) and\ncomposite attacks. For each attack type, we discuss the defense strategies\nproposed, highlighting their effectiveness, assumptions and potential areas for\nimprovement. Defense strategies have evolved from using a singular metric to\nexcluding malicious clients, to employing a multifaceted approach examining\nclient models at various phases. In this survey paper, our research indicates\nthat the to-learn data, the learning gradients, and the learned model at\ndifferent stages all can be manipulated to initiate malicious attacks that\nrange from undermining model performance, reconstructing private local data,\nand to inserting backdoors. We have also seen these threat are becoming more\ninsidious. While earlier studies typically amplified malicious gradients,\nrecent endeavors subtly alter the least significant weights in local models to\nbypass defense measures. This literature review provides a holistic\nunderstanding of the current FL threat landscape and highlights the importance\nof developing robust, efficient, and privacy-preserving defenses to ensure the\nsafe and trusted adoption of FL in real-world applications.\n",
        "title": "A Survey on Vulnerability of Federated Learning: A Learning Algorithm\n  Perspective",
        "date": "2023-11-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.02937",
        "abstract_url": "http://arxiv.org/abs/2312.02937",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Ayoosh"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhu",
                "first_name": "James"
            },
            {
                "last_name": "Cheng",
                "first_name": "Sheng"
            },
            {
                "last_name": "Gu",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Yoon",
                "first_name": "Hyung-Jin"
            },
            {
                "last_name": "Kim",
                "first_name": "Hunmin"
            },
            {
                "last_name": "Hovakimyan",
                "first_name": "Naira"
            },
            {
                "last_name": "Sha",
                "first_name": "Lui"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Perception, Planning, and Control form the essential components of autonomy\nin advanced air mobility. This work advances the holistic integration of these\ncomponents to enhance the performance and robustness of the complete\ncyber-physical system. We adapt Perception Simplex, a system for verifiable\ncollision avoidance amidst obstacle detection faults, to the vertical landing\nmaneuver for autonomous air mobility vehicles. We improve upon this system by\nreplacing static assumptions of control capabilities with dynamic confirmation,\ni.e., real-time confirmation of control limitations of the system, ensuring\nreliable fulfillment of safety maneuvers and overrides, without dependence on\noverly pessimistic assumptions. Parameters defining control system capabilities\nand limitations, e.g., maximum deceleration, are continuously tracked within\nthe system and used to make safety-critical decisions. We apply these\ntechniques to propose a verifiable collision avoidance solution for autonomous\naerial mobility vehicles operating in cluttered and potentially unsafe\nenvironments.\n",
        "title": "Synergistic Perception and Control Simplex for Verifiable Safe Vertical\n  Landing",
        "date": "2023-12-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.04219",
        "abstract_url": "http://arxiv.org/abs/2312.04219",
        "authors": [
            {
                "last_name": "Ferrer-i-Cancho",
                "first_name": "Ramon"
            },
            {
                "last_name": "Namboodiripad",
                "first_name": "Savithry"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Distance minimization is a general principle of language. A special case of\nthis principle in the domain of word order is swap distance minimization. This\nprinciple predicts that variations from a canonical order that are reached by\nfewer swaps of adjacent constituents are lest costly and thus more likely. Here\nwe investigate the principle in the context of the triple formed by subject\n(S), object (O) and verb (V). We introduce the concept of word order rotation\nas a cognitive underpinning of that prediction. When the canonical order of a\nlanguage is SOV, the principle predicts SOV < SVO, OSV < VSO, OVS < VOS, in\norder of increasing cognitive cost. We test the prediction in three flexible\norder SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese\n(Indo-European). Evidence of swap distance minimization is found in all three\nlanguages, but it is weaker in Sinhalese. Swap distance minimization is\nstronger than a preference for the canonical order in Korean and especially\nMalayalam.\n",
        "title": "Swap distance minimization in SOV languages. Cognitive and mathematical\n  foundations",
        "date": "2023-12-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.07895",
        "abstract_url": "http://arxiv.org/abs/2312.07895",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Yuqi"
            },
            {
                "last_name": "You",
                "first_name": "Li"
            },
            {
                "last_name": "Wang",
                "first_name": "Jue"
            },
            {
                "last_name": "Xu",
                "first_name": "Hao"
            },
            {
                "last_name": "Wong",
                "first_name": "Kai-Kit"
            },
            {
                "last_name": "Gao",
                "first_name": "Xiqi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  In conventional multiple-input multiple-output (MIMO) communication systems,\nthe positions of antennas are fixed. To take full advantage of spatial degrees\nof freedom, a new technology called fluid antenna (FA) is proposed to obtain\nhigher achievable rate and diversity gain. Most existing works on FA exploit\ninstantaneous channel state information (CSI). However, in FA-assisted systems,\nit is difficult to obtain instantaneous CSI since changes in the antenna\nposition will lead to channel variation. In this letter, we investigate a\nFA-assisted MIMO system using relatively slow-varying statistical CSI.\nSpecifically, in the criterion of rate maximization, we propose an algorithmic\nframework for transmit precoding and transmit/receive FAs position designs with\nstatistical CSI. Simulation results show that our proposed algorithm in\nFA-assisted systems significantly outperforms baselines in terms of rate\nperformance.\n",
        "title": "Fluid Antenna-Assisted MIMO Transmission Exploiting Statistical CSI",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.09676",
        "abstract_url": "http://arxiv.org/abs/2312.09676",
        "authors": [
            {
                "last_name": "Mlodzian",
                "first_name": "Leon"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhigang"
            },
            {
                "last_name": "Berkemeyer",
                "first_name": "Hendrik"
            },
            {
                "last_name": "Monka",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Wang",
                "first_name": "Zixu"
            },
            {
                "last_name": "Dietze",
                "first_name": "Stefan"
            },
            {
                "last_name": "Halilaj",
                "first_name": "Lavdim"
            },
            {
                "last_name": "Luettin",
                "first_name": "Juergen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "RO",
            "",
            "",
            ""
        ],
        "abstract": "  Trajectory prediction in traffic scenes involves accurately forecasting the\nbehaviour of surrounding vehicles. To achieve this objective it is crucial to\nconsider contextual information, including the driving path of vehicles, road\ntopology, lane dividers, and traffic rules. Although studies demonstrated the\npotential of leveraging heterogeneous context for improving trajectory\nprediction, state-of-the-art deep learning approaches still rely on a limited\nsubset of this information. This is mainly due to the limited availability of\ncomprehensive representations. This paper presents an approach that utilizes\nknowledge graphs to model the diverse entities and their semantic connections\nwithin traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a\nknowledge graph for the nuScenes dataset, that models explicitly all scene\nparticipants and road elements, as well as their semantic and spatial\nrelationships. To facilitate the usage of the nSKG via graph neural networks\nfor trajectory prediction, we provide the data in a format, ready-to-use by the\nPyG library. All artefacts can be found here:\nhttps://github.com/boschresearch/nuScenes_Knowledge_Graph\n",
        "title": "nuScenes Knowledge Graph -- A comprehensive semantic representation of\n  traffic scenes for trajectory prediction",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.13462",
        "abstract_url": "http://arxiv.org/abs/2312.13462",
        "authors": [
            {
                "last_name": "Kudrjavets",
                "first_name": "Gunnar"
            },
            {
                "last_name": "Kumar",
                "first_name": "Aditya"
            },
            {
                "last_name": "Thomas",
                "first_name": "Jeff"
            },
            {
                "last_name": "Rastogi",
                "first_name": "Ayushi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  An accepted practice to decrease applications' memory usage is to reduce the\namount and frequency of memory allocations. Factors such as (a) the prevalence\nof out-of-memory (OOM) killers, (b) memory allocations in modern programming\nlanguages done implicitly, (c) overcommitting being a default strategy in the\nLinux kernel, and (d) the rise in complexity and terminology related to memory\nmanagement makes the existing guidance inefficient. The industry needs detailed\nguidelines for optimizing memory usage targeting specific operating systems\n(OS) and programming language types.\n",
        "title": "What Do You Mean by Memory? When Engineers Are Lost in the Maze of\n  Complexity",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.13463",
        "abstract_url": "http://arxiv.org/abs/2312.13463",
        "authors": [
            {
                "last_name": "Kudrjavets",
                "first_name": "Gunnar"
            },
            {
                "last_name": "Kumar",
                "first_name": "Aditya"
            },
            {
                "last_name": "Thomas",
                "first_name": "Jeff"
            },
            {
                "last_name": "Rastogi",
                "first_name": "Ayushi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Engineers build large software systems for multiple architectures, operating\nsystems, and configurations. A set of inconsistent or missing compiler flags\ngenerates code that catastrophically impacts the system's behavior. In the\nauthors' industry experience, defects caused by an undesired combination of\ncompiler flags are common in nontrivial software projects. We are unaware of\nany build and CI/CD systems that track how the compiler produces a specific\nbinary in a structured manner. We postulate that a queryable database of how\nthe compiler compiled and linked the software system will help to detect\ndefects earlier and reduce the debugging time.\n",
        "title": "The Devil Is in the Command Line: Associating the Compiler Flags With\n  the Binary and Build Metadata",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.14848",
        "abstract_url": "http://arxiv.org/abs/2312.14848",
        "authors": [
            {
                "last_name": "Graber",
                "first_name": "Vanessa"
            },
            {
                "last_name": "Ronchi",
                "first_name": "Michele"
            },
            {
                "last_name": "Pardo-Araujo",
                "first_name": "Celsa"
            },
            {
                "last_name": "Rea",
                "first_name": "Nanda"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  We combine pulsar population synthesis with simulation-based inference to\nconstrain the magneto-rotational properties of isolated Galactic radio pulsars.\nWe first develop a flexible framework to model neutron-star birth properties\nand evolution, focusing on their dynamical, rotational and magnetic\ncharacteristics. In particular, we sample initial magnetic-field strengths,\n$B$, and spin periods, $P$, from log-normal distributions and capture the\nlate-time magnetic-field decay with a power law. Each log-normal is described\nby a mean, $\\mu_{\\log B}, \\mu_{\\log P}$, and standard deviation, $\\sigma_{\\log\nB}, \\sigma_{\\log P}$, while the power law is characterized by the index,\n$a_{\\rm late}$, resulting in five free parameters. We subsequently model the\nstars' radio emission and observational biases to mimic detections with three\nradio surveys, and produce a large database of synthetic $P$-$\\dot{P}$ diagrams\nby varying our input parameters. We then follow a simulation-based inference\napproach that focuses on neural posterior estimation and employ this database\nto train deep neural networks to directly infer the posterior distributions of\nthe five model parameters. After successfully validating these individual\nneural density estimators on simulated data, we use an ensemble of networks to\ninfer the posterior distributions for the observed pulsar population. We obtain\n$\\mu_{\\log B} = 13.10^{+0.08}_{-0.10}$, $\\sigma_{\\log B} =\n0.45^{+0.05}_{-0.05}$ and $\\mu_{\\log P} = -1.00^{+0.26}_{-0.21}$, $\\sigma_{\\log\nP} = 0.38^{+0.33}_{-0.18}$ for the log-normal distributions, and $a_{\\rm late}\n= -1.80^{+0.65}_{-0.61}$ for the power law at $95\\%$ credible interval. Our\napproach represents a crucial step towards robust statistical inference for\ncomplex population-synthesis frameworks and forms the basis for future\nmulti-wavelength analyses of Galactic pulsars.\n",
        "title": "Isolated pulsar population synthesis with simulation-based inference",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.15634",
        "abstract_url": "http://arxiv.org/abs/2312.15634",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hongjun"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Meng",
                "first_name": "Fanle"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This paper presents an innovative feature signal transmission approach\nincorpo-rating block-based haptic data reduction to address time-delayed\nteleoperation. Numerous data reduction techniques rely on perceptual deadband\n(DB). In the preceding block-based approaches, the whole block within the DB is\ndiscarded. However, disregarding all signals within the DB loses too much\ninformation and hinders effective haptic signal tracking, as these signals\ncontain valuable infor-mation for signal reconstruction. Consequently, we\npropose a feature signal transmission approach based on the block algorithm\nthat aggregates samples as a unit, enabling high-quality haptic data reduction.\nIn our proposed approach, we employ max-pooling to extract feature signals from\nthe signals within the DB. These feature signals are then transmitted by\nadjusting the content of the trans-mission block. This methodology enables the\ntransmission of more useful infor-mation without introducing additional delay,\naside from the inherent algorithmic delay. Experimental results demonstrate the\nsuperiority of our approach over oth-er state-of-the-art (SOTA) methods on\nvarious assessment measures under dis-tinct channel delays.\n",
        "title": "Incorporating Feature Signal Transmission with Block-based Haptic Data\n  Reduction for Time-delayed Teleoperation",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.16814",
        "abstract_url": "http://arxiv.org/abs/2312.16814",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Jindan"
            },
            {
                "last_name": "Xu",
                "first_name": "Wei"
            },
            {
                "last_name": "Yuen",
                "first_name": "Chau"
            },
            {
                "last_name": "Swindlehurst",
                "first_name": "A. Lee"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chunming"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Reconfigurable intelligent surface (RIS) technology is emerging as a\npromising technique for performance enhancement for next-generation wireless\nnetworks. This paper investigates the physical layer security of an\nRIS-assisted multiple-antenna communication system in the presence of random\nspatially distributed eavesdroppers. The RIS-to-ground channels are assumed to\nexperience Rician fading. Using stochastic geometry, exact distributions of the\nreceived signal-to-noise-ratios (SNRs) at the legitimate user and the\neavesdroppers located according to a Poisson point process (PPP) are derived,\nand closed-form expressions for the secrecy outage probability (SOP) and the\nergodic secrecy capacity (ESC) are obtained to provide insightful guidelines\nfor system design. First, the secrecy diversity order is obtained as\n$\\frac{2}{\\alpha_2}$, where $\\alpha_2$ denotes the path loss exponent of the\nRIS-to-ground links. Then, it is revealed that the secrecy performance is\nmainly affected by the number of RIS reflecting elements, $N$, and the impact\nof the number of transmit antennas and transmit power at the base station is\nmarginal. In addition, when the locations of the randomly located eavesdroppers\nare unknown, deploying the RIS closer to the legitimate user rather than to the\nbase station is shown to be more efficient. Moreover, it is also found that the\ndensity of randomly located eavesdroppers, $\\lambda_e$, has an additive effect\non the asymptotic ESC performance given by\n$\\log_2{\\left({1}/{\\lambda_e}\\right)}$. Finally, numerical simulations are\nconducted to verify the accuracy of these theoretical observations.\n",
        "title": "On Secrecy Performance of RIS-Assisted MISO Systems over Rician Channels\n  with Spatially Random Eavesdroppers",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.17127",
        "abstract_url": "http://arxiv.org/abs/2312.17127",
        "authors": [
            {
                "last_name": "Ackerman",
                "first_name": "Nathanael L."
            },
            {
                "last_name": "Freer",
                "first_name": "Cameron E."
            },
            {
                "last_name": "Kaddar",
                "first_name": "Younesse"
            },
            {
                "last_name": "Karwowski",
                "first_name": "Jacek"
            },
            {
                "last_name": "Moss",
                "first_name": "Sean K."
            },
            {
                "last_name": "Roy",
                "first_name": "Daniel M."
            },
            {
                "last_name": "Staton",
                "first_name": "Sam"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongseok"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "LO",
            ""
        ],
        "abstract": "  We study semantic models of probabilistic programming languages over graphs,\nand establish a connection to graphons from graph theory and combinatorics. We\nshow that every well-behaved equational theory for our graph probabilistic\nprogramming language corresponds to a graphon, and conversely, every graphon\narises in this way.\n  We provide three constructions for showing that every graphon arises from an\nequational theory. The first is an abstract construction, using Markov\ncategories and monoidal indeterminates. The second and third are more concrete.\nThe second is in terms of traditional measure theoretic probability, which\ncovers 'black-and-white' graphons. The third is in terms of probability monads\non the nominal sets of Gabbay and Pitts. Specifically, we use a variation of\nnominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi\ngraphons. In this way, we build new models of graph probabilistic programming\nfrom graphons.\n",
        "title": "Probabilistic programming interfaces for random graphs: Markov\n  categories, graphons, and nominal sets",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01060",
        "abstract_url": "http://arxiv.org/abs/2401.01060",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Shuzheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Wenxin"
            },
            {
                "last_name": "Gao",
                "first_name": "Cuiyun"
            },
            {
                "last_name": "Li",
                "first_name": "Li"
            },
            {
                "last_name": "Hu",
                "first_name": "Xing"
            },
            {
                "last_name": "Xia",
                "first_name": "Xin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Michael R."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Pre-trained code models have recently achieved substantial improvements in\nmany code intelligence tasks. These models are first pre-trained on large-scale\nunlabeled datasets in a task-agnostic manner using self-supervised learning,\nand then fine-tuned on labeled datasets in downstream tasks. However, the\nlabeled datasets are usually limited in size (i.e., human intensive efforts),\nwhich may hinder the performance of pre-trained code models in specific tasks.\nTo mitigate this, one possible solution is to leverage the large-scale\nunlabeled data in the tuning stage by pseudo-labeling. However, directly\nemploying the pseudo-labeled data can bring a large amount of noise, i.e.,\nincorrect labels, leading to suboptimal performance. How to effectively\nleverage the noisy pseudo-labeled data is a challenging yet under-explored\nproblem.In this paper, we propose a novel approach named HINT to improve\npre-trained code models with large-scale unlabeled datasets by better utilizing\nthe pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled\ndata selection and Noise-tolerant Training. In the hybrid pseudo-data selection\nmodule, considering the robustness issue, apart from directly measuring the\nquality of pseudo labels through training loss, we further propose to employ a\nretrieval-based method to filter low-quality pseudo-labeled data. The\nnoise-tolerant training module aims to further mitigate the influence of errors\nin pseudo labels by training the model with a noise-tolerant loss function and\nby regularizing the consistency of model predictions.The experimental results\nshow that HINT can better leverage those unlabeled data in a task-specific way\nand provide complementary benefits for pre-trained models, e.g., improving the\nbest baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect\ndetection, and assertion generation, respectively.\n",
        "title": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively\n  Tuning Pre-trained Code Models",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01554",
        "abstract_url": "http://arxiv.org/abs/2401.01554",
        "authors": [
            {
                "last_name": "Ortega",
                "first_name": "Sergio A."
            },
            {
                "last_name": "Martin-Delgado",
                "first_name": "Miguel A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "NI"
        ],
        "abstract": "  The quantum SearchRank algorithm is a promising tool for a future quantum\nsearch engine based on PageRank quantization. However, this algorithm loses its\nfunctionality when the $N/M$ ratio between the network size $N$ and the number\nof marked nodes $M$ is sufficiently large. We propose a modification of the\nalgorithm, replacing the underlying Szegedy quantum walk with a semiclassical\nwalk. To maintain the same time complexity as the quantum SearchRank algorithm\nwe propose a simplification of the algorithm. This new algorithm is called\nRandomized SearchRank, since it corresponds to a quantum walk over a randomized\nmixed state. The performance of the SearchRank algorithms is first analyzed on\nan example network, and then statistically on a set of different networks of\nincreasing size and different number of marked nodes. On the one hand, to test\nthe search ability of the algorithms, it is computed how the probability of\nmeasuring the marked nodes decreases with $N/M$ for the quantum SearchRank, but\nremarkably it remains at a high value around $0.9$ for our semiclassical\nalgorithms, solving the quantum SearchRank problem. The time complexity of the\nalgorithms is also analyzed, obtaining a quadratic speedup with respect to the\nclassical ones. On the other hand, the ranking functionality of the algorithms\nhas been investigated, obtaining a good agreement with the classical PageRank\ndistribution. Finally, the dependence of these algorithms on the intrinsic\nPageRank damping parameter has been clarified. Our results suggest that this\nparameter should be below a threshold so that the execution time does not\nincrease drastically.\n",
        "title": "Randomized SearchRank: A Semiclassical Approach to a Quantum Search\n  Engine",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01921",
        "abstract_url": "http://arxiv.org/abs/2401.01921",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Kai-Hsin"
            },
            {
                "last_name": "Lin",
                "first_name": "Chang-Teng"
            },
            {
                "last_name": "Hsu",
                "first_name": "Ke"
            },
            {
                "last_name": "Hung",
                "first_name": "Hao-Ti"
            },
            {
                "last_name": "Schneider",
                "first_name": "Manuel"
            },
            {
                "last_name": "Chung",
                "first_name": "Chia-Min"
            },
            {
                "last_name": "Kao",
                "first_name": "Ying-Jer"
            },
            {
                "last_name": "Chen",
                "first_name": "Pochung"
            }
        ],
        "primary_category": "MS",
        "categories": [
            "MS",
            ""
        ],
        "abstract": "  We introduce a tensor network library designed for classical and quantum\nphysics simulations called Cytnx (pronounced as sci-tens). This library\nprovides almost an identical interface and syntax for both C++ and Python,\nallowing users to effortlessly switch between two languages. Aiming at a quick\nlearning process for new users of tensor network algorithms, the interfaces\nresemble the popular Python scientific libraries like NumPy, Scipy, and\nPyTorch. Not only multiple global Abelian symmetries can be easily defined and\nimplemented, Cytnx also provides a new tool called Network that allows users to\nstore large tensor networks and perform tensor network contractions in an\noptimal order automatically. With the integration of cuQuantum, tensor\ncalculations can also be executed efficiently on GPUs. We present benchmark\nresults for tensor operations on both devices, CPU and GPU. We also discuss\nfeatures and higher-level interfaces to be added in the future.\n",
        "title": "The Cytnx Library for Tensor Networks",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02702",
        "abstract_url": "http://arxiv.org/abs/2401.02702",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Xie",
                "first_name": "Jun"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            },
            {
                "last_name": "Xu",
                "first_name": "Shaoqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhepeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  LiDAR-camera fusion can enhance the performance of 3D object detection by\nutilizing complementary information between depth-aware LiDAR points and\nsemantically rich images. Existing voxel-based methods face significant\nchallenges when fusing sparse voxel features with dense image features in a\none-to-one manner, resulting in the loss of the advantages of images, including\nsemantic and continuity information, leading to sub-optimal detection\nperformance, especially at long distances. In this paper, we present\nVoxelNextFusion, a multi-modal 3D object detection framework specifically\ndesigned for voxel-based methods, which effectively bridges the gap between\nsparse point clouds and dense images. In particular, we propose a voxel-based\nimage pipeline that involves projecting point clouds onto images to obtain both\npixel- and patch-level features. These features are then fused using a\nself-attention to obtain a combined representation. Moreover, to address the\nissue of background features present in patches, we propose a feature\nimportance module that effectively distinguishes between foreground and\nbackground features, thus minimizing the impact of the background features.\nExtensive experiments were conducted on the widely used KITTI and nuScenes 3D\nobject detection benchmarks. Notably, our VoxelNextFusion achieved around\n+3.20% in AP@0.7 improvement for car detection in hard level compared to the\nVoxel R-CNN baseline on the KITTI test dataset\n",
        "title": "VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework\n  for Multi-Modal 3D Object Detection",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02772",
        "abstract_url": "http://arxiv.org/abs/2401.02772",
        "authors": [
            {
                "last_name": "Stanisz",
                "first_name": "Tomasz"
            },
            {
                "last_name": "Dro\u017cd\u017c",
                "first_name": "Stanis\u0142aw"
            },
            {
                "last_name": "Kwapie\u0144",
                "first_name": "Jaros\u0142aw"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "",
            ""
        ],
        "abstract": "  The review summarizes the main methodological concepts used in studying\nnatural language from the perspective of complexity science and documents their\napplicability in identifying both universal and system-specific features of\nlanguage in its written representation. Three main complexity-related research\ntrends in quantitative linguistics are covered. The first part addresses the\nissue of word frequencies in texts and demonstrates that taking punctuation\ninto consideration restores scaling whose violation in the Zipf's law is often\nobserved for the most frequent words. The second part introduces methods\ninspired by time series analysis, used in studying various kinds of\ncorrelations in written texts. The related time series are generated on the\nbasis of text partition into sentences or into phrases between consecutive\npunctuation marks. It turns out that these series develop features often found\nin signals generated by complex systems, like long-range correlations or\n(multi)fractal structures. Moreover, it appears that the distances between\npunctuation marks comply with the discrete variant of the Weibull distribution.\nIn the third part, the application of the network formalism to natural language\nis reviewed, particularly in the context of the so-called word-adjacency\nnetworks. Parameters characterizing topology of such networks can be used for\nclassification of texts, for example, from a stylometric perspective. Network\napproach can also be applied to represent the organization of word\nassociations. Structure of word-association networks turns out to be\nsignificantly different from that observed in random networks, revealing\ngenuine properties of language. Finally, punctuation seems to have a\nsignificant impact not only on the language's information-carrying ability but\nalso on its key statistical properties, hence it is recommended to consider\npunctuation marks on a par with words.\n",
        "title": "Complex systems approach to natural language",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02959",
        "abstract_url": "http://arxiv.org/abs/2401.02959",
        "authors": [
            {
                "last_name": "Cazabet",
                "first_name": "Remy"
            },
            {
                "last_name": "Annen",
                "first_name": "Catherine"
            },
            {
                "last_name": "Moyen",
                "first_name": "Jean-Francois"
            },
            {
                "last_name": "Weinberg",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SI",
            ""
        ],
        "abstract": "  Magmas form at depth, move upwards and evolve chemically through a\ncombination of processes. Magmatic processes are investigated by means of\nfieldwork combined with geophysics, geochemistry, analog and numerical models,\nand many other approaches. However, scientists in the field still struggle to\nunderstand how the variety of magmatic products arises, and there is no\nconsensus yet on models of volcanic plumbing systems. This is because eruptions\nresult from the integration of multiple processes, rooted in the magma source\neither in the mantle or lower crust that feeds a complex network of magma\nbodies linking magma source and volcano. In this work, we investigate the\npotential of the network approach through a prototype of magma pool interaction\nand magma transfer across the crust. In network terms, it describes a diffusion\nprocess on a dynamic spatial network, in which diffusion and network evolution\nare intertwined: the diffusion affects the network structure, and reciprocally.\nThe diffusion process and network evolution mechanisms come from rules of\nbehaviour derived from rock mechanics and melting processes. Nodes represent\nmagma pools and edges physical connections between them, e.g., dykes or\nveinlets.\n",
        "title": "A toy model for approaching volcanic plumbing systems as complex systems",
        "date": "2023-07-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02960",
        "abstract_url": "http://arxiv.org/abs/2401.02960",
        "authors": [
            {
                "last_name": "Ratnarajah",
                "first_name": "Anton Jeran"
            },
            {
                "last_name": "Goonetilleke",
                "first_name": "Sahani"
            },
            {
                "last_name": "Tissera",
                "first_name": "Dumindu"
            },
            {
                "last_name": "Balagopalan",
                "first_name": "Kapilan"
            },
            {
                "last_name": "Rodrigo",
                "first_name": "Ranga"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CV"
        ],
        "abstract": "  Law enforcement officials heavily depend on Forensic Video Analytic (FVA)\nSoftware in their evidence extraction process. However present-day FVA software\nare complex, time consuming, equipment dependent and expensive. Developing\ncountries struggle to gain access to this gateway to a secure haven. The term\nforensic pertains the application of scientific methods to the investigation of\ncrime through post-processing, whereas surveillance is the close monitoring of\nreal-time feeds.\n  The principle objective of this Final Year Project was to develop an\nefficient and effective FVA Software, addressing the shortcomings through a\nstringent and systematic review of scholarly research papers, online databases\nand legal documentation. The scope spans multiple object detection, multiple\nobject tracking, anomaly detection, activity recognition, tampering detection,\ngeneral and specific image enhancement and video synopsis.\n  Methods employed include many machine learning techniques, GPU acceleration\nand efficient, integrated architecture development both for real-time and\npostprocessing. For this CNN, GMM, multithreading and OpenCV C++ coding were\nused. The implications of the proposed methodology would rapidly speed up the\nFVA process especially through the novel video synopsis research arena. This\nproject has resulted in three research outcomes Moving Object Based Collision\nFree Video Synopsis, Forensic and Surveillance Analytic Tool Architecture and\nTampering Detection Inter-Frame Forgery.\n  The results include forensic and surveillance panel outcomes with emphasis on\nvideo synopsis and Sri Lankan context. Principal conclusions include the\noptimization and efficient algorithm integration to overcome limitations in\nprocessing power, memory and compromise between real-time performance and\naccuracy.\n",
        "title": "Forensic Video Analytic Software",
        "date": "2023-09-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02961",
        "abstract_url": "http://arxiv.org/abs/2401.02961",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Manna"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yang"
            },
            {
                "last_name": "Yang",
                "first_name": "Feng"
            },
            {
                "last_name": "Chattoraj",
                "first_name": "Joyjit"
            },
            {
                "last_name": "Xia",
                "first_name": "Yingzhi"
            },
            {
                "last_name": "Xu",
                "first_name": "Xinxing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Weijiang"
            },
            {
                "last_name": "Dao",
                "first_name": "My Ha"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "",
            ""
        ],
        "abstract": "  Metasurfaces have widespread applications in fifth-generation (5G) microwave\ncommunication. Among the metasurface family, free-form metasurfaces excel in\nachieving intricate spectral responses compared to regular-shape counterparts.\nHowever, conventional numerical methods for free-form metasurfaces are\ntime-consuming and demand specialized expertise. Alternatively, recent studies\ndemonstrate that deep learning has great potential to accelerate and refine\nmetasurface designs. Here, we present XGAN, an extended generative adversarial\nnetwork (GAN) with a surrogate for high-quality free-form metasurface designs.\nThe proposed surrogate provides a physical constraint to XGAN so that XGAN can\naccurately generate metasurfaces monolithically from input spectral responses.\nIn comparative experiments involving 20000 free-form metasurface designs, XGAN\nachieves 0.9734 average accuracy and is 500 times faster than the conventional\nmethodology. This method facilitates the metasurface library building for\nspecific spectral responses and can be extended to various inverse design\nproblems, including optical metamaterials, nanophotonic devices, and drug\ndiscovery.\n",
        "title": "A Surrogate-Assisted Extended Generative Adversarial Network for\n  Parameter Optimization in Free-Form Metasurface Design",
        "date": "2023-10-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02962",
        "abstract_url": "http://arxiv.org/abs/2401.02962",
        "authors": [
            {
                "last_name": "Safarzadeh",
                "first_name": "Vahid Mohammadi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV"
        ],
        "abstract": "  Vessel structure is one of the most important parts of the retina which\nphysicians can detect many diseases by analysing its features. Localization of\nblood vessels in retina images is an important process in medical image\nanalysis. This process is also more challenging with the presence of bright and\ndark lesions. In this thesis, two automated vessel localization methods to\nhandle both healthy and unhealthy (pathological) retina images are analyzed.\nEach method consists of two major steps and the second step is the same in the\ntwo methods. In the first step, an algorithm is used to decrease the effect of\nbright lesions. In Method 1, this algorithm is based on K- Means segmentation,\nand in Method 2, it is based on a regularization procedure. In the second step\nof both methods, a multi-scale line operator is used to localize the\nline-shaped vascular structures and ignore the dark lesions which are generally\nassumed to have irregular patterns. After the introduction of the methods, a\ndetailed quantitative and qualitative comparison of the methods with one\nanother as well as the state-of-the-art solutions in the literature based on\nthe segmentation results on the images of the two publicly available datasets,\nDRIVE and STARE, is reported. The results demonstrate that the methods are\nhighly comparable with other solutions.\n",
        "title": "Automated Localization of Blood Vessels in Retinal Images",
        "date": "2023-10-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02965",
        "abstract_url": "http://arxiv.org/abs/2401.02965",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yu-Ning"
            },
            {
                "last_name": "Love",
                "first_name": "Michael I."
            },
            {
                "last_name": "Ronkowski",
                "first_name": "Cynthia Flaire"
            },
            {
                "last_name": "Deshpande",
                "first_name": "Dhrithi"
            },
            {
                "last_name": "Schriml",
                "first_name": "Lynn M."
            },
            {
                "last_name": "Wong-Beringer",
                "first_name": "Annie"
            },
            {
                "last_name": "Mons",
                "first_name": "Barend"
            },
            {
                "last_name": "Corbett-Detig",
                "first_name": "Russell"
            },
            {
                "last_name": "Hunter",
                "first_name": "Christopher I"
            },
            {
                "last_name": "Moore",
                "first_name": "Jason H."
            },
            {
                "last_name": "Garmire",
                "first_name": "Lana X."
            },
            {
                "last_name": "Reddy",
                "first_name": "T. B. K."
            },
            {
                "last_name": "Hide",
                "first_name": "Winston A."
            },
            {
                "last_name": "Butte",
                "first_name": "Atul J."
            },
            {
                "last_name": "Robinson",
                "first_name": "Mark D."
            },
            {
                "last_name": "Mangul",
                "first_name": "Serghei"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  Metadata, often termed \"data about data,\" is crucial for organizing,\nunderstanding, and managing vast omics datasets. It aids in efficient data\ndiscovery, integration, and interpretation, enabling users to access,\ncomprehend, and utilize data effectively. Its significance spans the domains of\nscientific research, facilitating data reproducibility, reusability, and\nsecondary analysis. However, numerous perceptual and technical barriers hinder\nthe sharing of metadata among researchers. These barriers compromise the\nreliability of research results and hinder integrative meta-analyses of omics\nstudies . This study highlights the key barriers to metadata sharing, including\nthe lack of uniform standards, privacy and legal concerns, limitations in study\ndesign, limited incentives, inadequate infrastructure, and the dearth of\nwell-trained personnel for metadata management and reuse. Proposed solutions\ninclude emphasizing the promotion of standardization, educational efforts, the\nrole of journals and funding agencies, incentives and rewards, and the\nimprovement of infrastructure. More accurate, reliable, and impactful research\noutcomes are achievable if the scientific community addresses these barriers,\nfacilitating more accurate, reliable, and impactful research outcomes.\n",
        "title": "Perceptual and technical barriers in sharing and formatting metadata\n  accompanying omics studies",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02968",
        "abstract_url": "http://arxiv.org/abs/2401.02968",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Qisong"
            },
            {
                "last_name": "Lin",
                "first_name": "Ji"
            },
            {
                "last_name": "Wei",
                "first_name": "Sijia"
            },
            {
                "last_name": "Liu",
                "first_name": "Neng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent studies, the focus has been on enhancing knowledge graph embedding\nlearning, which encodes entities and relations in knowledge graphs into\nlow-dimensional vector spaces. While current models mainly consider the\nstructural aspects of these graphs, there's a wealth of contextual and literal\ninformation in knowledge graphs that can be utilized for more effective\nembeddings. This paper introduces a novel model that incorporates both\ncontextual and literal information into entity and relation embeddings,\nutilizing graph convolutional networks. Specifically, for contextual\ninformation, we assess its significance through confidence and relatedness\nmetrics. A unique rule-based method is developed to calculate the confidence\nmetric, and the relatedness metric is derived from the literal information's\nrepresentations. We validated our model's performance with thorough experiments\non two established benchmark datasets.\n",
        "title": "Rule-Guided Joint Embedding Learning of Knowledge Graphs",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02971",
        "abstract_url": "http://arxiv.org/abs/2401.02971",
        "authors": [
            {
                "last_name": "Manolache",
                "first_name": "Andrei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Deep anomaly detection methods have become increasingly popular in recent\nyears, with methods like Stacked Autoencoders, Variational Autoencoders, and\nGenerative Adversarial Networks greatly improving the state-of-the-art. Other\nmethods rely on augmenting classical models (such as the One-Class Support\nVector Machine), by learning an appropriate kernel function using Neural\nNetworks. Recent developments in representation learning by self-supervision\nare proving to be very beneficial in the context of anomaly detection. Inspired\nby the advancements in anomaly detection using self-supervised learning in the\nfield of computer vision, this thesis aims to develop a method for detecting\nanomalies by exploiting pretext tasks tailored for text corpora. This approach\ngreatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG\nNews, for both semi-supervised and unsupervised anomaly detection, thus proving\nthe potential for self-supervised anomaly detectors in the field of natural\nlanguage processing.\n",
        "title": "Deep Anomaly Detection in Text",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02972",
        "abstract_url": "http://arxiv.org/abs/2401.02972",
        "authors": [
            {
                "last_name": "Sang",
                "first_name": "Erik Tjong Kim"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We describe the project REE-HDSC and outline our efforts to improve the\nquality of named entities extracted automatically from texts generated by\nhand-written text recognition (HTR) software. We describe a six-step processing\npipeline and test it by processing 19th and 20th century death certificates\nfrom the civil registry of Curacao. We find that the pipeline extracts dates\nwith high precision but that the precision of person name extraction is low.\nNext we show how name precision extraction can be improved by retraining HTR\nmodels with names, post-processing and by identifying and removing incorrect\nnames.\n",
        "title": "REE-HDSC: Recognizing Extracted Entities for the Historical Database\n  Suriname Curacao",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02974",
        "abstract_url": "http://arxiv.org/abs/2401.02974",
        "authors": [
            {
                "last_name": "Kwon",
                "first_name": "Taeksoo"
            },
            {
                "last_name": "Kim",
                "first_name": "Connor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "IR"
        ],
        "abstract": "  This paper examines the efficacy of utilizing large language models (LLMs) to\ndetect public threats posted online. Amid rising concerns over the spread of\nthreatening rhetoric and advance notices of violence, automated content\nanalysis techniques may aid in early identification and moderation. Custom data\ncollection tools were developed to amass post titles from a popular Korean\nonline community, comprising 500 non-threat examples and 20 threats. Various\nLLMs (GPT-3.5, GPT-4, PaLM) were prompted to classify individual posts as\neither \"threat\" or \"safe.\" Statistical analysis found all models demonstrated\nstrong accuracy, passing chi-square goodness of fit tests for both threat and\nnon-threat identification. GPT-4 performed best overall with 97.9% non-threat\nand 100% threat accuracy. Affordability analysis also showed PaLM API pricing\nas highly cost-efficient. The findings indicate LLMs can effectively augment\nhuman content moderation at scale to help mitigate emerging online risks.\nHowever, biases, transparency, and ethical oversight remain vital\nconsiderations before real-world implementation.\n",
        "title": "Efficacy of Utilizing Large Language Models to Detect Public Threat\n  Posted Online",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02975",
        "abstract_url": "http://arxiv.org/abs/2401.02975",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Yu"
            },
            {
                "last_name": "Ceross",
                "first_name": "Aaron"
            },
            {
                "last_name": "Bergmann",
                "first_name": "Jeroen H. M."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CL"
        ],
        "abstract": "  This study investigates the complexity of regulatory affairs in the medical\ndevice industry, a critical factor influencing market access and patient care.\nThrough qualitative research, we sought expert insights to understand the\nfactors contributing to this complexity. The study involved semi-structured\ninterviews with 28 professionals from medical device companies, specializing in\nvarious aspects of regulatory affairs. These interviews were analyzed using\nopen coding and Natural Language Processing (NLP) techniques. The findings\nreveal key sources of complexity within the regulatory landscape, divided into\nfive domains: (A) Regulatory language complexity, (B) Intricacies within the\nregulatory process, (C) Global-level complexities, (D) Database-related\nconsiderations, and (E) Product-level issues. The participants highlighted the\nneed for strategies to streamline regulatory compliance, enhance interactions\nbetween regulatory bodies and industry players, and develop adaptable\nframeworks for rapid technological advancements. Emphasizing interdisciplinary\ncollaboration and increased transparency, the study concludes that these\nelements are vital for establishing coherent and effective regulatory\nprocedures in the medical device sector.\n",
        "title": "Uncovering Regulatory Affairs Complexity in Medical Products: A\n  Qualitative Assessment Utilizing Open Coding and Natural Language Processing\n  (NLP)",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02976",
        "abstract_url": "http://arxiv.org/abs/2401.02976",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jiahang"
            },
            {
                "last_name": "Chen",
                "first_name": "Taoyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuanli"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This study introduces a novel approach for analyzing and modifying entity\nrelationships in GPT models, diverging from ROME's entity-focused methods. We\ndevelop a relation tracing technique to understand the influence of language\nmodel computations on relationship judgments. Using the FewRel dataset, we\nidentify key roles of MLP modules and attention mechanisms in processing\nrelationship information. Our method, tested against ROME on a new dataset,\nshows improved balance in specificity and generalization, underscoring the\npotential of manipulating early-layer modules for enhanced model understanding\nand accuracy.\n",
        "title": "Trace and Edit Relation Associations in GPT",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02978",
        "abstract_url": "http://arxiv.org/abs/2401.02978",
        "authors": [
            {
                "last_name": "Brinkman",
                "first_name": "Donald"
            },
            {
                "last_name": "Grudin",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC",
            ""
        ],
        "abstract": "  For generative AI to succeed, how engaging a conversationalist must it be?\nFor almost sixty years, some conversational agents have responded to any\nquestion or comment to keep a conversation going. In recent years, several\nutilized machine learning or sophisticated language processing, such as Tay,\nXiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they\nfocused on engagement, not expertise. Millions of people were motivated to\nengage with them. What were the attractions? Will generative AI do better if it\nis equally engaging, or should it be less engaging? Prior to the emergence of\ngenerative AI, we conducted a large-scale quantitative and qualitative analysis\nto learn what motivated millions of people to engage with one such 'virtual\ncompanion,' Microsoft's Zo. We examined the complete chat logs of 2000\nanonymized people. We identified over a dozen motivations that people had for\ninteracting with this software. Designers learned different ways to increase\nengagement. Generative conversational AI does not yet have a clear revenue\nmodel to address its high cost. It might benefit from being more engaging, even\nas it supports productivity and creativity. Our study and analysis point to\nopportunities and challenges.\n",
        "title": "Learning from a Generative AI Predecessor -- The Many Motivations for\n  Interacting with Conversational Agents",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02979",
        "abstract_url": "http://arxiv.org/abs/2401.02979",
        "authors": [
            {
                "last_name": "Peter",
                "first_name": "Silvan David"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Shreyan"
            },
            {
                "last_name": "Cancino-Chac\u00f3n",
                "first_name": "Carlos Eduardo"
            },
            {
                "last_name": "Widmer",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "IR"
        ],
        "abstract": "  Semantic embeddings play a crucial role in natural language-based information\nretrieval. Embedding models represent words and contexts as vectors whose\nspatial configuration is derived from the distribution of words in large text\ncorpora. While such representations are generally very powerful, they might\nfail to account for fine-grained domain-specific nuances. In this article, we\ninvestigate this uncertainty for the domain of characterizations of expressive\npiano performance. Using a music research dataset of free text performance\ncharacterizations and a follow-up study sorting the annotations into clusters,\nwe derive a ground truth for a domain-specific semantic similarity structure.\nWe test five embedding models and their similarity structure for correspondence\nwith the ground truth. We further assess the effects of contextualizing\nprompts, hubness reduction, cross-modal similarity, and k-means clustering. The\nquality of embedding models shows great variability with respect to this task;\nmore general models perform better than domain-adapted ones and the best model\nconfigurations reach human-level agreement.\n",
        "title": "Are we describing the same sound? An analysis of word embedding spaces\n  of expressive piano performance",
        "date": "2023-12-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02981",
        "abstract_url": "http://arxiv.org/abs/2401.02981",
        "authors": [
            {
                "last_name": "Jeong",
                "first_name": "Cheonsu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.\n",
        "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02982",
        "abstract_url": "http://arxiv.org/abs/2401.02982",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Shu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shangqing"
            },
            {
                "last_name": "Jia",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Xinlin"
            },
            {
                "last_name": "Long",
                "first_name": "Zhaoguang"
            },
            {
                "last_name": "Lan",
                "first_name": "Man"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of tasks. However, their proficiency and reliability in the\nspecialized domain of Data Analysis, particularly with a focus on data-driven\nthinking, remain uncertain. To bridge this gap, we introduce BIBench, a\ncomprehensive benchmark designed to evaluate the data analysis capabilities of\nLLMs within the context of Business Intelligence (BI). BIBench assesses LLMs\nacross three dimensions: 1) BI foundational knowledge, evaluating the models'\nnumerical reasoning and familiarity with financial concepts; 2) BI knowledge\napplication, determining the models' ability to quickly comprehend textual\ninformation and generate analysis questions from multiple views; and 3) BI\ntechnical skills, examining the models' use of technical knowledge to address\nreal-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning\nthree categories of task types: classification, extraction, and generation.\nAdditionally, we've developed BIChat, a domain-specific dataset with over a\nmillion data points, to fine-tune LLMs. We will release BIBenchmark, BIChat,\nand the evaluation scripts at \\url{https://github.com/cubenlp/BIBench}. This\nbenchmark aims to provide a measure for in-depth analysis of LLM abilities and\nfoster the advancement of LLMs in the field of data analysis.\n",
        "title": "BIBench: Benchmarking Data Analysis Knowledge of Large Language Models",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02984",
        "abstract_url": "http://arxiv.org/abs/2401.02984",
        "authors": [
            {
                "last_name": "Hua",
                "first_name": "Yining"
            },
            {
                "last_name": "Liu",
                "first_name": "Fenglin"
            },
            {
                "last_name": "Yang",
                "first_name": "Kailai"
            },
            {
                "last_name": "Li",
                "first_name": "Zehan"
            },
            {
                "last_name": "Sheu",
                "first_name": "Yi-han"
            },
            {
                "last_name": "Zhou",
                "first_name": "Peilin"
            },
            {
                "last_name": "Moran",
                "first_name": "Lauren V."
            },
            {
                "last_name": "Ananiadou",
                "first_name": "Sophia"
            },
            {
                "last_name": "Beam",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Objective: The growing use of large language models (LLMs) stimulates a need\nfor a comprehensive review of their applications and outcomes in mental health\ncare contexts. This scoping review aims to critically analyze the existing\ndevelopment and applications of LLMs in mental health care, highlighting their\nsuccesses and identifying their challenges and limitations in these specialized\nfields. Materials and Methods: A broad literature search was conducted in\nNovember 2023 using six databases (PubMed, Web of Science, Google Scholar,\narXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A\ntotal of 313 publications were initially identified, and after applying the\nstudy inclusion criteria, 34 publications were selected for the final review.\nResults: We identified diverse applications of LLMs in mental health care,\nincluding diagnosis, therapy, patient engagement enhancement, etc. Key\nchallenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. Conclusion: LLMs show\npromising potential in advancing mental health care, with applications in\ndiagnostics, and patient support. Continued advancements depend on\ncollaborative, multidisciplinary efforts focused on framework enhancement,\nrigorous dataset development, technological refinement, and ethical integration\nto ensure the effective and safe application of LLMs in mental health care.\n",
        "title": "Large Language Models in Mental Health Care: a Scoping Review",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02985",
        "abstract_url": "http://arxiv.org/abs/2401.02985",
        "authors": [
            {
                "last_name": "Ashrafimoghari",
                "first_name": "Vahid"
            },
            {
                "last_name": "G\u00fcrkan",
                "first_name": "Necdet"
            },
            {
                "last_name": "Suchow",
                "first_name": "Jordan W."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The rapid evolution of artificial intelligence (AI), especially in the domain\nof Large Language Models (LLMs) and generative AI, has opened new avenues for\napplication across various fields, yet its role in business education remains\nunderexplored. This study introduces the first benchmark to assess the\nperformance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and\nGPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models\n(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission\nprocess for graduate business programs. Our analysis shows that most LLMs\noutperform human candidates, with GPT-4 Turbo not only outperforming the other\nmodels but also surpassing the average scores of graduate students at top\nbusiness schools. Through a case study, this research examines GPT-4 Turbo's\nability to explain answers, evaluate responses, identify errors, tailor\ninstructions, and generate alternative scenarios. The latest LLM versions,\nGPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in\nreasoning tasks compared to their predecessors, underscoring their potential\nfor complex problem-solving. While AI's promise in education, assessment, and\ntutoring is clear, challenges remain. Our study not only sheds light on LLMs'\nacademic potential but also emphasizes the need for careful development and\napplication of AI in education. As AI technology advances, it is imperative to\nestablish frameworks and protocols for AI interaction, verify the accuracy of\nAI-generated content, ensure worldwide access for diverse learners, and create\nan educational environment where AI supports human expertise. This research\nsets the stage for further exploration into the responsible use of AI to enrich\neducational experiences and improve exam preparation and assessment methods.\n",
        "title": "Evaluating Large Language Models on the GMAT: Implications for the\n  Future of Business Education",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02986",
        "abstract_url": "http://arxiv.org/abs/2401.02986",
        "authors": [
            {
                "last_name": "Sai",
                "first_name": "Catherine"
            },
            {
                "last_name": "Sadiq",
                "first_name": "Shazia"
            },
            {
                "last_name": "Han",
                "first_name": "Lei"
            },
            {
                "last_name": "Demartini",
                "first_name": "Gianluca"
            },
            {
                "last_name": "Rinderle-Ma",
                "first_name": "Stefanie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Organizations face the challenge of ensuring compliance with an increasing\namount of requirements from various regulatory documents. Which requirements\nare relevant depends on aspects such as the geographic location of the\norganization, its domain, size, and business processes. Considering these\ncontextual factors, as a first step, relevant documents (e.g., laws,\nregulations, directives, policies) are identified, followed by a more detailed\nanalysis of which parts of the identified documents are relevant for which step\nof a given business process. Nowadays the identification of regulatory\nrequirements relevant to business processes is mostly done manually by domain\nand legal experts, posing a tremendous effort on them, especially for a large\nnumber of regulatory documents which might frequently change. Hence, this work\nexamines how legal and domain experts can be assisted in the assessment of\nrelevant requirements. For this, we compare an embedding-based NLP ranking\nmethod, a generative AI method using GPT-4, and a crowdsourced method with the\npurely manual method of creating relevancy labels by experts. The proposed\nmethods are evaluated based on two case studies: an Australian insurance case\ncreated with domain experts and a global banking use case, adapted from SAP\nSignavio's workflow example of an international guideline. A gold standard is\ncreated for both BPMN2.0 processes and matched to real-world textual\nrequirements from multiple regulatory documents. The evaluation and discussion\nprovide insights into strengths and weaknesses of each method regarding\napplicability, automation, transparency, and reproducibility and provide\nguidelines on which method combinations will maximize benefits for given\ncharacteristics such as process usage, impact, and dynamics of an application\nscenario.\n",
        "title": "Identification of Regulatory Requirements Relevant to Business\n  Processes: A Comparative Study on Generative AI, Embedding-based Ranking,\n  Crowd and Expert-driven Methods",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02988",
        "abstract_url": "http://arxiv.org/abs/2401.02988",
        "authors": [
            {
                "last_name": "Muzumdar",
                "first_name": "Prathamesh"
            },
            {
                "last_name": "Kurian",
                "first_name": "George"
            },
            {
                "last_name": "Basyal",
                "first_name": "Ganga Prasad"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Crowdfunding in the realm of the Social Web has received substantial\nattention, with prior research examining various aspects of campaigns,\nincluding project objectives, durations, and influential project categories for\nsuccessful fundraising. These factors are crucial for entrepreneurs seeking\ndonor support. However, the terrain of charity crowdfunding within the Social\nWeb remains relatively unexplored, lacking comprehension of the motivations\ndriving donations that often lack concrete reciprocation. Distinct from\nconventional crowdfunding that offers tangible returns, charity crowdfunding\nrelies on intangible rewards like tax advantages, recognition posts, or\nadvisory roles. Such details are often embedded within campaign narratives,\nyet, the analysis of textual content in charity crowdfunding is limited. This\nstudy introduces an inventive text analytics framework, utilizing Latent\nDirichlet Allocation (LDA) to extract latent themes from textual descriptions\nof charity campaigns. The study has explored four different themes, two each in\ncampaign and incentive descriptions. Campaign description themes are focused on\nchild and elderly health mainly the ones who are diagnosed with terminal\ndiseases. Incentive description themes are based on tax benefits, certificates,\nand appreciation posts. These themes, combined with numerical parameters,\npredict campaign success. The study was successful in using Random Forest\nClassifier to predict success of the campaign using both thematic and numerical\nparameters. The study distinguishes thematic categories, particularly medical\nneed-based charity and general causes, based on project and incentive\ndescriptions. In conclusion, this research bridges the gap by showcasing topic\nmodelling utility in uncharted charity crowdfunding domains.\n",
        "title": "A Latent Dirichlet Allocation (LDA) Semantic Text Analytics Approach to\n  Explore Topical Features in Charity Crowdfunding Campaigns",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02989",
        "abstract_url": "http://arxiv.org/abs/2401.02989",
        "authors": [
            {
                "last_name": "Zbinden",
                "first_name": "Robin"
            },
            {
                "last_name": "van Tiel",
                "first_name": "Nina"
            },
            {
                "last_name": "Kellenberger",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Hughes",
                "first_name": "Lloyd"
            },
            {
                "last_name": "Tuia",
                "first_name": "Devis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Species distribution modeling is a highly versatile tool for understanding\nthe intricate relationship between environmental conditions and species\noccurrences. However, the available data often lacks information on confirmed\nspecies absence and is limited to opportunistically sampled, presence-only\nobservations. To overcome this limitation, a common approach is to employ\npseudo-absences, which are specific geographic locations designated as negative\nsamples. While pseudo-absences are well-established for single-species\ndistribution models, their application in the context of multi-species neural\nnetworks remains underexplored. Notably, the significant class imbalance\nbetween species presences and pseudo-absences is often left unaddressed.\nMoreover, the existence of different types of pseudo-absences (e.g., random and\ntarget-group background points) adds complexity to the selection process.\nDetermining the optimal combination of pseudo-absences types is difficult and\ndepends on the characteristics of the data, particularly considering that\ncertain types of pseudo-absences can be used to mitigate geographic biases. In\nthis paper, we demonstrate that these challenges can be effectively tackled by\nintegrating pseudo-absences in the training of multi-species neural networks\nthrough modifications to the loss function. This adjustment involves assigning\ndifferent weights to the distinct terms of the loss function, thereby\naddressing both the class imbalance and the choice of pseudo-absence types.\nAdditionally, we propose a strategy to set these loss weights using spatial\nblock cross-validation with presence-only data. We evaluate our approach using\na benchmark dataset containing independent presence-absence data from six\ndifferent regions and report improved results when compared to competing\napproaches.\n",
        "title": "On the selection and effectiveness of pseudo-absences for species\n  distribution modeling with deep learning",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02991",
        "abstract_url": "http://arxiv.org/abs/2401.02991",
        "authors": [
            {
                "last_name": "Kharyal",
                "first_name": "Chaitanya"
            },
            {
                "last_name": "Gottipati",
                "first_name": "Sai Krishna"
            },
            {
                "last_name": "Sinha",
                "first_name": "Tanmay Kumar"
            },
            {
                "last_name": "Das",
                "first_name": "Srijita"
            },
            {
                "last_name": "Taylor",
                "first_name": "Matthew E."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  One of the final frontiers in the development of complex human - AI\ncollaborative systems is the ability of AI agents to comprehend the natural\nlanguage and perform tasks accordingly. However, training efficient\nReinforcement Learning (RL) agents grounded in natural language has been a\nlong-standing challenge due to the complexity and ambiguity of the language and\nsparsity of the rewards, among other factors. Several advances in reinforcement\nlearning, curriculum learning, continual learning, language models have\nindependently contributed to effective training of grounded agents in various\nenvironments. Leveraging these developments, we present a novel algorithm,\nGrounded Language Instruction through DEmonstration in RL (GLIDE-RL) that\nintroduces a teacher-instructor-student curriculum learning framework for\ntraining an RL agent capable of following natural language instructions that\ncan generalize to previously unseen language instructions. In this multi-agent\nframework, the teacher and the student agents learn simultaneously based on the\nstudent's current skill level. We further demonstrate the necessity for\ntraining the student agent with not just one, but multiple teacher agents.\nExperiments on a complex sparse reward environment validates the effectiveness\nof our proposed approach.\n",
        "title": "GLIDE-RL: Grounded Language Instruction through DEmonstration in RL",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02992",
        "abstract_url": "http://arxiv.org/abs/2401.02992",
        "authors": [
            {
                "last_name": "Peng",
                "first_name": "Jiahui"
            },
            {
                "last_name": "Gao",
                "first_name": "Jing"
            },
            {
                "last_name": "Tong",
                "first_name": "Xin"
            },
            {
                "last_name": "Guo",
                "first_name": "Jing"
            },
            {
                "last_name": "Yang",
                "first_name": "Hang"
            },
            {
                "last_name": "Qi",
                "first_name": "Jianchuan"
            },
            {
                "last_name": "Li",
                "first_name": "Ruiqiao"
            },
            {
                "last_name": "Li",
                "first_name": "Nan"
            },
            {
                "last_name": "Xu",
                "first_name": "Ming"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In the evolving field of corporate sustainability, analyzing unstructured\nEnvironmental, Social, and Governance (ESG) reports is a complex challenge due\nto their varied formats and intricate content. This study introduces an\ninnovative methodology utilizing the \"Unstructured Core Library\", specifically\ntailored to address these challenges by transforming ESG reports into\nstructured, analyzable formats. Our approach significantly advances the\nexisting research by offering high-precision text cleaning, adept\nidentification and extraction of text from images, and standardization of\ntables within these reports. Emphasizing its capability to handle diverse data\ntypes, including text, images, and tables, the method adeptly manages the\nnuances of differing page layouts and report styles across industries. This\nresearch marks a substantial contribution to the fields of industrial ecology\nand corporate sustainability assessment, paving the way for the application of\nadvanced NLP technologies and large language models in the analysis of\ncorporate governance and sustainability. Our code is available at\nhttps://github.com/linancn/TianGong-AI-Unstructure.git.\n",
        "title": "Advanced Unstructured Data Processing for ESG Reports: A Methodology for\n  Structured Transformation and Enhanced Analysis",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02993",
        "abstract_url": "http://arxiv.org/abs/2401.02993",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Shangyu"
            },
            {
                "last_name": "Xiong",
                "first_name": "Ying"
            },
            {
                "last_name": "Cui",
                "first_name": "Yufei"
            },
            {
                "last_name": "Liu",
                "first_name": "Xue"
            },
            {
                "last_name": "Tang",
                "first_name": "Buzhou"
            },
            {
                "last_name": "Kuo",
                "first_name": "Tei-Wei"
            },
            {
                "last_name": "Xue",
                "first_name": "Chun Jason"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Retrieval-based augmentations that aim to incorporate knowledge from an\nexternal database into language models have achieved great success in various\nknowledge-intensive (KI) tasks, such as question-answering and text generation.\nHowever, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as\ntext classification, is still challenging. Existing works focus on\nconcatenating retrievals to inputs as context to form the prompt-based inputs.\nUnfortunately, such methods require language models to have the capability to\nhandle long texts. Besides, inferring such concatenated data would also consume\na significant amount of computational resources.\n  To solve these challenges, we propose \\textbf{ReFusion} in this paper, a\ncomputation-efficient \\textbf{Re}trieval representation \\textbf{Fusion} with\nneural architecture search. The main idea is to directly fuse the retrieval\nrepresentations into the language models. Specifically, we first propose an\nonline retrieval module that retrieves representations of similar sentences.\nThen, we present a retrieval fusion module including two effective ranking\nschemes, i.e., reranker-based scheme and ordered-mask-based scheme, to fuse the\nretrieval representations with hidden states. Furthermore, we use Neural\nArchitecture Search (NAS) to seek the optimal fusion structure across different\nlayers. Finally, we conduct comprehensive experiments, and the results\ndemonstrate our ReFusion can achieve superior and robust performance on various\nNKI tasks.\n",
        "title": "Improving Natural Language Understanding with Computation-Efficient\n  Retrieval Representation Fusion",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02995",
        "abstract_url": "http://arxiv.org/abs/2401.02995",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Yuntao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuzhe"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shuyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "CV",
            ""
        ],
        "abstract": "  Multimodal depression detection is an important research topic that aims to\npredict human mental states using multimodal data. Previous methods treat\ndifferent modalities equally and fuse each modality by na\\\"ive mathematical\noperations without measuring the relative importance between them, which cannot\nobtain well-performed multimodal representations for downstream depression\ntasks. In order to tackle the aforementioned concern, we present a Cross-modal\nAttention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for\nmultimodal depression detection. CANAMRF is constructed by a multimodal feature\nextractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid\nAttention Module. Through experimentation on two benchmark datasets, CANAMRF\ndemonstrates state-of-the-art performance, underscoring the effectiveness of\nour proposed approach.\n",
        "title": "CANAMRF: An Attention-Based Model for Multimodal Depression Detection",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02996",
        "abstract_url": "http://arxiv.org/abs/2401.02996",
        "authors": [
            {
                "last_name": "Saeed",
                "first_name": "Tabish"
            },
            {
                "last_name": "Ijaz",
                "first_name": "Aneeqa"
            },
            {
                "last_name": "Sadiq",
                "first_name": "Ismail"
            },
            {
                "last_name": "Qureshi",
                "first_name": "Haneya N."
            },
            {
                "last_name": "Rizwan",
                "first_name": "Ali"
            },
            {
                "last_name": "Imran",
                "first_name": "Ali"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            ""
        ],
        "abstract": "  Cough-based diagnosis for Respiratory Diseases (RDs) using Artificial\nIntelligence (AI) has attracted considerable attention, yet many existing\nstudies overlook confounding variables in their predictive models. These\nvariables can distort the relationship between cough recordings (input data)\nand RD status (output variable), leading to biased associations and unrealistic\nmodel performance. To address this gap, we propose the Bias Free Network\n(RBFNet), an end to end solution that effectively mitigates the impact of\nconfounders in the training data distribution. RBFNet ensures accurate and\nunbiased RD diagnosis features, emphasizing its relevance by incorporating a\nCOVID19 dataset in this study. This approach aims to enhance the reliability of\nAI based RD diagnosis models by navigating the challenges posed by confounding\nvariables. A hybrid of a Convolutional Neural Networks (CNN) and Long-Short\nTerm Memory (LSTM) networks is proposed for the feature encoder module of\nRBFNet. An additional bias predictor is incorporated in the classification\nscheme to formulate a conditional Generative Adversarial Network (cGAN) which\nhelps in decorrelating the impact of confounding variables from RD prediction.\nThe merit of RBFNet is demonstrated by comparing classification performance\nwith State of The Art (SoTA) Deep Learning (DL) model (CNN LSTM) after training\non different unbalanced COVID-19 data sets, created by using a large scale\nproprietary cough data set. RBF-Net proved its robustness against extremely\nbiased training scenarios by achieving test set accuracies of 84.1%, 84.6%, and\n80.5% for the following confounding variables gender, age, and smoking status,\nrespectively. RBF-Net outperforms the CNN-LSTM model test set accuracies by\n5.5%, 7.7%, and 8.2%, respectively\n",
        "title": "An AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough\n  Audio: A Case Study for COVID-19",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02997",
        "abstract_url": "http://arxiv.org/abs/2401.02997",
        "authors": [
            {
                "last_name": "Dom\u00ednguez",
                "first_name": "Jos\u00e9 Manuel"
            },
            {
                "last_name": "Err\u00e1zuriz",
                "first_name": "Benjam\u00edn"
            },
            {
                "last_name": "Daher",
                "first_name": "Patricio"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large Language Models (LLMs) have gained considerable notoriety in the field\nof natural language to SQL tasks (NL2SQL). In this study, we show how task\ndecomposition can greatly benefit LLMs in database understanding and query\ngeneration in order to answer human questions with an SQL query.\n  We fined-tuned open source models, specifically Llama-2 and Code Llama, by\ncombining 2 different models each designated to focus on one of two tasks in\norder to leverage each model's core competency to further increase the accuracy\nof the final SQL query.\n  We propose a new framework to divide the schema into chunks in order to fit\nmore information into a limited context. Our results are comparable with those\nobtained by GPT-4 at the same time being 135 times smaller, 90 times faster and\nmore than 100 times cheaper than GPT-4.\n",
        "title": "Blar-SQL: Faster, Stronger, Smaller NL2SQL",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02999",
        "abstract_url": "http://arxiv.org/abs/2401.02999",
        "authors": [
            {
                "last_name": "Nesterov",
                "first_name": "Alexander I"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "NI",
            "SI"
        ],
        "abstract": "  The clustering coefficient is a valuable tool for understanding the structure\nof complex networks. It is widely used to analyze social networks, biological\nnetworks, and other complex systems. While there is generally a single common\ndefinition for the local clustering coefficient, there are two different ways\nto calculate the global clustering coefficient. The first approach takes the\naverage of the local clustering coefficients for each node in the network. The\nsecond one is based on the ratio of closed triplets to all triplets. It is\nshown that these two definitions of the global clustering coefficients are\nstrongly inequivalent and may significantly impact the accuracy of the outcome.\n",
        "title": "On Clustering Coefficients in Complex Networks",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03000",
        "abstract_url": "http://arxiv.org/abs/2401.03000",
        "authors": [
            {
                "last_name": "Muaz",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Paull",
                "first_name": "Nathan"
            },
            {
                "last_name": "Malagavalli",
                "first_name": "Jahnavi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "LG",
            ""
        ],
        "abstract": "  This paper presents an innovative approach to address the challenges of\ntranslating multi-modal emotion recognition models to a more practical and\nresource-efficient uni-modal counterpart, specifically focusing on speech-only\nemotion recognition. Recognizing emotions from speech signals is a critical\ntask with applications in human-computer interaction, affective computing, and\nmental health assessment. However, existing state-of-the-art models often rely\non multi-modal inputs, incorporating information from multiple sources such as\nfacial expressions and gestures, which may not be readily available or feasible\nin real-world scenarios. To tackle this issue, we propose a novel framework\nthat leverages knowledge distillation and masked training techniques.\n",
        "title": "Bridging Modalities: Knowledge Distillation and Masked Training for\n  Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion\n  Recognition",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03001",
        "abstract_url": "http://arxiv.org/abs/2401.03001",
        "authors": [
            {
                "last_name": "chu",
                "first_name": "Li"
            },
            {
                "last_name": "bingjia",
                "first_name": "Xiao"
            },
            {
                "last_name": "qiping",
                "first_name": "Yuan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, Transformer-base models have made significant progress in the field\nof time series prediction which have achieved good results and become baseline\nmodels beyond Dlinear. The paper proposes an U-Net time series prediction model\n(UnetTSF) with linear complexity, which adopts the U-Net architecture. We are\nthe first to use FPN technology to extract features from time series data,\nreplacing the method of decomposing time series data into trend and seasonal\nterms, while designing a fusion structure suitable for time series data. After\ntesting on 8 open-source datasets, compared to the best linear model DLiner.\nOut of 32 testing projects, 31 achieved the best results. The average decrease\nin mse is 10.1%, while the average decrease in mae is 9.1%. Compared with the\ncomplex transformer-base PatchTST, UnetTSF obtained 9 optimal results for mse\nand 15 optimal results for mae in 32 testing projects.\n",
        "title": "UnetTSF: A Better Performance Linear Complexity Time Series Prediction\n  Model",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03002",
        "abstract_url": "http://arxiv.org/abs/2401.03002",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Chi"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhen"
            },
            {
                "last_name": "Ju",
                "first_name": "Lie"
            },
            {
                "last_name": "Mahapatra",
                "first_name": "Dwarikanath"
            },
            {
                "last_name": "Betz-Stablein",
                "first_name": "Brigid"
            },
            {
                "last_name": "Mar",
                "first_name": "Victoria"
            },
            {
                "last_name": "Janda",
                "first_name": "Monika"
            },
            {
                "last_name": "Soyer",
                "first_name": "Peter"
            },
            {
                "last_name": "Ge",
                "first_name": "Zongyuan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Deep learning models for medical image analysis easily suffer from\ndistribution shifts caused by dataset artifacts bias, camera variations,\ndifferences in the imaging station, etc., leading to unreliable diagnoses in\nreal-world clinical settings. Domain generalization (DG) methods, which aim to\ntrain models on multiple domains to perform well on unseen domains, offer a\npromising direction to solve the problem. However, existing DG methods assume\ndomain labels of each image are available and accurate, which is typically\nfeasible for only a limited number of medical datasets. To address these\nchallenges, we propose a novel DG framework for medical image classification\nwithout relying on domain labels, called Prompt-driven Latent Domain\nGeneralization (PLDG). PLDG consists of unsupervised domain discovery and\nprompt learning. This framework first discovers pseudo domain labels by\nclustering the bias-associated style features, then leverages collaborative\ndomain prompts to guide a Vision Transformer to learn knowledge from discovered\ndiverse domains. To facilitate cross-domain knowledge learning between\ndifferent prompts, we introduce a domain prompt generator that enables\nknowledge sharing between domain prompts and a shared prompt. A domain mixup\nstrategy is additionally employed for more flexible decision margins and\nmitigates the risk of incorrect domain assignments. Extensive experiments on\nthree medical image classification tasks and one debiasing task demonstrate\nthat our method can achieve comparable or even superior performance than\nconventional DG algorithms without relying on domain labels. Our code will be\npublicly available upon the paper is accepted.\n",
        "title": "Prompt-driven Latent Domain Generalization for Medical Image\n  Classification",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03003",
        "abstract_url": "http://arxiv.org/abs/2401.03003",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Linyuan"
            },
            {
                "last_name": "Elhoushi",
                "first_name": "Mostafa"
            },
            {
                "last_name": "Cheung",
                "first_name": "Alvin"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "LG"
        ],
        "abstract": "  Large language models (LLMs) have made significant advancements in\ncode-related tasks, yet many LLMs treat code as simple sequences, neglecting\nits structured nature. We introduce AST-T5, a novel pretraining paradigm that\nleverages the Abstract Syntax Tree (AST) for enhanced code generation,\ntranspilation, and understanding. Using dynamic programming, our AST-Aware\nSegmentation retains code structure, while our AST-Aware Span Corruption\nobjective equips the model to reconstruct various code structures. Unlike other\nmodels, AST-T5 avoids intricate program analyses or architectural changes, so\nit integrates seamlessly with any encoder-decoder Transformer. Evaluations show\nthat AST-T5 consistently outperforms similar-sized LMs across various\ncode-related tasks. Structure-awareness makes AST-T5 particularly powerful in\ncode-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the\nBugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in\nCodeXGLUE. Our code and model are publicly available at\nhttps://github.com/gonglinyuan/ast_t5.\n",
        "title": "AST-T5: Structure-Aware Pretraining for Code Generation and\n  Understanding",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03005",
        "abstract_url": "http://arxiv.org/abs/2401.03005",
        "authors": [
            {
                "last_name": "Saha",
                "first_name": "Sudipan"
            },
            {
                "last_name": "Verma",
                "first_name": "Tushar"
            },
            {
                "last_name": "Oliveira",
                "first_name": "Dario Augusto Borges"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  With the global population on the rise, our cities have been expanding to\naccommodate the growing number of people. The expansion of cities generally\nleads to the engulfment of peripheral areas. However, such expansion of urban\nareas is likely to cause increment in areas with increased land surface\ntemperature (LST). By considering each summer as a data point, we form LST\nmulti-year time-series and cluster it to obtain spatio-temporal pattern. We\nobserve several interesting phenomena from these patterns, e.g., some clusters\nshow reasonable similarity to the built-up area, whereas the locations with\nhigh temporal variation are seen more in the peripheral areas. Furthermore, the\nLST center of mass shifts over the years for cities with development activities\ntilted towards a direction. We conduct the above-mentioned studies for three\ndifferent cities in three different continents.\n",
        "title": "Evolution of urban areas and land surface temperature",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03006",
        "abstract_url": "http://arxiv.org/abs/2401.03006",
        "authors": [
            {
                "last_name": "Meijer",
                "first_name": "Caspar"
            },
            {
                "last_name": "Chen",
                "first_name": "Lydia Y."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  This survey delves into the application of diffusion models in time-series\nforecasting. Diffusion models are demonstrating state-of-the-art results in\nvarious fields of generative AI. The paper includes comprehensive background\ninformation on diffusion models, detailing their conditioning methods and\nreviewing their use in time-series forecasting. The analysis covers 11 specific\ntime-series implementations, the intuition and theory behind them, the\neffectiveness on different datasets, and a comparison among each other. Key\ncontributions of this work are the thorough exploration of diffusion models'\napplications in time-series forecasting and a chronologically ordered overview\nof these models. Additionally, the paper offers an insightful discussion on the\ncurrent state-of-the-art in this domain and outlines potential future research\ndirections. This serves as a valuable resource for researchers in AI and\ntime-series analysis, offering a clear view of the latest advancements and\nfuture potential of diffusion models.\n",
        "title": "The Rise of Diffusion Models in Time-Series Forecasting",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03011",
        "abstract_url": "http://arxiv.org/abs/2401.03011",
        "authors": [
            {
                "last_name": "Bousquet",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM"
        ],
        "abstract": "  We say that a graph is $k$-mixing if it is possible to transform any\n$k$-coloring into any other via a sequence of single vertex recolorings keeping\na proper coloring all along. Cereceda, van den Heuvel and Johnson proved that\ndeciding if a graph is $3$-mixing is co-NP-complete and left open the case $k\n\\ge 4$. We prove that for every $k \\ge 4$, $k$-mixing is co-NP-hard.\n",
        "title": "A Note on the Complexity of Graph Recoloring",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03012",
        "abstract_url": "http://arxiv.org/abs/2401.03012",
        "authors": [
            {
                "last_name": "Raghavan",
                "first_name": "Aneesh"
            },
            {
                "last_name": "Johansson",
                "first_name": "Karl Henrik"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We consider the problem of function estimation by a multi-agent system\ncomprising of two agents and a fusion center. Each agent receives data\ncomprising of samples of an independent variable (input) and the corresponding\nvalues of the dependent variable (output). The data remains local and is not\nshared with other members in the system. The objective of the system is to\ncollaboratively estimate the function from the input to the output. To this\nend, we develop an iterative distributed algorithm for this function estimation\nproblem. Each agent solves a local estimation problem in a Reproducing Kernel\nHilbert Space (RKHS) and uploads the function to the fusion center. At the\nfusion center, the functions are fused by first estimating the data points that\nwould have generated the uploaded functions and then subsequently solving a\nleast squares estimation problem using the estimated data from both functions.\nThe fused function is downloaded by the agents and is subsequently used for\nestimation at the next iteration along with incoming data. This procedure is\nexecuted sequentially and stopped when the difference between consecutively\nestimated functions becomes small enough. To analyze the algorithm, we define\nlearning operators for the agents, fusion center and the system. We study the\nasymptotic properties of the norm of the learning operators and find sufficient\nconditions under which they converge to $1$. Given a sequence of data points,\nwe define and prove the existence of the learning operator for the system. We\nprove that the porposed learning algorithm is consistent and demonstrate the\nsame using an example. The paper has been submitted to L4DC 2024.\n",
        "title": "Distributed Learning and Function Fusion in Reproducing Kernel Hilbert\n  Space",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03030",
        "abstract_url": "http://arxiv.org/abs/2401.03030",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Weizi"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CL",
            "CY"
        ],
        "abstract": "  With the rise of human-machine communication, machines are increasingly\ndesigned with humanlike characteristics, such as gender, which can\ninadvertently trigger cognitive biases. Many conversational agents (CAs), such\nas voice assistants and chatbots, default to female personas, leading to\nconcerns about perpetuating gender stereotypes and inequality. Critiques have\nemerged regarding the potential objectification of females and reinforcement of\ngender stereotypes by these technologies. This research, situated in\nconversational AI design, aims to delve deeper into the impacts of gender\nbiases in human-CA interactions. From a behavioral and communication research\nstandpoint, this program focuses not only on perceptions but also the\nlinguistic styles of users when interacting with CAs, as previous research has\nrarely explored. It aims to understand how pre-existing gender biases might be\ntriggered by CAs' gender designs. It further investigates how CAs' gender\ndesigns may reinforce gender biases and extend them to human-human\ncommunication. The findings aim to inform ethical design of conversational\nagents, addressing whether gender assignment in CAs is appropriate and how to\npromote gender equality in design.\n",
        "title": "Exploring Gender Biases in Language Patterns of Human-Conversational\n  Agent Conversations",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03031",
        "abstract_url": "http://arxiv.org/abs/2401.03031",
        "authors": [
            {
                "last_name": "Bentbib",
                "first_name": "Abdeslem Hafid"
            },
            {
                "last_name": "jbilou",
                "first_name": "Khalide"
            },
            {
                "last_name": "Tahiri",
                "first_name": "Ridwane"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The proximal gradient method is a generic technique introduced to tackle the\nnon-smoothness in optimization problems, wherein the objective function is\nexpressed as the sum of a differentiable convex part and a non-differentiable\nregularization term. Such problems with tensor format are of interest in many\nfields of applied mathematics such as image and video processing. Our goal in\nthis paper is to address the solution of such problems with a more general form\nof the regularization term. An adapted iterative proximal gradient method is\nintroduced for this purpose. Due to the slowness of the proposed algorithm, we\nuse new tensor extrapolation methods to enhance its convergence. Numerical\nexperiments on color image deblurring are conducted to illustrate the\nefficiency of our approach.\n",
        "title": "Multidimensional extrapolated global proximal gradient and applications\n  for image processing",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03037",
        "abstract_url": "http://arxiv.org/abs/2401.03037",
        "authors": [
            {
                "last_name": "Talemi",
                "first_name": "Niloufar Alipour"
            },
            {
                "last_name": "Kashiani",
                "first_name": "Hossein"
            },
            {
                "last_name": "Nasrabadi",
                "first_name": "Nasser M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Although face recognition (FR) has achieved great success in recent years, it\nis still challenging to accurately recognize faces in low-quality images due to\nthe obscured facial details. Nevertheless, it is often feasible to make\npredictions about specific soft biometric (SB) attributes, such as gender, and\nbaldness even in dealing with low-quality images. In this paper, we propose a\nnovel multi-branch neural network that leverages SB attribute information to\nboost the performance of FR. To this end, we propose a cross-attribute-guided\ntransformer fusion (CATF) module that effectively captures the long-range\ndependencies and relationships between FR and SB feature representations. The\nsynergy created by the reciprocal flow of information in the dual\ncross-attention operations of the proposed CATF module enhances the performance\nof FR. Furthermore, we introduce a novel self-attention distillation framework\nthat effectively highlights crucial facial regions, such as landmarks by\naligning low-quality images with those of their high-quality counterparts in\nthe feature space. The proposed self-attention distillation regularizes our\nnetwork to learn a unified quality-invariant feature representation in\nunconstrained environments. We conduct extensive experiments on various FR\nbenchmarks varying in quality. Experimental results demonstrate the superiority\nof our FR method compared to state-of-the-art FR studies.\n",
        "title": "CATFace: Cross-Attribute-Guided Transformer with Self-Attention\n  Distillation for Low-Quality Face Recognition",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03038",
        "abstract_url": "http://arxiv.org/abs/2401.03038",
        "authors": [
            {
                "last_name": "Shankar",
                "first_name": "Shreya"
            },
            {
                "last_name": "Li",
                "first_name": "Haotian"
            },
            {
                "last_name": "Asawa",
                "first_name": "Parth"
            },
            {
                "last_name": "Hulsebos",
                "first_name": "Madelon"
            },
            {
                "last_name": "Lin",
                "first_name": "Yiming"
            },
            {
                "last_name": "Zamfirescu-Pereira",
                "first_name": "J. D."
            },
            {
                "last_name": "Chase",
                "first_name": "Harrison"
            },
            {
                "last_name": "Fu-Hinthorn",
                "first_name": "Will"
            },
            {
                "last_name": "Parameswaran",
                "first_name": "Aditya G."
            },
            {
                "last_name": "Wu",
                "first_name": "Eugene"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "SE"
        ],
        "abstract": "  Operationalizing large language models (LLMs) for custom, repetitive data\npipelines is challenging, particularly due to their unpredictable and\npotentially catastrophic failures. Acknowledging the inevitability of these\nerrors, we focus on identifying when LLMs may be generating incorrect responses\nwhen used repeatedly as part of data generation pipelines. We present SPADE, a\nmethod for automatically synthesizing assertions that identify bad LLM outputs.\nSPADE analyzes prompt version histories to create candidate assertion functions\nand then selects a minimal set that fulfills both coverage and accuracy\nrequirements. In testing across nine different real-world LLM pipelines, SPADE\nefficiently reduces the number of assertions by 14% and decreases false\nfailures by 21% when compared to simpler baselines.\n",
        "title": "SPADE: Synthesizing Assertions for Large Language Model Pipelines",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03040",
        "abstract_url": "http://arxiv.org/abs/2401.03040",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Kebin"
            },
            {
                "last_name": "Li",
                "first_name": "Wenbin"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xiaofei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV",
            "DC"
        ],
        "abstract": "  Traffic accident analysis is pivotal for enhancing public safety and\ndeveloping road regulations. Traditional approaches, although widely used, are\noften constrained by manual analysis processes, subjective decisions, uni-modal\noutputs, as well as privacy issues related to sensitive data. This paper\nintroduces the idea of AccidentGPT, a foundation model of traffic accident\nanalysis, which incorporates multi-modal input data to automatically\nreconstruct the accident process video with dynamics details, and furthermore\nprovide multi-task analysis with multi-modal outputs. The design of the\nAccidentGPT is empowered with a multi-modality prompt with feedback for\ntask-oriented adaptability, a hybrid training schema to leverage labelled and\nunlabelled data, and a edge-cloud split configuration for data privacy. To\nfully realize the functionalities of this model, we proposes several research\nopportunities. This paper serves as the stepping stone to fill the gaps in\ntraditional approaches of traffic accident analysis and attract the research\ncommunity attention for automatic, objective, and privacy-preserving traffic\naccident analysis.\n",
        "title": "AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident\n  Analysis",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03041",
        "abstract_url": "http://arxiv.org/abs/2401.03041",
        "authors": [
            {
                "last_name": "Holzer",
                "first_name": "Markus"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Travis"
            },
            {
                "last_name": "Leonardi",
                "first_name": "Christopher R."
            },
            {
                "last_name": "Ruede",
                "first_name": "Ulrich"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This study develops a computationally efficient phase-field lattice Boltzmann\nmodel with the capability to simulate thermocapillary flows. The model was\nimplemented into the open-source simulation framework, waLBerla, and extended\nto conduct the collision stage using central moments. The multiphase model was\ncoupled with both a passive-scalar thermal LB, and a RK solution to the energy\nequation in order to resolve temperature-dependent surface tension phenomena.\nVarious lattice stencils (D3Q7, D3Q15, D3Q19, D3Q27) were tested for the\npassive-scalar LB and both the second- and fourth-order RK methods were\ninvestigated. There was no significant difference observed in the accuracy of\nthe LB or RK schemes. The passive scalar D3Q7 LB discretisation tended to\nprovide computational benefits, while the second order RK scheme is superior in\nmemory usage. This paper makes contributions relating to the modelling of\nthermocapillary flows and to understanding the behaviour of droplet capture\nwith thermal sources analogous to thermal tweezers. Four primary contributions\nto the literature are identified. First, a new 3D thermocapillary,\ncentral-moment phase-field LB model is presented and implemented in the\nopen-source software, waLBerla. Second, the accuracy and computational\nperformance of various techniques to resolve the energy equation for\nmultiphase, incompressible fluids is investigated. Third, the dynamic droplet\ntransport behaviour in the presence of thermal sources is studied and insight\nis provided on the potential ability to manipulate droplets based on local\ndomain heating. Finally, a concise analysis of the computational performance\ntogether with near-perfect scaling results on NVIDIA and AMD GPU-clusters is\nshown. This research enables the detailed study of droplet manipulation and\ncontrol in thermocapillary devices.\n",
        "title": "Development of a central-moment phase-field lattice Boltzmann model for\n  thermocapillary flows: Droplet capture and computational performance",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03043",
        "abstract_url": "http://arxiv.org/abs/2401.03043",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Qihua"
            },
            {
                "last_name": "Chen",
                "first_name": "Xuejin"
            },
            {
                "last_name": "Wang",
                "first_name": "Chenxuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Yixiong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Wu",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The current neuron reconstruction pipeline for electron microscopy (EM) data\nusually includes automatic image segmentation followed by extensive human\nexpert proofreading. In this work, we aim to reduce human workload by\npredicting connectivity between over-segmented neuron pieces, taking both\nmicroscopy image and 3D morphology features into account, similar to human\nproofreading workflow. To this end, we first construct a dataset, named\nFlyTracing, that contains millions of pairwise connections of segments\nexpanding the whole fly brain, which is three orders of magnitude larger than\nexisting datasets for neuron segment connection. To learn sophisticated\nbiological imaging features from the connectivity annotations, we propose a\nnovel connectivity-aware contrastive learning method to generate dense\nvolumetric EM image embedding. The learned embeddings can be easily\nincorporated with any point or voxel-based morphological representations for\nautomatic neuron tracing. Extensive comparisons of different combination\nschemes of image and morphological representation in identifying split errors\nacross the whole fly brain demonstrate the superiority of the proposed\napproach, especially for the locations that contain severe imaging artifacts,\nsuch as section missing and misalignment. The dataset and code are available at\nhttps://github.com/Levishery/Flywire-Neuron-Tracing.\n",
        "title": "Learning Multimodal Volumetric Features for Large-Scale Neuron Tracing",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03048",
        "abstract_url": "http://arxiv.org/abs/2401.03048",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Xin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaohui"
            },
            {
                "last_name": "Jia",
                "first_name": "Gengyun"
            },
            {
                "last_name": "Chen",
                "first_name": "Xinyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Li",
                "first_name": "Yuan-Fang"
            },
            {
                "last_name": "Chen",
                "first_name": "Cunjian"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose a novel Latent Diffusion Transformer, namely Latte, for video\ngeneration. Latte first extracts spatio-temporal tokens from input videos and\nthen adopts a series of Transformer blocks to model video distribution in the\nlatent space. In order to model a substantial number of tokens extracted from\nvideos, four efficient variants are introduced from the perspective of\ndecomposing the spatial and temporal dimensions of input videos. To improve the\nquality of generated videos, we determine the best practices of Latte through\nrigorous experimental analysis, including video clip patch embedding, model\nvariants, timestep-class information injection, temporal positional embedding,\nand learning strategies. Our comprehensive evaluation demonstrates that Latte\nachieves state-of-the-art performance across four standard video generation\ndatasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In\naddition, we extend Latte to text-to-video generation (T2V) task, where Latte\nachieves comparable results compared to recent T2V models. We strongly believe\nthat Latte provides valuable insights for future research on incorporating\nTransformers into diffusion models for video generation.\n",
        "title": "Latte: Latent Diffusion Transformer for Video Generation",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03051",
        "abstract_url": "http://arxiv.org/abs/2401.03051",
        "authors": [
            {
                "last_name": "Estienne",
                "first_name": "Lautaro"
            },
            {
                "last_name": "Hansen",
                "first_name": "Roberta"
            },
            {
                "last_name": "Vera",
                "first_name": "Matias"
            },
            {
                "last_name": "Ferrer",
                "first_name": "Luciana"
            },
            {
                "last_name": "Piantanida",
                "first_name": "Pablo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Calibration is an essential key in machine leaning. Semi Unsupervised\nCalibration through Prior Adaptation (SUCPA) is a calibration algorithm used in\n(but not limited to) large-scale language models defined by a {system of\nfirst-order difference equation. The map derived by this system} has the\npeculiarity of being non-hyperbolic {with a non-bounded set of non-isolated\nfixed points}. In this work, we prove several convergence properties of this\nalgorithm from the perspective of dynamical systems. For a binary\nclassification problem, it can be shown that the algorithm always converges,\n{more precisely, the map is globally asymptotically stable, and the orbits\nconverge} to a single line of fixed points. Finally, we perform numerical\nexperiments on real-world application to support the presented results.\nExperiment codes are available online.\n",
        "title": "On the Convergence of Semi Unsupervised Calibration through Prior\n  Adaptation Algorithm",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03058",
        "abstract_url": "http://arxiv.org/abs/2401.03058",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Ruichen"
            },
            {
                "last_name": "Raman",
                "first_name": "Parameswaran"
            },
            {
                "last_name": "Sabach",
                "first_name": "Shoham"
            },
            {
                "last_name": "Mokhtari",
                "first_name": "Aryan"
            },
            {
                "last_name": "Hong",
                "first_name": "Mingyi"
            },
            {
                "last_name": "Cevher",
                "first_name": "Volkan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Second-order optimization methods, such as cubic regularized Newton methods,\nare known for their rapid convergence rates; nevertheless, they become\nimpractical in high-dimensional problems due to their substantial memory\nrequirements and computational costs. One promising approach is to execute\nsecond-order updates within a lower-dimensional subspace, giving rise to\nsubspace second-order methods. However, the majority of existing subspace\nsecond-order methods randomly select subspaces, consequently resulting in\nslower convergence rates depending on the problem's dimension $d$. In this\npaper, we introduce a novel subspace cubic regularized Newton method that\nachieves a dimension-independent global convergence rate of\n${O}\\left(\\frac{1}{mk}+\\frac{1}{k^2}\\right)$ for solving convex optimization\nproblems. Here, $m$ represents the subspace dimension, which can be\nsignificantly smaller than $d$. Instead of adopting a random subspace, our\nprimary innovation involves performing the cubic regularized Newton update\nwithin the Krylov subspace associated with the Hessian and the gradient of the\nobjective function. This result marks the first instance of a\ndimension-independent convergence rate for a subspace second-order method.\nFurthermore, when specific spectral conditions of the Hessian are met, our\nmethod recovers the convergence rate of a full-dimensional cubic regularized\nNewton method. Numerical experiments show our method converges faster than\nexisting random subspace methods, especially for high-dimensional problems.\n",
        "title": "Krylov Cubic Regularized Newton: A Subspace Second-Order Method with\n  Dimension-Free Convergence Rate",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03059",
        "abstract_url": "http://arxiv.org/abs/2401.03059",
        "authors": [
            {
                "last_name": "Semiari",
                "first_name": "Omid"
            },
            {
                "last_name": "Nikopour",
                "first_name": "Hosein"
            },
            {
                "last_name": "Talwar",
                "first_name": "Shilpa"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "IT",
            "NI",
            ""
        ],
        "abstract": "  Ultra-reliable low-latency communication (URLLC) is the cornerstone for a\nbroad range of emerging services in next-generation wireless networks. URLLC\nfundamentally relies on the network's ability to proactively determine whether\nsufficient resources are available to support the URLLC traffic, and thus,\nprevent so-called cell overloads. Nonetheless, achieving accurate\nquality-of-service (QoS) predictions for URLLC user equipment (UEs) and\npreventing cell overloads are very challenging tasks. This is due to dependency\nof the QoS metrics (latency and reliability) on traffic and channel statistics,\nusers' mobility, and interdependent performance across UEs. In this paper, a\nnew QoS-aware UE admission control approach is developed to proactively\nestimate QoS for URLLC UEs, prior to associating them with a cell, and\naccordingly, admit only a subset of UEs that do not lead to a cell overload. To\nthis end, an optimization problem is formulated to find an efficient UE\nadmission control policy, cognizant of UEs' QoS requirements and cell-level\nload dynamics. To solve this problem, a new machine learning based method is\nproposed that builds on (deep) neural contextual bandits, a suitable framework\nfor dealing with nonlinear bandit problems. In fact, the UE admission\ncontroller is treated as a bandit agent that observes a set of network\nmeasurements (context) and makes admission control decisions based on\ncontext-dependent QoS (reward) predictions. The simulation results show that\nthe proposed scheme can achieve near-optimal performance and yield substantial\ngains in terms of cell-level service reliability and efficient resource\nutilization.\n",
        "title": "Reliability-Optimized User Admission Control for URLLC Traffic: A Neural\n  Contextual Bandit Approach",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03060",
        "abstract_url": "http://arxiv.org/abs/2401.03060",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Ho Hin"
            },
            {
                "last_name": "Saunders",
                "first_name": "Adam M."
            },
            {
                "last_name": "Kim",
                "first_name": "Michael E."
            },
            {
                "last_name": "Remedios",
                "first_name": "Samuel W."
            },
            {
                "last_name": "Tang",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Qi"
            },
            {
                "last_name": "Yu",
                "first_name": "Xin"
            },
            {
                "last_name": "Bao",
                "first_name": "Shunxing"
            },
            {
                "last_name": "Cho",
                "first_name": "Chloe"
            },
            {
                "last_name": "Mawn",
                "first_name": "Louise A."
            },
            {
                "last_name": "Rex",
                "first_name": "Tonia S."
            },
            {
                "last_name": "Schey",
                "first_name": "Kevin L."
            },
            {
                "last_name": "Dewey",
                "first_name": "Blake E."
            },
            {
                "last_name": "Spraggins",
                "first_name": "Jeffrey M."
            },
            {
                "last_name": "Prince",
                "first_name": "Jerry L."
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            },
            {
                "last_name": "Landman",
                "first_name": "Bennett A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Eye morphology varies significantly across the population, especially for the\norbit and optic nerve. These variations limit the feasibility and robustness of\ngeneralizing population-wise features of eye organs to an unbiased spatial\nreference. To tackle these limitations, we propose a process for creating\nhigh-resolution unbiased eye atlases. First, to restore spatial details from\nscans with a low through-plane resolution compared to a high in-plane\nresolution, we apply a deep learning-based super-resolution algorithm. Then, we\ngenerate an initial unbiased reference with an iterative metric-based\nregistration using a small portion of subject scans. We register the remaining\nscans to this template and refine the template using an unsupervised deep\nprobabilistic approach that generates a more expansive deformation field to\nenhance the organ boundary alignment. We demonstrate this framework using\nmagnetic resonance images across four different MRI tissue contrasts,\ngenerating four atlases in separate spatial alignments. For each tissue\ncontrast, we find a significant improvement in the average Dice score across\nfour labeled regions compared to a standard registration framework consisting\nof rigid, affine, and deformable transformations. These results highlight the\neffective alignment of eye organs and boundaries using our proposed process. By\ncombining super-resolution preprocessing and deep probabilistic models, we\naddress the challenge of generating an eye atlas to serve as a standardized\nreference across a largely variable population.\n",
        "title": "Super-Resolution Multi-Contrast Unbiased Eye Atlases With Deep\n  Probabilistic Refinement",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03062",
        "abstract_url": "http://arxiv.org/abs/2401.03062",
        "authors": [
            {
                "last_name": "Rech",
                "first_name": "Alberto"
            },
            {
                "last_name": "Badia",
                "first_name": "Leonardo"
            },
            {
                "last_name": "Tomasin",
                "first_name": "Stefano"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI",
            ""
        ],
        "abstract": "  The technical limitations of the intelligent reflecting surface (IRS)\n(re)configurations in terms of both communication overhead and energy\nefficiency must be considered when IRSs are used in cellular networks. In this\npaper, we investigate the downlink time-frequency scheduling of an IRS-assisted\nmulti-user system in the orthogonal frequency-division multiple access (OFDMA)\nframework wherein both the set of possible IRS configurations and the number of\nIRS reconfigurations within a time frame are limited. We formulate the sum rate\nmaximization problem as a non-polynomial (NP)-complete generalized\nmulti-knapsack problem. A heuristic greedy algorithm for the joint IRS\nconfiguration and time-frequency scheduling is also proposed. Numerical\nsimulations prove the effectiveness of our greedy solution.\n",
        "title": "Scheduling for Downlink OFDMA With IRS Reconfiguration Constraints",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03065",
        "abstract_url": "http://arxiv.org/abs/2401.03065",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Alex"
            },
            {
                "last_name": "Rozi\u00e8re",
                "first_name": "Baptiste"
            },
            {
                "last_name": "Leather",
                "first_name": "Hugh"
            },
            {
                "last_name": "Solar-Lezama",
                "first_name": "Armando"
            },
            {
                "last_name": "Synnaeve",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Wang",
                "first_name": "Sida I."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "LG"
        ],
        "abstract": "  We present CRUXEval (Code Reasoning, Understanding, and eXecution\nEvaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each\nfunction comes with an input-output pair, leading to two natural tasks: input\nprediction and output prediction. First, we propose a generic recipe for\ngenerating our execution benchmark which can be used to create future variation\nof the benchmark. Second, we evaluate twenty code models on our benchmark and\ndiscover that many recent high-scoring models on HumanEval do not show the same\nimprovements on our benchmark. Third, we show that simple CoT and fine-tuning\nschemes can improve performance on our benchmark but remain far from solving\nit. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75%\nand 81% on input and output prediction, respectively. In contrast, Code Llama\n34B achieves a pass@1 of 50% and 46% on input and output prediction,\nhighlighting the gap between open and closed source models. As no model is\nclose to acing CRUXEval, we provide examples of consistent GPT-4 failures on\nsimple programs as a lens into its code reasoning capabilities and areas for\nimprovement.\n",
        "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03069",
        "abstract_url": "http://arxiv.org/abs/2401.03069",
        "authors": [
            {
                "last_name": "Shah",
                "first_name": "Mehil B."
            },
            {
                "last_name": "Rahman",
                "first_name": "Mohammad Masudur"
            },
            {
                "last_name": "Khomh",
                "first_name": "Foutse"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Context: Deep learning has achieved remarkable progress in various domains.\nHowever, like traditional software systems, deep learning systems contain bugs,\nwhich can have severe impacts, as evidenced by crashes involving autonomous\nvehicles. Despite substantial advancements in deep learning techniques, little\nresearch has focused on reproducing deep learning bugs, which hinders resolving\nthem. Existing literature suggests that only 3% of deep learning bugs are\nreproducible, underscoring the need for further research.\n  Objective: This paper examines the reproducibility of deep learning bugs. We\nidentify edit actions and useful information that could improve deep learning\nbug reproducibility.\n  Method: First, we construct a dataset of 668 deep learning bugs from Stack\nOverflow and Defects4ML across 3 frameworks and 22 architectures. Second, we\nselect 102 bugs using stratified sampling and try to determine their\nreproducibility. While reproducing these bugs, we identify edit actions and\nuseful information necessary for their reproduction. Third, we used the Apriori\nalgorithm to identify useful information and edit actions required to reproduce\nspecific bug types. Finally, we conduct a user study with 22 developers to\nassess the effectiveness of our findings in real-life settings.\n  Results: We successfully reproduced 85 bugs and identified ten edit actions\nand five useful information categories that can help us reproduce deep learning\nbugs. Our findings improved bug reproducibility by 22.92% and reduced\nreproduction time by 24.35% based on our user study.\n  Conclusions: Our research addresses the critical issue of deep learning bug\nreproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.\n",
        "title": "Towards Enhancing the Reproducibility of Deep Learning Bugs: An\n  Empirical Study",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03070",
        "abstract_url": "http://arxiv.org/abs/2401.03070",
        "authors": [
            {
                "last_name": "Agorku",
                "first_name": "Geoffery"
            },
            {
                "last_name": "PhD",
                "first_name": "Sarah Hernandez"
            },
            {
                "last_name": "Falquez",
                "first_name": "Maria"
            },
            {
                "last_name": "PhD",
                "first_name": "Subhadipto Poddar"
            },
            {
                "last_name": "Amankwah-Nkyi",
                "first_name": "Kwadwo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Inland waterways are critical for freight movement, but limited means exist\nfor monitoring their performance and usage by freight-carrying vessels, e.g.,\nbarges. While methods to track vessels, e.g., tug and tow boats, are publicly\navailable through Automatic Identification Systems (AIS), ways to track freight\ntonnages and commodity flows carried on barges along these critical marine\nhighways are non-existent, especially in real-time settings. This paper\ndevelops a method to detect barge traffic on inland waterways using existing\ntraffic cameras with opportune viewing angles. Deep learning models,\nspecifically, You Only Look Once (YOLO), Single Shot MultiBox Detector (SSD),\nand EfficientDet are employed. The model detects the presence of vessels and/or\nbarges from video and performs a classification (no vessel or barge, vessel\nwithout barge, vessel with barge, and barge). A dataset of 331 annotated images\nwas collected from five existing traffic cameras along the Mississippi and Ohio\nRivers for model development. YOLOv8 achieves an F1-score of 96%, outperforming\nYOLOv5, SSD, and EfficientDet models with 86%, 79%, and 77% respectively.\nSensitivity analysis was carried out regarding weather conditions (fog and\nrain) and location (Mississippi and Ohio rivers). A background subtraction\ntechnique was used to normalize video images across the various locations for\nthe location sensitivity analysis. This model can be used to detect the\npresence of barges along river segments, which can be used for anonymous bulk\ncommodity tracking and monitoring. Such data is valuable for long-range\ntransportation planning efforts carried out by public transportation agencies,\nin addition to operational and maintenance planning conducted by federal\nagencies such as the US Army Corp of Engineers.\n",
        "title": "Traffic Cameras to detect inland waterway barge traffic: An Application\n  of machine learning",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03071",
        "abstract_url": "http://arxiv.org/abs/2401.03071",
        "authors": [
            {
                "last_name": "Herron",
                "first_name": "Connor W."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  The purpose of this work is to provide some notes on a software\nimplementation for digital filtering via Tustins Bilinear Transform. The first\nsection discusses how to solve for the input and output coefficients by hand\nusing a generalized approach called Horners method. The second section presents\nsome results of this generalized digital filtering approach using the IHMC Open\nRobotics Software stack and Simulation Construction Set 2. This generalized\napproach can solve for the digital coefficients for any causal transfer\nfunction.\n",
        "title": "Software Implementation of Digital Filtering via Tustin's Bilinear\n  Transform",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03073",
        "abstract_url": "http://arxiv.org/abs/2401.03073",
        "authors": [
            {
                "last_name": "Atchekzai",
                "first_name": "Roxane"
            },
            {
                "last_name": "Claeys",
                "first_name": "Xavier"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The generalized optimised Schwarz method proposed in [Claeys & Parolin, 2022]\nis a variant of the Despr\\'es algorithm for solving harmonic wave problems\nwhere transmission conditions are enforced by means of a non-local exchange\noperator. We introduce and analyse an acceleration technique that significantly\nreduces the cost of applying this exchange operator without deteriorating the\nprecision and convergence speed of the overall domain decomposition algorithm.\n",
        "title": "Accelerating non-local exchange in generalized optimized Schwarz methods",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03074",
        "abstract_url": "http://arxiv.org/abs/2401.03074",
        "authors": [
            {
                "last_name": "Sanz-Alonso",
                "first_name": "Daniel"
            },
            {
                "last_name": "Waniorek",
                "first_name": "Nathan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  This paper analyzes hierarchical Bayesian inverse problems using techniques\nfrom high-dimensional statistics. Our analysis leverages a property of\nhierarchical Bayesian regularizers that we call approximate decomposability to\nobtain non-asymptotic bounds on the reconstruction error attained by maximum a\nposteriori estimators. The new theory explains how hierarchical Bayesian models\nthat exploit sparsity, group sparsity, and sparse representations of the\nunknown parameter can achieve accurate reconstructions in high-dimensional\nsettings.\n",
        "title": "Hierarchical Bayesian Inverse Problems: A High-Dimensional Statistics\n  Viewpoint",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03077",
        "abstract_url": "http://arxiv.org/abs/2401.03077",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Xiaoxue"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Ning",
                "first_name": "Yue"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Continual learning on graphs tackles the problem of training a graph neural\nnetwork (GNN) where graph data arrive in a streaming fashion and the model\ntends to forget knowledge from previous tasks when updating with new data.\nTraditional continual learning strategies such as Experience Replay can be\nadapted to streaming graphs, however, these methods often face challenges such\nas inefficiency in preserving graph topology and incapability of capturing the\ncorrelation between old and new tasks. To address these challenges, we propose\nTA$\\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual\nlearning framework that stores information from previous tasks as a reduced\ngraph. At each time period, this reduced graph expands by combining with a new\ngraph and aligning shared nodes, and then it undergoes a \"zoom out\" process by\nreduction to maintain a stable size. We design a graph coarsening algorithm\nbased on node representation proximities to efficiently reduce a graph and\npreserve topological information. We empirically demonstrate the learning\nprocess on the reduced graph can approximate that of the original graph. Our\nexperiments validate the effectiveness of the proposed framework on three\nreal-world datasets using different backbone GNN models.\n",
        "title": "A Topology-aware Graph Coarsening Framework for Continual Graph Learning",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03078",
        "abstract_url": "http://arxiv.org/abs/2401.03078",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Yang"
            },
            {
                "last_name": "Kartynnik",
                "first_name": "Yury"
            },
            {
                "last_name": "Li",
                "first_name": "Yunpeng"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiuqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Xing"
            },
            {
                "last_name": "Sung",
                "first_name": "George"
            },
            {
                "last_name": "Grundmann",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "SD"
        ],
        "abstract": "  We present StreamVC, a streaming voice conversion solution that preserves the\ncontent and prosody of any source speech while matching the voice timbre from\nany target speech. Unlike previous approaches, StreamVC produces the resulting\nwaveform at low latency from the input signal even on a mobile platform, making\nit applicable to real-time communication scenarios like calls and video\nconferencing, and addressing use cases such as voice anonymization in these\nscenarios. Our design leverages the architecture and training strategy of the\nSoundStream neural audio codec for lightweight high-quality speech synthesis.\nWe demonstrate the feasibility of learning soft speech units causally, as well\nas the effectiveness of supplying whitened fundamental frequency information to\nimprove pitch stability without leaking the source timbre information.\n",
        "title": "StreamVC: Real-Time Low-Latency Voice Conversion",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03079",
        "abstract_url": "http://arxiv.org/abs/2401.03079",
        "authors": [
            {
                "last_name": "Naughton",
                "first_name": "Patrick"
            },
            {
                "last_name": "Nam",
                "first_name": "James Seungbum"
            },
            {
                "last_name": "Stratton",
                "first_name": "Andrew"
            },
            {
                "last_name": "Hauser",
                "first_name": "Kris"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Teleoperated avatar robots allow people to transport their manipulation\nskills to environments that may be difficult or dangerous to work in. Current\nsystems are able to give operators direct control of many components of the\nrobot to immerse them in the remote environment, but operators still struggle\nto complete tasks as competently as they could in person. We present a\nframework for incorporating open-world shared control into avatar robots to\ncombine the benefits of direct and shared control. This framework preserves the\nfluency of our avatar interface by minimizing obstructions to the operator's\nview and using the same interface for direct, shared, and fully autonomous\ncontrol. In a human subjects study (N=19), we find that operators using this\nframework complete a range of tasks significantly more quickly and reliably\nthan those that do not.\n",
        "title": "Integrating Open-World Shared Control in Immersive Avatars",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03082",
        "abstract_url": "http://arxiv.org/abs/2401.03082",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Lin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kai"
            },
            {
                "last_name": "Li",
                "first_name": "Qingyuan"
            },
            {
                "last_name": "Lou",
                "first_name": "Renze"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Multimodal information extraction (MIE) gains significant attention as the\npopularity of multimedia content increases. However, current MIE methods often\nresort to using task-specific model structures, which results in limited\ngeneralizability across tasks and underutilizes shared knowledge across MIE\ntasks. To address these issues, we propose UMIE, a unified multimodal\ninformation extractor to unify three MIE tasks as a generation problem using\ninstruction tuning, being able to effectively extract both textual and visual\nmentions. Extensive experiments show that our single UMIE outperforms various\nstate-of-the-art (SoTA) methods across six MIE datasets on three tasks.\nFurthermore, in-depth analysis demonstrates UMIE's strong generalization in the\nzero-shot setting, robustness to instruction variants, and interpretability.\nOur research serves as an initial step towards a unified MIE model and\ninitiates the exploration into both instruction tuning and large language\nmodels within the MIE domain. Our code, data, and model are available at\nhttps://github.com/ZUCC-AI/UMIE\n",
        "title": "UMIE: Unified Multimodal Information Extraction with Instruction Tuning",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03083",
        "abstract_url": "http://arxiv.org/abs/2401.03083",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xusheng"
            },
            {
                "last_name": "Chiu",
                "first_name": "Cho-Chun"
            },
            {
                "last_name": "He",
                "first_name": "Ting"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC",
            ""
        ],
        "abstract": "  This work aims at improving the energy efficiency of decentralized learning\nby optimizing the mixing matrix, which controls the communication demands\nduring the learning process. Through rigorous analysis based on a\nstate-of-the-art decentralized learning algorithm, the problem is formulated as\na bi-level optimization, with the lower level solved by graph sparsification. A\nsolution with guaranteed performance is proposed for the special case of\nfully-connected base topology and a greedy heuristic is proposed for the\ngeneral case. Simulations based on real topology and dataset show that the\nproposed solution can lower the energy consumption at the busiest node by\n54%-76% while maintaining the quality of the trained model.\n",
        "title": "Energy-efficient Decentralized Learning via Graph Sparsification",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03085",
        "abstract_url": "http://arxiv.org/abs/2401.03085",
        "authors": [
            {
                "last_name": "Brimoh",
                "first_name": "Paul"
            },
            {
                "last_name": "Olisah",
                "first_name": "Chollette C."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  A genuine signer's signature is naturally unstable even at short\ntime-intervals whereas, expert forgers always try to perfectly mimic a genuine\nsigner's signature. This presents a challenge which puts a genuine signer at\nrisk of being denied access, while a forge signer is granted access. The\nimplication is a high false acceptance rate (FAR) which is the percentage of\nforge signature classified as belonging to a genuine class. Existing work have\nonly scratched the surface of signature verification because the\nmisclassification error remains high. In this paper, a consensus-threshold\ndistance-based classifier criterion is proposed for offline writer-dependent\nsignature verification. Using features extracted from SigNet and SigNet-F deep\nconvolutional neural network models, the proposed classifier minimizes FAR.\nThis is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR\nand Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier\nimproves the state-of-the-art performance by achieving a 1.27% FAR compared to\n8.73% and 17.31% recorded in literature. This performance is consistent across\nother datasets and guarantees that the risk of imposters gaining access to\nsensitive documents or transactions is minimal.\n",
        "title": "Consensus-Threshold Criterion for Offline Signature Verification using\n  Convolutional Neural Network Learned Representations",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03088",
        "abstract_url": "http://arxiv.org/abs/2401.03088",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Delgado",
                "first_name": "David"
            },
            {
                "last_name": "Zeng",
                "first_name": "Daniel"
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Robots that cooperate with humans must be effective at communicating with\nthem. However, people have varied preferences for communication based on many\ncontextual factors, such as culture, environment, and past experience. To\ncommunicate effectively, robots must take those factors into consideration. In\nthis work, we present the Robot Signal Design (RoSiD) tool to empower people to\neasily self-specify communicative preferences for collaborative robots. We show\nthrough a participatory design study that the RoSiD tool enables users to\ncreate signals that align with their communicative preferences, and we\nilluminate how this tool can be further improved.\n",
        "title": "The RoSiD Tool: Empowering Users to Design Multimodal Signals for\n  Human-Robot Collaboration",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03089",
        "abstract_url": "http://arxiv.org/abs/2401.03089",
        "authors": [
            {
                "last_name": "Dzanic",
                "first_name": "Tarik"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  For finite element approximations of transport phenomena, it is often\nnecessary to apply a form of limiting to ensure that the discrete solution\nremains well-behaved and satisfies physical constraints. However, these\nlimiting procedures are typically performed at discrete nodal locations, which\nis not sufficient to ensure the robustness of the scheme when the solution must\nbe evaluated at arbitrary locations (e.g., for adaptive mesh refinement,\nremapping in arbitrary Lagragian--Eulerian solvers, overset meshes, etc.). In\nthis work, a novel limiting approach for discontinuous Galerkin methods is\npresented which ensures that the solution is continuously bounds-preserving\n(i.e., across the entire solution polynomial) for any arbitrary choice of\nbasis, approximation order, and mesh element type. Through a modified\nformulation for the constraint functionals, the proposed approach requires only\nthe solution of a single spatial scalar minimization problem per element for\nwhich a highly efficient numerical optimization procedure is presented. The\nefficacy of this approach is shown in numerical experiments by enforcing\ncontinuous constraints in high-order unstructured discontinuous Galerkin\ndiscretizations of hyperbolic conservation laws, ranging from scalar transport\nwith maximum principle preserving constraints to compressible gas dynamics with\npositivity-preserving constraints.\n",
        "title": "Continuously bounds-preserving discontinuous Galerkin methods for\n  hyperbolic conservation laws",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03092",
        "abstract_url": "http://arxiv.org/abs/2401.03092",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Zezheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Chunmei"
            },
            {
                "last_name": "Yang",
                "first_name": "Haizhao"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC",
            "",
            ""
        ],
        "abstract": "  Complex network data pervades various real-world domains, including physical,\ntechnological, and biological systems. Despite the prevalence of such data,\npredicting trends and understanding behavioral patterns in complex systems\nremains challenging due to poorly understood underlying mechanisms. While\ndata-driven methods have made strides in uncovering governing equations from\ntime series data, efforts to extract physical laws from network data are\nlimited and often struggle with incomplete or noisy data. To address these\nchallenges, we introduce a novel approach called the Finite Expression Method\n(FEX) and its fast algorithm for this learning problem on complex networks. FEX\nrepresents dynamics on complex networks using binary trees composed of finite\nmathematical operators. The nodes within these trees are trained through a\ncombinatorial optimization process guided by reinforcement learning techniques.\nThis unique configuration allows FEX to capture complex dynamics with minimal\nprior knowledge of the system and a small dictionary of mathematical operators.\nOur extensive numerical experiments demonstrate that FEX excels in accurately\nidentifying dynamics across diverse network topologies and dynamic behaviors.\n",
        "title": "Finite Expression Method for Learning Dynamics on Complex Networks",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03093",
        "abstract_url": "http://arxiv.org/abs/2401.03093",
        "authors": [
            {
                "last_name": "Kalmykov",
                "first_name": "V. L."
            },
            {
                "last_name": "Kalmykov",
                "first_name": "L. V."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Artificial intelligence based on neural networks has made significant\nprogress. However, there are concerns about the reliability and security of\nthis approach due to its lack of transparency. This is the black box problem of\nAI. Here we show how this problem can be solved using symbolic AI, which has a\ntransparent white box nature. The widespread use of symbolic AI is hindered by\nthe opacity of mathematical models and natural language terms, the lack of a\nunified ontology, and the combinatorial explosion of search options. To solve\nthe AI black box problem and to implement general-purpose symbolic AI, we\npropose to use deterministic logic cellular automata with rules based on first\nprinciples of the general theory of the relevant domain. In this case, the\ngeneral theory of the relevant domain plays the role of a knowledge base for\nthe cellular automaton inference. A cellular automaton implements automatic\nparallel logical inference at three levels of organization of a complex system.\nOur verification of several ecological hypotheses provides a successful\nprecedent for the implementation of white-box AI. Finally, we discuss a program\nfor creating a general-purpose symbolic AI capable of processing knowledge and\nensuring the reliability and safety of automated decisions.\n",
        "title": "A white box solution to the black box problem of AI",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03097",
        "abstract_url": "http://arxiv.org/abs/2401.03097",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Xiaobin"
            },
            {
                "last_name": "Liu",
                "first_name": "Zeyuan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Benben"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY",
            ""
        ],
        "abstract": "  Machine learning methods based on AdaBoost have been widely applied to\nvarious classification problems across many mission-critical applications\nincluding healthcare, law and finance. However, there is a growing concern\nabout the unfairness and discrimination of data-driven classification models,\nwhich is inevitable for classical algorithms including AdaBoost. In order to\nachieve fair classification, a novel fair AdaBoost (FAB) approach is proposed\nthat is an interpretable fairness-improving variant of AdaBoost. We mainly\ninvestigate binary classification problems and focus on the fairness of three\ndifferent indicators (i.e., accuracy, false positive rate and false negative\nrate). By utilizing a fairness-aware reweighting technique for base\nclassifiers, the proposed FAB approach can achieve fair classification while\nmaintaining the advantage of AdaBoost with negligible sacrifice of predictive\nperformance. In addition, a hyperparameter is introduced in FAB to show\npreferences for the fairness-accuracy trade-off. An upper bound for the target\nloss function that quantifies error rate and unfairness is theoretically\nderived for FAB, which provides a strict theoretical support for the\nfairness-improving methods designed for AdaBoost. The effectiveness of the\nproposed method is demonstrated on three real-world datasets (i.e., Adult,\nCOMPAS and HSLS) with respect to the three fairness indicators. The results are\naccordant with theoretic analyses, and show that (i) FAB significantly improves\nclassification fairness at a small cost of accuracy compared with AdaBoost; and\n(ii) FAB outperforms state-of-the-art fair classification methods including\nequalized odds method, exponentiated gradient method, and disparate\nmistreatment method in terms of the fairness-accuracy trade-off.\n",
        "title": "Adaptive Boosting with Fairness-aware Reweighting Technique for Fair\n  Classification",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03103",
        "abstract_url": "http://arxiv.org/abs/2401.03103",
        "authors": [
            {
                "last_name": "Adhikari",
                "first_name": "K."
            },
            {
                "last_name": "Patrick",
                "first_name": "J. F."
            },
            {
                "last_name": "Nakshatrala",
                "first_name": "K. B."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Fiber-reinforced composites (FRC) provide structural systems with unique\nfeatures that appeal to various civilian and military sectors. Often, one needs\nto modulate the temperature field to achieve the intended functionalities\n(e.g., self-healing) in these lightweight structures. Vascular-based active\ncooling offers one efficient way of thermal regulation in such material\nsystems. However, the thermophysical properties (e.g., thermal conductivity,\nspecific heat capacity) of FRC and their base constituents depend on\ntemperature, and such structures are often subject to a broad spectrum of\ntemperatures. Notably, prior active cooling modeling studies did not account\nfor such temperature dependence. Thus, the primary aim of this paper is to\nreveal the effect of temperature-dependent material properties -- obtained via\nmaterial characterization -- on the qualitative and quantitative behaviors of\nactive cooling. By applying mathematical analysis and conducting numerical\nsimulations, we show this dependence does not affect qualitative attributes,\nsuch as minimum and maximum principles (in the same spirit as \\textsc{Hopf}'s\nresults for elliptic partial differential equations). However, the dependence\nslightly affects quantitative results, such as the mean surface temperature and\nthermal efficiency. The import of our study is that it provides a deeper\nunderstanding of thermal regulation systems under practical scenarios and can\nguide researchers and practitioners in perfecting associated designs.\n",
        "title": "Effect of temperature-dependent material properties on thermal\n  regulation in microvascular composites",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03104",
        "abstract_url": "http://arxiv.org/abs/2401.03104",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Haihang"
            },
            {
                "last_name": "Wang",
                "first_name": "Wei"
            },
            {
                "last_name": "Malepathirana",
                "first_name": "Tamasha"
            },
            {
                "last_name": "Senanayake",
                "first_name": "Damith"
            },
            {
                "last_name": "Oetomo",
                "first_name": "Denny"
            },
            {
                "last_name": "Halgamuge",
                "first_name": "Saman"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural growth is the process of growing a small neural network to a large\nnetwork and has been utilized to accelerate the training of deep neural\nnetworks. One crucial aspect of neural growth is determining the optimal growth\ntiming. However, few studies investigate this systematically. Our study reveals\nthat neural growth inherently exhibits a regularization effect, whose intensity\nis influenced by the chosen policy for growth timing. While this regularization\neffect may mitigate the overfitting risk of the model, it may lead to a notable\naccuracy drop when the model underfits. Yet, current approaches have not\naddressed this issue due to their lack of consideration of the regularization\neffect from neural growth. Motivated by these findings, we propose an\nunder/over fitting risk-aware growth timing policy, which automatically adjusts\nthe growth timing informed by the level of potential under/overfitting risks to\naddress both risks. Comprehensive experiments conducted using CIFAR-10/100 and\nImageNet datasets show that the proposed policy achieves accuracy improvements\nof up to 1.3% in models prone to underfitting while achieving similar\naccuracies in models suffering from overfitting compared to the existing\nmethods.\n",
        "title": "When To Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep\n  Neural Networks",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03105",
        "abstract_url": "http://arxiv.org/abs/2401.03105",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Xin"
            },
            {
                "last_name": "Wei",
                "first_name": "Longhui"
            },
            {
                "last_name": "Xie",
                "first_name": "Lingxi"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  Multimodal Large Language Models (MLLMs) are experiencing rapid growth,\nyielding a plethora of noteworthy contributions in recent months. The\nprevailing trend involves adopting data-driven methodologies, wherein diverse\ninstruction-following datasets are collected. However, a prevailing challenge\npersists in these approaches, specifically in relation to the limited visual\nperception ability, as CLIP-like encoders employed for extracting visual\ninformation from inputs. Though these encoders are pre-trained on billions of\nimage-text pairs, they still grapple with the information loss dilemma, given\nthat textual captions only partially capture the contents depicted in images.\nTo address this limitation, this paper proposes to improve the visual\nperception ability of MLLMs through a mixture-of-experts knowledge enhancement\nmechanism. Specifically, we introduce a novel method that incorporates\nmulti-task encoders and visual tools into the existing MLLMs training and\ninference pipeline, aiming to provide a more comprehensive and accurate\nsummarization of visual inputs. Extensive experiments have evaluated its\neffectiveness of advancing MLLMs, showcasing improved visual perception\nachieved through the integration of visual experts.\n",
        "title": "Incorporating Visual Experts to Resolve the Information Loss in\n  Multimodal Large Language Models",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03108",
        "abstract_url": "http://arxiv.org/abs/2401.03108",
        "authors": [
            {
                "last_name": "Naik",
                "first_name": "Shanthika"
            },
            {
                "last_name": "Singh",
                "first_name": "Kunwar"
            },
            {
                "last_name": "Srivastava",
                "first_name": "Astitva"
            },
            {
                "last_name": "Sirikonda",
                "first_name": "Dhawal"
            },
            {
                "last_name": "Raj",
                "first_name": "Amit"
            },
            {
                "last_name": "Jampani",
                "first_name": "Varun"
            },
            {
                "last_name": "Sharma",
                "first_name": "Avinash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose a novel self-supervised framework for retargeting\nnon-parameterized 3D garments onto 3D human avatars of arbitrary shapes and\nposes, enabling 3D virtual try-on (VTON). Existing self-supervised 3D\nretargeting methods only support parametric and canonical garments, which can\nonly be draped over parametric body, e.g. SMPL. To facilitate the\nnon-parametric garments and body, we propose a novel method that introduces\nIsomap Embedding based correspondences matching between the garment and the\nhuman body to get a coarse alignment between the two meshes. We perform neural\nrefinement of the coarse alignment in a self-supervised setting. Further, we\nleverage a Laplacian detail integration method for preserving the inherent\ndetails of the input garment. For evaluating our 3D non-parametric garment\nretargeting framework, we propose a dataset of 255 real-world garments with\nrealistic noise and topological deformations. The dataset contains $44$ unique\ngarments worn by 15 different subjects in 5 distinctive poses, captured using a\nmulti-view RGBD capture setup. We show superior retargeting quality on\nnon-parametric garments and human avatars over existing state-of-the-art\nmethods, acting as the first-ever baseline on the proposed dataset for\nnon-parametric 3D garment retargeting.\n",
        "title": "Dress-Me-Up: A Dataset & Method for Self-Supervised 3D Garment\n  Retargeting",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03114",
        "abstract_url": "http://arxiv.org/abs/2401.03114",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Zhongshu"
            },
            {
                "last_name": "Jing",
                "first_name": "Bin"
            },
            {
                "last_name": "Wan",
                "first_name": "Xiaopei"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhizhen"
            },
            {
                "last_name": "Liang",
                "first_name": "Lei"
            },
            {
                "last_name": "zhou",
                "first_name": "Jun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  As a powerful tool for modeling graph data, Graph Neural Networks (GNNs) have\nreceived increasing attention in both academia and industry. Nevertheless, it\nis notoriously difficult to deploy GNNs on industrial scale graphs, due to\ntheir huge data size and complex topological structures. In this paper, we\npropose GLISP, a sampling based GNN learning system for industrial scale\ngraphs. By exploiting the inherent structural properties of graphs, such as\npower law distribution and data locality, GLISP addresses the scalability and\nperformance issues that arise at different stages of the graph learning\nprocess. GLISP consists of three core components: graph partitioner, graph\nsampling service and graph inference engine. The graph partitioner adopts the\nproposed vertex-cut graph partitioning algorithm AdaDNE to produce balanced\npartitioning for power law graphs, which is essential for sampling based GNN\nsystems. The graph sampling service employs a load balancing design that allows\nthe one hop sampling request of high degree vertices to be handled by multiple\nservers. In conjunction with the memory efficient data structure, the\nefficiency and scalability are effectively improved. The graph inference engine\nsplits the $K$-layer GNN into $K$ slices and caches the vertex embeddings\nproduced by each slice in the data locality aware hybrid caching system for\nreuse, thus completely eliminating redundant computation caused by the data\ndependency of graph. Extensive experiments show that GLISP achieves up to\n$6.53\\times$ and $70.77\\times$ speedups over existing GNN systems for training\nand inference tasks, respectively, and can scale to the graph with over 10\nbillion vertices and 40 billion edges with limited resources.\n",
        "title": "GLISP: A Scalable GNN Learning System by Exploiting Inherent Structural\n  Properties of Graphs",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03115",
        "abstract_url": "http://arxiv.org/abs/2401.03115",
        "authors": [
            {
                "last_name": "Sui",
                "first_name": "Yang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhuohang"
            },
            {
                "last_name": "Ding",
                "first_name": "Ding"
            },
            {
                "last_name": "Pan",
                "first_name": "Xiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaozhong"
            },
            {
                "last_name": "Liu",
                "first_name": "Shan"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhenzhong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM",
            ""
        ],
        "abstract": "  Adversarial attacks can readily disrupt the image classification system,\nrevealing the vulnerability of DNN-based recognition tasks. While existing\nadversarial perturbations are primarily applied to uncompressed images or\ncompressed images by the traditional image compression method, i.e., JPEG,\nlimited studies have investigated the robustness of models for image\nclassification in the context of DNN-based image compression. With the rapid\nevolution of advanced image compression, DNN-based learned image compression\nhas emerged as the promising approach for transmitting images in many\nsecurity-critical applications, such as cloud-based face recognition and\nautonomous driving, due to its superior performance over traditional\ncompression. Therefore, there is a pressing need to fully investigate the\nrobustness of a classification system post-processed by learned image\ncompression. To bridge this research gap, we explore the adversarial attack on\na new pipeline that targets image classification models that utilize learned\nimage compressors as pre-processing modules. Furthermore, to enhance the\ntransferability of perturbations across various quality levels and\narchitectures of learned image compression models, we introduce a saliency\nscore-based sampling method to enable the fast generation of transferable\nperturbation. Extensive experiments with popular attack methods demonstrate the\nenhanced transferability of our proposed method when attacking images that have\nbeen post-processed with different learned image compression models.\n",
        "title": "Transferable Learned Image Compression-Resistant Adversarial\n  Perturbations",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03116",
        "abstract_url": "http://arxiv.org/abs/2401.03116",
        "authors": [
            {
                "last_name": "Alfatemi",
                "first_name": "Ali"
            },
            {
                "last_name": "Rahouti",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Amin",
                "first_name": "Ruhul"
            },
            {
                "last_name": "ALJamal",
                "first_name": "Sarah"
            },
            {
                "last_name": "Xiong",
                "first_name": "Kaiqi"
            },
            {
                "last_name": "Xin",
                "first_name": "Yufeng"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Distributed Denial of Service (DDoS) attacks pose a significant threat to the\nstability and reliability of online systems. Effective and early detection of\nsuch attacks is pivotal for safeguarding the integrity of networks. In this\nwork, we introduce an enhanced approach for DDoS attack detection by leveraging\nthe capabilities of Deep Residual Neural Networks (ResNets) coupled with\nsynthetic oversampling techniques. Because of the inherent class imbalance in\nmany cyber-security datasets, conventional methods often struggle with false\nnegatives, misclassifying subtle DDoS patterns as benign. By applying the\nSynthetic Minority Over-sampling Technique (SMOTE) to the CICIDS dataset, we\nbalance the representation of benign and malicious data points, enabling the\nmodel to better discern intricate patterns indicative of an attack. Our deep\nresidual network, tailored for this specific task, further refines the\ndetection process. Experimental results on a real-world dataset demonstrate\nthat our approach achieves an accuracy of 99.98%, significantly outperforming\ntraditional methods. This work underscores the potential of combining advanced\ndata augmentation techniques with deep learning models to bolster\ncyber-security defenses.\n",
        "title": "Advancing DDoS Attack Detection: A Synergistic Approach Using Deep\n  Residual Neural Networks and Synthetic Oversampling",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03118",
        "abstract_url": "http://arxiv.org/abs/2401.03118",
        "authors": [
            {
                "last_name": "Pawn",
                "first_name": ""
            },
            {
                "last_name": "Rookie",
                "first_name": ""
            },
            {
                "last_name": "Cheng",
                "first_name": "Zhuan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  NuLink provides privacy-preserving technology for decentralized applications\nvia APIs. Users can securely store its valuable data, trade with others and so\non. To ensure the privacy and security of service provided by NuLink,\n(zero-knowledge) proof systems are necessary. Zero-knowledge proof systems\nallow the prover to make the verifier believe that a certain conclusion is\ncorrect without providing any useful information to the verifier. In NuLink, we\nare going to use (zero-knowledge) proof system in the following three methods:\n1. Users store their data through NuLink in a decentralized manner. To ensure\nthat the storage clients are indeed storing the data, we employ proof of\nstorage systems. In this system, users prepare certain challenges that can only\nbe correctly answered by those who are actually storing the data. 2. Users have\nthe option to outsource computations to NuLink. To verify the correctness of\nthe computation results provided by the compute node, we require the node to\nprovide a proof of correctness via SNARK systems. When sensitive parameters are\nused as inputs for computation, we utilize zk-SNARKs to prevent any potential\nleakage of these parameters. 3. Users may choose to trade their data through\nNuLink. To confirm that the buyer has sufficient digital funds and the seller\npossesses the desired data, both parties can provide a proof via zk-SNARKs.\nThis builds confidence and prevents cheating during transactions. Using\nzero-knowledge proof systems, we can ensure that all nodes in NuLink behaves\nhonestly and avoid cheating in the whole system.\n",
        "title": "Zero-Knowledge Proof in NuLink",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03122",
        "abstract_url": "http://arxiv.org/abs/2401.03122",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xuran"
            },
            {
                "last_name": "Xu",
                "first_name": "Ziqiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhihan"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhengpeng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Mingzhe"
            },
            {
                "last_name": "Stankovic",
                "first_name": "LJubisa"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            ""
        ],
        "abstract": "  Speckle noise poses a significant challenge in maintaining the quality of\nsynthetic aperture radar (SAR) images, so SAR despeckling techniques have drawn\nincreasing attention. Despite the tremendous advancements of deep learning in\nfixed-scale SAR image despeckling, these methods still struggle to deal with\nlarge-scale SAR images. To address this problem, this paper introduces a novel\ndespeckling approach termed Region Denoising Diffusion Probabilistic Model\n(R-DDPM) based on generative models. R-DDPM enables versatile despeckling of\nSAR images across various scales, accomplished within a single training\nsession. Moreover, The artifacts in the fused SAR images can be avoided\neffectively with the utilization of region-guided inverse sampling. Experiments\nof our proposed R-DDPM on Sentinel-1 data demonstrates superior performance to\nexisting methods.\n",
        "title": "SAR Despeckling via Regional Denoising Diffusion Probabilistic Model",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03123",
        "abstract_url": "http://arxiv.org/abs/2401.03123",
        "authors": [
            {
                "last_name": "Shin",
                "first_name": "Jungmin"
            },
            {
                "last_name": "Shin",
                "first_name": "Seung Jun"
            },
            {
                "last_name": "Bang",
                "first_name": "Sungwan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We propose a deep neural network (DNN) based least distance (LD) estimator\n(DNN-LD) for a multivariate regression problem, addressing the limitations of\nthe conventional methods. Due to the flexibility of a DNN structure, both\nlinear and nonlinear conditional mean functions can be easily modeled, and a\nmultivariate regression model can be realized by simply adding extra nodes at\nthe output layer. The proposed method is more efficient in capturing the\ndependency structure among responses than the least squares loss, and robust to\noutliers. In addition, we consider $L_1$-type penalization for variable\nselection, crucial in analyzing high-dimensional data. Namely, we propose what\nwe call (A)GDNN-LD estimator that enjoys variable selection and model\nestimation simultaneously, by applying the (adaptive) group Lasso penalty to\nweight parameters in the DNN structure. For the computation, we propose a\nquadratic smoothing approximation method to facilitate optimizing the\nnon-smooth objective function based on the least distance loss. The simulation\nstudies and a real data analysis demonstrate the promising performance of the\nproposed method.\n",
        "title": "A least distance estimator for a multivariate regression model using\n  deep neural networks",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03124",
        "abstract_url": "http://arxiv.org/abs/2401.03124",
        "authors": [
            {
                "last_name": "Fraccaroli",
                "first_name": "Enrico"
            },
            {
                "last_name": "Jang",
                "first_name": "Seongik"
            },
            {
                "last_name": "Stach",
                "first_name": "Logan"
            },
            {
                "last_name": "Yang",
                "first_name": "Hoeseok"
            },
            {
                "last_name": "Park",
                "first_name": "Sangyoung"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Samarjit"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Due to manufacturing variabilities and temperature gradients within an\nelectric vehicle's battery pack, the capacities of cells in it decrease\ndifferently over time. This reduces the usable capacity of the battery - the\ncharge levels of one or more cells might be at the minimum threshold while most\nof the other cells have residual charge. Active cell balancing (i.e.,\ntransferring charge among cells) can equalize their charge levels, thereby\nincreasing the battery pack's usable capacity. But performing balancing means\nadditional charge transfer, which can result in energy loss and cell aging,\nakin to memory aging in storage technologies due to writing. This paper studies\nwhen cell balancing should be optimally triggered to minimize aging while\nmaintaining the necessary driving capability. In particular, we propose\noptimization strategies for cell balancing while minimizing their impact on\naging. By borrowing terminology from the storage domain, we refer to this as\n\"wear leveling-aware\" active balancing.\n",
        "title": "To Balance or to Not? Battery Aging-Aware Active Cell Balancing for\n  Electric Vehicles",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03128",
        "abstract_url": "http://arxiv.org/abs/2401.03128",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xuran"
            },
            {
                "last_name": "Zhu",
                "first_name": "Mingzhe"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanjing"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhenpeng"
            },
            {
                "last_name": "Stankovic",
                "first_name": "LJubisa"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Explainable artificial intelligence (XAI) holds immense significance in\nenhancing the deep neural network's transparency and credibility, particularly\nin some risky and high-cost scenarios, like synthetic aperture radar (SAR).\nShapley is a game-based explanation technique with robust mathematical\nfoundations. However, Shapley assumes that model's features are independent,\nrendering Shapley explanation invalid for high dimensional models. This study\nintroduces a manifold-based Shapley method by projecting high-dimensional\nfeatures into low-dimensional manifold features and subsequently obtaining\nFusion-Shap, which aims at (1) addressing the issue of erroneous explanations\nencountered by traditional Shap; (2) resolving the challenge of\ninterpretability that traditional Shap faces in complex scenarios.\n",
        "title": "Manifold-based Shapley for SAR Recognization Network Explanation",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03129",
        "abstract_url": "http://arxiv.org/abs/2401.03129",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Chen-An"
            },
            {
                "last_name": "Lee",
                "first_name": "Hung-Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n",
        "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language\n  Models",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03131",
        "abstract_url": "http://arxiv.org/abs/2401.03131",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Junhuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Hanchen"
            },
            {
                "last_name": "Sheng",
                "first_name": "Yi"
            },
            {
                "last_name": "Lin",
                "first_name": "Youzuo"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV",
            "",
            ""
        ],
        "abstract": "  Full-waveform inversion (FWI) plays a vital role in geoscience to explore the\nsubsurface. It utilizes the seismic wave to image the subsurface velocity map.\nAs the machine learning (ML) technique evolves, the data-driven approaches\nusing ML for FWI tasks have emerged, offering enhanced accuracy and reduced\ncomputational cost compared to traditional physics-based methods. However, a\ncommon challenge in geoscience, the unprivileged data, severely limits ML\neffectiveness. The issue becomes even worse during model pruning, a step\nessential in geoscience due to environmental complexities. To tackle this, we\nintroduce the EdGeo toolkit, which employs a diffusion-based model guided by\nphysics principles to generate high-fidelity velocity maps. The toolkit uses\nthe acoustic wave equation to generate corresponding seismic waveform data,\nfacilitating the fine-tuning of pruned ML models. Our results demonstrate\nsignificant improvements in SSIM scores and reduction in both MAE and MSE\nacross various pruning ratios. Notably, the ML model fine-tuned using data\ngenerated by EdGeo yields superior quality of velocity maps, especially in\nrepresenting unprivileged features, outperforming other existing methods.\n",
        "title": "A Physics-guided Generative AI Toolkit for Geophysical Monitoring",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03132",
        "abstract_url": "http://arxiv.org/abs/2401.03132",
        "authors": [
            {
                "last_name": "Akan",
                "first_name": "Taymaz"
            },
            {
                "last_name": "Alp",
                "first_name": "Sait"
            },
            {
                "last_name": "Bhuiyanb",
                "first_name": "Mohammad A. N"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Alzheimer's is a brain disease that gets worse over time and affects memory,\nthinking, and behavior. Alzheimer's disease (AD) can be treated and managed if\nit is diagnosed early, which can slow the progression of symptoms and improve\nquality of life. In this study, we suggested using the Visual Transformer (ViT)\nand bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used\nViT to extract features from the MRI and then map them to a feature sequence.\nThen, we used Bi-LSTM sequence modeling to keep the interdependencies between\nrelated features. In addition, we evaluated the performance of the proposed\nmodel for the binary classification of AD patients using data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our\nmethod against other deep learning models in the literature. The proposed\nmethod performs well in terms of accuracy, precision, F-score, and recall for\nthe diagnosis of AD.\n",
        "title": "Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from\n  3D MRI",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03134",
        "abstract_url": "http://arxiv.org/abs/2401.03134",
        "authors": [
            {
                "last_name": "Maheshwari",
                "first_name": "Paridhi"
            },
            {
                "last_name": "Ren",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanan"
            },
            {
                "last_name": "Sosic",
                "first_name": "Rok"
            },
            {
                "last_name": "Leskovec",
                "first_name": "Jure"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Many real-world systems exhibit temporal, dynamic behaviors, which are\ncaptured as time series of complex agent interactions. To perform temporal\nreasoning, current methods primarily encode temporal dynamics through simple\nsequence-based models. However, in general these models fail to efficiently\ncapture the full spectrum of rich dynamics in the input, since the dynamics is\nnot uniformly distributed. In particular, relevant information might be harder\nto extract and computing power is wasted for processing all individual\ntimesteps, even if they contain no significant changes or no new information.\nHere we propose TimeGraphs, a novel approach that characterizes dynamic\ninteractions as a hierarchical temporal graph, diverging from traditional\nsequential representations. Our approach models the interactions using a\ncompact graph-based representation, enabling adaptive reasoning across diverse\ntime scales. Adopting a self-supervised method, TimeGraphs constructs a\nmulti-level event hierarchy from a temporal input, which is then used to\nefficiently reason about the unevenly distributed dynamics. This construction\nprocess is scalable and incremental to accommodate streaming data. We evaluate\nTimeGraphs on multiple datasets with complex, dynamic agent interactions,\nincluding a football simulator, the Resistance game, and the MOMA human\nactivity dataset. The results demonstrate both robustness and efficiency of\nTimeGraphs on a range of temporal reasoning tasks. Our approach obtains\nstate-of-the-art performance and leads to a performance increase of up to 12.2%\non event prediction and recognition tasks over current approaches. Our\nexperiments further demonstrate a wide array of capabilities including\nzero-shot generalization, robustness in case of data sparsity, and adaptability\nto streaming data flow.\n",
        "title": "TimeGraphs: Graph-based Temporal Reasoning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03135",
        "abstract_url": "http://arxiv.org/abs/2401.03135",
        "authors": [
            {
                "last_name": "Ping",
                "first_name": "Xubin"
            },
            {
                "last_name": "Zimenko",
                "first_name": "Konstantin"
            },
            {
                "last_name": "Polyakov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Efimov",
                "first_name": "Denis"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Homogeneous observer for linear multi-input multi-output (MIMO) system is\ndesigned. A prefilter of the output is utilized in order to improve robustness\nof the observer with respect to measurement noises. The use of such a prefilter\nalso simplifies tuning, since the observer gains in this case are parameterized\nby a linear matrix inequality (LMI) being always feasible for observable\nsystem. In particular case, the observer is shown to be applicable in the\npresence of the state and the output bounded perturbations. Theoretical results\nare supported by numerical simulations.\n",
        "title": "Filtering Homogeneous Observer for MIMO System",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03137",
        "abstract_url": "http://arxiv.org/abs/2401.03137",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Dohyeok"
            },
            {
                "last_name": "Han",
                "first_name": "Seungyub"
            },
            {
                "last_name": "Cho",
                "first_name": "Taehyun"
            },
            {
                "last_name": "Lee",
                "first_name": "Jungwoo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Alleviating overestimation bias is a critical challenge for deep\nreinforcement learning to achieve successful performance on more complex tasks\nor offline datasets containing out-of-distribution data. In order to overcome\noverestimation bias, ensemble methods for Q-learning have been investigated to\nexploit the diversity of multiple Q-functions. Since network initialization has\nbeen the predominant approach to promote diversity in Q-functions,\nheuristically designed diversity injection methods have been studied in the\nliterature. However, previous studies have not attempted to approach guaranteed\nindependence over an ensemble from a theoretical perspective. By introducing a\nnovel regularization loss for Q-ensemble independence based on random matrix\ntheory, we propose spiked Wishart Q-ensemble independence regularization (SPQR)\nfor reinforcement learning. Specifically, we modify the intractable hypothesis\ntesting criterion for the Q-ensemble independence into a tractable KL\ndivergence between the spectral distribution of the Q-ensemble and the target\nWigner's semicircle distribution. We implement SPQR in several online and\noffline ensemble Q-learning algorithms. In the experiments, SPQR outperforms\nthe baseline algorithms in both online and offline RL benchmarks.\n",
        "title": "SPQR: Controlling Q-ensemble Independence with Spiked Random Model for\n  Reinforcement Learning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03138",
        "abstract_url": "http://arxiv.org/abs/2401.03138",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "ChungYi"
            },
            {
                "last_name": "Tung",
                "first_name": "Shen-Lung"
            },
            {
                "last_name": "Su",
                "first_name": "Hung-Ting"
            },
            {
                "last_name": "Hsu",
                "first_name": "Winston H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  To address the limitations of traffic prediction from location-bound\ndetectors, we present Geographical Cellular Traffic (GCT) flow, a novel data\nsource that leverages the extensive coverage of cellular traffic to capture\nmobility patterns. Our extensive analysis validates its potential for\ntransportation. Focusing on vehicle-related GCT flow prediction, we propose a\ngraph neural network that integrates multivariate, temporal, and spatial facets\nfor improved accuracy. Experiments reveal our model's superiority over\nbaselines, especially in long-term predictions. We also highlight the potential\nfor GCT flow integration into transportation systems.\n",
        "title": "TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation\n  and Prediction via Multifaceted Graph Modeling",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03141",
        "abstract_url": "http://arxiv.org/abs/2401.03141",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jun"
            },
            {
                "last_name": "Zhao",
                "first_name": "Dexin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Youxi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Feitian"
            },
            {
                "last_name": "Shen",
                "first_name": "Tongsheng"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  An artificial lateral line (ALL) is a bioinspired flow sensing system of an\nunderwater robot that consists of distributed flow sensors. The ALL has\nachieved great success in sensing the motion states of bioinspired underwater\nrobots, e.g., robotic fish, that are driven by body undulation and/or tail\nflapping. However, the ALL has not been systematically tested and studied in\nthe sensing of underwater robots driven by rotating propellers due to the\nhighly dynamic and complex flow field therein. This paper makes a bold\nhypothesis that the distributed flow measurements sampled from the propeller\nwake flow, although infeasible to represent the entire flow dynamics, provides\nsufficient information for estimating the lateral motion states of the leader\nunderwater robot. An experimental testbed is constructed to investigate the\nfeasibility of such a state estimator which comprises a cylindrical ALL sensory\nsystem, a rotating leader propeller, and a water tank with a planar sliding\nguide. Specifically, a hybrid network that consists of a one-dimensional\nconvolution network (1DCNN) and a bidirectional long short-term memory network\n(BiLSTM) is designed to extract the spatiotemporal features of the time series\nof distributed pressure measurements. A multi-output deep learning network is\nadopted to estimate the lateral motion states of the leader propeller. In\naddition, the state estimator is optimized using the whale optimization\nalgorithm (WOA) considering the comprehensive estimation performance. Extensive\nexperiments are conducted the results of which validate the proposed\ndata-driven algorithm in estimating the motion states of the leader underwater\nrobot by propeller wake sensing.\n",
        "title": "Estimating the Lateral Motion States of an Underwater Robot by Propeller\n  Wake Sensing Using an Artificial Lateral Line",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03142",
        "abstract_url": "http://arxiv.org/abs/2401.03142",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Liangtao"
            },
            {
                "last_name": "Zhong",
                "first_name": "Bineng"
            },
            {
                "last_name": "Liang",
                "first_name": "Qihua"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shengping"
            },
            {
                "last_name": "Li",
                "first_name": "Xianxian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  How to effectively exploit spatio-temporal information is crucial to capture\ntarget appearance changes in visual tracking. However, most deep learning-based\ntrackers mainly focus on designing a complicated appearance model or template\nupdating strategy, while lacking the exploitation of context between\nconsecutive frames and thus entailing the \\textit{when-and-how-to-update}\ndilemma. To address these issues, we propose a novel explicit visual prompts\nframework for visual tracking, dubbed \\textbf{EVPTrack}. Specifically, we\nutilize spatio-temporal tokens to propagate information between consecutive\nframes without focusing on updating templates. As a result, we cannot only\nalleviate the challenge of \\textit{when-to-update}, but also avoid the\nhyper-parameters associated with updating strategies. Then, we utilize the\nspatio-temporal tokens to generate explicit visual prompts that facilitate\ninference in the current frame. The prompts are fed into a transformer encoder\ntogether with the image tokens without additional processing. Consequently, the\nefficiency of our model is improved by avoiding \\textit{how-to-update}. In\naddition, we consider multi-scale information as explicit visual prompts,\nproviding multiscale template features to enhance the EVPTrack's ability to\nhandle target scale changes. Extensive experimental results on six benchmarks\n(i.e., LaSOT, LaSOT\\rm $_{ext}$, GOT-10k, UAV123, TrackingNet, and TNL2K.)\nvalidate that our EVPTrack can achieve competitive performance at a real-time\nspeed by effectively exploiting both spatio-temporal and multi-scale\ninformation. Code and models are available at\nhttps://github.com/GXNU-ZhongLab/EVPTrack.\n",
        "title": "Explicit Visual Prompts for Visual Object Tracking",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03144",
        "abstract_url": "http://arxiv.org/abs/2401.03144",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Xinying"
            },
            {
                "last_name": "Ericson",
                "first_name": "Barbara J."
            },
            {
                "last_name": "Wang",
                "first_name": "Xu"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC"
        ],
        "abstract": "  Novice programmers need to learn how to write basic code but often face\ndifficulties when coding independently. To assist struggling students, we have\nrecently implemented personalized Parsons problems as a pop-up scaffolding.\nStudents found them to be more engaging and helpful for learning compared to\nsimply receiving the correct answer, such as the response they might get from\nLarge Language Model (LLM) tools like ChatGPT. However, a drawback of using\nParsons problems as scaffolding is that students may be able to put the code\nblocks back in place without fully understanding the rationale of the correct\nsolution. As a result, the learning benefits of such scaffolding are\ncompromised. Our goal is to enhance the advantages of using personalized\nParsons problems as scaffolding by improving their comprehension through code\nexplanations. In this poster, we propose designs that incorporate multiple\nlevels of textual explanations in the Parsons problems. This design will be\nused for future technical evaluation and classroom experiments. These\nexperiments will explore the effectiveness of adding textual explanations to\nParsons problems to improve instructional benefits.\n",
        "title": "Integrating Personalized Parsons Problems with Multi-Level Textual\n  Explanations to Scaffold Code Writing",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03145",
        "abstract_url": "http://arxiv.org/abs/2401.03145",
        "authors": [
            {
                "last_name": "Tu",
                "first_name": "Yuanpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Boshen"
            },
            {
                "last_name": "Liu",
                "first_name": "Liang"
            },
            {
                "last_name": "Li",
                "first_name": "Yuxi"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenhai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiangning"
            },
            {
                "last_name": "Wang",
                "first_name": "Yabiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Chengjie"
            },
            {
                "last_name": "Zhao",
                "first_name": "Cai Rong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Industrial anomaly detection is generally addressed as an unsupervised task\nthat aims at locating defects with only normal training samples. Recently,\nnumerous 2D anomaly detection methods have been proposed and have achieved\npromising results, however, using only the 2D RGB data as input is not\nsufficient to identify imperceptible geometric surface anomalies. Hence, in\nthis work, we focus on multi-modal anomaly detection. Specifically, we\ninvestigate early multi-modal approaches that attempted to utilize models\npre-trained on large-scale visual datasets, i.e., ImageNet, to construct\nfeature databases. And we empirically find that directly using these\npre-trained models is not optimal, it can either fail to detect subtle defects\nor mistake abnormal features as normal ones. This may be attributed to the\ndomain gap between target industrial data and source data.Towards this problem,\nwe propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method\nto finetune the adaptors and learn task-oriented representation toward anomaly\ndetection.Both intra-modal adaptation and cross-modal alignment are optimized\nfrom a local-to-global perspective in LSFA to ensure the representation quality\nand consistency in the inference stage.Extensive experiments demonstrate that\nour method not only brings a significant performance boost to feature embedding\nbased approaches, but also outperforms previous State-of-The-Art (SoTA) methods\nprominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves\n97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%.\n",
        "title": "Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03147",
        "abstract_url": "http://arxiv.org/abs/2401.03147",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Jingyi"
            },
            {
                "last_name": "Tang",
                "first_name": "Min"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  It is of great interest to solve the inverse problem of stationary radiative\ntransport equation (RTE) in optical tomography. The standard way is to\nformulate the inverse problem into an optimization problem, but the bottleneck\nis that one has to solve the forward problem repeatedly, which is\ntime-consuming. Due to the optical property of biological tissue, in real\napplications, optical thin and thick regions coexist and are adjacent to each\nother, and the geometry can be complex. To use coarse meshes and save the\ncomputational cost, the forward solver has to be asymptotic preserving across\nthe interface (APAL).\n  In this paper, we propose an offline/online solver for RTE. The cost at the\noffline stage is comparable to classical methods, while the cost at the online\nstage is much lower. Two cases are considered. One is to solve the RTE with\nfixed scattering and absorption cross sections while the boundary conditions\nvary; the other is when cross sections vary in a small domain and the boundary\nconditions change many times. The solver can be decomposed into offline/online\nstages in these two cases. One only needs to calculate the offline stage once\nand update the online stage when the parameters vary. Our proposed solver is\nmuch cheaper when one needs to solve RTE with multiple right-hand sides or when\nthe cross sections vary in a small domain, thus can accelerate the speed of\nsolving inverse RTE problems. We illustrate the online/offline decomposition\nbased on the Tailored Finite Point Method (TFPM), which is APAL on general\nquadrilateral meshes.\n",
        "title": "A fast offline/online forward solver for stationary transport equation\n  with multiple inflow boundary conditions and varying coefficients",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03149",
        "abstract_url": "http://arxiv.org/abs/2401.03149",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yixin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Han",
                "first_name": "Boran"
            },
            {
                "last_name": "He",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this work, we introduce Context-Aware MultiModal Learner (CaMML), for\ntuning large multimodal models (LMMs). CaMML, a lightweight module, is crafted\nto seamlessly integrate multimodal contextual samples into large models,\nthereby empowering the model to derive knowledge from analogous,\ndomain-specific, up-to-date information and make grounded inferences.\nImportantly, CaMML is highly scalable and can efficiently handle lengthy\nmultimodal context examples owing to its hierarchical design. Based on CaMML,\nwe have developed two multimodal models, CaMML-7B and CaMML-13B, that have\nshown exceptional performance across an array of benchmark datasets for\nmultimodal tasks. Remarkably, CaMML-13B achieves the state-of-the-art\nperformance on over ten widely recognized multimodal benchmark datasets,\nsurpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any\nexternal resources. Moreover, we have conducted extensive ablative studies to\ninspect the inner workings of CaMML and performed qualitative analyses to\nshowcase its effectiveness in handling real-world challenging cases.\n",
        "title": "CaMML: Context-Aware Multimodal Learner for Large Models",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03151",
        "abstract_url": "http://arxiv.org/abs/2401.03151",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Yingying"
            },
            {
                "last_name": "Pei",
                "first_name": "Xiaobing"
            },
            {
                "last_name": "Shen",
                "first_name": "Lihong"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Log anomaly detection plays a critical role in ensuring the security and\nmaintenance of modern software systems. At present, the primary approach for\ndetecting anomalies in log data is through supervised anomaly detection.\nNonetheless, existing supervised methods heavily rely on labeled data, which\ncan be frequently limited in real-world scenarios. In this paper, we propose a\nsemi-supervised log anomaly detection method that combines the DQN algorithm\nfrom deep reinforcement learning, which is called DQNLog. DQNLog leverages a\nsmall amount of labeled data and a large-scale unlabeled dataset, effectively\naddressing the challenges of imbalanced data and limited labeling. This\napproach not only learns known anomalies by interacting with an environment\nbiased towards anomalies but also discovers unknown anomalies by actively\nexploring the unlabeled dataset. Additionally, DQNLog incorporates a\ncross-entropy loss term to prevent model overestimation during Deep\nReinforcement Learning (DRL). Our evaluation on three widely-used datasets\ndemonstrates that DQNLog significantly improves recall rate and F1-score while\nmaintaining precision, validating its practicality.\n",
        "title": "Semi-supervised learning via DQN for log anomaly detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03152",
        "abstract_url": "http://arxiv.org/abs/2401.03152",
        "authors": [
            {
                "last_name": "Valvano",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Agostino",
                "first_name": "Antonino"
            },
            {
                "last_name": "De Magistris",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Graziano",
                "first_name": "Antonino"
            },
            {
                "last_name": "Veneri",
                "first_name": "Giacomo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Training supervised deep neural networks that perform defect detection and\nsegmentation requires large-scale fully-annotated datasets, which can be hard\nor even impossible to obtain in industrial environments. Generative AI offers\nopportunities to enlarge small industrial datasets artificially, thus enabling\nthe usage of state-of-the-art supervised approaches in the industry.\nUnfortunately, also good generative models need a lot of data to train, while\nindustrial datasets are often tiny. Here, we propose a new approach for reusing\ngeneral-purpose pre-trained generative models on industrial data, ultimately\nallowing the generation of self-labelled defective images. First, we let the\nmodel learn the new concept, entailing the novel data distribution. Then, we\nforce it to learn to condition the generative process, producing industrial\nimages that satisfy well-defined topological characteristics and show defects\nwith a given geometry and location. To highlight the advantage of our approach,\nwe use the synthetic dataset to optimise a crack segmentor for a real\nindustrial use case. When the available data is small, we observe considerable\nperformance increase under several metrics, showing the method's potential in\nproduction environments.\n",
        "title": "Controllable Image Synthesis of Industrial Data Using Stable Diffusion",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03153",
        "abstract_url": "http://arxiv.org/abs/2401.03153",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Bo"
            },
            {
                "last_name": "Han",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Suo",
                "first_name": "Jinli"
            },
            {
                "last_name": "Dai",
                "first_name": "Qionghai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Event cameras or dynamic vision sensors (DVS) record asynchronous response to\nbrightness changes instead of conventional intensity frames, and feature\nultra-high sensitivity at low bandwidth. The new mechanism demonstrates great\nadvantages in challenging scenarios with fast motion and large dynamic range.\nHowever, the recorded events might be highly sparse due to either limited\nhardware bandwidth or extreme photon starvation in harsh environments. To\nunlock the full potential of event cameras, we propose an inventive event\nsequence completion approach conforming to the unique characteristics of event\ndata in both the processing stage and the output form. Specifically, we treat\nevent streams as 3D event clouds in the spatiotemporal domain, develop a\ndiffusion-based generative model to generate dense clouds in a coarse-to-fine\nmanner, and recover exact timestamps to maintain the temporal resolution of raw\ndata successfully. To validate the effectiveness of our method comprehensively,\nwe perform extensive experiments on three widely used public datasets with\ndifferent spatial resolutions, and additionally collect a novel event dataset\ncovering diverse scenarios with highly dynamic motions and under harsh\nillumination. Besides generating high-quality dense events, our method can\nbenefit downstream applications such as object classification and intensity\nframe reconstruction.\n",
        "title": "An Event-Oriented Diffusion-Refinement Method for Sparse Events\n  Completion",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03156",
        "abstract_url": "http://arxiv.org/abs/2401.03156",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yihan"
            },
            {
                "last_name": "Liu",
                "first_name": "Shuang"
            },
            {
                "last_name": "Gao",
                "first_name": "Xiao-Shan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Stability analysis is an essential aspect of studying the generalization\nability of deep learning, as it involves deriving generalization bounds for\nstochastic gradient descent-based training algorithms. Adversarial training is\nthe most widely used defense against adversarial example attacks. However,\nprevious generalization bounds for adversarial training have not included\ninformation regarding the data distribution. In this paper, we fill this gap by\nproviding generalization bounds for stochastic gradient descent-based\nadversarial training that incorporate data distribution information. We utilize\nthe concepts of on-average stability and high-order approximate Lipschitz\nconditions to examine how changes in data distribution and adversarial budget\ncan affect robust generalization gaps. Our derived generalization bounds for\nboth convex and non-convex losses are at least as good as the uniform\nstability-based counterparts which do not include data distribution\ninformation. Furthermore, our findings demonstrate how distribution shifts from\ndata poisoning attacks can impact robust generalization.\n",
        "title": "Data-Dependent Stability Analysis of Adversarial Training",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03157",
        "abstract_url": "http://arxiv.org/abs/2401.03157",
        "authors": [
            {
                "last_name": "Dissanayaka",
                "first_name": "Sahan"
            },
            {
                "last_name": "Mudanayaka",
                "first_name": "Oshan"
            },
            {
                "last_name": "Halloluwa",
                "first_name": "Thilina"
            },
            {
                "last_name": "De Silva",
                "first_name": "Chameera"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image processing holds immense potential for societal benefit, yet its full\npotential is often accessible only to tech-savvy experts. Bridging this\nknowledge gap and providing accessible tools for users of all backgrounds\nremains an unexplored frontier. This paper introduces \"ImageLab,\" a novel tool\ndesigned to democratize image processing, catering to both novices and experts\nby prioritizing interactive learning over theoretical complexity. ImageLab not\nonly serves as a valuable educational resource but also offers a practical\ntesting environment for seasoned practitioners. Through a comprehensive\nevaluation of ImageLab's features, we demonstrate its effectiveness through a\nuser study done for a focused group of school children and university students\nwhich enables us to get positive feedback on the tool. Our work represents a\nsignificant stride toward enhancing image processing education and practice,\nmaking it more inclusive and approachable for all.\n",
        "title": "ImageLab: Simplifying Image Processing Exploration for Novices and\n  Experts Alike",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03158",
        "abstract_url": "http://arxiv.org/abs/2401.03158",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuanben"
            },
            {
                "last_name": "Han",
                "first_name": "Zhonghe"
            },
            {
                "last_name": "Hou",
                "first_name": "Yingyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Lei"
            },
            {
                "last_name": "Liu",
                "first_name": "Siye"
            },
            {
                "last_name": "Gong",
                "first_name": "Qihang"
            },
            {
                "last_name": "Ge",
                "first_name": "Yunping"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Short Text Classification (STC) is crucial for processing and comprehending\nthe brief but substantial content prevalent on contemporary digital platforms.\nThe STC encounters difficulties in grasping semantic and syntactic intricacies,\nan issue that is apparent in traditional pre-trained language models. Although\nGraph Convolutional Networks enhance performance by integrating external\nknowledge bases, these methods are limited by the quality and extent of the\nknowledge applied. Recently, the emergence of Large Language Models (LLMs) and\nChain-of-Thought (CoT) has significantly improved the performance of complex\nreasoning tasks. However, some studies have highlighted the limitations of\ntheir application in fundamental NLP tasks. Consequently, this study sought to\nemploy CoT to investigate the capabilities of LLMs in STC tasks. This study\nintroduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This\nframework primarily incorporates Syntactic and Semantic Enrichment CoT,\neffectively decomposing the STC task into four distinct steps: (i) essential\nconcept identification, (ii) common-sense knowledge retrieval, (iii) text\nrewriting, and (iv) classification. This elicits the inherent knowledge and\nabilities of LLMs to address the challenges in STC. Surprisingly, we found that\nQLFR can also improve the performance of smaller models. Therefore, we\ndeveloped a CoT-Driven Multi-task learning (QLFR-CML) method to facilitate the\nknowledge transfer from LLMs to smaller models. Extensive experimentation\nacross six short-text benchmarks validated the efficacy of the proposed\nmethods. Notably, QLFR achieved state-of-the-art performance on all datasets,\nwith significant improvements, particularly on the Ohsumed and TagMyNews\ndatasets.\n",
        "title": "Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing\n  Short Text Classification",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03159",
        "abstract_url": "http://arxiv.org/abs/2401.03159",
        "authors": [
            {
                "last_name": "Cha",
                "first_name": "Narisu"
            },
            {
                "last_name": "Chang",
                "first_name": "Long"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Federated learning is an emerging distributed machine learning framework in\nthe Internet of Vehicles (IoV). In IoV, millions of vehicles are willing to\ntrain the model to share their knowledge. Maintaining an active state means the\nparticipants must update their state to the FL server in a fixed interval and\nparticipate to next round. However, the cost by maintaining an active state is\nvery large when there are a huge number of participating vehicles. In this\npaper, we proposed a distributed client selection scheme to reduce the cost of\nmaintaining the active state for all participants. The clients with the highest\nevaluation are elected among the neighbours. In the evaluator, four variables\nare considered including sample quantity, throughput available, computational\ncapability and the quality of the local dataset. We adopted fuzzy logic as the\nevaluator since the closed-form solution over four variables does not exist.\nExtensive simulation results show our proposal approximates the centralized\nclient selection in terms of accuracy and can significantly reduce the\ncommunication overhead.\n",
        "title": "Distributed client selection with multi-objective in federated learning\n  assisted Internet of Vehicles",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03161",
        "abstract_url": "http://arxiv.org/abs/2401.03161",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Jingyi"
            },
            {
                "last_name": "Liang",
                "first_name": "Jiuyang"
            },
            {
                "last_name": "Perthame",
                "first_name": "Benoit"
            },
            {
                "last_name": "tang",
                "first_name": "Min"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The motile micro-organisms such as E. coli, sperm, or some seaweed are\nusually modelled by self-propelled particles that move with the run-and-tumble\nprocess. Individual-based stochastic models are usually employed to model the\naggregation phenomenon at the boundary, which is an active research field that\nhas attracted a lot of biologists and biophysicists. Self-propelled particles\nat the microscale have complex behaviors, while characteristics at the\npopulation level are more important for practical applications but rely on\nindividual behaviors. Kinetic PDE models that describe the time evolution of\nthe probability density distribution of the motile micro-organisms are widely\nused. However, how to impose the appropriate boundary conditions that take into\naccount the boundary aggregation phenomena is rarely studied. In this paper, we\npropose the boundary conditions for a 2D confined run-and-tumble model (CRTM)\nfor self-propelled particle populations moving between two parallel plates with\na run-and-tumble process. The proposed model satisfies the relative entropy\ninequality and thus long-time convergence. We establish the relation between\nCRTM and the confined Fokker-Planck model (CFPM) studied in [22]. We prove\ntheoretically that when the tumble is highly forward peaked and frequent\nenough, CRTM converges asymptotically to the CFPM. A numerical comparison of\nthe CRTM with aggregation and CFPM is given. The time evolution of both the\ndeterministic PDE model and individual-based stochastic simulations are\ndisplayed, which match each other well.\n",
        "title": "Confined run-and-tumble model with boundary aggregation: long time\n  behavior and convergence to the confined Fokker-Planck model",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03162",
        "abstract_url": "http://arxiv.org/abs/2401.03162",
        "authors": [
            {
                "last_name": "Choi",
                "first_name": "Jeongwhan"
            },
            {
                "last_name": "Ryu",
                "first_name": "Duksan"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG",
            "SE"
        ],
        "abstract": "  With the rapid growth of cloud services driven by advancements in web service\ntechnology, selecting a high-quality service from a wide range of options has\nbecome a complex task. This study aims to address the challenges of data\nsparsity and the cold-start problem in web service recommendation using Quality\nof Service (QoS). We propose a novel approach called QoS-aware graph\ncontrastive learning (QAGCL) for web service recommendation. Our model\nharnesses the power of graph contrastive learning to handle cold-start problems\nand improve recommendation accuracy effectively. By constructing contextually\naugmented graphs with geolocation information and randomness, our model\nprovides diverse views. Through the use of graph convolutional networks and\ngraph contrastive learning techniques, we learn user and service embeddings\nfrom these augmented graphs. The learned embeddings are then utilized to\nseamlessly integrate QoS considerations into the recommendation process.\nExperimental results demonstrate the superiority of our QAGCL model over\nseveral existing models, highlighting its effectiveness in addressing data\nsparsity and the cold-start problem in QoS-aware service recommendations. Our\nresearch contributes to the potential for more accurate recommendations in\nreal-world scenarios, even with limited user-service interaction data.\n",
        "title": "QoS-Aware Graph Contrastive Learning for Web Service Recommendation",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03163",
        "abstract_url": "http://arxiv.org/abs/2401.03163",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Kewen"
            },
            {
                "last_name": "Vamplew",
                "first_name": "Peter"
            },
            {
                "last_name": "Foale",
                "first_name": "Cameron"
            },
            {
                "last_name": "Dazeley",
                "first_name": "Richard"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  One common approach to solve multi-objective reinforcement learning (MORL)\nproblems is to extend conventional Q-learning by using vector Q-values in\ncombination with a utility function. However issues can arise with this\napproach in the context of stochastic environments, particularly when\noptimising for the Scalarised Expected Reward (SER) criterion. This paper\nextends prior research, providing a detailed examination of the factors\ninfluencing the frequency with which value-based MORL Q-learning algorithms\nlearn the SER-optimal policy for an environment with stochastic state\ntransitions. We empirically examine several variations of the core\nmulti-objective Q-learning algorithm as well as reward engineering approaches,\nand demonstrate the limitations of these methods. In particular, we highlight\nthe critical impact of the noisy Q-value estimates issue on the stability and\nconvergence of these algorithms.\n",
        "title": "An Empirical Investigation of Value-Based Multi-objective Reinforcement\n  Learning for Stochastic Environments",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03166",
        "abstract_url": "http://arxiv.org/abs/2401.03166",
        "authors": [
            {
                "last_name": "Dalal",
                "first_name": "Vibhu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Variational Autoencoders (VAEs) are powerful generative models, however their\ngenerated samples are known to suffer from a characteristic blurriness, as\ncompared to the outputs of alternative generating techniques. Extensive\nresearch efforts have been made to tackle this problem, and several works have\nfocused on modifying the reconstruction term of the evidence lower bound\n(ELBO). In particular, many have experimented with augmenting the\nreconstruction loss with losses in the frequency domain. Such loss functions\nusually employ the Fourier transform to explicitly penalise the lack of higher\nfrequency components in the generated samples, which are responsible for sharp\nvisual features. In this paper, we explore the aspects of previous such\napproaches which aren't well understood, and we propose an augmentation to the\nreconstruction term in response to them. Our reasoning leads us to use the\nshort-time Fourier transform and to emphasise on local phase coherence between\nthe input and output samples. We illustrate the potential of our proposed loss\non the MNIST dataset by providing both qualitative and quantitative results.\n",
        "title": "Short-Time Fourier Transform for deblurring Variational Autoencoders",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03167",
        "abstract_url": "http://arxiv.org/abs/2401.03167",
        "authors": [
            {
                "last_name": "She",
                "first_name": "Rui"
            },
            {
                "last_name": "Wang",
                "first_name": "Sijie"
            },
            {
                "last_name": "Kang",
                "first_name": "Qiyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Kai"
            },
            {
                "last_name": "Song",
                "first_name": "Yang"
            },
            {
                "last_name": "Tay",
                "first_name": "Wee Peng"
            },
            {
                "last_name": "Geng",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Jian",
                "first_name": "Xingchao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Point cloud registration is a crucial technique in 3D computer vision with a\nwide range of applications. However, this task can be challenging, particularly\nin large fields of view with dynamic objects, environmental noise, or other\nperturbations. To address this challenge, we propose a model called PosDiffNet.\nOur approach performs hierarchical registration based on window-level,\npatch-level, and point-level correspondence. We leverage a graph neural partial\ndifferential equation (PDE) based on Beltrami flow to obtain high-dimensional\nfeatures and position embeddings for point clouds. We incorporate position\nembeddings into a Transformer module based on a neural ordinary differential\nequation (ODE) to efficiently represent patches within points. We employ the\nmulti-level correspondence derived from the high feature similarity scores to\nfacilitate alignment between point clouds. Subsequently, we use registration\nmethods such as SVD-based algorithms to predict the transformation using\ncorresponding point pairs. We evaluate PosDiffNet on several 3D point cloud\ndatasets, verifying that it achieves state-of-the-art (SOTA) performance for\npoint cloud registration in large fields of view with perturbations. The\nimplementation code of experiments is available at\nhttps://github.com/AI-IT-AVs/PosDiffNet.\n",
        "title": "PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in\n  a Large Field of View with Perturbations",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03170",
        "abstract_url": "http://arxiv.org/abs/2401.03170",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Chujie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianren"
            },
            {
                "last_name": "Chen",
                "first_name": "Feng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Domain generalization (DG) aims to improve the generalization ability of the\nmodel trained on several known training domains over unseen test domains.\nPrevious work has shown that self-supervised contrastive pre-training improves\nthe robustness of the model on downstream tasks. However, in this paper, we\nfind that self-supervised models do not exhibit better generalization\nperformance than supervised models pre-trained on the same dataset in the DG\nsetting. We argue that this is owing to the fact that the richer intra-class\ndiscriminative features extracted by self-supervised contrastive learning,\nwhich we term silent features, are suppressed during supervised fine-tuning.\nThese silent features are likely to contain features that are more\ngeneralizable on the test domain. In this work, we model and analyze this\nfeature suppression phenomenon and theoretically prove that preserving silent\nfeatures can achieve lower expected test domain risk under certain conditions.\nIn light of this, we propose a simple yet effective method termed STEP (Silent\nFeature Preservation) to improve the generalization performance of the\nself-supervised contrastive learning pre-trained model by alleviating the\nsuppression of silent features during the supervised fine-tuning process.\nExperimental results show that STEP exhibits state-of-the-art performance on\nstandard DG benchmarks with significant distribution shifts.\n",
        "title": "Preserving Silent Features for Domain Generalization",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03171",
        "abstract_url": "http://arxiv.org/abs/2401.03171",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Qiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Yufeng"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Hefeng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CY"
        ],
        "abstract": "  In contemporary society, the escalating pressures of life and work have\npropelled psychological disorders to the forefront of modern health concerns,\nan issue that has been further accentuated by the COVID-19 pandemic. The\nprevalence of depression among adolescents is steadily increasing, and\ntraditional diagnostic methods, which rely on scales or interviews, prove\nparticularly inadequate for detecting depression in young people. Addressing\nthese challenges, numerous AI-based methods for assisting in the diagnosis of\nmental health issues have emerged. However, most of these methods center around\nfundamental issues with scales or use multimodal approaches like facial\nexpression recognition. Diagnosis of depression risk based on everyday habits\nand behaviors has been limited to small-scale qualitative studies. Our research\nleverages adolescent census data to predict depression risk, focusing on\nchildren's experiences with depression and their daily life situations. We\nintroduced a method for managing severely imbalanced high-dimensional data and\nan adaptive predictive approach tailored to data structure characteristics.\nFurthermore, we proposed a cloud-based architecture for automatic online\nlearning and data updates. This study utilized publicly available NSCH youth\ncensus data from 2020 to 2022, encompassing nearly 150,000 data entries. We\nconducted basic data analyses and predictive experiments, demonstrating\nsignificant performance improvements over standard machine learning and deep\nlearning algorithms. This affirmed our data processing method's broad\napplicability in handling imbalanced medical data. Diverging from typical\npredictive method research, our study presents a comprehensive architectural\nsolution, considering a wider array of user needs.\n",
        "title": "Exploration of Adolescent Depression Risk Prediction Based on Census\n  Surveys and General Life Issues",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03173",
        "abstract_url": "http://arxiv.org/abs/2401.03173",
        "authors": [
            {
                "last_name": "Minh",
                "first_name": "Tran Cao"
            },
            {
                "last_name": "Quoc",
                "first_name": "Nguyen Kim"
            },
            {
                "last_name": "Vinh",
                "first_name": "Phan Cong"
            },
            {
                "last_name": "Phu",
                "first_name": "Dang Nhu"
            },
            {
                "last_name": "Chi",
                "first_name": "Vuong Xuan"
            },
            {
                "last_name": "Tan",
                "first_name": "Ha Minh"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  In the field of medical imaging, breast ultrasound has emerged as a crucial\ndiagnostic tool for early detection of breast cancer. However, the accuracy of\ndiagnosing the location of the affected area and the extent of the disease\ndepends on the experience of the physician. In this paper, we propose a novel\nmodel called UGGNet, combining the power of the U-Net and VGG architectures to\nenhance the performance of breast ultrasound image analysis. The U-Net\ncomponent of the model helps accurately segment the lesions, while the VGG\ncomponent utilizes deep convolutional layers to extract features. The fusion of\nthese two architectures in UGGNet aims to optimize both segmentation and\nfeature representation, providing a comprehensive solution for accurate\ndiagnosis in breast ultrasound images. Experimental results have demonstrated\nthat the UGGNet model achieves a notable accuracy of 78.2% on the \"Breast\nUltrasound Images Dataset.\"\n",
        "title": "UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03175",
        "abstract_url": "http://arxiv.org/abs/2401.03175",
        "authors": [
            {
                "last_name": "Pathak",
                "first_name": "Dhrubajyoti"
            },
            {
                "last_name": "Narzary",
                "first_name": "Sanjib"
            },
            {
                "last_name": "Nandi",
                "first_name": "Sukumar"
            },
            {
                "last_name": "Som",
                "first_name": "Bidisha"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG",
            ""
        ],
        "abstract": "  Language Processing systems such as Part-of-speech tagging, Named entity\nrecognition, Machine translation, Speech recognition, and Language modeling\n(LM) are well-studied in high-resource languages. Nevertheless, research on\nthese systems for several low-resource languages, including Bodo, Mizo,\nNagamese, and others, is either yet to commence or is in its nascent stages.\nLanguage model plays a vital role in the downstream tasks of modern NLP.\nExtensive studies are carried out on LMs for high-resource languages.\nNevertheless, languages such as Bodo, Rabha, and Mising continue to lack\ncoverage. In this study, we first present BodoBERT, a language model for the\nBodo language. To the best of our knowledge, this work is the first such effort\nto develop a language model for Bodo. Secondly, we present an ensemble DL-based\nPOS tagging model for Bodo. The POS tagging model is based on combinations of\nBiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We\ncover several language models in the experiment to see how well they work in\nPOS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A\ncomparative experiment was also conducted on Assamese POS taggers, considering\nthat the language is spoken in the same region as Bodo.\n",
        "title": "Part-of-Speech Tagger for Bodo Language using Deep Learning approach",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03177",
        "abstract_url": "http://arxiv.org/abs/2401.03177",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Qian"
            },
            {
                "last_name": "Su",
                "first_name": "Lixin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jiashu"
            },
            {
                "last_name": "Xia",
                "first_name": "Long"
            },
            {
                "last_name": "Cai",
                "first_name": "Hengyi"
            },
            {
                "last_name": "Cheng",
                "first_name": "Suqi"
            },
            {
                "last_name": "Tang",
                "first_name": "Hengzhu"
            },
            {
                "last_name": "Wang",
                "first_name": "Junfeng"
            },
            {
                "last_name": "Yin",
                "first_name": "Dawei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Text-video retrieval is a challenging task that aims to identify relevant\nvideos given textual queries. Compared to conventional textual retrieval, the\nmain obstacle for text-video retrieval is the semantic gap between the textual\nnature of queries and the visual richness of video content. Previous works\nprimarily focus on aligning the query and the video by finely aggregating\nword-frame matching signals. Inspired by the human cognitive process of\nmodularly judging the relevance between text and video, the judgment needs\nhigh-order matching signal due to the consecutive and complex nature of video\ncontents. In this paper, we propose chunk-level text-video matching, where the\nquery chunks are extracted to describe a specific retrieval unit, and the video\nchunks are segmented into distinct clips from videos. We formulate the\nchunk-level matching as n-ary correlations modeling between words of the query\nand frames of the video and introduce a multi-modal hypergraph for n-ary\ncorrelation modeling. By representing textual units and video frames as nodes\nand using hyperedges to depict their relationships, a multi-modal hypergraph is\nconstructed. In this way, the query and the video can be aligned in a\nhigh-order semantic space. In addition, to enhance the model's generalization\nability, the extracted features are fed into a variational inference component\nfor computation, obtaining the variational representation under the Gaussian\ndistribution. The incorporation of hypergraphs and variational inference allows\nour model to capture complex, n-ary interactions among textual and visual\ncontents. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance on the text-video retrieval task.\n",
        "title": "Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03179",
        "abstract_url": "http://arxiv.org/abs/2401.03179",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Lei",
                "first_name": "Jie"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Yang",
                "first_name": "Geng"
            },
            {
                "last_name": "Li",
                "first_name": "Daixun"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            },
            {
                "last_name": "Seghouane",
                "first_name": "Karim"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In multimodal land cover classification (MLCC), a common challenge is the\nredundancy in data distribution, where irrelevant information from multiple\nmodalities can hinder the effective integration of their unique features. To\ntackle this, we introduce the Multimodal Informative Vit (MIVit), a system with\nan innovative information aggregate-distributing mechanism. This approach\nredefines redundancy levels and integrates performance-aware elements into the\nfused representation, facilitating the learning of semantics in both forward\nand backward directions. MIVit stands out by significantly reducing redundancy\nin the empirical distribution of each modality's separate and fused features.\nIt employs oriented attention fusion (OAF) for extracting shallow local\nfeatures across modalities in horizontal and vertical dimensions, and a\nTransformer feature extractor for extracting deep global features through\nlong-range attention. We also propose an information aggregation constraint\n(IAC) based on mutual information, designed to remove redundant information and\npreserve complementary information within embedded features. Additionally, the\ninformation distribution flow (IDF) in MIVit enhances performance-awareness by\ndistributing global classification information across different modalities'\nfeature maps. This architecture also addresses missing modality challenges with\nlightweight independent modality classifiers, reducing the computational load\ntypically associated with Transformers. Our results show that MIVit's\nbidirectional aggregate-distributing mechanism between modalities is highly\neffective, achieving an average overall accuracy of 95.56% across three\nmultimodal datasets. This performance surpasses current state-of-the-art\nmethods in MLCC. The code for MIVit is accessible at\nhttps://github.com/icey-zhang/MIViT.\n",
        "title": "Multimodal Informative ViT: Information Aggregation and Distribution for\n  Hyperspectral and LiDAR Classification",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03181",
        "abstract_url": "http://arxiv.org/abs/2401.03181",
        "authors": [
            {
                "last_name": "Sukhwal",
                "first_name": "Prakash Chandra"
            },
            {
                "last_name": "Rajan",
                "first_name": "Vaibhav"
            },
            {
                "last_name": "Kankanhalli",
                "first_name": "Atreyi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Medical question answer (QA) assistants respond to lay users' health-related\nqueries by synthesizing information from multiple sources using natural\nlanguage processing and related techniques. They can serve as vital tools to\nalleviate issues of misinformation, information overload, and complexity of\nmedical language, thus addressing lay users' information needs while reducing\nthe burden on healthcare professionals. QA systems, the engines of such\nassistants, have typically used either language models (LMs) or knowledge\ngraphs (KG), though the approaches could be complementary. LM-based QA systems\nexcel at understanding complex questions and providing well-formed answers, but\nare prone to factual mistakes. KG-based QA systems, which represent facts well,\nare mostly limited to answering short-answer questions with pre-created\ntemplates. While a few studies have jointly used LM and KG approaches for\ntext-based QA, this was done to answer multiple-choice questions. Extant QA\nsystems also have limitations in terms of automation and performance. We\naddress these challenges by designing a novel, automated disease QA system\nwhich effectively utilizes both LM and KG techniques through a joint-reasoning\napproach to answer disease-related questions appropriate for lay users. Our\nevaluation of the system using a range of quality metrics demonstrates its\nefficacy over benchmark systems, including the popular ChatGPT.\n",
        "title": "A Joint-Reasoning based Disease Q&A System",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03182",
        "abstract_url": "http://arxiv.org/abs/2401.03182",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Lei",
                "first_name": "Jie"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Jiang",
                "first_name": "Kai"
            },
            {
                "last_name": "Cao",
                "first_name": "Mingxiang"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate cloud recognition and warning are crucial for various applications,\nincluding in-flight support, weather forecasting, and climate research.\nHowever, recent deep learning algorithms have predominantly focused on\ndetecting cloud regions in satellite imagery, with insufficient attention to\nthe specificity required for accurate cloud recognition. This limitation\ninspired us to develop the novel FY-4A-Himawari-8 (FYH) dataset, which includes\nnine distinct cloud categories and uses precise domain adaptation methods to\nalign 70,419 image-label pairs in terms of projection, temporal resolution, and\nspatial resolution, thereby facilitating the training of supervised deep\nlearning networks. Given the complexity and diversity of cloud formations, we\nhave thoroughly analyzed the challenges inherent to cloud recognition tasks,\nexamining the intricate characteristics and distribution of the data. To\neffectively address these challenges, we designed a Distribution-aware\nInteractive-Attention Network (DIAnet), which preserves pixel-level details\nthrough a high-resolution branch and a parallel multi-resolution cross-branch.\nWe also integrated a distribution-aware loss (DAL) to mitigate the imbalance\nacross cloud categories. An Interactive Attention Module (IAM) further enhances\nthe robustness of feature extraction combined with spatial and channel\ninformation. Empirical evaluations on the FYH dataset demonstrate that our\nmethod outperforms other cloud recognition networks, achieving superior\nperformance in terms of mean Intersection over Union (mIoU). The code for\nimplementing DIAnet is available at https://github.com/icey-zhang/DIAnet.\n",
        "title": "Distribution-aware Interactive Attention Network and Large-scale Cloud\n  Recognition Benchmark on FY-4A Satellite Image",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03183",
        "abstract_url": "http://arxiv.org/abs/2401.03183",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Shaobo"
            },
            {
                "last_name": "Milikic",
                "first_name": "Lazar"
            },
            {
                "last_name": "Feng",
                "first_name": "Yiyang"
            },
            {
                "last_name": "Ismayilzada",
                "first_name": "Mete"
            },
            {
                "last_name": "Paul",
                "first_name": "Debjit"
            },
            {
                "last_name": "Bosselut",
                "first_name": "Antoine"
            },
            {
                "last_name": "Faltings",
                "first_name": "Boi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present {\\delta}-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n{\\delta}-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin {\\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by {\\delta}-CAUSAL.\n",
        "title": "{\\delta}-CAUSAL: Exploring Defeasibility in Causal Reasoning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03189",
        "abstract_url": "http://arxiv.org/abs/2401.03189",
        "authors": [
            {
                "last_name": "Santos",
                "first_name": "Herman L. dos"
            },
            {
                "last_name": "Vejling",
                "first_name": "Martin Voigt"
            },
            {
                "last_name": "Abr\u00e3o",
                "first_name": "Taufik"
            },
            {
                "last_name": "Popovski",
                "first_name": "Petar"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "NI",
            ""
        ],
        "abstract": "  Intelligent metasurfaces are one of the favorite technologies for integrating\nsixth-generation (6G) networks, especially the reconfigurable intelligent\nsurface (RIS) that has been extensively researched in various applications. In\nthis context, a feature that deserves further exploration is the frequency\nscattering that occurs when the elements are periodically switched, referred to\nas Space-Time-Coding metasurface (STCM) topology. This type of topology causes\nimpairments to the established communication methods by generating undesirable\ninterference both in frequency and space, which is worsened when using wideband\nsignals. Nevertheless, it has the potential to bring forward useful features\nfor sensing and localization. This work exploits STCM sensing capabilities in\ntarget detection, localization, and classification using narrowband downlink\npilot signals at the base station (BS). The results of this novel approach\nreveal the ability to retrieve a scattering point (SP) localization within the\nsub-centimeter and sub-decimeter accuracy depending on the SP position in\nspace. We also analyze the associated detection and classification\nprobabilities, which show reliable detection performance in the whole analyzed\nenvironment. In contrast, the classification is bounded by physical\nconstraints, and we conclude that this method presents a promising approach for\nfuture integrated sensing and communications (ISAC) protocols by providing a\ntool to perform sensing and localization services using legacy communication\nsignals.\n",
        "title": "Assessing the Potential of Space-Time-Coding Metasurfaces for Sensing\n  and Localization",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03190",
        "abstract_url": "http://arxiv.org/abs/2401.03190",
        "authors": [
            {
                "last_name": "Si",
                "first_name": "Nianwen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weiqiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "CV"
        ],
        "abstract": "  Large language models are known for encoding a vast amount of factual\nknowledge, but they often becomes outdated due to the ever-changing nature of\nexternal information. A promising solution to this challenge is the utilization\nof model editing methods to update the knowledge in an efficient manner.\nHowever, the majority of existing model editing techniques are limited to\nmonolingual frameworks, thus failing to address the crucial issue of\ncross-lingual knowledge synchronization for multilingual models. To tackle this\nproblem, we propose a simple yet effective method that trains multilingual\npatch neuron to store cross-lingual knowledge. It can be easily adapted to\nexisting approaches to enhance their cross-lingual editing capabilities. To\nevaluate our method, we conduct experiments using both the XNLI dataset and a\nself-constructed XFEVER dataset. Experimental results demonstrate that our\nproposed method achieves improved performance in cross-lingual editing tasks\nwithout requiring excessive modifications to the original methodology, thereby\nshowcasing its user-friendly characteristics. Codes will be released soon.\n",
        "title": "MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model\n  Editing",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03191",
        "abstract_url": "http://arxiv.org/abs/2401.03191",
        "authors": [
            {
                "last_name": "Panariello",
                "first_name": "Aniello"
            },
            {
                "last_name": "Mancusi",
                "first_name": "Gianluca"
            },
            {
                "last_name": "Ali",
                "first_name": "Fedy Haj"
            },
            {
                "last_name": "Porrello",
                "first_name": "Angelo"
            },
            {
                "last_name": "Calderara",
                "first_name": "Simone"
            },
            {
                "last_name": "Cucchiara",
                "first_name": "Rita"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate per-object distance estimation is crucial in safety-critical\napplications such as autonomous driving, surveillance, and robotics. Existing\napproaches rely on two scales: local information (i.e., the bounding box\nproportions) or global information, which encodes the semantics of the scene as\nwell as the spatial relations with neighboring objects. However, these\napproaches may struggle with long-range objects and in the presence of strong\nocclusions or unusual visual patterns. In this respect, our work aims to\nstrengthen both local and global cues. Our architecture -- named DistFormer --\nbuilds upon three major components acting jointly: i) a robust context encoder\nextracting fine-grained per-object representations; ii) a masked\nencoder-decoder module exploiting self-supervision to promote the learning of\nuseful per-object features; iii) a global refinement module that aggregates\nobject representations and computes a joint, spatially-consistent estimation.\nTo evaluate the effectiveness of DistFormer, we conduct experiments on the\nstandard KITTI dataset and the large-scale NuScenes and MOTSynth datasets. Such\ndatasets cover various indoor/outdoor environments, changing weather\nconditions, appearances, and camera viewpoints. Our comprehensive analysis\nshows that DistFormer outperforms existing methods. Moreover, we further delve\ninto its generalization capabilities, showing its regularization benefits in\nzero-shot synth-to-real transfer.\n",
        "title": "DistFormer: Enhancing Local and Global Features for Monocular Per-Object\n  Distance Estimation",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03192",
        "abstract_url": "http://arxiv.org/abs/2401.03192",
        "authors": [
            {
                "last_name": "Boull\u00e9",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Colbrook",
                "first_name": "Matthew J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  In this work, we study the convergence of Hermitian Dynamic Mode\nDecomposition (DMD) to the spectral properties of self-adjoint Koopman\noperators. Hermitian DMD is a data-driven method for approximating the Koopman\noperator associated with an unknown nonlinear dynamical system from\ndiscrete-time snapshots, while preserving the self-adjointness of the operator\non its finite-dimensional approximations. We show that, under suitable\nconditions, the eigenvalues and eigenfunctions of HDMD converge to the spectral\nproperties of the underlying Koopman operator. Along the way, we establish a\ngeneral theorem on the convergence of spectral measures, and demonstrate our\nresults numerically on the two-dimensional Schr\\\"odinger equation.\n",
        "title": "On the Convergence of Hermitian Dynamic Mode Decomposition",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03193",
        "abstract_url": "http://arxiv.org/abs/2401.03193",
        "authors": [
            {
                "last_name": "Stoikov",
                "first_name": "Sasha"
            },
            {
                "last_name": "Borzillo",
                "first_name": "Stefano"
            },
            {
                "last_name": "Raub",
                "first_name": "Steffen"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "",
            ""
        ],
        "abstract": "  It has been established in the literature that the number of ratings and the\nscores restaurants obtain on online rating systems (ORS) significantly impact\ntheir revenue. However, when a restaurant has a limited number of ratings, it\nmay be challenging to predict its future performance. It may well be that\nratings reveal more about the user who did the rating than about the quality of\nthe restaurant. This motivates us to segment users into \"inflating raters\", who\ntend to give unusually high ratings, and \"deflating raters\", who tend to give\nunusually low ratings, and compare the rankings generated by these two\npopulations. Using a public dataset provided by Yelp, we find that deflating\nraters are better at predicting restaurants that will achieve a top rating (4.5\nand above) in the future. As such, these deflating raters may have an important\nrole in restaurant discovery.\n",
        "title": "Picky Eaters Make For Better Raters",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03194",
        "abstract_url": "http://arxiv.org/abs/2401.03194",
        "authors": [
            {
                "last_name": "Kong",
                "first_name": "Dexu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Anping"
            },
            {
                "last_name": "Li",
                "first_name": "Yang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Dynamic community detection methods often lack effective mechanisms to ensure\ntemporal consistency, hindering the analysis of network evolution. In this\npaper, we propose a novel deep graph clustering framework with temporal\nconsistency regularization on inter-community structures, inspired by the\nconcept of minimal network topological changes within short intervals.\nSpecifically, to address the representation collapse problem, we first\nintroduce MFC, a matrix factorization-based deep graph clustering algorithm\nthat preserves node embedding. Based on static clustering results, we construct\nprobabilistic community networks and compute their persistence homology, a\nrobust topological measure, to assess structural similarity between them.\nMoreover, a novel neural network regularization TopoReg is introduced to ensure\nthe preservation of topological similarity between inter-community structures\nover time intervals. Our approach enhances temporal consistency and clustering\naccuracy on real-world datasets with both fixed and varying numbers of\ncommunities. It is also a pioneer application of TDA in temporally persistent\ncommunity detection, offering an insightful contribution to field of network\nanalysis. Code and data are available at the public git repository:\nhttps://github.com/kundtx/MFC_TopoReg\n",
        "title": "Learning Persistent Community Structures in Dynamic Networks via\n  Topological Data Analysis",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03195",
        "abstract_url": "http://arxiv.org/abs/2401.03195",
        "authors": [
            {
                "last_name": "Falahati",
                "first_name": "Ali"
            },
            {
                "last_name": "Safavi",
                "first_name": "Mohammad Karim"
            },
            {
                "last_name": "Elahi",
                "first_name": "Ardavan"
            },
            {
                "last_name": "Pakdaman",
                "first_name": "Farhad"
            },
            {
                "last_name": "Gabbouj",
                "first_name": "Moncef"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "CV",
            "LG",
            ""
        ],
        "abstract": "  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n",
        "title": "Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03196",
        "abstract_url": "http://arxiv.org/abs/2401.03196",
        "authors": [
            {
                "last_name": "\u00c7olhak",
                "first_name": "Furkan"
            },
            {
                "last_name": "Ecevit",
                "first_name": "Mert \u0130lhan"
            },
            {
                "last_name": "Da\u011f",
                "first_name": "Hasan"
            },
            {
                "last_name": "Creutzburg",
                "first_name": "Reiner"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  Rising cyber threats, with miscreants registering thousands of new domains\ndaily for Internet-scale attacks like spam, phishing, and drive-by downloads,\nemphasize the need for innovative detection methods. This paper introduces a\ncutting-edge approach for identifying suspicious domains at the onset of the\nregistration process. The accompanying data pipeline generates crucial features\nby comparing new domains to registered domains,emphasizing the crucial\nsimilarity score. Leveraging a novel combination of Natural Language Processing\n(NLP) techniques, including a pretrained Canine model, and Multilayer\nPerceptron (MLP) models, our system analyzes semantic and numerical attributes,\nproviding a robust solution for early threat detection. This integrated\napproach significantly reduces the window of vulnerability, fortifying defenses\nagainst potential threats. The findings demonstrate the effectiveness of the\nintegrated approach and contribute to the ongoing efforts in developing\nproactive strategies to mitigate the risks associated with illicit online\nactivities through the early identification of suspicious domain registrations.\n",
        "title": "SecureReg: A Combined Framework for Proactively Exposing Malicious\n  Domain Name Registrations",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03197",
        "abstract_url": "http://arxiv.org/abs/2401.03197",
        "authors": [
            {
                "last_name": "Pettet",
                "first_name": "Ava"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yunuo"
            },
            {
                "last_name": "Luo",
                "first_name": "Baiting"
            },
            {
                "last_name": "Wray",
                "first_name": "Kyle"
            },
            {
                "last_name": "Baier",
                "first_name": "Hendrik"
            },
            {
                "last_name": "Laszka",
                "first_name": "Aron"
            },
            {
                "last_name": "Dubey",
                "first_name": "Abhishek"
            },
            {
                "last_name": "Mukhopadhyay",
                "first_name": "Ayan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Sequential decision-making under uncertainty is present in many important\nproblems. Two popular approaches for tackling such problems are reinforcement\nlearning and online search (e.g., Monte Carlo tree search). While the former\nlearns a policy by interacting with the environment (typically done before\nexecution), the latter uses a generative model of the environment to sample\npromising action trajectories at decision time. Decision-making is particularly\nchallenging in non-stationary environments, where the environment in which an\nagent operates can change over time. Both approaches have shortcomings in such\nsettings -- on the one hand, policies learned before execution become stale\nwhen the environment changes and relearning takes both time and computational\neffort. Online search, on the other hand, can return sub-optimal actions when\nthere are limitations on allowed runtime. In this paper, we introduce\n\\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines\naction-value estimates from an out-of-date policy with an online search using\nan up-to-date model of the environment. We prove theoretical results showing\nconditions under which PA-MCTS selects the one-step optimal action and also\nbound the error accrued while following PA-MCTS as a policy. We compare and\ncontrast our approach with AlphaZero, another hybrid planning approach, and\nDeep Q Learning on several OpenAI Gym environments. Through extensive\nexperiments, we show that under non-stationary settings with limited time\nconstraints, PA-MCTS outperforms these baselines.\n",
        "title": "Decision Making in Non-Stationary Environments with Policy-Augmented\n  Search",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03198",
        "abstract_url": "http://arxiv.org/abs/2401.03198",
        "authors": [
            {
                "last_name": "Jabari",
                "first_name": "Issam K. O"
            },
            {
                "last_name": "Shofiyah",
                "first_name": ""
            },
            {
                "last_name": "S",
                "first_name": "Pradiptya Kahvi"
            },
            {
                "last_name": "Putriwijaya",
                "first_name": "Novi Nur"
            },
            {
                "last_name": "Yudistira",
                "first_name": "Novanto"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Learning augmented is a machine learning concept built to improve the\nperformance of a method or model, such as enhancing its ability to predict and\ngeneralize data or features, or testing the reliability of the method by\nintroducing noise and other factors. On the other hand, clustering is a\nfundamental aspect of data analysis and has long been used to understand the\nstructure of large datasets. Despite its long history, the k-means algorithm\nstill faces challenges. One approach, as suggested by Ergun et al,is to use a\npredictor to minimize the sum of squared distances between each data point and\na specified centroid. However, it is known that the computational cost of this\nalgorithm increases with the value of k, and it often gets stuck in local\nminima. In response to these challenges, we propose a solution to reduce the\ndimensionality of the dataset using Principal Component Analysis (PCA). It is\nworth noting that when using k values of 10 and 25, the proposed algorithm\nyields lower cost results compared to running it without PCA. \"Principal\ncomponent analysis (PCA) is the problem of fitting a low-dimensional affine\nsubspace to a set of data points in a high-dimensional space. PCA is\nwell-established in the literature and has become one of the most useful tools\nfor data modeling, compression, and visualization.\"\n",
        "title": "Learning-Augmented K-Means Clustering Using Dimensional Reduction",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03200",
        "abstract_url": "http://arxiv.org/abs/2401.03200",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Hiroyasu"
            },
            {
                "last_name": "Souma",
                "first_name": "Wataru"
            },
            {
                "last_name": "Fujiwara",
                "first_name": "Yoshi"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            ""
        ],
        "abstract": "  The novel coronavirus SARS-CoV-2, commonly referred to as COVID-19, triggered\nthe global pandemic. Although the nature of the international spread of\ninfection is an important issue, extracting diffusion networks from\nobservations is challenging because of its inherent complexity. In this paper,\nwe investigate the process of infection worldwide, including time delays, based\non global infection case data collected from January 3, 2020 to December 31,\n2022. We approach the data with a complex Hilbert principal component analysis,\nwhich can consider not only the concurrent relationships between elements, but\nalso the leading and lagging relationships. Then, we examine the interactions\namong countries by considering six factors: geography, population, GDP,\nstringency of countermeasures, vaccination rates, and government type. The\nresults show two primary trends occurring in 2020 and in 2021-2022 and they\ninterchange with each other. Specifically, European, highly populated, and\ndemocratic countries, i.e., countries with high mobility rates, show leading\ntrends in 2020. In contrast, African and nondemocratic countries show leading\ntrends in 2021-2022, followed by countries with high vaccination rates and\nadvanced countermeasures. The results reveal that, although factors that\nincrease infection risk lead to certain trends at the beginning of the\npandemic, these trends dynamically changes over time due to socioeconomic\nfactors, especially the introduction of countermeasures. The findings suggest\nthat international efforts to promote countermeasures in developing countries\ncan contribute to pandemic containment.\n",
        "title": "The Coexistence of Infection Spread Patterns in the Global Dynamics of\n  COVID-19 Dissemination",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03201",
        "abstract_url": "http://arxiv.org/abs/2401.03201",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zeju"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Ren",
                "first_name": "Ruilong"
            },
            {
                "last_name": "Xu",
                "first_name": "Yifan"
            },
            {
                "last_name": "Ma",
                "first_name": "Ruifei"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiangde"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  The remarkable potential of multi-modal large language models (MLLMs) in\ncomprehending both vision and language information has been widely\nacknowledged. However, the scarcity of 3D scenes-language pairs in comparison\nto their 2D counterparts, coupled with the inadequacy of existing approaches in\nunderstanding of 3D scenes by LLMs, poses a significant challenge. In response,\nwe collect and construct an extensive dataset comprising 75K\ninstruction-response pairs tailored for 3D scenes. This dataset addresses tasks\nrelated to 3D VQA, 3D grounding, and 3D conversation. To further enhance the\nintegration of 3D spatial information into LLMs, we introduce a novel and\nefficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment\nstage between 3D scenes and language and extends the instruction prompt with\nthe 3D modality information including the entire scene and segmented objects.\nWe evaluate the effectiveness of our method across diverse tasks in the 3D\nscene domain and find that our approach serves as a strategic means to enrich\nLLMs' comprehension of the 3D world. Our code is available at\nhttps://github.com/staymylove/3DMIT.\n",
        "title": "3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03203",
        "abstract_url": "http://arxiv.org/abs/2401.03203",
        "authors": [
            {
                "last_name": "Hua",
                "first_name": "Tongyan"
            },
            {
                "last_name": "Bai",
                "first_name": "Haotian"
            },
            {
                "last_name": "Cao",
                "first_name": "Zidong"
            },
            {
                "last_name": "Liu",
                "first_name": "Ming"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we introduce Hi-Map, a novel monocular dense mapping approach\nbased on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to\nachieve efficient and high-fidelity mapping using only posed RGB inputs. Our\nmethod eliminates the need for external depth priors derived from e.g., a depth\nestimation model. Our key idea is to represent the scene as a hierarchical\nfeature grid that encodes the radiance and then factorizes it into feature\nplanes and vectors. As such, the scene representation becomes simpler and more\ngeneralizable for fast and smooth convergence on new observations. This allows\nfor efficient computation while alleviating noise patterns by reducing the\ncomplexity of the scene representation. Buttressed by the hierarchical\nfactorized representation, we leverage the Sign Distance Field (SDF) as a proxy\nof rendering for inferring the volume density, demonstrating high mapping\nfidelity. Moreover, we introduce a dual-path encoding strategy to strengthen\nthe photometric cues and further boost the mapping quality, especially for the\ndistant and textureless regions. Extensive experiments demonstrate our method's\nsuperiority in geometric and textural accuracy over the state-of-the-art\nNeRF-based monocular mapping methods.\n",
        "title": "Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity\n  Monocular Dense Mapping",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03204",
        "abstract_url": "http://arxiv.org/abs/2401.03204",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Feifei"
            },
            {
                "last_name": "Ke",
                "first_name": "Pinhui"
            },
            {
                "last_name": "Xiao",
                "first_name": "Lingmei"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Recently, Jiang et al. proposed several new classes of quaternary sequences\nwith low autocorrelation and high linear complexity by using the inverse Gray\nmapping (JAMC, \\textbf{69} (2023): 689--706). In this paper, we estimate the\n4-adic complexity of these quaternary sequences. Our results show that these\nsequences have large 4-adic complexity to resist the attack of the rational\napproximation algorithm.\n",
        "title": "The 4-adic complexity of quaternary sequences with low autocorrelation\n  and high linear complexity",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03205",
        "abstract_url": "http://arxiv.org/abs/2401.03205",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Junyi"
            },
            {
                "last_name": "Chen",
                "first_name": "Jie"
            },
            {
                "last_name": "Ren",
                "first_name": "Ruiyang"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xiaoxue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Nie",
                "first_name": "Jian-Yun"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n",
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination\n  in Large Language Models",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03206",
        "abstract_url": "http://arxiv.org/abs/2401.03206",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Siwei"
            },
            {
                "last_name": "Ma",
                "first_name": "Ke"
            },
            {
                "last_name": "Goetz",
                "first_name": "Stephan M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  We propose a new method to improve the convergence speed of the Robbins-Monro\nalgorithm by introducing prior information about the target point into the\nRobbins-Monro iteration. We achieve the incorporation of prior information\nwithout the need of a -- potentially wrong -- regression model, which would\nalso entail additional constraints. We show that this prior-information\nRobbins-Monro sequence is convergent for a wide range of prior distributions,\neven wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel\ndensity estimate, as well as bounded arbitrary distribution functions greater\nthan zero. We furthermore analyse the sequence numerically to understand its\nperformance and the influence of parameters. The results demonstrate that the\nprior-information Robbins-Monro sequence converges faster than the standard\none, especially during the first steps, which are particularly important for\napplications where the number of function measurements is limited, and when the\nnoise of observing the underlying function is large. We finally propose a rule\nto select the parameters of the sequence.\n",
        "title": "A Robbins--Monro Sequence That Can Exploit Prior Information For Faster\n  Convergence",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03214",
        "abstract_url": "http://arxiv.org/abs/2401.03214",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Ruofeng"
            },
            {
                "last_name": "Li",
                "first_name": "Xiangyuan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bo"
            },
            {
                "last_name": "Li",
                "first_name": "Shuai"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Self-supervised learning (SSL) has empirically shown its data representation\nlearnability in many downstream tasks. There are only a few theoretical works\non data representation learnability, and many of those focus on final data\nrepresentation, treating the nonlinear neural network as a ``black box\".\nHowever, the accurate learning results of neural networks are crucial for\ndescribing the data distribution features learned by SSL models. Our paper is\nthe first to analyze the learning results of the nonlinear SSL model\naccurately. We consider a toy data distribution that contains two features: the\nlabel-related feature and the hidden feature. Unlike previous linear setting\nwork that depends on closed-form solutions, we use the gradient descent\nalgorithm to train a 1-layer nonlinear SSL model with a certain initialization\nregion and prove that the model converges to a local minimum. Furthermore,\ndifferent from the complex iterative analysis, we propose a new analysis\nprocess which uses the exact version of Inverse Function Theorem to accurately\ndescribe the features learned by the local minimum. With this local minimum, we\nprove that the nonlinear SSL model can capture the label-related feature and\nhidden feature at the same time. In contrast, the nonlinear supervised learning\n(SL) model can only learn the label-related feature. We also present the\nlearning processes and results of the nonlinear SSL and SL model via simulation\nexperiments.\n",
        "title": "Understanding Representation Learnability of Nonlinear Self-Supervised\n  Learning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03215",
        "abstract_url": "http://arxiv.org/abs/2401.03215",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Yujing"
            },
            {
                "last_name": "Ma",
                "first_name": "Xingjun"
            },
            {
                "last_name": "Erfani",
                "first_name": "Sarah Monazam"
            },
            {
                "last_name": "Li",
                "first_name": "Yige"
            },
            {
                "last_name": "Bailey",
                "first_name": "James"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Backdoor attacks present a substantial security concern for deep learning\nmodels, especially those utilized in applications critical to safety and\nsecurity. These attacks manipulate model behavior by embedding a hidden trigger\nduring the training phase, allowing unauthorized control over the model's\noutput during inference time. Although numerous defenses exist for image\nclassification models, there is a conspicuous absence of defenses tailored for\ntime series data, as well as an end-to-end solution capable of training clean\nmodels on poisoned data. To address this gap, this paper builds upon\nAnti-Backdoor Learning (ABL) and introduces an innovative method, End-to-End\nAnti-Backdoor Learning (E2ABL), for robust training against backdoor attacks.\nUnlike the original ABL, which employs a two-stage training procedure, E2ABL\naccomplishes end-to-end training through an additional classification head\nlinked to the shallow layers of a Deep Neural Network (DNN). This secondary\nhead actively identifies potential backdoor triggers, allowing the model to\ndynamically cleanse these samples and their corresponding labels during\ntraining. Our experiments reveal that E2ABL significantly improves on existing\ndefenses and is effective against a broad range of backdoor attacks in both\nimage and time series domains.\n",
        "title": "End-to-End Anti-Backdoor Learning on Images and Time Series",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03217",
        "abstract_url": "http://arxiv.org/abs/2401.03217",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Callie Y."
            },
            {
                "last_name": "Lee",
                "first_name": "Christine P."
            },
            {
                "last_name": "Mutlu",
                "first_name": "Bilge"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Large-language models (LLMs) hold significant promise in improving\nhuman-robot interaction, offering advanced conversational skills and\nversatility in managing diverse, open-ended user requests in various tasks and\ndomains. Despite the potential to transform human-robot interaction, very\nlittle is known about the distinctive design requirements for utilizing LLMs in\nrobots, which may differ from text and voice interaction and vary by task and\ncontext. To better understand these requirements, we conducted a user study (n\n= 32) comparing an LLM-powered social robot against text- and voice-based\nagents, analyzing task-based requirements in conversational tasks, including\nchoose, generate, execute, and negotiate. Our findings show that LLM-powered\nrobots elevate expectations for sophisticated non-verbal cues and excel in\nconnection-building and deliberation, but fall short in logical communication\nand may induce anxiety. We provide design implications both for robots\nintegrating LLMs and for fine-tuning LLMs for use with robots.\n",
        "title": "Understanding Large-Language Model (LLM)-powered Human-Robot Interaction",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03218",
        "abstract_url": "http://arxiv.org/abs/2401.03218",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shenao"
            },
            {
                "last_name": "Li",
                "first_name": "Yuekang"
            },
            {
                "last_name": "Wang",
                "first_name": "Kailong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Chao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yanjie"
            },
            {
                "last_name": "Deng",
                "first_name": "Gelei"
            },
            {
                "last_name": "Shi",
                "first_name": "Ling"
            },
            {
                "last_name": "Li",
                "first_name": "Hui"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoyu"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  The advent of MiniApps, operating within larger SuperApps, has revolutionized\nuser experiences by offering a wide range of services without the need for\nindividual app downloads. However, this convenience has raised significant\nprivacy concerns, as these MiniApps often require access to sensitive data,\npotentially leading to privacy violations. Our research addresses the critical\ngaps in the analysis of MiniApps' privacy practices, especially focusing on\nWeChat MiniApps in the Android ecosystem. Despite existing privacy regulations\nand platform guidelines, there is a lack of effective mechanisms to safeguard\nuser privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis\napproach, specifically designed for the MiniApp environment. This approach\novercomes the limitations of existing static analysis techniques by\nincorporating dynamic UI exploration for complete code coverage and accurate\nprivacy practice identification. Our methodology includes modeling UI\ntransition states, resolving cross-package callback control flows, and\nautomated iterative UI exploration. This allows for a comprehensive\nunderstanding of MiniApps' privacy practices, addressing the unique challenges\nof sub-package loading and event-driven callbacks. Our empirical evaluation of\nover 120K MiniApps using MiniScope demonstrates its effectiveness in\nidentifying privacy inconsistencies. The results reveal significant issues,\nwith 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data\ncollection. These findings emphasize the urgent need for more precise privacy\nmonitoring systems and highlight the responsibility of SuperApp operators to\nenforce stricter privacy measures.\n",
        "title": "MiniScope: Automated UI Exploration and Privacy Inconsistency Detection\n  of MiniApps via Two-phase Iterative Hybrid Analysis",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03220",
        "abstract_url": "http://arxiv.org/abs/2401.03220",
        "authors": [
            {
                "last_name": "Souza",
                "first_name": "Matheus"
            },
            {
                "last_name": "Heidrich",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image signal processors (ISPs) are historically grown legacy software systems\nfor reconstructing color images from noisy raw sensor measurements. Each\nsmartphone manufacturer has developed its ISPs with its own characteristic\nheuristics for improving the color rendition, for example, skin tones and other\nvisually essential colors. The recent interest in replacing the historically\ngrown ISP systems with deep-learned pipelines to match DSLR's image quality\nimproves structural features in the image. However, these works ignore the\nsuperior color processing based on semantic scene analysis that distinguishes\nmobile phone ISPs from DSLRs. Here, we present MetaISP, a single model designed\nto learn how to translate between the color and local contrast characteristics\nof different devices. MetaISP takes the RAW image from device A as input and\ntranslates it to RGB images that inherit the appearance characteristics of\ndevices A, B, and C. We achieve this result by employing a lightweight deep\nlearning technique that conditions its output appearance based on the device of\ninterest. In this approach, we leverage novel attention mechanisms inspired by\ncross-covariance to learn global scene semantics. Additionally, we use the\nmetadata that typically accompanies RAW images and estimate scene illuminants\nwhen they are unavailable.\n",
        "title": "MetaISP -- Exploiting Global Scene Structure for Accurate Multi-Device\n  Color Rendition",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03221",
        "abstract_url": "http://arxiv.org/abs/2401.03221",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Yupei"
            },
            {
                "last_name": "Xian",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Shi",
                "first_name": "Yukai"
            },
            {
                "last_name": "Lin",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recently, text-to-image diffusion models become a new paradigm in image\nprocessing fields, including content generation, image restoration and\nimage-to-image translation. Given a target prompt, Denoising Diffusion\nProbabilistic Models (DDPM) are able to generate realistic yet eligible images.\nWith this appealing property, the image translation task has the potential to\nbe free from target image samples for supervision. By using a target text\nprompt for domain adaption, the diffusion model is able to implement zero-shot\nimage-to-image translation advantageously. However, the sampling and inversion\nprocesses of DDPM are stochastic, and thus the inversion process often fail to\nreconstruct the input content. Specifically, the displacement effect will\ngradually accumulated during the diffusion and inversion processes, which led\nto the reconstructed results deviating from the source domain. To make\nreconstruction explicit, we propose a prompt redescription strategy to realize\na mirror effect between the source and reconstructed image in the diffusion\nmodel (MirrorDiffusion). More specifically, a prompt redescription mechanism is\ninvestigated to align the text prompts with latent code at each time step of\nthe Denoising Diffusion Implicit Models (DDIM) inversion to pursue a\nstructure-preserving reconstruction. With the revised DDIM inversion,\nMirrorDiffusion is able to realize accurate zero-shot image translation by\nediting optimized text prompts and latent code. Extensive experiments\ndemonstrate that MirrorDiffusion achieves superior performance over the\nstate-of-the-art methods on zero-shot image translation benchmarks by clear\nmargins and practical model stability.\n",
        "title": "MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image\n  Translation by Prompts Redescription and Beyond",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03223",
        "abstract_url": "http://arxiv.org/abs/2401.03223",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Wei"
            },
            {
                "last_name": "Gao",
                "first_name": "Zaifeng"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Insights: - The human-centered AI (HCAI) approach and the sociotechnical\nsystems (STS) theory share the same goal: ensuring that new technologies such\nas AI best serve humans in a sociotechnical environment. - HCAI practice needs\nto fully embrace sociotechnical systems thinking, while traditional STS needs\nto evolve to address the emerging characteristics of AI technology. - We\npropose a conceptual framework for intelligent sociotechnical systems (iSTS) to\nenhance traditional STS theory in the AI era. - Based on iSTS, we further\npropose a sociotechnical-based hierarchical HCAI approach as a paradigmatic\nextension to existing HCAI practice, further advancing HCAI practice.\n",
        "title": "An intelligent sociotechnical systems (iSTS) framework: Toward a\n  sociotechnically-based hierarchical human-centered AI approach",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03226",
        "abstract_url": "http://arxiv.org/abs/2401.03226",
        "authors": [
            {
                "last_name": "Di Giacomo",
                "first_name": "Emilio"
            },
            {
                "last_name": "F\u00f6rster",
                "first_name": "Henry"
            },
            {
                "last_name": "Kokhovich",
                "first_name": "Daria"
            },
            {
                "last_name": "Mchedlidze",
                "first_name": "Tamara"
            },
            {
                "last_name": "Montecchiani",
                "first_name": "Fabrizio"
            },
            {
                "last_name": "Symvonis",
                "first_name": "Antonios"
            },
            {
                "last_name": "Villedieu",
                "first_name": "Ana\u00efs"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  We study the upward point-set embeddability of digraphs on one-sided convex\npoint sets with at most 1 bend per edge. We provide an algorithm to compute a\n1-bend upward point-set embedding of outerplanar $st$-digraphs on arbitrary\none-sided convex point sets. We complement this result by proving that for\nevery $n \\geq 18$ there exists a $2$-outerplanar $st$-digraph $G$ with $n$\nvertices and a one-sided convex point set $S$ so that $G$ does not admit a\n1-bend upward point-set embedding on $S$.\n",
        "title": "On 1-bend Upward Point-set Embeddings of $st$-digraphs",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03228",
        "abstract_url": "http://arxiv.org/abs/2401.03228",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Wei"
            },
            {
                "last_name": "Chen",
                "first_name": "Yu"
            },
            {
                "last_name": "Yang",
                "first_name": "Nicole Tianjiao"
            },
            {
                "last_name": "Du",
                "first_name": "Hengrong"
            },
            {
                "last_name": "Feng",
                "first_name": "Qi"
            },
            {
                "last_name": "Chen",
                "first_name": "Ricky T. Q."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Diffusion models have become the go-to method for large-scale generative\nmodels in real-world applications. These applications often involve data\ndistributions confined within bounded domains, typically requiring ad-hoc\nthresholding techniques for boundary enforcement. Reflected diffusion models\n(Lou23) aim to enhance generalizability by generating the data distribution\nthrough a backward process governed by reflected Brownian motion. However,\nreflected diffusion models may not easily adapt to diverse domains without the\nderivation of proper diffeomorphic mappings and do not guarantee optimal\ntransport properties. To overcome these limitations, we introduce the Reflected\nSchrodinger Bridge algorithm: an entropy-regularized optimal transport approach\ntailored for generating data within diverse bounded domains. We derive elegant\nreflected forward-backward stochastic differential equations with Neumann and\nRobin boundary conditions, extend divergence-based likelihood training to\nbounded domains, and explore natural connections to entropic optimal transport\nfor the study of approximate linear convergence - a valuable insight for\npractical training. Our algorithm yields robust generative modeling in diverse\ndomains, and its scalability is demonstrated in real-world constrained\ngenerative modeling through standard image benchmarks.\n",
        "title": "Reflected Schr\\\"odinger Bridge for Constrained Generative Modeling",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03229",
        "abstract_url": "http://arxiv.org/abs/2401.03229",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Wansen"
            },
            {
                "last_name": "Yang",
                "first_name": "Weiyi"
            },
            {
                "last_name": "Li",
                "first_name": "Juanjuan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zhengqiu"
            },
            {
                "last_name": "Chen",
                "first_name": "Bin"
            },
            {
                "last_name": "Qiu",
                "first_name": "Sihang"
            },
            {
                "last_name": "Peng",
                "first_name": "Yong"
            },
            {
                "last_name": "Wang",
                "first_name": "Fei-Yue"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The precise characterization and modeling of Cyber-Physical-Social Systems\n(CPSS) requires more comprehensive and accurate data, which imposes heightened\ndemands on intelligent sensing capabilities. To address this issue,\nCrowdsensing Intelligence (CSI) has been proposed to collect data from CPSS by\nharnessing the collective intelligence of a diverse workforce. Our first and\nsecond Distributed/Decentralized Hybrid Workshop on Crowdsensing Intelligence\n(DHW-CSI) have focused on principles and high-level processes of organizing and\noperating CSI, as well as the participants, methods, and stages involved in\nCSI. This letter reports the outcomes of the latest DHW-CSI, focusing on\nAutonomous Crowdsensing (ACS) enabled by a range of technologies such as\ndecentralized autonomous organizations and operations, large language models,\nand human-oriented operating systems. Specifically, we explain what ACS is and\nexplore its distinctive features in comparison to traditional crowdsensing.\nMoreover, we present the ``6A-goal\" of ACS and propose potential avenues for\nfuture research.\n",
        "title": "Autonomous Crowdsensing: Operating and Organizing Crowdsensing for\n  Sensing Automation",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03230",
        "abstract_url": "http://arxiv.org/abs/2401.03230",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jianqing"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Hua",
                "first_name": "Yang"
            },
            {
                "last_name": "Cao",
                "first_name": "Jian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "DC"
        ],
        "abstract": "  Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due\nto its ability to support heterogeneous models and data. To reduce the high\ncommunication cost of transmitting model parameters, a major challenge in HtFL,\nprototype-based HtFL methods are proposed to solely share class\nrepresentatives, a.k.a, prototypes, among heterogeneous clients while\nmaintaining the privacy of clients' models. However, these prototypes are\nnaively aggregated into global prototypes on the server using weighted\naveraging, resulting in suboptimal global knowledge which negatively impacts\nthe performance of clients. To overcome this challenge, we introduce a novel\nHtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced\nContrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the\nserver. By incorporating ACL, our approach enhances prototype separability\nwhile preserving semantic meaning. Extensive experiments with twelve\nheterogeneous models demonstrate that our FedTGP surpasses state-of-the-art\nmethods by up to 9.08% in accuracy while maintaining the communication and\nprivacy advantages of prototype-based HtFL. Our code is available at\nhttps://github.com/TsingZ0/FedTGP.\n",
        "title": "FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced\n  Contrastive Learning for Data and Model Heterogeneity in Federated Learning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03231",
        "abstract_url": "http://arxiv.org/abs/2401.03231",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Seongbeom"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  Many countries around the world, including Korea, use the school choice\nlottery system. However, this method has a problem in that many students are\nassigned to less-preferred schools based on the lottery results. In addition,\nthe task of finding a good assignment with ties often has a time complexity of\nNP, making it a very difficult problem to improve the quality of the\nassignment.\n  In this paper, we prove that the problem of finding a stable matching that\nmaximizes the student-oriented preference utility in a two-sided market with\none-sided preference can be solved in polynomial time, and we verify through\nexperiments that the quality of assignment is improved. The main contributions\nof this paper are as follows. We found that stable student-oriented allocation\nin a two-sided market with one-sided preferences is the same as stable\nallocation in a two-sided market with symmetric preferences. In addition, we\ndefined a method to quantify the quality of allocation from a preference\nutilitarian perspective. Based on the above two, it was proven that the problem\nof finding a stable match that maximizes the preference utility in a two-sided\nmarket with homogeneous preferences can be reduced to an allocation problem. In\nthis paper, through an experiment, we quantitatively verified that optimal\nstudent assignment assigns more students to schools of higher preference, even\nin situations where many students are assigned to schools of low preference\nusing the existing assignment method.\n",
        "title": "Stable Marriage with One-Sided Preference",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03232",
        "abstract_url": "http://arxiv.org/abs/2401.03232",
        "authors": [
            {
                "last_name": "Vrahatis",
                "first_name": "Michael N."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CG",
            "",
            ""
        ],
        "abstract": "  The Apollonius theorem gives the length of a median of a triangle in terms of\nthe lengths of its sides. The straightforward generalization of this theorem\nobtained for m-simplices in the n-dimensional Euclidean space for n greater\nthan or equal to m is given. Based on this, generalizations of properties\nrelated to the medians of a triangle are presented. In addition, applications\nof the generalized Apollonius' theorem and the related to the medians results,\nare given for obtaining: (a) the minimal spherical surface that encloses a\ngiven simplex or a given bounded set, (b) the thickness of a simplex that it\nprovides a measure for the quality or how well shaped a simplex is, and (c) the\nconvergence and error estimates of the root-finding bisection method applied on\nsimplices.\n",
        "title": "Generalization of the Apollonius theorem for simplices and related\n  problems",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03233",
        "abstract_url": "http://arxiv.org/abs/2401.03233",
        "authors": [
            {
                "last_name": "Marinova",
                "first_name": "Matea"
            },
            {
                "last_name": "Denkovski",
                "first_name": "Daniel"
            },
            {
                "last_name": "Gjoreski",
                "first_name": "Hristijan"
            },
            {
                "last_name": "Hadzi-Velkov",
                "first_name": "Zoran"
            },
            {
                "last_name": "Rakovic",
                "first_name": "Valentin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Split Learning (SL) is a promising Distributed Learning approach in\nelectromyography (EMG) based prosthetic control, due to its applicability\nwithin resource-constrained environments. Other learning approaches, such as\nDeep Learning and Federated Learning (FL), provide suboptimal solutions, since\nprosthetic devices are extremely limited in terms of processing power and\nbattery life. The viability of implementing SL in such scenarios is caused by\nits inherent model partitioning, with clients executing the smaller model\nsegment. However, selecting an inadequate cut layer hinders the training\nprocess in SL systems. This paper presents an algorithm for optimal cut layer\nselection in terms of maximizing the convergence rate of the model. The\nperformance evaluation demonstrates that the proposed algorithm substantially\naccelerates the convergence in an EMG pattern recognition task for improving\nprosthetic device control.\n",
        "title": "Convergence Rate Maximization for Split Learning-based Control of EMG\n  Prosthetic Devices",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03235",
        "abstract_url": "http://arxiv.org/abs/2401.03235",
        "authors": [
            {
                "last_name": "Thomasian",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "OS",
            "PF"
        ],
        "abstract": "  RAID proposal advocated replacing large disks with arrays of PC disks, but as\nthe capacity of small disks increased 100-fold in 1990s the production of large\ndisks was discontinued. Storage dependability is increased via replication or\nerasure coding. Cloud storage providers store multiple copies of data obviating\nfor need for further redundancy. Varitaions of RAID based on local recovery\ncodes, partial MDS reduce recovery cost. NAND flash Solid State Disks - SSDs\nhave low latency and high bandwidth, are more reliable, consume less power and\nhave a lower TCO than Hard Disk Drives, which are more viable for hyperscalers.\n",
        "title": "RAID Organizations for Improved Reliability and Performance: A Not\n  Entirely Unbiased Tutorial (1st revision)",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03236",
        "abstract_url": "http://arxiv.org/abs/2401.03236",
        "authors": [
            {
                "last_name": "Kujanp\u00e4\u00e4",
                "first_name": "Kalle"
            },
            {
                "last_name": "Baimukashev",
                "first_name": "Daulet"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shibei"
            },
            {
                "last_name": "Azam",
                "first_name": "Shoaib"
            },
            {
                "last_name": "Munir",
                "first_name": "Farzeen"
            },
            {
                "last_name": "Alcan",
                "first_name": "Gokhan"
            },
            {
                "last_name": "Kyrki",
                "first_name": "Ville"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Building simulation environments for developing and testing autonomous\nvehicles necessitates that the simulators accurately model the statistical\nrealism of the real-world environment, including the interaction with other\nvehicles driven by human drivers. To address this requirement, an accurate\nhuman behavior model is essential to incorporate the diversity and consistency\nof human driving behavior. We propose a mathematical framework for designing a\ndata-driven simulation model that simulates human driving behavior more\nrealistically than the currently used physics-based simulation models.\nExperiments conducted using the NGSIM dataset validate our hypothesis regarding\nthe necessity of considering the complexity, diversity, and consistency of\nhuman driving behavior when aiming to develop realistic simulators.\n",
        "title": "Challenges of Data-Driven Simulation of Diverse and Consistent Human\n  Driving Behaviors",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03238",
        "abstract_url": "http://arxiv.org/abs/2401.03238",
        "authors": [
            {
                "last_name": "Kakarla",
                "first_name": "Sanjit"
            },
            {
                "last_name": "Thomas",
                "first_name": "Danielle"
            },
            {
                "last_name": "Lin",
                "first_name": "Jionghao"
            },
            {
                "last_name": "Gupta",
                "first_name": "Shivang"
            },
            {
                "last_name": "Koedinger",
                "first_name": "Kenneth R."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CY"
        ],
        "abstract": "  Research suggests that tutors should adopt a strategic approach when\naddressing math errors made by low-efficacy students. Rather than drawing\ndirect attention to the error, tutors should guide the students to identify and\ncorrect their mistakes on their own. While tutor lessons have introduced this\npedagogical skill, human evaluation of tutors applying this strategy is arduous\nand time-consuming. Large language models (LLMs) show promise in providing\nreal-time assessment to tutors during their actual tutoring sessions, yet\nlittle is known regarding their accuracy in this context. In this study, we\ninvestigate the capacity of generative AI to evaluate real-life tutors'\nperformance in responding to students making math errors. By analyzing 50\nreal-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate\nproficiency in assessing the criteria related to reacting to students making\nerrors. However, both models exhibit limitations in recognizing instances where\nthe student made an error. Notably, GPT-4 tends to overidentify instances of\nstudents making errors, often attributing student uncertainty or inferring\npotential errors where human evaluators did not. Future work will focus on\nenhancing generalizability by assessing a larger dataset of dialogues and\nevaluating learning transfer. Specifically, we will analyze the performance of\ntutors in real-life scenarios when responding to students' math errors before\nand after lesson completion on this crucial tutoring skill.\n",
        "title": "Using Large Language Models to Assess Tutors' Performance in Reacting to\n  Students Making Math Errors",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03239",
        "abstract_url": "http://arxiv.org/abs/2401.03239",
        "authors": [
            {
                "last_name": "De Paoli",
                "first_name": "Stefano"
            },
            {
                "last_name": "Mathis",
                "first_name": "Walter Stan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY"
        ],
        "abstract": "  This paper presents a set of reflections on saturation and the use of Large\nLanguage Models (LLMs) for performing Thematic Analysis (TA). The paper\nsuggests that initial thematic saturation (ITS) could be used as a metric to\nassess part of the transactional validity of TA with LLM, focusing on the\ninitial coding. The paper presents the initial coding of two datasets of\ndifferent sizes, and it reflects on how the LLM reaches some form of analytical\nsaturation during the coding. The procedure proposed in this work leads to the\ncreation of two codebooks, one comprising the total cumulative initial codes\nand the other the total unique codes. The paper proposes a metric to\nsynthetically measure ITS using a simple mathematical calculation employing the\nratio between slopes of cumulative codes and unique codes. The paper\ncontributes to the initial body of work exploring how to perform qualitative\nanalysis with LLMs.\n",
        "title": "Reflections on Inductive Thematic Saturation as a potential metric for\n  measuring the validity of an inductive Thematic Analysis with LLMs",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03240",
        "abstract_url": "http://arxiv.org/abs/2401.03240",
        "authors": [
            {
                "last_name": "Suh",
                "first_name": "Min-Kook"
            },
            {
                "last_name": "Seo",
                "first_name": "Seung-Woo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  We address the challenge of estimating the learning rate for adaptive\ngradient methods used in training deep neural networks. While several\nlearning-rate-free approaches have been proposed, they are typically tailored\nfor steepest descent. However, although steepest descent methods offer an\nintuitive approach to finding minima, many deep learning applications require\nadaptive gradient methods to achieve faster convergence. In this paper, we\ninterpret adaptive gradient methods as steepest descent applied on\nparameter-scaled networks, proposing learning-rate-free adaptive gradient\nmethods. Experimental results verify the effectiveness of this approach,\ndemonstrating comparable performance to hand-tuned learning rates across\nvarious scenarios. This work extends the applicability of learning-rate-free\nmethods, enhancing training with adaptive gradient methods.\n",
        "title": "Interpreting Adaptive Gradient Methods by Parameter Scaling for\n  Learning-Rate-Free Optimization",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03244",
        "abstract_url": "http://arxiv.org/abs/2401.03244",
        "authors": [
            {
                "last_name": "Fan",
                "first_name": "Zhenan"
            },
            {
                "last_name": "Ghaddar",
                "first_name": "Bissan"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinglu"
            },
            {
                "last_name": "Xing",
                "first_name": "Linzi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zirui"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The rapid advancement of artificial intelligence (AI) techniques has opened\nup new opportunities to revolutionize various fields, including operations\nresearch (OR). This survey paper explores the integration of AI within the OR\nprocess (AI4OR) to enhance its effectiveness and efficiency across multiple\nstages, such as parameter generation, model formulation, and model\noptimization. By providing a comprehensive overview of the state-of-the-art and\nexamining the potential of AI to transform OR, this paper aims to inspire\nfurther research and innovation in the development of AI-enhanced OR methods\nand tools. The synergy between AI and OR is poised to drive significant\nadvancements and novel solutions in a multitude of domains, ultimately leading\nto more effective and efficient decision-making.\n",
        "title": "Artificial Intelligence for Operations Research: Revolutionizing the\n  Operations Research Process",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03245",
        "abstract_url": "http://arxiv.org/abs/2401.03245",
        "authors": [
            {
                "last_name": "Alasseur",
                "first_name": "Cl\u00e9mence"
            },
            {
                "last_name": "Bensaid",
                "first_name": "Zakaria"
            },
            {
                "last_name": "Dumitrescu",
                "first_name": "Roxana"
            },
            {
                "last_name": "Warin",
                "first_name": "Xavier"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  In this paper, we introduce various machine learning solvers for (coupled)\nforward-backward systems of stochastic differential equations (FBSDEs) driven\nby a Brownian motion and a Poisson random measure. We provide a rigorous\ncomparison of the different algorithms and demonstrate their effectiveness in\nvarious applications, such as cases derived from pricing with jumps and\nmean-field games. In particular, we show the efficiency of the deep-learning\nalgorithms to solve a coupled multi-dimensional FBSDE system driven by a\ntime-inhomogeneous jump process with stochastic intensity, which describes the\nNash equilibria for a specific mean-field game (MFG) problem for which we also\nprovide the complete theoretical resolution. More precisely, we develop an\nextension of the MFG model for smart grids introduced in Alasseur, Campi,\nDumitrescu and Zeng (Annals of Operations Research, 2023) to the case when the\nrandom jump times correspond to the jump times of a doubly Poisson process. We\nfirst provide an existence result of an equilibria and derive its semi-explicit\ncharacterization in terms of a system of FBSDEs in the linear-quadratic\nsetting. We then compare the MFG solution to the optimal strategy of a central\nplanner and provide several numerical illustrations using the deep-learning\nsolvers presented in the first part of the paper.\n",
        "title": "Deep learning algorithms for FBSDEs with jumps: Applications to option\n  pricing and a MFG model for smart grids",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03246",
        "abstract_url": "http://arxiv.org/abs/2401.03246",
        "authors": [
            {
                "last_name": "Udovichenko",
                "first_name": "Igor"
            },
            {
                "last_name": "Shvetsov",
                "first_name": "Egor"
            },
            {
                "last_name": "Divitsky",
                "first_name": "Denis"
            },
            {
                "last_name": "Osin",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Trofimov",
                "first_name": "Ilya"
            },
            {
                "last_name": "Glushenko",
                "first_name": "Anatoly"
            },
            {
                "last_name": "Sukharev",
                "first_name": "Ivan"
            },
            {
                "last_name": "Berestenev",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Burnaev",
                "first_name": "Evgeny"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Neural Architecture Search (NAS) methods are widely used in various\nindustries to obtain high quality taskspecific solutions with minimal human\nintervention. Event Sequences find widespread use in various industrial\napplications including churn prediction customer segmentation fraud detection\nand fault diagnosis among others. Such data consist of categorical and\nreal-valued components with irregular timestamps. Despite the usefulness of NAS\nmethods previous approaches only have been applied to other domains images\ntexts or time series. Our work addresses this limitation by introducing a novel\nNAS algorithm SeqNAS specifically designed for event sequence classification.\nWe develop a simple yet expressive search space that leverages commonly used\nbuilding blocks for event sequence classification including multihead self\nattention convolutions and recurrent cells. To perform the search we adopt\nsequential Bayesian Optimization and utilize previously trained models as an\nensemble of teachers to augment knowledge distillation. As a result of our work\nwe demonstrate that our method surpasses state of the art NAS methods and\npopular architectures suitable for sequence classification and holds great\npotential for various industrial applications.\n",
        "title": "SeqNAS: Neural Architecture Search for Event Sequence Classification",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03248",
        "abstract_url": "http://arxiv.org/abs/2401.03248",
        "authors": [
            {
                "last_name": "Golkar",
                "first_name": "Siavash"
            },
            {
                "last_name": "Berman",
                "first_name": "Jules"
            },
            {
                "last_name": "Lipshutz",
                "first_name": "David"
            },
            {
                "last_name": "Haret",
                "first_name": "Robert Mihai"
            },
            {
                "last_name": "Gollisch",
                "first_name": "Tim"
            },
            {
                "last_name": "Chklovskii",
                "first_name": "Dmitri B."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  To generate actions in the face of physiological delays, the brain must\npredict the future. Here we explore how prediction may lie at the core of brain\nfunction by considering a neuron predicting the future of a scalar time series\ninput. Assuming that the dynamics of the lag vector (a vector composed of\nseveral consecutive elements of the time series) are locally linear, Normal\nMode Decomposition decomposes the dynamics into independently evolving\n(eigen-)modes allowing for straightforward prediction. We propose that a neuron\nlearns the top mode and projects its input onto the associated subspace. Under\nthis interpretation, the temporal filter of a neuron corresponds to the left\neigenvector of a generalized eigenvalue problem. We mathematically analyze the\noperation of such an algorithm on noisy observations of synthetic data\ngenerated by a linear system. Interestingly, the shape of the temporal filter\nvaries with the signal-to-noise ratio (SNR): a noisy input yields a monophasic\nfilter and a growing SNR leads to multiphasic filters with progressively\ngreater number of phases. Such variation in the temporal filter with input SNR\nresembles that observed experimentally in biological neurons.\n",
        "title": "Neuronal Temporal Filters as Normal Mode Extractors",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03250",
        "abstract_url": "http://arxiv.org/abs/2401.03250",
        "authors": [
            {
                "last_name": "Ji",
                "first_name": "Wenqi"
            },
            {
                "last_name": "liu",
                "first_name": "Fang"
            },
            {
                "last_name": "Du",
                "first_name": "Xinxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Niqi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Chao"
            },
            {
                "last_name": "Yu",
                "first_name": "Mingjin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Guozhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong-Jin"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CV"
        ],
        "abstract": "  Interpersonal relationship quality is pivotal in social and occupational\ncontexts. Existing analysis of interpersonal relationships mostly rely on\nsubjective self-reports, whereas objective quantification remains challenging.\nIn this paper, we propose a novel social relationship analysis framework using\nspatio-temporal patterns derived from dyadic EEG signals, which can be applied\nto quantitatively measure team cooperation in corporate team building, and\nevaluate interpersonal dynamics between therapists and patients in psychiatric\ntherapy. First, we constructed a dyadic-EEG dataset from 72 pairs of\nparticipants with two relationships (stranger or friend) when watching\nemotional videos simultaneously. Then we proposed a deep neural network on\ndyadic-subject EEG signals, in which we combine the dynamic graph convolutional\nneural network for characterizing the interpersonal relationships among the EEG\nchannels and 1-dimension convolution for extracting the information from the\ntime sequence. To obtain the feature vectors from two EEG recordings that well\nrepresent the relationship of two subjects, we integrate deep canonical\ncorrelation analysis and triplet loss for training the network. Experimental\nresults show that the social relationship type (stranger or friend) between two\nindividuals can be effectively identified through their EEG data.\n",
        "title": "Interpersonal Relationship Analysis with Dyadic EEG Signals via Learning\n  Spatial-Temporal Patterns",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03251",
        "abstract_url": "http://arxiv.org/abs/2401.03251",
        "authors": [
            {
                "last_name": "Ravi",
                "first_name": "Nagarathna"
            },
            {
                "last_name": "T",
                "first_name": "Thishyan Raj"
            },
            {
                "last_name": "Arora",
                "first_name": "Vipul"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "SD",
            ""
        ],
        "abstract": "  Confidence estimation of predictions from an End-to-End (E2E) Automatic\nSpeech Recognition (ASR) model benefits ASR's downstream and upstream tasks.\nClass-probability-based confidence scores do not accurately represent the\nquality of overconfident ASR predictions. An ancillary Confidence Estimation\nModel (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use\nbinary target scores for CEM training. However, the binary labels do not reveal\nthe granular information of predicted words, such as temporal alignment between\nreference and hypothesis and whether the predicted word is entirely incorrect\nor contains spelling errors. Addressing this issue, we propose a novel\nTemporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address\nthe data imbalance of target scores while training CEM, we use shrinkage loss\nto focus on hard-to-learn data points and minimise the impact of easily learned\ndata points. We conduct experiments with ASR models trained in three languages,\nnamely Hindi, Tamil, and Kannada, with varying training data sizes. Experiments\nshow that TeLeS generalises well across domains. To demonstrate the\napplicability of the proposed method, we formulate a TeLeS-based Acquisition\n(TeLeS-A) function for sampling uncertainty in active learning. We observe a\nsignificant reduction in the Word Error Rate (WER) as compared to SOTA methods.\n",
        "title": "TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in\n  End-to-End ASR",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03253",
        "abstract_url": "http://arxiv.org/abs/2401.03253",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shuhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yulong"
            },
            {
                "last_name": "Jiang",
                "first_name": "Weisen"
            },
            {
                "last_name": "Lu",
                "first_name": "Jiangang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "LG"
        ],
        "abstract": "  Recent advances achieved by deep learning models rely on the independent and\nidentically distributed assumption, hindering their applications in real-world\nscenarios with domain shifts. To address the above issues, cross-domain\nlearning aims at extracting domain-invariant knowledge to reduce the domain\nshift between training and testing data. However, in visual cross-domain\nlearning, traditional methods concentrate solely on the image modality,\nneglecting the use of the text modality to alleviate the domain shift. In this\nwork, we propose Large Language models as Visual cross-dOmain learners (LLaVO).\nLLaVO uses vision-language models to convert images into detailed textual\ndescriptions. A large language model is then finetuned on textual descriptions\nof the source/target domain generated by a designed instruction template.\nExtensive experimental results on various cross-domain tasks under the domain\ngeneralization and unsupervised domain adaptation settings have demonstrated\nthe effectiveness of the proposed method.\n",
        "title": "Large Language Models as Visual Cross-Domain Learners",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03257",
        "abstract_url": "http://arxiv.org/abs/2401.03257",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mengfei"
            },
            {
                "last_name": "Lu",
                "first_name": "Ming"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaofang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shanghang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D\nconsistency, achieving impressive results in 3D scene modeling and\nhigh-fidelity novel-view synthesis. However, there are limitations. First,\nexisting methods assume enough high-quality images are available for training\nthe NeRF model, ignoring real-world image degradation. Second, previous methods\nstruggle with ambiguity in the training set due to unmodeled inconsistencies\namong different views. In this work, we present RustNeRF for real-world\nhigh-quality NeRF. To improve NeRF's robustness under real-world inputs, we\ntrain a 3D-aware preprocessing network that incorporates real-world degradation\nmodeling. We propose a novel implicit multi-view guidance to address\ninformation loss during image degradation and restoration. Extensive\nexperiments demonstrate RustNeRF's advantages over existing approaches under\nreal-world degradation. The code will be released.\n",
        "title": "RustNeRF: Robust Neural Radiance Field with Low-Quality Images",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03262",
        "abstract_url": "http://arxiv.org/abs/2401.03262",
        "authors": [
            {
                "last_name": "Thilakarathne",
                "first_name": "Haritha"
            },
            {
                "last_name": "Nibali",
                "first_name": "Aiden"
            },
            {
                "last_name": "He",
                "first_name": "Zhen"
            },
            {
                "last_name": "Morgan",
                "first_name": "Stuart"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Group activity recognition in video is a complex task due to the need for a\nmodel to recognise the actions of all individuals in the video and their\ncomplex interactions. Recent studies propose that optimal performance is\nachieved by individually tracking each person and subsequently inputting the\nsequence of poses or cropped images/optical flow into a model. This helps the\nmodel to recognise what actions each person is performing before they are\nmerged to arrive at the group action class. However, all previous models are\nhighly reliant on high quality tracking and have only been evaluated using\nground truth tracking information. In practice it is almost impossible to\nachieve highly reliable tracking information for all individuals in a group\nactivity video. We introduce an innovative deep learning-based group activity\nrecognition approach called Rendered Pose based Group Activity Recognition\nSystem (RePGARS) which is designed to be tolerant of unreliable tracking and\npose information. Experimental results confirm that RePGARS outperforms all\nexisting group activity recognition algorithms tested which do not use ground\ntruth detection and tracking information.\n",
        "title": "Group Activity Recognition using Unreliable Tracked Pose",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03263",
        "abstract_url": "http://arxiv.org/abs/2401.03263",
        "authors": [
            {
                "last_name": "Armbruster",
                "first_name": "Susanne"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  Recent improvements in adder optimization could be achieved by optimizing the\nAND-trees occurring within the constructed circuits. The overlap of such trees\nand its potential for pure size optimization has not been taken into account\nthough. Motivated by this, we examine the fundamental problem of minimizing the\nsize of a circuit for multiple AND-functions on intersecting variable sets. Our\nformulation generalizes the overlapping \\AND-trees within adder optimization\nbut is in NP, in contrast to general Boolean circuit optimization which is in\n$\\Sigma_2^p$ (and thus suspected not to be in NP). While restructuring the AND-\nor XOR-trees simultaneously, we optimize the total number of gates needed for\nall functions to be computed. We show that this problem is APX-hard already for\nfunctions of few variables and present efficient approximation algorithms for\nthe case in which the Boolean functions depend on at most 3 or 4 variables\neach, achieving guarantees of $\\frac 43$ and $1.9$, respectively. To conclude,\nwe give a polynomial approximation algorithm with guarantee $\\frac 23k$ for\nAND-functions of up to $k$ variables. To achieve these results, the key\ntechnique is to determine how much overlap among the variable sets makes tree\nconstruction cheap and how little makes the optimum solution large.\n",
        "title": "Size Minimization For Multi-Output AND-Functions",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03265",
        "abstract_url": "http://arxiv.org/abs/2401.03265",
        "authors": [
            {
                "last_name": "Greati",
                "first_name": "Vitor"
            },
            {
                "last_name": "Marcelino",
                "first_name": "S\u00e9rgio"
            },
            {
                "last_name": "Rivieccio",
                "first_name": "Umberto"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LO",
            "",
            ""
        ],
        "abstract": "  Multiple-conclusion Hilbert-style systems allow us to finitely axiomatize\nevery logic defined by a finite matrix. Having obtained such axiomatizations\nfor Paraconsistent Weak Kleene and Bochvar-Kleene logics, we modify them by\nreplacing the multiple-conclusion rules with carefully selected\nsingle-conclusion ones. In this way we manage to introduce the first finite\nHilbert-style single-conclusion axiomatizations for these logics.\n",
        "title": "Finite Hilbert systems for Weak Kleene logics",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03267",
        "abstract_url": "http://arxiv.org/abs/2401.03267",
        "authors": [
            {
                "last_name": "Gerstenslager",
                "first_name": "Andrew"
            },
            {
                "last_name": "Lewis",
                "first_name": "Jomol"
            },
            {
                "last_name": "McKenna",
                "first_name": "Liam"
            },
            {
                "last_name": "Patel",
                "first_name": "Poorva"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "CV",
            "LG",
            ""
        ],
        "abstract": "  This paper explores the application of CNN-DNN network fusion to construct a\nrobot navigation controller within a simulated environment. The simulated\nenvironment is constructed to model a subterranean rescue situation, such that\nan autonomous agent is tasked with finding a goal within an unknown cavernous\nsystem. Imitation learning is used to train the control algorithm to use LiDAR\nand camera data to navigate the space and find the goal. The trained model is\nthen tested for robustness using Monte-Carlo.\n",
        "title": "Autonomous Navigation in Complex Environments",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03271",
        "abstract_url": "http://arxiv.org/abs/2401.03271",
        "authors": [
            {
                "last_name": "Lahr",
                "first_name": "Isaiah"
            },
            {
                "last_name": "Alfasly",
                "first_name": "Saghir"
            },
            {
                "last_name": "Nejat",
                "first_name": "Peyman"
            },
            {
                "last_name": "Khan",
                "first_name": "Jibran"
            },
            {
                "last_name": "Kottom",
                "first_name": "Luke"
            },
            {
                "last_name": "Kumbhar",
                "first_name": "Vaishnavi"
            },
            {
                "last_name": "Alsaafin",
                "first_name": "Areej"
            },
            {
                "last_name": "Shafique",
                "first_name": "Abubakr"
            },
            {
                "last_name": "Hemati",
                "first_name": "Sobhan"
            },
            {
                "last_name": "Alabtah",
                "first_name": "Ghazal"
            },
            {
                "last_name": "Comfere",
                "first_name": "Nneka"
            },
            {
                "last_name": "Murphee",
                "first_name": "Dennis"
            },
            {
                "last_name": "Mangold",
                "first_name": "Aaron"
            },
            {
                "last_name": "Yasir",
                "first_name": "Saba"
            },
            {
                "last_name": "Meroueh",
                "first_name": "Chady"
            },
            {
                "last_name": "Boardman",
                "first_name": "Lisa"
            },
            {
                "last_name": "Shah",
                "first_name": "Vijay H."
            },
            {
                "last_name": "Garcia",
                "first_name": "Joaquin J."
            },
            {
                "last_name": "Tizhoosh",
                "first_name": "H. R."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "IR"
        ],
        "abstract": "  Searching for similar images in archives of histology and histopathology\nimages is a crucial task that may aid in patient matching for various purposes,\nranging from triaging and diagnosis to prognosis and prediction. Whole slide\nimages (WSIs) are highly detailed digital representations of tissue specimens\nmounted on glass slides. Matching WSI to WSI can serve as the critical method\nfor patient matching. In this paper, we report extensive analysis and\nvalidation of four search methods bag of visual words (BoVW), Yottixel, SISH,\nRetCCL, and some of their potential variants. We analyze their algorithms and\nstructures and assess their performance. For this evaluation, we utilized four\ninternal datasets ($1269$ patients) and three public datasets ($1207$\npatients), totaling more than $200,000$ patches from $38$ different\nclasses/subtypes across five primary sites. Certain search engines, for\nexample, BoVW, exhibit notable efficiency and speed but suffer from low\naccuracy. Conversely, search engines like Yottixel demonstrate efficiency and\nspeed, providing moderately accurate results. Recent proposals, including SISH,\ndisplay inefficiency and yield inconsistent outcomes, while alternatives like\nRetCCL prove inadequate in both accuracy and efficiency. Further research is\nimperative to address the dual aspects of accuracy and minimal storage\nrequirements in histopathological image search.\n",
        "title": "Analysis and Validation of Image Search Engines in Histopathology",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03274",
        "abstract_url": "http://arxiv.org/abs/2401.03274",
        "authors": [
            {
                "last_name": "Greati",
                "first_name": "Vitor"
            },
            {
                "last_name": "Greco",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Marcelino",
                "first_name": "S\u00e9rgio"
            },
            {
                "last_name": "Palmigiano",
                "first_name": "Alessandra"
            },
            {
                "last_name": "Rivieccio",
                "first_name": "Umberto"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LO",
            "",
            ""
        ],
        "abstract": "  In general, providing an axiomatization for an arbitrary logic is a task that\nmay require some ingenuity. In the case of logics defined by a finite logical\nmatrix (three-valued logics being a particularly simple example), the\ngeneration of suitable finite axiomatizations can be completely automatized,\nessentially by expressing the matrix tables via inference rules. In this\nchapter we illustrate how two formalisms, the 3-labelled calculi of Baaz,\nFerm\\\"uller and Zach and the multiple-conclusion (or Set-Set) Hilbert-style\ncalculi of Shoesmith and Smiley, may be uniformly employed to axiomatize logics\ndefined by a three-valued logical matrix. The generating procedure common to\nboth formalisms can be described as follows: first (i) convert the matrix\nsemantics into rule form (we refer to this step as the generating subprocedure)\nand then (ii) simplify the set of rules thus obtained, essentially relying on\nthe defining properties of any Tarskian consequence relation (we refer to this\nstep as the streamlining subprocedure). We illustrate through some examples\nthat, if a minimal expressiveness assumption is met (namely, if the matrix\ndefining the logic is monadic), then it is straightforward to define effective\ntranslations guaranteeing the equivalence between the 3-labelled and the\nSet-Set approach.\n",
        "title": "Generating proof systems for three-valued propositional logics",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03275",
        "abstract_url": "http://arxiv.org/abs/2401.03275",
        "authors": [
            {
                "last_name": "Guettala",
                "first_name": "Walid"
            },
            {
                "last_name": "Sayah",
                "first_name": "Ali"
            },
            {
                "last_name": "Kahloul",
                "first_name": "Laid"
            },
            {
                "last_name": "Tibermacine",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  One of the most important problems in computer vision and remote sensing is\nobject detection, which identifies particular categories of diverse things in\npictures. Two crucial data sources for public security are the thermal infrared\n(TIR) remote sensing multi-scenario photos and videos produced by unmanned\naerial vehicles (UAVs). Due to the small scale of the target, complex scene\ninformation, low resolution relative to the viewable videos, and dearth of\npublicly available labeled datasets and training models, their object detection\nprocedure is still difficult. A UAV TIR object detection framework for pictures\nand videos is suggested in this study. The Forward-looking Infrared (FLIR)\ncameras used to gather ground-based TIR photos and videos are used to create\nthe ``You Only Look Once'' (YOLO) model, which is based on CNN architecture.\nResults indicated that in the validating task, detecting human object had an\naverage precision at IOU (Intersection over Union) = 0.5, which was 72.5\\%,\nusing YOLOv7 (YOLO version 7) state of the art model \\cite{1}, while the\ndetection speed around 161 frames per second (FPS/second). The usefulness of\nthe YOLO architecture is demonstrated in the application, which evaluates the\ncross-detection performance of people in UAV TIR videos under a YOLOv7 model in\nterms of the various UAVs' observation angles. The qualitative and quantitative\nevaluation of object detection from TIR pictures and videos using deep-learning\nmodels is supported favorably by this work.\n",
        "title": "Real Time Human Detection by Unmanned Aerial Vehicles",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03284",
        "abstract_url": "http://arxiv.org/abs/2401.03284",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Sen"
            },
            {
                "last_name": "Li",
                "first_name": "Dong"
            },
            {
                "last_name": "Huang",
                "first_name": "Shao-Yu"
            },
            {
                "last_name": "Deng",
                "first_name": "Xuanliang"
            },
            {
                "last_name": "Sifat",
                "first_name": "Ashrarul H."
            },
            {
                "last_name": "Jung",
                "first_name": "Changhee"
            },
            {
                "last_name": "Williams",
                "first_name": "Ryan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Haibo"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In real-time systems optimization, designers often face a challenging problem\nposed by the non-convex and non-continuous schedulability conditions, which may\neven lack an analytical form to understand their properties. To tackle this\nchallenging problem, we treat the schedulability analysis as a black box that\nonly returns true/false results. We propose a general and scalable framework to\noptimize real-time systems, named Numerical Optimizer with Real-Time Highlight\n(NORTH). NORTH is built upon the gradient-based active-set methods from the\nnumerical optimization literature but with new methods to manage active\nconstraints for the non-differentiable schedulability constraints. In addition,\nwe also generalize NORTH to NORTH+, to collaboratively optimize certain types\nof discrete variables (\\eg priority assignments, categorical variables) with\ncontinuous variables based on numerical optimization algorithms. We demonstrate\nthe algorithm performance with two example applications: energy minimization\nbased on dynamic voltage and frequency scaling (DVFS), and optimization of\ncontrol system performance. In these experiments, NORTH achieved $10^2$ to\n$10^5$ times speed improvements over state-of-the-art methods while maintaining\nsimilar or better solution quality. NORTH+ outperforms NORTH by 30\\% with\nsimilar algorithm scalability. Both NORTH and NORTH+ support black-box\nschedulability analysis, ensuring broad applicability.\n",
        "title": "A General and Scalable Method for Optimizing Real-Time Systems",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03286",
        "abstract_url": "http://arxiv.org/abs/2401.03286",
        "authors": [
            {
                "last_name": "Tourbabin",
                "first_name": "Vladimir"
            },
            {
                "last_name": "Rafaely",
                "first_name": "Boaz"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "RO",
            "SD"
        ],
        "abstract": "  An important aspect of a humanoid robot is audition. Previous work has\npresented robot systems capable of sound localization and source segregation\nbased on microphone arrays with various configurations. However, no theoretical\nframework for the design of these arrays has been presented. In the current\npaper, a design framework is proposed based on a novel array quality measure.\nThe measure is based on the effective rank of a matrix composed of the\ngeneralized head related transfer functions (GHRTFs) that account for\nmicrophone positions other than the ears. The measure is shown to be\ntheoretically related to standard array performance measures such as\nbeamforming robustness and DOA estimation accuracy. Then, the measure is\napplied to produce sample designs of microphone arrays. Their performance is\ninvestigated numerically, verifying the advantages of array design based on the\nproposed theoretical framework.\n",
        "title": "Theoretical Framework for the Optimization of Microphone Array\n  Configuration for Humanoid Robot Audition",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03291",
        "abstract_url": "http://arxiv.org/abs/2401.03291",
        "authors": [
            {
                "last_name": "Morgenstern",
                "first_name": "Hai"
            },
            {
                "last_name": "Rafaely",
                "first_name": "Boaz"
            },
            {
                "last_name": "Noisternig",
                "first_name": "Markus"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Spherical microphone arrays (SMAs) and spherical loudspeaker arrays (SLAs)\nfacilitate the study of room acoustics due to the three-dimensional analysis\nthey provide. More recently, systems that combine both arrays, referred to as\nmultiple-input multiple-output (MIMO) systems, have been proposed due to the\nadded spatial diversity they facilitate. The literature provides frameworks for\ndesigning SMAs and SLAs separately, including error analysis from which the\noperating frequency range (OFR) of an array is defined. However, such a\nframework does not exist for the joint design of a SMA and a SLA that comprise\na MIMO system. This paper develops a design framework for MIMO systems based on\na model that addresses errors and highlights the importance of a matched\ndesign. Expanding on a free-field assumption, errors are incorporated\nseparately for each array and error bounds are defined, facilitating error\nanalysis for the system. The dependency of the error bounds on the SLA and SMA\nparameters is studied and it is recommended that parameters should be chosen to\nassure matched OFRs of the arrays in MIMO system design. A design example is\nprovided, demonstrating the superiority of a matched system over an unmatched\nsystem in the synthesis of directional room impulse responses.\n",
        "title": "Design framework for spherical microphone and loudspeaker arrays in a\n  multiple-input multiple-output system",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03297",
        "abstract_url": "http://arxiv.org/abs/2401.03297",
        "authors": [
            {
                "last_name": "Asaeedi",
                "first_name": "Saeed"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  The Colored Points Traveling Salesman Problem (Colored Points TSP) is\nintroduced in this work as a novel variation of the traditional Traveling\nSalesman Problem (TSP) in which the set of points is partitioned into multiple\nclasses, each of which is represented by a distinct color (or label). The goal\nis to find a minimum cost cycle $C$ that visits all the colors and only makes\neach one appears once. This issue has various applications in the fields of\ntransportation, goods distribution network, postal network, inspection,\ninsurance, banking, etc. By reducing the traditional TSP to it, we can\ndemonstrate that Colored Points TSP is NP-hard. Here, we offer a $\\frac{2\\pi\nr}{3}$-approximation algorithm to solve this issue, where $r$ denotes the\nradius of the points' smallest color-spanning circle. The algorithm has been\nimplemented, executed on random datasets, and compared against the brute force\nmethod.\n",
        "title": "Colored Points Traveling Salesman Problem",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03298",
        "abstract_url": "http://arxiv.org/abs/2401.03298",
        "authors": [
            {
                "last_name": "Benz",
                "first_name": "Christian"
            },
            {
                "last_name": "Rodehorst",
                "first_name": "Volker"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  For effective structural damage assessment, the instances of damages need to\nbe localized in the world of a 3D model. Due to a lack of data, the detection\nof structural anomalies can currently not be directly learned and performed in\n3D space. In this work, a three-stage approach is presented, which uses the\ngood performance of detection models on image level to segment instances of\nanomalies in the 3D space. In the detection stage, semantic segmentation\npredictions are produced on image level. The mapping stage transfers the\nimage-level prediction onto the respective point cloud. In the extraction\nstage, 3D anomaly instances are extracted from the segmented point cloud. Cloud\ncontraction is used to transform cracks into their medial axis representation.\nFor areal anomalies the bounding polygon is extracted by means of alpha shapes.\nThe approach covers the classes crack, spalling, and corrosion and the three\nimage-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are\ncompared. Granted a localization tolerance of 4cm, IoUs of over 90% can be\nachieved for crack and corrosion and 41% for spalling, which appears to be a\nspecifically challenging class. Detection on instance-level measured in AP is\nabout 45% for crack and spalling and 73% for corrosion.\n",
        "title": "Multi-View 3D Instance Segmentation of Structural Anomalies for Enhanced\n  Structural Inspection of Concrete Bridges",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03301",
        "abstract_url": "http://arxiv.org/abs/2401.03301",
        "authors": [
            {
                "last_name": "Nguyen-Tang",
                "first_name": "Thanh"
            },
            {
                "last_name": "Arora",
                "first_name": "Raman"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We seek to understand what facilitates sample-efficient learning from\nhistorical datasets for sequential decision-making, a problem that is popularly\nknown as offline reinforcement learning (RL). Further, we are interested in\nalgorithms that enjoy sample efficiency while leveraging (value) function\napproximation. In this paper, we address these fundamental questions by (i)\nproposing a notion of data diversity that subsumes the previous notions of\ncoverage measures in offline RL and (ii) using this notion to {unify} three\ndistinct classes of offline RL algorithms based on version spaces (VS),\nregularized optimization (RO), and posterior sampling (PS). We establish that\nVS-based, RO-based, and PS-based algorithms, under standard assumptions,\nachieve \\emph{comparable} sample efficiency, which recovers the\nstate-of-the-art sub-optimality bounds for finite and linear model classes with\nthe standard assumptions. This result is surprising, given that the prior work\nsuggested an unfavorable sample complexity of the RO-based algorithm compared\nto the VS-based algorithm, whereas posterior sampling is rarely considered in\noffline RL due to its explorative nature. Notably, our proposed model-free\nPS-based algorithm for offline RL is {novel}, with sub-optimality bounds that\nare {frequentist} (i.e., worst-case) in nature.\n",
        "title": "On Sample-Efficient Offline Reinforcement Learning: Data Diversity,\n  Posterior Sampling, and Beyond",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03302",
        "abstract_url": "http://arxiv.org/abs/2401.03302",
        "authors": [
            {
                "last_name": "Hashemi",
                "first_name": "Seyed Mohammad Hossein"
            },
            {
                "last_name": "Safari",
                "first_name": "Leila"
            },
            {
                "last_name": "Taromi",
                "first_name": "Amirhossein Dadashzade"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV",
            "LG",
            ""
        ],
        "abstract": "  In the field of medical sciences, reliable detection and classification of\nbrain tumors from images remains a formidable challenge due to the rarity of\ntumors within the population of patients. Therefore, the ability to detect\ntumors in anomaly scenarios is paramount for ensuring timely interventions and\nimproved patient outcomes. This study addresses the issue by leveraging deep\nlearning (DL) techniques to detect and classify brain tumors in challenging\nsituations. The curated data set from the National Brain Mapping Lab (NBML)\ncomprises 81 patients, including 30 Tumor cases and 51 Normal cases. The\ndetection and classification pipelines are separated into two consecutive\ntasks. The detection phase involved comprehensive data analysis and\npre-processing to modify the number of image samples and the number of patients\nof each class to anomaly distribution (9 Normal per 1 Tumor) to comply with\nreal world scenarios. Next, in addition to common evaluation metrics for the\ntesting, we employed a novel performance evaluation method called Patient to\nPatient (PTP), focusing on the realistic evaluation of the model. In the\ndetection phase, we fine-tuned a YOLOv8n detection model to detect the tumor\nregion. Subsequent testing and evaluation yielded competitive performance both\nin Common Evaluation Metrics and PTP metrics. Furthermore, using the Data\nEfficient Image Transformer (DeiT) module, we distilled a Vision Transformer\n(ViT) model from a fine-tuned ResNet152 as a teacher in the classification\nphase. This approach demonstrates promising strides in reliable tumor detection\nand classification, offering potential advancements in tumor diagnosis for\nreal-world medical imaging scenarios.\n",
        "title": "Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical\n  Images Using YOLOv8 and DeiT",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03303",
        "abstract_url": "http://arxiv.org/abs/2401.03303",
        "authors": [
            {
                "last_name": "Lisan",
                "first_name": "Aliza"
            },
            {
                "last_name": "Norris",
                "first_name": "Boyana"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  A critical issue faced by open-source software projects is the risk of key\npersonnel leaving the project. This risk is exacerbated in large projects that\nhave been under development for a long time and experienced growth in their\ndevelopment teams. One way to quantify this risk is to measure the\nconcentration of knowledge about the project among its developers. Formally\nknown as the Bus Factor (BF) of a project and defined as 'the number of key\ndevelopers who would need to be incapacitated to make a project unable to\nproceed'. Most of the proposed algorithms for BF calculation measure a\ndeveloper's knowledge of a file based on the number of commits. In this work,\nwe propose using other metrics like lines of code changes (LOCC) and cosine\ndifference of lines of code (change-size-cos) to calculate the BF. We use these\nmetrics for BF calculation for five open-source GitHub projects using the CST\nalgorithm and the RIG algorithm, which is git-blame-based. Moreover, we\ncalculate the BF on project sub-directories that have seen the most active\ndevelopment recently. Lastly, we compare the results of the two algorithms in\naccuracy, similarity in results, execution time, and trends in BF values over\ntime.\n",
        "title": "Guiding Effort Allocation in Open-Source Software Projects Using Bus\n  Factor Analysis",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03306",
        "abstract_url": "http://arxiv.org/abs/2401.03306",
        "authors": [
            {
                "last_name": "Rafailov",
                "first_name": "Rafael"
            },
            {
                "last_name": "Hatch",
                "first_name": "Kyle"
            },
            {
                "last_name": "Kolev",
                "first_name": "Victor"
            },
            {
                "last_name": "Martin",
                "first_name": "John D."
            },
            {
                "last_name": "Phielipp",
                "first_name": "Mariano"
            },
            {
                "last_name": "Finn",
                "first_name": "Chelsea"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "RO"
        ],
        "abstract": "  We study the problem of offline pre-training and online fine-tuning for\nreinforcement learning from high-dimensional observations in the context of\nrealistic robot tasks. Recent offline model-free approaches successfully use\nonline fine-tuning to either improve the performance of the agent over the data\ncollection policy or adapt to novel tasks. At the same time, model-based RL\nalgorithms have achieved significant progress in sample efficiency and the\ncomplexity of the tasks they can solve, yet remain under-utilized in the\nfine-tuning setting. In this work, we argue that existing model-based offline\nRL methods are not suitable for offline-to-online fine-tuning in\nhigh-dimensional domains due to issues with distribution shifts, off-dynamics\ndata, and non-stationary rewards. We propose an on-policy model-based method\nthat can efficiently reuse prior data through model-based value expansion and\npolicy regularization, while preventing model exploitation by controlling\nepistemic uncertainty. We find that our approach successfully solves tasks from\nthe MetaWorld benchmark, as well as the Franka Kitchen robot manipulation\nenvironment completely from images. To the best of our knowledge, MOTO is the\nfirst method to solve this environment from pixels.\n",
        "title": "MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot\n  Learning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03307",
        "abstract_url": "http://arxiv.org/abs/2401.03307",
        "authors": [
            {
                "last_name": "Mori",
                "first_name": "J. Carlos Mart\u00ednez"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zhanzhan"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Gentrification is a process of neighborhood change in which the primary\nbeneficiaries tend to be homeowners and newcomers, as opposed to incumbent\nrenters. However, operational definitions of gentrification and other concepts\nof neighborhood change are elusive, making them and their interactions with\npolicy interventions difficult to quantify. In this paper, we propose\nformulating processes of neighborhood change as instances of no-regret\ndynamics; a collective learning process in which a set of strategic agents\nrapidly reach a state of approximate equilibrium. We mathematize concepts of\nneighborhood change to model the incentive structures impacting individual\ndwelling-site decision-making. Our model accounts for affordability, access to\nrelevant amenities, community ties, and site upkeep. We showcase our model with\ncomputational experiments that provide semi-quantitative insights on the\nspatial economics of neighborhood change, particularly on the influence of\nresidential zoning policy and the placement of urban amenities.\n",
        "title": "Modeling Processes of Neighborhood Change",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03310",
        "abstract_url": "http://arxiv.org/abs/2401.03310",
        "authors": [
            {
                "last_name": "Borges",
                "first_name": "Jo\u00e3o"
            },
            {
                "last_name": "Bastos",
                "first_name": "Felipe"
            },
            {
                "last_name": "Correa",
                "first_name": "Ilan"
            },
            {
                "last_name": "Batista",
                "first_name": "Pedro"
            },
            {
                "last_name": "Klautau",
                "first_name": "Aldebaro"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "",
            ""
        ],
        "abstract": "  Digital twins are an important technology for advancing mobile\ncommunications, specially in use cases that require simultaneously simulating\nthe wireless channel, 3D scenes and machine learning. Aiming at providing a\nsolution to this demand, this work describes a modular co-simulation\nmethodology called CAVIAR. Here, CAVIAR is upgraded to support a message\npassing library and enable the virtual counterpart of a digital twin system\nusing different 6G-related simulators. The main contributions of this work are\nthe detailed description of different CAVIAR architectures, the implementation\nof this methodology to assess a 6G use case of UAV-based search and rescue\nmission (SAR), and the generation of benchmarking data about the computational\nresource usage. For executing the SAR co-simulation we adopt five open-source\nsolutions: the physical and link level network simulator Sionna, the simulator\nfor autonomous vehicles AirSim, scikit-learn for training a decision tree for\nMIMO beam selection, Yolov8 for the detection of rescue targets and NATS for\nmessage passing. Results for the implemented SAR use case suggest that the\nmethodology can run in a single machine, with the main demanded resources being\nthe CPU processing and the GPU memory.\n",
        "title": "CAVIAR: Co-simulation of 6G Communications, 3D Scenarios and AI for\n  Digital Twins",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03312",
        "abstract_url": "http://arxiv.org/abs/2401.03312",
        "authors": [
            {
                "last_name": "Bhalla",
                "first_name": "Arjun"
            },
            {
                "last_name": "Levenson",
                "first_name": "Daniel"
            },
            {
                "last_name": "Bernhard",
                "first_name": "Jan"
            },
            {
                "last_name": "Abilov",
                "first_name": "Anton"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  This work investigates how hierarchically structured data can help neural\nnetworks learn conceptual representations of cathedrals. The underlying\nWikiScenes dataset provides a spatially organized hierarchical structure of\ncathedral components. We propose a novel hierarchical contrastive training\napproach that leverages a triplet margin loss to represent the data's spatial\nhierarchy in the encoder's latent space. As such, the proposed approach\ninvestigates if the dataset structure provides valuable information for\nself-supervised learning. We apply t-SNE to visualize the resultant latent\nspace and evaluate the proposed approach by comparing it with other\ndataset-specific contrastive learning methods using a common downstream\nclassification task. The proposed method outperforms the comparable\nweakly-supervised and baseline methods. Our findings suggest that dataset\nstructure is a valuable modality for weakly-supervised learning.\n",
        "title": "Exploiting Data Hierarchy as a New Modality for Contrastive Learning",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03314",
        "abstract_url": "http://arxiv.org/abs/2401.03314",
        "authors": [
            {
                "last_name": "Ambilduke",
                "first_name": "Kshitij"
            },
            {
                "last_name": "Shetye",
                "first_name": "Aneesh"
            },
            {
                "last_name": "Bagade",
                "first_name": "Diksha"
            },
            {
                "last_name": "Bhagwatkar",
                "first_name": "Rishika"
            },
            {
                "last_name": "Fitter",
                "first_name": "Khurshed"
            },
            {
                "last_name": "Vagdargi",
                "first_name": "Prasad"
            },
            {
                "last_name": "Chiddarwar",
                "first_name": "Shital"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Neural machine translation benefits from semantically rich representations.\nConsiderable progress in learning such representations has been achieved by\nlanguage modelling and mutual information maximization objectives using\ncontrastive learning. The language-dependent nature of language modelling\nintroduces a trade-off between the universality of the learned representations\nand the model's performance on the language modelling tasks. Although\ncontrastive learning improves performance, its success cannot be attributed to\nmutual information alone. We propose a novel Context Enhancement step to\nimprove performance on neural machine translation by maximizing mutual\ninformation using the Barlow Twins loss. Unlike other approaches, we do not\nexplicitly augment the data but view languages as implicit augmentations,\neradicating the risk of disrupting semantic information. Further, our method\ndoes not learn embeddings from scratch and can be generalised to any set of\npre-trained embeddings. Finally, we evaluate the language-agnosticism of our\nembeddings through language classification and use them for neural machine\ntranslation to compare with state-of-the-art approaches.\n",
        "title": "Enhancing Context Through Contrast",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03315",
        "abstract_url": "http://arxiv.org/abs/2401.03315",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Zilong"
            },
            {
                "last_name": "Cui",
                "first_name": "Jian"
            },
            {
                "last_name": "Liao",
                "first_name": "Xiaojing"
            },
            {
                "last_name": "Wang",
                "first_name": "XiaoFeng"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The underground exploitation of large language models (LLMs) for malicious\nservices (i.e., Malla) is witnessing an uptick, amplifying the cyber threat\nlandscape and posing questions about the trustworthiness of LLM technologies.\nHowever, there has been little effort to understand this new cybercrime, in\nterms of its magnitude, impact, and techniques. In this paper, we conduct the\nfirst systematic study on 212 real-world Mallas, uncovering their proliferation\nin underground marketplaces and exposing their operational modalities. Our\nstudy discloses the Malla ecosystem, revealing its significant growth and\nimpact on today's public LLM services. Through examining 212 Mallas, we\nuncovered eight backend LLMs used by Mallas, along with 182 prompts that\ncircumvent the protective measures of public LLM APIs. We further demystify the\ntactics employed by Mallas, including the abuse of uncensored LLMs and the\nexploitation of public LLM APIs through jailbreak prompts. Our findings enable\na better understanding of the real-world exploitation of LLMs by\ncybercriminals, offering insights into strategies to counteract this\ncybercrime.\n",
        "title": "Malla: Demystifying Real-world Large Language Model Integrated Malicious\n  Services",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03317",
        "abstract_url": "http://arxiv.org/abs/2401.03317",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Qian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chengzhu"
            },
            {
                "last_name": "Liang",
                "first_name": "Xin"
            },
            {
                "last_name": "Reshniak",
                "first_name": "Viktor"
            },
            {
                "last_name": "Chen",
                "first_name": "Jieyang"
            },
            {
                "last_name": "Rangarajan",
                "first_name": "Anand"
            },
            {
                "last_name": "Ranka",
                "first_name": "Sanjay"
            },
            {
                "last_name": "Vidal",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Wan",
                "first_name": "Lipeng"
            },
            {
                "last_name": "Ullrich",
                "first_name": "Paul"
            },
            {
                "last_name": "Podhorszki",
                "first_name": "Norbert"
            },
            {
                "last_name": "Jacob",
                "first_name": "Robert"
            },
            {
                "last_name": "Klasky",
                "first_name": "Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Scientific discoveries are increasingly constrained by limited storage space\nand I/O capacities. For time-series simulations and experiments, their data\noften need to be decimated over timesteps to accommodate storage and I/O\nlimitations. In this paper, we propose a technique that addresses storage costs\nwhile improving post-analysis accuracy through spatiotemporal adaptive,\nerror-controlled lossy compression. We investigate the trade-off between data\nprecision and temporal output rates, revealing that reducing data precision and\nincreasing timestep frequency lead to more accurate analysis outcomes.\nAdditionally, we integrate spatiotemporal feature detection with data\ncompression and demonstrate that performing adaptive error-bounded compression\nin higher dimensional space enables greater compression ratios, leveraging the\nerror propagation theory of a transformation-based compressor.\n  To evaluate our approach, we conduct experiments using the well-known E3SM\nclimate simulation code and apply our method to compress variables used for\ncyclone tracking. Our results show a significant reduction in storage size\nwhile enhancing the quality of cyclone tracking analysis, both quantitatively\nand qualitatively, in comparison to the prevalent timestep decimation approach.\nCompared to three state-of-the-art lossy compressors lacking feature\npreservation capabilities, our adaptive compression framework improves\nperfectly matched cases in TC tracking by 26.4-51.3% at medium compression\nratios and by 77.3-571.1% at large compression ratios, with a merely 5-11%\ncomputational overhead.\n",
        "title": "Spatiotemporally adaptive compression for scientific dataset with\n  feature preservation -- a case study on simulation data with extreme climate\n  events analysis",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03319",
        "abstract_url": "http://arxiv.org/abs/2401.03319",
        "authors": [
            {
                "last_name": "Mehran",
                "first_name": "Narges"
            },
            {
                "last_name": "Haghighi",
                "first_name": "Arman"
            },
            {
                "last_name": "Aminharati",
                "first_name": "Pedram"
            },
            {
                "last_name": "Nikolov",
                "first_name": "Nikolay"
            },
            {
                "last_name": "Soylu",
                "first_name": "Ahmet"
            },
            {
                "last_name": "Roman",
                "first_name": "Dumitru"
            },
            {
                "last_name": "Prodan",
                "first_name": "Radu"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "",
            "LG",
            "NE"
        ],
        "abstract": "  Today, many users deploy their microservice-based applications with various\ninterconnections on a cluster of Cloud machines, subject to stochastic changes\ndue to dynamic user requirements. To address this problem, we compare three\nmachine learning (ML) models for predicting the microservice call rates based\non the microservice times and aiming at estimating the scalability\nrequirements. We apply the linear regression (LR), multilayer perception (MLP),\nand gradient boosting regression (GBR) models on the Alibaba microservice\ntraces. The prediction results reveal that the LR model reaches a lower\ntraining time than the GBR and MLP models. However, the GBR reduces the mean\nabsolute error and the mean absolute percentage error compared to LR and MLP\nmodels. Moreover, the prediction results show that the required number of\nreplicas for each microservice by the gradient boosting model is close to the\nactual test data without any prediction.\n",
        "title": "Comparison of Microservice Call Rate Predictions for Replication in the\n  Cloud",
        "date": "2023-10-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03321",
        "abstract_url": "http://arxiv.org/abs/2401.03321",
        "authors": [
            {
                "last_name": "Tai",
                "first_name": "Yintao"
            },
            {
                "last_name": "Liao",
                "first_name": "Xiyang"
            },
            {
                "last_name": "Suglia",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Vergari",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent works showed the possibility of building open-vocabulary large\nlanguage models (LLMs) that directly operate on pixel representations and are\nimplemented as encoder-decoder models that reconstruct masked image patches of\nrendered text. However, these pixel-based LLMs are limited to autoencoding\ntasks and cannot generate new text as images. As such, they cannot be used for\nopen-answer or generative language tasks. In this work, we overcome this\nlimitation and introduce PIXAR, the first pixel-based autoregressive LLM that\ndoes not rely on a pre-defined vocabulary for both input and output text.\nConsisting of only a decoder, PIXAR can answer free-form generative tasks while\nkeeping the text representation learning performance on par with previous\nencoder-decoder models. Furthermore, we highlight the challenges to\nautoregressively generate non-blurred text as images and link this to the usual\nmaximum likelihood objective. We propose a simple adversarial pretraining that\nsignificantly improves the readability and performance of PIXAR making it\ncomparable to GPT2 on short text generation tasks. This paves the way to\nbuilding open-vocabulary LLMs that are usable for free-form generative tasks\nand questions the necessity of the usual symbolic input representation -- text\nas tokens -- for these challenging tasks.\n",
        "title": "PIXAR: Auto-Regressive Language Modeling in Pixel Space",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03322",
        "abstract_url": "http://arxiv.org/abs/2401.03322",
        "authors": [
            {
                "last_name": "Najafi",
                "first_name": "Seyed Amirhossein"
            },
            {
                "last_name": "Asemani",
                "first_name": "Mohammad Hassan"
            },
            {
                "last_name": "Setoodeh",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  This paper introduces a hybrid attention and autoencoder (AE) model for\nunsupervised online anomaly detection in time series. The autoencoder captures\nlocal structural patterns in short embeddings, while the attention model learns\nlong-term features, facilitating parallel computing with positional encoding.\nUnique in its approach, our proposed hybrid model combines attention and\nautoencoder for the first time in time series anomaly detection. It employs an\nattention-based mechanism, akin to the deep transformer model, with key\narchitectural modifications for predicting the next time step window in the\nautoencoder's latent space. The model utilizes a threshold from the validation\ndataset for anomaly detection and introduces an alternative method based on\nanalyzing the first statistical moment of error, improving accuracy without\ndependence on a validation dataset. Evaluation on diverse real-world benchmark\ndatasets and comparing with other well-established models, confirms the\neffectiveness of our proposed model in anomaly detection.\n",
        "title": "Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly\n  Detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03324",
        "abstract_url": "http://arxiv.org/abs/2401.03324",
        "authors": [
            {
                "last_name": "Vahdatpour",
                "first_name": "Mohammad Saleh"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  The \"0-1 knapsack problem\" stands as a classical combinatorial optimization\nconundrum, necessitating the selection of a subset of items from a given set.\nEach item possesses inherent values and weights, and the primary objective is\nto formulate a selection strategy that maximizes the total value while adhering\nto a predefined capacity constraint. In this research paper, we introduce a\nnovel variant of Cultural Algorithms tailored specifically for solving 0-1\nknapsack problems, a well-known combinatorial optimization challenge. Our\nproposed algorithm incorporates a belief space to refine the population and\nintroduces two vital functions for dynamically adjusting the crossover and\nmutation rates during the evolutionary process. Through extensive\nexperimentation, we provide compelling evidence of the algorithm's remarkable\nefficiency in consistently locating the global optimum, even in knapsack\nproblems characterized by high dimensions and intricate constraints.\n",
        "title": "Addressing The Knapsack Challenge Through Cultural Algorithm\n  Optimization",
        "date": "2023-10-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03329",
        "abstract_url": "http://arxiv.org/abs/2401.03329",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Emily"
            },
            {
                "last_name": "Shi",
                "first_name": "Zhonghao"
            },
            {
                "last_name": "Qiao",
                "first_name": "Xiaoyang"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja J"
            },
            {
                "last_name": "Bittner",
                "first_name": "Ava K"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Socially assistive robots (SARs) have shown great promise in supplementing\nand augmenting interventions to support the physical and mental well-being of\nolder adults. However, past work has not yet explored the potential of applying\nSAR to lower the barriers of long-term low vision rehabilitation (LVR)\ninterventions for older adults. In this work, we present a user-informed design\nprocess to validate the motivation and identify major design principles for\ndeveloping SAR for long-term LVR. To evaluate user-perceived usefulness and\nacceptance of SAR in this novel domain, we performed a two-phase study through\nuser surveys. First, a group (n=38) of older adults with LV completed a\nmailed-in survey. Next, a new group (n=13) of older adults with LV saw an\nin-clinic SAR demo and then completed the survey. The study participants\nreported that SARs would be useful, trustworthy, easy to use, and enjoyable\nwhile providing socio-emotional support to augment LVR interventions. The\nin-clinic demo group reported significantly more positive opinions of the SAR's\ncapabilities than did the baseline survey group that used mailed-in forms\nwithout the SAR demo.\n",
        "title": "Designing a Socially Assistive Robot to Support Older Adults with Low\n  Vision",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03330",
        "abstract_url": "http://arxiv.org/abs/2401.03330",
        "authors": [
            {
                "last_name": "Bentbib",
                "first_name": "A. H."
            },
            {
                "last_name": "Ghomari",
                "first_name": "M. EL"
            },
            {
                "last_name": "Jbilou",
                "first_name": "K."
            },
            {
                "last_name": "Sadek",
                "first_name": "EL. M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  In the present paper, we propose a block variant of the extended Hessenberg\nprocess for computing approximations of matrix functions and other problems\nproducing large-scale matrices. Applications to the computation of a matrix\nfunction such as f(A)V, where A is an nxn large sparse matrix, V is an nxp\nblock with p<<n, and f is a function are presented. Solving shifted linear\nsystems with multiple right hand sides are also given. Computing approximations\nof these matrix problems appear in many scientific and engineering\napplications. Different numerical experiments are provided to show the\neffectiveness of the proposed method for these problems.\n",
        "title": "Extended block Hessenberg process for the evaluation of matrix functions",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03331",
        "abstract_url": "http://arxiv.org/abs/2401.03331",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Kaiming"
            },
            {
                "last_name": "Lei",
                "first_name": "Tong"
            },
            {
                "last_name": "Halubok",
                "first_name": "Maryia"
            },
            {
                "last_name": "Bailey",
                "first_name": "Brian N."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  The accurate identification of walnuts within orchards brings forth a\nplethora of advantages, profoundly amplifying the efficiency and productivity\nof walnut orchard management. Nevertheless, the unique characteristics of\nwalnut trees, characterized by their closely resembling shapes, colors, and\ntextures between the walnuts and leaves, present a formidable challenge in\nprecisely distinguishing between them during the annotation process. In this\nstudy, we present a novel approach to improve walnut detection efficiency,\nutilizing YOLOv5 trained on an enriched image set that incorporates both real\nand synthetic RGB and NIR images. Our analysis comparing results from our\noriginal and augmented datasets shows clear improvements in detection when\nusing the synthetic images.\n",
        "title": "Walnut Detection Through Deep Learning Enhanced by Multispectral\n  Synthetic Images",
        "date": "2023-10-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03333",
        "abstract_url": "http://arxiv.org/abs/2401.03333",
        "authors": [
            {
                "last_name": "Hoglund",
                "first_name": "Andreas"
            },
            {
                "last_name": "Mozaffari",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Yang",
                "first_name": "Yanpeng"
            },
            {
                "last_name": "Moschetti",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Kittichokechai",
                "first_name": "Kittipong"
            },
            {
                "last_name": "Nory",
                "first_name": "Ravikiran"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Enhancing the energy efficiency of devices stands as one of the key\nrequirements in the fifth-generation (5G) cellular network and its evolutions\ntoward the next generation wireless technology. Specifically, for\nbattery-limited Internet-of-Things (IoT) devices where downlink monitoring\nsignificantly contributes to energy consumption, efficient solutions are\nrequired for power saving while addressing performance tradeoffs. In this\nregard, the use of a low-power wake-up receiver (WUR) and wake-up signal (WUS)\nis an attractive solution for reducing the energy consumption of devices\nwithout compromising the downlink latency. This paper provides an overview of\nthe standardization study on the design of low-power WUR and WUS within Release\n18 of the third-generation partnership project (3GPP). We describe design\nprinciples, receiver architectures, waveform characteristics, and device\nprocedures upon detection of WUS. In addition, we provide representative\nresults to show the performance of the WUR in terms of power saving, coverage,\nand network overhead along with highlighting design tradeoffs.\n",
        "title": "3GPP Release 18 Wake-up Receiver: Feature Overview and Evaluations",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03336",
        "abstract_url": "http://arxiv.org/abs/2401.03336",
        "authors": [
            {
                "last_name": "Wolf",
                "first_name": "Florian"
            },
            {
                "last_name": "List",
                "first_name": "Florian"
            },
            {
                "last_name": "Rodd",
                "first_name": "Nicholas L."
            },
            {
                "last_name": "Hahn",
                "first_name": "Oliver"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            "LG"
        ],
        "abstract": "  Astronomical observations typically provide three-dimensional maps, encoding\nthe distribution of the observed flux in (1) the two angles of the celestial\nsphere and (2) energy/frequency. An important task regarding such maps is to\nstatistically characterize populations of point sources too dim to be\nindividually detected. As the properties of a single dim source will be poorly\nconstrained, instead one commonly studies the population as a whole, inferring\na source-count distribution (SCD) that describes the number density of sources\nas a function of their brightness. Statistical and machine learning methods for\nrecovering SCDs exist; however, they typically entirely neglect spectral\ninformation associated with the energy distribution of the flux. We present a\ndeep learning framework able to jointly reconstruct the spectra of different\nemission components and the SCD of point-source populations. In a\nproof-of-concept example, we show that our method accurately extracts even\ncomplex-shaped spectra and SCDs from simulated maps.\n",
        "title": "A deep learning framework for jointly extracting spectra and\n  source-count distributions in astronomy",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03337",
        "abstract_url": "http://arxiv.org/abs/2401.03337",
        "authors": [
            {
                "last_name": "Shah",
                "first_name": "Nishaant"
            },
            {
                "last_name": "Tiwari",
                "first_name": "Kshitij"
            },
            {
                "last_name": "Bera",
                "first_name": "Aniket"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Urban search and rescue missions require rapid first response to minimize\nloss of life and damage. Often, such efforts are assisted by humanitarian\nrobots which need to handle dynamic operational conditions such as uneven and\nrough terrains, especially during mass casualty incidents like an earthquake.\nQuadruped robots, owing to their versatile design, have the potential to assist\nin such scenarios. However, control of quadruped robots in dynamic and rough\nterrain environments is a challenging problem due to the many degrees of\nfreedom of these robots. Current locomotion controllers for quadrupeds are\nlimited in their ability to produce multiple adaptive gaits, solve tasks in a\ntime and resource-efficient manner, and require tedious training and manual\ntuning procedures. To address these challenges, we propose MTAC: a multi-gait\nterrain-adaptive controller, which utilizes a Hierarchical reinforcement\nlearning (HRL) approach while being time and memory-efficient. We show that our\nproposed method scales well to a diverse range of environments with similar\ncompute times as state-of-the-art methods. Our method showed greater than 75%\non most tasks, outperforming previous work on the majority of test cases.\n",
        "title": "MTAC: Hierarchical Reinforcement Learning-based Multi-gait\n  Terrain-adaptive Quadruped Controller",
        "date": "2023-11-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03338",
        "abstract_url": "http://arxiv.org/abs/2401.03338",
        "authors": [
            {
                "last_name": "Fay",
                "first_name": "Yvann Le"
            },
            {
                "last_name": "S\u00e4rkk\u00e4",
                "first_name": "Simo"
            },
            {
                "last_name": "Corenflos",
                "first_name": "Adrien"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Probabilistic ordinary differential equation (ODE) solvers have been\nintroduced over the past decade as uncertainty-aware numerical integrators.\nThey typically proceed by assuming a functional prior to the ODE solution,\nwhich is then queried on a grid to form a posterior distribution over the ODE\nsolution. As the queries span the integration interval, the approximate\nposterior solution then converges to the true deterministic one. Gaussian ODE\nfilters, in particular, have enjoyed a lot of attention due to their\ncomputational efficiency, the simplicity of their implementation, as well as\ntheir provable fast convergence rates. In this article, we extend the\nmethodology to stochastic differential equations (SDEs) and propose a\nprobabilistic simulator for SDEs. Our approach involves transforming the SDE\ninto a sequence of random ODEs using piecewise differentiable approximations of\nthe Brownian motion. We then apply probabilistic ODE solvers to the individual\nODEs, resulting in a pathwise probabilistic solution to the SDE\\@. We establish\nworst-case strong $1.5$ local and $1.0$ global convergence orders for a\nspecific instance of our method. We further show how we can marginalise the\nBrownian approximations, by incorporating its coefficients as part of the prior\nODE model, allowing for computing exact transition densities under our model.\nFinally, we numerically validate the theoretical findings, showcasing\nreasonable weak convergence properties in the marginalised version.\n",
        "title": "Modelling pathwise uncertainty of Stochastic Differential Equations\n  samplers via Probabilistic Numerics",
        "date": "2023-11-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03339",
        "abstract_url": "http://arxiv.org/abs/2401.03339",
        "authors": [
            {
                "last_name": "Conradi",
                "first_name": "Jacobus"
            },
            {
                "last_name": "Driemel",
                "first_name": "Anne"
            },
            {
                "last_name": "Kolbe",
                "first_name": "Benedikt"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            "",
            ""
        ],
        "abstract": "  Since its introduction to computational geometry by Alt and Godau in 1992,\nthe Fr\\'echet distance has been a mainstay of algorithmic research on curve\nsimilarity computations. The focus of the research has been on comparing\npolygonal curves, with the notable exception of an algorithm for the decision\nproblem for planar piecewise smooth curves due to Rote (2007). We present an\nalgorithm for the decision problem for piecewise smooth curves that is both\nconceptually simpler and naturally extends to the first algorithm for the\nproblem for piecewise smooth curves in $\\mathbb{R}^d$. We assume that the\nalgorithm is given two continuous curves, each consisting of a sequence of $m$,\nresp.\\ $n$, smooth pieces, where each piece belongs to a sufficiently\nwell-behaved class of curves, such as the set of algebraic curves of bounded\ndegree. We introduce a decomposition of the free space diagram into a\ncontrolled number of pieces that can be used to solve the decision problem\nsimilarly to the polygonal case, in $O(mn)$ time, leading to a computation of\nthe Fr\\'echet distance that runs in $O(mn\\log(mn))$ time. Furthermore, we study\napproximation algorithms for piecewise smooth curves that are also $c$-packed\nfor some fixed value $c$. We adapt the existing framework for\n$(1+\\epsilon)$-approximations and show that an approximate decision can be\ncomputed in $O(cn/\\epsilon)$ time for any $\\epsilon > 0$.\n",
        "title": "Revisiting the Fr\\'echet distance between piecewise smooth curves",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03340",
        "abstract_url": "http://arxiv.org/abs/2401.03340",
        "authors": [
            {
                "last_name": "Vajjarapu",
                "first_name": "Dheeraj"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces the CowStallNumbers dataset, a collection of images\nextracted from videos focusing on cow teats, designed to advance the field of\ncow stall number detection. The dataset comprises 1042 training images and 261\ntest images, featuring stall numbers ranging from 0 to 60. To enhance the\ndataset, we performed fine-tuning on a YOLO model and applied data augmentation\ntechniques, including random crop, center crop, and random rotation. The\nexperimental outcomes demonstrate a notable 95.4\\% accuracy in recognizing\nstall numbers.\n",
        "title": "Classifying cow stall numbers using YOLO",
        "date": "2023-11-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03341",
        "abstract_url": "http://arxiv.org/abs/2401.03341",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Zhangkai"
            },
            {
                "last_name": "Cao",
                "first_name": "Longbing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Junxian"
            },
            {
                "last_name": "Chen",
                "first_name": "Hui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Due to their unsupervised training and uncertainty estimation, deep\nVariational Autoencoders (VAEs) have become powerful tools for\nreconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based\nTSAD methods, either statistical or deep, tune meta-priors to estimate the\nlikelihood probability for effectively capturing spatiotemporal dependencies in\nthe data. However, these methods confront the challenge of inherent data\nscarcity, which is often the case in anomaly detection tasks. Such scarcity\neasily leads to latent holes, discontinuous regions in latent space, resulting\nin non-robust reconstructions on these discontinuous spaces. We propose a novel\ngenerative framework that combines VAEs with self-supervised learning (SSL) to\naddress this issue.\n",
        "title": "Weakly Augmented Variational Autoencoder in Time Series Anomaly\n  Detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03343",
        "abstract_url": "http://arxiv.org/abs/2401.03343",
        "authors": [
            {
                "last_name": "Dutta",
                "first_name": "B."
            },
            {
                "last_name": "Arzoo",
                "first_name": "S."
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL",
            ""
        ],
        "abstract": "  The present study puts forward a novel biographical knowledge graph (KG) on\nProf. S. R. Ranganathan, one of the pioneering figures in the Library and\nInformation Science (LIS) domain. It has been found that most of the relevant\nfacts about Ranganathan exist in a variety of resources (e.g., books, essays,\njournal articles, websites, blogs, etc.), offering information in a fragmented\nand piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to\nfurnish a 360-degree view of his life and achievements. To the best of our\nknowledge, such a dedicated representation is unparalleled in its scope and\ncoverage: using state-of-the-art technology for anyone to openly access,\nuse/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the\nKG was developed using a \"facet-based methodology\" at two levels: in the\nidentification of the vital biographical aspects and the development of the\nontological model. Finally, with this study, we call for a community-driven\neffort to enhance the KG and pay homage to the Father of Library Science on the\nhundredth anniversary of his revitalizing the LIS domain through his enduring\nparticipation.\n",
        "title": "Rediscovering Ranganathan: A Prismatic View of His Life through the\n  Knowledge Graph Spectrum",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03346",
        "abstract_url": "http://arxiv.org/abs/2401.03346",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Keyan"
            },
            {
                "last_name": "Hu",
                "first_name": "Alexander"
            },
            {
                "last_name": "Mu",
                "first_name": "Jaden"
            },
            {
                "last_name": "Shi",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Ziming"
            },
            {
                "last_name": "Vishwamitra",
                "first_name": "Nishant"
            },
            {
                "last_name": "Hu",
                "first_name": "Hongxin"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "",
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.\n",
        "title": "An Investigation of Large Language Models for Real-World Hate Speech\n  Detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03349",
        "abstract_url": "http://arxiv.org/abs/2401.03349",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Anji"
            },
            {
                "last_name": "Niepert",
                "first_name": "Mathias"
            },
            {
                "last_name": "Broeck",
                "first_name": "Guy Van den"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion models are the current state of the art for generating\nphotorealistic images. Controlling the sampling process for constrained image\ngeneration tasks such as inpainting, however, remains challenging since exact\nconditioning on such constraints is intractable. While existing methods use\nvarious techniques to approximate the constrained posterior, this paper\nproposes to exploit the ability of Tractable Probabilistic Models (TPMs) to\nexactly and efficiently compute the constrained posterior, and to leverage this\nsignal to steer the denoising process of diffusion models. Specifically, this\npaper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs).\nBuilding upon prior advances, we further scale up PCs and make them capable of\nguiding the image generation process of diffusion models. Empirical results\nsuggest that our approach can consistently improve the overall quality and\nsemantic coherence of inpainted images across three natural image datasets\n(i.e., CelebA-HQ, ImageNet, and LSUN) with only ~10% additional computational\noverhead brought by the TPM. Further, with the help of an image encoder and\ndecoder, our method can readily accept semantic constraints on specific regions\nof the image, which opens up the potential for more controlled image generation\ntasks. In addition to proposing a new framework for constrained image\ngeneration, this paper highlights the benefit of more tractable models and\nmotivates the development of expressive TPMs.\n",
        "title": "Image Inpainting via Tractable Steering of Diffusion Models",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03350",
        "abstract_url": "http://arxiv.org/abs/2401.03350",
        "authors": [
            {
                "last_name": "Trivedi",
                "first_name": "Puja"
            },
            {
                "last_name": "Heimann",
                "first_name": "Mark"
            },
            {
                "last_name": "Anirudh",
                "first_name": "Rushil"
            },
            {
                "last_name": "Koutra",
                "first_name": "Danai"
            },
            {
                "last_name": "Thiagarajan",
                "first_name": "Jayaraman J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While graph neural networks (GNNs) are widely used for node and graph\nrepresentation learning tasks, the reliability of GNN uncertainty estimates\nunder distribution shifts remains relatively under-explored. Indeed, while\npost-hoc calibration strategies can be used to improve in-distribution\ncalibration, they need not also improve calibration under distribution shift.\nHowever, techniques which produce GNNs with better intrinsic uncertainty\nestimates are particularly valuable, as they can always be combined with\npost-hoc strategies later. Therefore, in this work, we propose G-$\\Delta$UQ, a\nnovel training framework designed to improve intrinsic GNN uncertainty\nestimates. Our framework adapts the principle of stochastic data centering to\ngraph data through novel graph anchoring strategies, and is able to support\npartially stochastic GNNs. While, the prevalent wisdom is that fully stochastic\nnetworks are necessary to obtain reliable estimates, we find that the\nfunctional diversity induced by our anchoring strategies when sampling\nhypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on\npretrained models. Indeed, through extensive evaluation under covariate,\nconcept and graph size shifts, we show that G-$\\Delta$UQ leads to better\ncalibrated GNNs for node and graph classification. Further, it also improves\nperformance on the uncertainty-based tasks of out-of-distribution detection and\ngeneralization gap estimation. Overall, our work provides insights into\nuncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ\nin obtaining reliable estimates.\n",
        "title": "Accurate and Scalable Estimation of Epistemic Uncertainty for Graph\n  Neural Networks",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03351",
        "abstract_url": "http://arxiv.org/abs/2401.03351",
        "authors": [
            {
                "last_name": "Razumovsky",
                "first_name": "A. V."
            },
            {
                "last_name": "Saramud",
                "first_name": "M. V."
            },
            {
                "last_name": "Tkachev",
                "first_name": "S. B."
            },
            {
                "last_name": "Shtabel",
                "first_name": "N. V."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Algorithms for creating optimal configurations of warehouse threedimensional\nsystems consisting of unified transport and warehouse modules are presented in\nthe article. The constructions of these modules, and methods for constructing a\nwarehouse array map are described. Each module has a cubic shape and is\nconnected to each other with any of the 6 faces. In order to save money, some\nof the modules do not have drives to move the load in the vertical direction.\nWhen building a warehouse, a problem to determine the optimal warehouse\nconfiguration in terms of cost-performance ratio arises.\n",
        "title": "Algorithms for synthesis of three-dimensional warehouse systems\n  configurations optimal in terms of minimum cost and maximum speed",
        "date": "2023-08-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03352",
        "abstract_url": "http://arxiv.org/abs/2401.03352",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Rui"
            },
            {
                "last_name": "Pourmousavi",
                "first_name": "S. Ali"
            },
            {
                "last_name": "Soong",
                "first_name": "Wen L."
            },
            {
                "last_name": "Liisberg",
                "first_name": "Jon A. R."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Detecting behind-the-meter (BTM) equipment and major appliances at the\nresidential level and tracking their changes in real time is important for\naggregators and traditional electricity utilities. In our previous work, we\ndeveloped a systematic solution called IRMAC to identify residential users' BTM\nequipment and applications from their imported energy data. As a part of IRMAC,\na Similarity Profile (SP) was proposed for dimensionality reduction and\nextracting self-join similarity from the end users' daily electricity usage\ndata. The proposed SP calculation, however, was computationally expensive and\nrequired a significant amount of memory at the user's end. To realise the\nbenefits of edge computing, in this paper, we propose and assess three\ncomputationally-efficient updating solutions, namely additive, fixed memory,\nand codebook-based updating methods. Extensive simulation studies are carried\nout using real PV users' data to evaluate the performance of the proposed\nmethods in identifying PV users, tracking changes in real time, and examining\nmemory usage. We found that the Codebook-based solution reduces more than 30\\%\nof the required memory without compromising the performance of extracting\nusers' features. When the end users' data storage and computation speed are\nconcerned, the fixed-memory method outperforms the others. In terms of tracking\nthe changes, different variations of the fixed-memory method show various\ninertia levels, making them suitable for different applications.\n",
        "title": "Dynamic and Memory-efficient Shape Based Methodologies for User Type\n  Identification in Smart Grid Applications",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03353",
        "abstract_url": "http://arxiv.org/abs/2401.03353",
        "authors": [
            {
                "last_name": "Heller",
                "first_name": "Thomas"
            },
            {
                "last_name": "Diehl",
                "first_name": "Patrick"
            },
            {
                "last_name": "Byerly",
                "first_name": "Zachary"
            },
            {
                "last_name": "Biddiscombe",
                "first_name": "John"
            },
            {
                "last_name": "Kaiser",
                "first_name": "Hartmut"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  To achieve scalability with today's heterogeneous HPC resources, we need a\ndramatic shift in our thinking; MPI+X is not enough. Asynchronous Many Task\n(AMT) runtime systems break down the global barriers imposed by the Bulk\nSynchronous Programming model. HPX is an open-source, C++ Standards compliant\nAMT runtime system that is developed by a diverse international community of\ncollaborators called The Ste||ar Group. HPX provides features which allow\napplication developers to naturally use key design patterns, such as\noverlapping communication and computation, decentralizing of control flow,\noversubscribing execution resources and sending work to data instead of data to\nwork. The Ste||ar Group comprises physicists, engineers, and computer\nscientists; men and women from many different institutions and affiliations,\nand over a dozen different countries. We are committed to advancing the\ndevelopment of scalable parallel applications by providing a platform for\ncollaborating and exchanging ideas. In this paper, we give a detailed\ndescription of the features HPX provides and how they help achieve scalability\nand programmability, a list of applications of HPX including two large NSF\nfunded collaborations (STORM, for storm surge forecasting; and STAR (OctoTiger)\nan astro-physics project which runs at 96.8% parallel efficiency on 643,280\ncores), and we end with a description of how HPX and the Ste||ar Group fit into\nthe open source community.\n",
        "title": "HPX -- An open source C++ Standard Library for Parallelism and\n  Concurrency",
        "date": "2023-08-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03357",
        "abstract_url": "http://arxiv.org/abs/2401.03357",
        "authors": [
            {
                "last_name": "Chizhik",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Du",
                "first_name": "Jinfeng"
            },
            {
                "last_name": "Valenzuela",
                "first_name": "Reinaldo"
            },
            {
                "last_name": "Bedin",
                "first_name": "Andrea"
            },
            {
                "last_name": "Moisio",
                "first_name": "Martti"
            },
            {
                "last_name": "Feick",
                "first_name": "Rodolfo"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  28 GHz outdoor-indoor coverage into modern office buildings with high thermal\nefficiency windows is found to be severely limited due to 46 dB median\npenetration loss at normal incidence and additional 15 dB median oblique\nincidence loss. The study is based on measurements of path gain over 280\noutdoor-indoor links, at ranges up to 100 m. A simple theoretical path gain\nmodel is extended to include building penetration through multiple sides of the\nbuilding as well as a reflection from another building. The theoretical model\naccounts for the building orientation relative to the source, resulting in 4.9\ndB RMSE relative to data, as compared to 5.7 dB RMSE from a linear fit and 14.7\ndB RMSE for the 3GPP recommended model. Only coarse description of the\nbuildings is required: building orientation and exterior wall composition,\nwithout any interior details. Coverage range for SNR>-8 dB from an outdoor base\nto a terminal just inside a high-efficiency building is under 35 m\n",
        "title": "Measured and Modeled Outdoor Indoor Coverage at 28 GHz into High Thermal\n  Efficiency Buildings",
        "date": "2023-09-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03358",
        "abstract_url": "http://arxiv.org/abs/2401.03358",
        "authors": [
            {
                "last_name": "Bendel",
                "first_name": "Oliver"
            },
            {
                "last_name": "Graf",
                "first_name": "Emanuel"
            },
            {
                "last_name": "Bollier",
                "first_name": "Kevin"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            ""
        ],
        "abstract": "  Semi-autonomous machines, autonomous machines and robots inhabit closed,\nsemi-closed and open environments, more structured environments like the\nhousehold or more unstructured environments like cultural landscapes or the\nwilderness. There they encounter domestic animals, farm animals, working\nanimals, and wild animals. These creatures could be disturbed, displaced,\ninjured, or killed by the machines. Within the context of machine ethics and\nsocial robotics, the School of Business FHNW developed several design studies\nand prototypes for animal-friendly machines, which can be understood as moral\nand social machines in the spirit of these disciplines. In 2019-20, a team led\nby the main author developed a prototype robot lawnmower that can recognize\nhedgehogs, interrupt its work for them and thus protect them. Every year many\nof these animals die worldwide because of traditional service robots. HAPPY\nHEDGEHOG (HHH), as the invention is called, could be a solution to this\nproblem. This article begins by providing an introduction to the background.\nThen it focuses on navigation (where the machine comes across certain objects\nthat need to be recognized) and thermal and image recognition (with the help of\nmachine learning) of the machine. It also presents obvious weaknesses and\npossible improvements. The results could be relevant for an industry that wants\nto market their products as animal-friendly machines.\n",
        "title": "The HAPPY HEDGEHOG Project",
        "date": "2023-08-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03359",
        "abstract_url": "http://arxiv.org/abs/2401.03359",
        "authors": [
            {
                "last_name": "Perini",
                "first_name": "Massimo"
            },
            {
                "last_name": "Nikolic",
                "first_name": "Milos"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  Missing data is a widespread problem in many domains, creating challenges in\ndata analysis and decision making. Traditional techniques for dealing with\nmissing data, such as excluding incomplete records or imputing simple estimates\n(e.g., mean), are computationally efficient but may introduce bias and disrupt\nvariable relationships, leading to inaccurate analyses. Model-based imputation\ntechniques offer a more robust solution that preserves the variability and\nrelationships in the data, but they demand significantly more computation time,\nlimiting their applicability to small datasets.\n  This work enables efficient, high-quality, and scalable data imputation\nwithin a database system using the widely used MICE method. We adapt this\nmethod to exploit computation sharing and a ring abstraction for faster model\ntraining. To impute both continuous and categorical values, we develop\ntechniques for in-database learning of stochastic linear regression and\nGaussian discriminant analysis models. Our MICE implementations in PostgreSQL\nand DuckDB outperform alternative MICE implementations and model-based\nimputation techniques by up to two orders of magnitude in terms of computation\ntime, while maintaining high imputation quality.\n",
        "title": "In-Database Data Imputation",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03360",
        "abstract_url": "http://arxiv.org/abs/2401.03360",
        "authors": [
            {
                "last_name": "Mishra",
                "first_name": "Utkarsh A."
            },
            {
                "last_name": "Xue",
                "first_name": "Shangjie"
            },
            {
                "last_name": "Chen",
                "first_name": "Yongxin"
            },
            {
                "last_name": "Xu",
                "first_name": "Danfei"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Long-horizon tasks, usually characterized by complex subtask dependencies,\npresent a significant challenge in manipulation planning. Skill chaining is a\npractical approach to solving unseen tasks by combining learned skill priors.\nHowever, such methods are myopic if sequenced greedily and face scalability\nissues with search-based planning strategy. To address these challenges, we\nintroduce Generative Skill Chaining~(GSC), a probabilistic framework that\nlearns skill-centric diffusion models and composes their learned distributions\nto generate long-horizon plans during inference. GSC samples from all skill\nmodels in parallel to efficiently solve unseen tasks while enforcing geometric\nconstraints. We evaluate the method on various long-horizon tasks and\ndemonstrate its capability in reasoning about action dependencies, constraint\nhandling, and generalization, along with its ability to replan in the face of\nperturbations. We show results in simulation and on real robot to validate the\nefficiency and scalability of GSC, highlighting its potential for advancing\nlong-horizon task planning. More details are available at:\nhttps://generative-skill-chaining.github.io/\n",
        "title": "Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion\n  Models",
        "date": "2023-10-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03362",
        "abstract_url": "http://arxiv.org/abs/2401.03362",
        "authors": [
            {
                "last_name": "Alvarez-Hidalgo",
                "first_name": "Laura"
            },
            {
                "last_name": "Howard",
                "first_name": "Ian S."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  We describe the construction and evaluation of two robotic grippers for berry\npicking. Using a pneumatic cylinder drive, one was constructed from hard\nmaterials and the other from soft materials. A novel evaluation paradigm using\na handle mechanism was developed, so the grippers could be directly op-erated\nby human participants. An artificial bush was also constructed and used for\nevaluation purposes. Overall, both grippers performed worse than the human\nhand, indicating that further development is needed.\n",
        "title": "Human evaluation of robotic grippers for berry picking",
        "date": "2023-09-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03363",
        "abstract_url": "http://arxiv.org/abs/2401.03363",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Tao"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Wen",
                "first_name": "Guanghui"
            },
            {
                "last_name": "Duan",
                "first_name": "Zhisheng"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper revisits the event-triggered control problem from a data-driven\nperspective, where unknown continuous-time linear systems subject to\ndisturbances are taken into account. Using data information collected off-line\ninstead of accurate system model information, a data-driven dynamic\nevent-triggered control scheme is developed in this paper. The dynamic property\nis reflected by that the designed event-triggering function embedded in the\nevent-triggering mechanism (ETM) is dynamically updated as a whole. Thanks to\nthis dynamic design, a strictly positive minimum inter-event time (MIET) is\nguaranteed without sacrificing control performance. Specifically, exponential\ninput-to-state stability (ISS) of the closed-loop system with respect to\ndisturbances is achieved in this paper, which is superior to some existing\nresults that only guarantee a practical exponential ISS property. The dynamic\nETM is easy-to-implement in practical operation since all designed parameters\nare determined only by a simple data-driven linear matrix inequality (LMI),\nwithout additional complicated conditions as required in relevant literature.\nAs quantization is the most common signal constraint in practice, the developed\ncontrol scheme is further extended to the case where state transmission is\naffected by a uniform or logarithmic quantization effect. Finally, adequate\nsimulations are performed to show the validity and superiority of the proposed\ncontrol schemes.\n",
        "title": "Data-driven Dynamic Event-triggered Control",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03365",
        "abstract_url": "http://arxiv.org/abs/2401.03365",
        "authors": [
            {
                "last_name": "Khabibulin",
                "first_name": "Marat Ildarovich"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Currently, the area of geometric modeling and the construction of 3D models\nbased on point clouds from laser sensors is actively developing. One of the\nbasic tasks of geometric modeling is the reconstruction of a surface from a\ncloud of points. The aim of this work is to develop a parallel least-squares\nsurface reconstruction method on a distributed memory supercomputer that allows\nachieving optimal results both in scalability and surface reconstruction\nquality. The focus of the work is on the surface reconstruction algorithm based\non the least squares method, in connection with which the algorithm was called\nthe moving least squares method.\n",
        "title": "Investigation of the efficiency of the moving least squares method in\n  the reconstruction of a three-dimensional surface on a supercomputer",
        "date": "2023-09-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03370",
        "abstract_url": "http://arxiv.org/abs/2401.03370",
        "authors": [
            {
                "last_name": "R.",
                "first_name": "Aiswarya"
            },
            {
                "last_name": "Shaik",
                "first_name": "Rasheed"
            },
            {
                "last_name": "Jose",
                "first_name": "Jobin"
            },
            {
                "last_name": "Varma",
                "first_name": "Hari R."
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Himadri S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Time delay in a projectile-target scattering is a fundamental tool in\nunderstanding their interactions by probing the temporal domain. The present\nstudy focuses on computing and analyzing the Eisenbud-Wigner-Smith (EWS) time\ndelay in low energy elastic e C60 scattering. The investigation is carried out\nin the framework of a non-relativistic partial wave analysis (PWA) technique.\nThe projectile-target interaction is described in (1) Density Functional Theory\n(DFT) and (2) Annular Square Well (ASW) static model, and their final results\nare compared in details. The impact of polarization on resonant and\nnon-resonant time delay is also investigated.\n",
        "title": "EWS time delay in low energy e C60 elastic scattering",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03372",
        "abstract_url": "http://arxiv.org/abs/2401.03372",
        "authors": [
            {
                "last_name": "Kumar",
                "first_name": "Prabhat"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Demands for pneumatic-driven soft robots are constantly rising for various\napplications. However, they are often designed manually due to the lack of\nsystematic methods. Moreover, design-dependent characteristics of pneumatic\nactuation pose distinctive challenges. This paper provides a compact MATLAB\ncode, named SoRoTop, and its various extensions for designing pneumatic-driven\nsoft robots using topology optimization. The code uses the method of moving\nasymptotes as the optimizer and builds upon the approach initially presented in\nKumar et al.(Struct Multidiscip Optim 61 (4): 1637-1655, 2020). The pneumatic\nload is modeled using Darcy's law with a conceptualized drainage term.\nConsistent nodal loads are determined from the resultant pressure field using\nthe conventional finite element approach. The robust formulation is employed,\ni.e., the eroded and blueprint design descriptions are used. A min-max\noptimization problem is formulated using the output displacements of the eroded\nand blueprint designs. A volume constraint is imposed on the blueprint design,\nwhile the eroded design is used to apply a conceptualized strain energy\nconstraint. The latter constraint aids in attaining optimized designs that can\nendure the applied load without compromising their performance. Sensitivities\nrequired for optimization are computed using the adjoint-variable method. The\ncode is explained in detail, and various extensions are also presented. It is\nstructured into pre-optimization, MMA optimization, and post-optimization\noperations, each of which is comprehensively detailed. The paper also\nillustrates the impact of load sensitivities on the optimized designs. SoRoTop\nis provided in Appendix A and is available with extensions in the supplementary\nmaterial and publicly at \\url{https://github.com/PrabhatIn/SoRoTop}.\n",
        "title": "SoRoTop: a hitchhiker's guide to topology optimization MATLAB code for\n  design-dependent pneumatic-driven soft robots",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03374",
        "abstract_url": "http://arxiv.org/abs/2401.03374",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Nafis Tanveer"
            },
            {
                "last_name": "Khoury",
                "first_name": "Joseph"
            },
            {
                "last_name": "Seong",
                "first_name": "Andrew"
            },
            {
                "last_name": "Parra",
                "first_name": "Gonzalo De La Torre"
            },
            {
                "last_name": "Bou-Harb",
                "first_name": "Elias"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  In software development, the predominant emphasis on functionality often\nsupersedes security concerns, a trend gaining momentum with AI-driven\nautomation tools like GitHub Copilot. These tools significantly improve\ndevelopers' efficiency in functional code development. Nevertheless, it remains\na notable concern that such tools are also responsible for creating insecure\ncode, predominantly because of pre-training on publicly available repositories\nwith vulnerable code. Moreover, developers are called the \"weakest link in the\nchain\" since they have very minimal knowledge of code security. Although\nexisting solutions provide a reasonable solution to vulnerable code, they must\nadequately describe and educate the developers on code security to ensure that\nthe security issues are not repeated. Therefore we introduce a multipurpose\ncode vulnerability analysis system \\texttt{SecRepair}, powered by a large\nlanguage model, CodeGen2 assisting the developer in identifying and generating\nfixed code along with a complete description of the vulnerability with a code\ncomment. Our innovative methodology uses a reinforcement learning paradigm to\ngenerate code comments augmented by a semantic reward mechanism. Inspired by\nhow humans fix code issues, we propose an instruction-based dataset suitable\nfor vulnerability analysis with LLMs. We further identify zero-day and N-day\nvulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings\nunderscore that incorporating reinforcement learning coupled with semantic\nreward augments our model's performance, thereby fortifying its capacity to\naddress code vulnerabilities with improved efficacy.\n",
        "title": "LLM-Powered Code Vulnerability Repair with Reinforcement Learning and\n  Semantic Reward",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03378",
        "abstract_url": "http://arxiv.org/abs/2401.03378",
        "authors": [
            {
                "last_name": "Rudi",
                "first_name": "Johann"
            },
            {
                "last_name": "Lee",
                "first_name": "Youngjun"
            },
            {
                "last_name": "Chadha",
                "first_name": "Aidan H."
            },
            {
                "last_name": "Wahib",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Weide",
                "first_name": "Klaus"
            },
            {
                "last_name": "O'Neal",
                "first_name": "Jared P."
            },
            {
                "last_name": "Dubey",
                "first_name": "Anshu"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            ""
        ],
        "abstract": "  CG-Kit is a new code generation toolkit that we propose as a solution for\nportability and maintainability for scientific computing applications. The\ndevelopment of CG-Kit is rooted in the urgent need created by the shifting\nlandscape of high-performance computing platforms and the algorithmic\ncomplexities of a particular large-scale multiphysics application: Flash-X.\nThis combination leads to unique challenges including handling an existing\nlarge code base in Fortran and/or C/C++, subdivision of code into a great\nvariety of units supporting a wide range of physics and numerical methods,\ndifferent parallelization techniques for distributed- and shared-memory systems\nand accelerator devices, and heterogeneity of computing platforms requiring\ncoexisting variants of parallel algorithms. The challenges demand that\ndevelopers determine custom abstractions and granularity for code generation.\nCG-Kit tackles this with standalone tools that can be combined into highly\nspecific and, we argue, highly effective portability and maintainability tool\nchains. Here we present the design of our new tools: parametrized source trees,\ncontrol flow graphs, and recipes. The tools are implemented in Python. Although\nthe tools are agnostic to the programming language of the source code, we focus\non C/C++ and Fortran. Code generation experiments demonstrate the generation of\nvariants of parallel algorithms: first, multithreaded variants of the basic\nAXPY operation (scalar-vector addition and vector-vector multiplication) to\nintroduce the application of CG-Kit tool chains; and second, variants of\nparallel algorithms within a hydrodynamics solver, called Spark, from Flash-X\nthat operates on block-structured adaptive meshes. In summary, code generated\nby CG-Kit achieves a reduction by over 60% of the original C/C++/Fortran source\ncode.\n",
        "title": "CG-Kit: Code Generation Toolkit for Performant and Maintainable Variants\n  of Source Code Applied to Flash-X Hydrodynamics Simulations",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03379",
        "abstract_url": "http://arxiv.org/abs/2401.03379",
        "authors": [
            {
                "last_name": "Kong",
                "first_name": "Xiangtao"
            },
            {
                "last_name": "Dong",
                "first_name": "Chao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While single task image restoration (IR) has achieved significant successes,\nit remains a challenging issue to train a single model which can tackle\nmultiple IR tasks. In this work, we investigate in-depth the multiple-in-one\n(MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO\nIR faces two pivotal challenges: the optimization of diverse objectives and the\nadaptation to multiple tasks. To tackle these challenges, we present two simple\nyet effective strategies. The first strategy, referred to as sequential\nlearning, attempts to address how to optimize the diverse objectives, which\nguides the network to incrementally learn individual IR tasks in a sequential\nmanner rather than mixing them together. The second strategy, i.e., prompt\nlearning, attempts to address how to adapt to the different IR tasks, which\nassists the network to understand the specific task and improves the\ngeneralization ability. By evaluating on 19 test sets, we demonstrate that the\nsequential and prompt learning strategies can significantly enhance the MiO\nperformance of commonly used CNN and Transformer backbones. Our experiments\nalso reveal that the two strategies can supplement each other to learn better\ndegradation representations and enhance the model robustness. It is expected\nthat our proposed MiO IR formulation and strategies could facilitate the\nresearch on how to train IR models with higher generalization capabilities.\n",
        "title": "Towards Effective Multiple-in-One Image Restoration: A Sequential and\n  Prompt Learning Strategy",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03381",
        "abstract_url": "http://arxiv.org/abs/2401.03381",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Lun"
            },
            {
                "last_name": "Yang",
                "first_name": "Haoxiang"
            },
            {
                "last_name": "Cao",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Guan",
                "first_name": "Xiaohong"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Unscheduled islanding events of microgrids result in the transition between\ngrid-connected and islanded modes and induce a sudden and unknown power\nimbalance, posing a threat to frequency security. To achieve seamless\nislanding, we propose a distributionally robust frequency-constrained microgrid\nscheduling model considering unscheduled islanding events. This model\nco-optimizes unit commitments, power dispatch, upward/downward primary\nfrequency response reserves, virtual inertia provisions from renewable energy\nsources (RESs), deloading ratios of RESs, and battery operations, while\nensuring the system frequency security during unscheduled islanding. We\nestablish an affine relationship between the actual power exchange and RES\nuncertainty in grid-connected mode, describe RES uncertainty with a\nWasserstein-metric ambiguity set, and formulate frequency constraints under\nuncertain post-islanding power imbalance as distributionally robust quadratic\nchance constraints, which are further transformed by a tight conic relaxation.\nWe solve the proposed mixed-integer convex program and demonstrate its\neffectiveness through case studies.\n",
        "title": "Distributionally Robust Frequency-Constrained Microgrid Scheduling\n  Towards Seamless Islanding",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03384",
        "abstract_url": "http://arxiv.org/abs/2401.03384",
        "authors": [
            {
                "last_name": "Rabbani",
                "first_name": "Tahseen"
            },
            {
                "last_name": "Su",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Chan",
                "first_name": "David"
            },
            {
                "last_name": "Sangston",
                "first_name": "Geoffrey"
            },
            {
                "last_name": "Huang",
                "first_name": "Furong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Modern ConvNets continue to achieve state-of-the-art results over a vast\narray of vision and image classification tasks, but at the cost of increasing\nparameters. One strategy for compactifying a network without sacrificing much\nexpressive power is to reshape it into a tensorial neural network (TNN), which\nis a higher-order tensorization of its layers, followed by a factorization,\nsuch as a CP-decomposition, which strips a weight down to its critical basis\ncomponents. Passes through TNNs can be represented as sequences of multilinear\noperations (MLOs), where the evaluation path can greatly affect the number of\nfloating point operations (FLOPs) incurred. While functions such as the popular\neinsum can evaluate simple MLOs such as contractions, existing implementations\ncannot process multi-way convolutions, resulting in scant assessments of how\noptimal evaluation paths through tensorized convolutional layers can improve\ntraining speed. In this paper, we develop a unifying framework for representing\ntensorial convolution layers as einsum-like strings and a meta-algorithm\nconv_einsum which is able to evaluate these strings in a FLOPs-minimizing\nmanner. Comprehensive experiments, using our open-source implementation, over a\nwide range of models, tensor decompositions, and diverse tasks, demonstrate\nthat conv_einsum significantly increases both computational and\nmemory-efficiency of convolutional TNNs.\n",
        "title": "conv_einsum: A Framework for Representation and Fast Evaluation of\n  Multilinear Operations in Convolutional Tensorial Neural Networks",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03386",
        "abstract_url": "http://arxiv.org/abs/2401.03386",
        "authors": [
            {
                "last_name": "Aram",
                "first_name": "Khalid Y."
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  This study focuses on order dispatch decisions within two-echelon supply\nchains, where order dispatch creates economic shipments to reduce delivery\ncosts. Dispatching orders is often constrained by delivery windows, leading to\npenalty costs for untimely deliveries. Prolonged dispatch times can increase\nthe lead time of orders and potentially violate these delivery windows. To\nbalance the trade-offs between lead time and economic delivery, this study\nintroduces a simulation-optimization approach for determining optimal ordering\nand dispatch rules. It emphasizes the intricacies of the order dispatch process\nand explores how these can be integrated into the simulation-optimization\nprocedure to improve ordering and delivery decisions. The study evaluates\nvarious options for implementing dispatch rules, including the number of\ndispatch queues and prioritized dispatch. The results indicate that a\nsingle-queue, quantity-based, first-in-first-out dispatch approach achieves the\ngreatest cost reduction while maintaining a desirable service level.\n",
        "title": "Optimizing Order Dispatch Decisions under Delivery Window Constraints",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03388",
        "abstract_url": "http://arxiv.org/abs/2401.03388",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Connie"
            },
            {
                "last_name": "Xu",
                "first_name": "Yiqing"
            },
            {
                "last_name": "Hsu",
                "first_name": "David"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CL",
            "LG"
        ],
        "abstract": "  The advantages of pre-trained large language models (LLMs) are apparent in a\nvariety of language processing tasks. But can a language model's knowledge be\nfurther harnessed to effectively disambiguate objects and navigate\ndecision-making challenges within the realm of robotics? Our study reveals the\nLLM's aptitude for solving complex decision making challenges that are often\npreviously modeled by Partially Observable Markov Decision Processes (POMDPs).\nA pivotal focus of our research is the object disambiguation capability of\nLLMs. We detail the integration of an LLM into a tabletop environment\ndisambiguation task, a decision making problem where the robot's task is to\ndiscern and retrieve a user's desired object from an arbitrarily large and\ncomplex cluster of objects. Despite multiple query attempts with zero-shot\nprompt engineering (details can be found in the Appendix), the LLM struggled to\ninquire about features not explicitly provided in the scene description. In\nresponse, we have developed a few-shot prompt engineering system to improve the\nLLM's ability to pose disambiguating queries. The result is a model capable of\nboth using given features when they are available and inferring new relevant\nfeatures when necessary, to successfully generate and navigate down a precise\ndecision tree to the correct object--even when faced with identical options.\n",
        "title": "LLMs for Robotic Object Disambiguation",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03389",
        "abstract_url": "http://arxiv.org/abs/2401.03389",
        "authors": [
            {
                "last_name": "Hemel",
                "first_name": "Md. Shahriar Khan"
            },
            {
                "last_name": "Reaz",
                "first_name": "Mamun Bin Ibne"
            },
            {
                "last_name": "Ali",
                "first_name": "Sawal Hamid Bin Md"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Mohammad Arif Sobhan"
            },
            {
                "last_name": "Miraz",
                "first_name": "Mahdi H."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The Internet of Things (IoT) is pivotal in transforming the way we live and\ninteract with our surroundings. To cope with the advancement in technologies,\nit is vital to acquire accuracy with the speed. A phase frequency detector\n(PFD) is a critical device to regulate and provide accurate frequency in IoT\ndevices. Designing a PFD poses challenges in achieving precise phase detection,\nminimising dead zones, optimising power consumption, and ensuring robust\nperformance across various operational frequencies, necessitating complex\nengineering and innovative solutions. This study delves into optimising a PFD\ncircuit, designed using 90 nm standard CMOS technology, aiming to achieve\nsuperior operational frequencies. An efficient and high-frequency PFD design is\ncrafted and analysed using cadence virtuoso. The study focused on investigating\nthe impact of optimising PFD design. With the optimised PFD, an operational\nfrequency of 5 GHz has been achieved, along with a power consumption of only 29\n{\\mu}W. The dead zone of the PFD was only 25 ps.\n",
        "title": "Optimisation and Performance Computation of a Phase Frequency Detector\n  Module for IoT Devices",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03390",
        "abstract_url": "http://arxiv.org/abs/2401.03390",
        "authors": [
            {
                "last_name": "Aawar",
                "first_name": "Majd Al"
            },
            {
                "last_name": "Mutnuri",
                "first_name": "Srikar"
            },
            {
                "last_name": "Montazerin",
                "first_name": "Mansooreh"
            },
            {
                "last_name": "Srivastava",
                "first_name": "Ajitesh"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  During the COVID-19 pandemic, a major driver of new surges has been the\nemergence of new variants. When a new variant emerges in one or more countries,\nother nations monitor its spread in preparation for its potential arrival. The\nimpact of the variant and the timing of epidemic peaks in a country highly\ndepend on when the variant arrives. The current methods for predicting the\nspread of new variants rely on statistical modeling, however, these methods\nwork only when the new variant has already arrived in the region of interest\nand has a significant prevalence. The question arises: Can we predict when (and\nif) a variant that exists elsewhere will arrive in a given country and reach a\ncertain prevalence? We propose a variant-dynamics-informed Graph Neural Network\n(GNN) approach. First, We derive the dynamics of variant prevalence across\npairs of regions (countries) that applies to a large class of epidemic models.\nThe dynamics suggest that ratios of variant proportions lead to simpler\npatterns. Therefore, we use ratios of variant proportions along with some\nparameters estimated from the dynamics as features in a GNN. We develop a\nbenchmarking tool to evaluate variant emergence prediction over 87 countries\nand 36 variants. We leverage this tool to compare our GNN-based approach\nagainst our dynamics-only model and a number of machine learning models.\nResults show that the proposed dynamics-informed GNN method retrospectively\noutperforms all the baselines, including the currently pervasive framework of\nPhysics-Informed Neural Networks (PINNs) that incorporates the dynamics in the\nloss function.\n",
        "title": "Global Prediction of COVID-19 Variant Emergence Using Dynamics-Informed\n  Graph Neural Networks",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03391",
        "abstract_url": "http://arxiv.org/abs/2401.03391",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Yansheng"
            },
            {
                "last_name": "Heng",
                "first_name": "Ziling"
            },
            {
                "last_name": "Li",
                "first_name": "Chengju"
            },
            {
                "last_name": "Ding",
                "first_name": "Cunsheng"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  MDS codes have diverse practical applications in communication systems, data\nstorage, and quantum codes due to their algebraic properties and optimal\nerror-correcting capability. In this paper, we focus on a class of linear codes\nand establish some sufficient and necessary conditions for them being MDS.\nNotably, these codes differ from Reed-Solomon codes up to monomial equivalence.\nAdditionally, we also explore the cases in which these codes are almost MDS or\nnear MDS. Applying our main results, we determine the covering radii and deep\nholes of the dual codes associated with specific Roth-Lempel codes and discover\nan infinite family of (almost) optimally extendable codes with dimension three.\n",
        "title": "More MDS codes of non-Reed-Solomon type",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03395",
        "abstract_url": "http://arxiv.org/abs/2401.03395",
        "authors": [
            {
                "last_name": "Quan",
                "first_name": "Weize"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiaxi"
            },
            {
                "last_name": "Liu",
                "first_name": "Yanli"
            },
            {
                "last_name": "Yan",
                "first_name": "Dong-Ming"
            },
            {
                "last_name": "Wonka",
                "first_name": "Peter"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image and video inpainting is a classic problem in computer vision and\ncomputer graphics, aiming to fill in the plausible and realistic content in the\nmissing areas of images and videos. With the advance of deep learning, this\nproblem has achieved significant progress recently. The goal of this paper is\nto comprehensively review the deep learning-based methods for image and video\ninpainting. Specifically, we sort existing methods into different categories\nfrom the perspective of their high-level inpainting pipeline, present different\ndeep learning architectures, including CNN, VAE, GAN, diffusion models, etc.,\nand summarize techniques for module design. We review the training objectives\nand the common benchmark datasets. We present evaluation metrics for low-level\npixel and high-level perceptional similarity, conduct a performance evaluation,\nand discuss the strengths and weaknesses of representative inpainting methods.\nWe also discuss related real-world applications. Finally, we discuss open\nchallenges and suggest potential future research directions.\n",
        "title": "Deep Learning-based Image and Video Inpainting: A Survey",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03401",
        "abstract_url": "http://arxiv.org/abs/2401.03401",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Wei"
            },
            {
                "last_name": "Mao",
                "first_name": "Shaoguang"
            },
            {
                "last_name": "Zheng",
                "first_name": "Chanjing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models have demonstrated exceptional capabilities in tasks\ninvolving natural language generation, reasoning, and comprehension. This study\naims to construct prompts and comments grounded in the diverse scoring criteria\ndelineated within the official TOEFL guide. The primary objective is to assess\nthe capabilities and constraints of ChatGPT, a prominent representative of\nlarge language models, within the context of automated essay scoring. The\nprevailing methodologies for automated essay scoring involve the utilization of\ndeep neural networks, statistical machine learning techniques, and fine-tuning\npre-trained models. However, these techniques face challenges when applied to\ndifferent contexts or subjects, primarily due to their substantial data\nrequirements and limited adaptability to small sample sizes. In contrast, this\nstudy employs ChatGPT to conduct an automated evaluation of English essays,\neven with a small sample size, employing an experimental approach. The\nempirical findings indicate that ChatGPT can provide operational functionality\nfor automated essay scoring, although the results exhibit a regression effect.\nIt is imperative to underscore that the effective design and implementation of\nChatGPT prompts necessitate a profound domain expertise and technical\nproficiency, as these prompts are subject to specific threshold criteria.\nKeywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent\nWriting Task\n",
        "title": "Empirical Study of Large Language Models as Automated Essay Scoring\n  Tools in English Composition__Taking TOEFL Independent Writing Task for\n  Example",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03403",
        "abstract_url": "http://arxiv.org/abs/2401.03403",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Hao"
            },
            {
                "last_name": "Long",
                "first_name": "Da"
            },
            {
                "last_name": "Yuan",
                "first_name": "Li"
            },
            {
                "last_name": "Tian",
                "first_name": "Yonghong"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinchang"
            },
            {
                "last_name": "Mo",
                "first_name": "Fanyang"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Chiral molecule assignation is crucial for asymmetric catalysis, functional\nmaterials, and the drug industry. The conventional approach requires\ntheoretical calculations of electronic circular dichroism (ECD) spectra, which\nis time-consuming and costly. To speed up this process, we have incorporated\ndeep learning techniques for the ECD prediction. We first set up a large-scale\ndataset of Chiral Molecular ECD spectra (CMCDS) with calculated ECD spectra. We\nfurther develop the ECDFormer model, a Transformer-based model to learn the\nchiral molecular representations and predict corresponding ECD spectra with\nimproved efficiency and accuracy. Unlike other models for spectrum prediction,\nour ECDFormer creatively focused on peak properties rather than the whole\nspectrum sequence for prediction, inspired by the scenario of chiral molecule\nassignation. Specifically, ECDFormer predicts the peak properties, including\nnumber, position, and symbol, then renders the ECD spectra from these peak\nproperties, which significantly outperforms other models in ECD prediction, Our\nECDFormer reduces the time of acquiring ECD spectra from 1-100 hours per\nmolecule to 1.5s.\n",
        "title": "Deep peak property learning for efficient chiral molecules ECD spectra\n  prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03406",
        "abstract_url": "http://arxiv.org/abs/2401.03406",
        "authors": [
            {
                "last_name": "Zare",
                "first_name": "Nader"
            },
            {
                "last_name": "Amini",
                "first_name": "Omid"
            },
            {
                "last_name": "Sayareh",
                "first_name": "Aref"
            },
            {
                "last_name": "Sarvmaili",
                "first_name": "Mahtab"
            },
            {
                "last_name": "Firouzkouhi",
                "first_name": "Arad"
            },
            {
                "last_name": "Matwin",
                "first_name": "Stan"
            },
            {
                "last_name": "Soares",
                "first_name": "Amilcar"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "LG"
        ],
        "abstract": "  The RoboCup competition was started in 1997, and is known as the oldest\nRoboCup league. The RoboCup 2D Soccer Simulation League is a stochastic,\npartially observable soccer environment in which 24 autonomous agents play on\ntwo opposing teams. In this paper, we detail the main strategies and\nfunctionalities of CYRUS, the RoboCup 2021 2D Soccer Simulation League\nchampions. The new functionalities presented and discussed in this work are (i)\nMulti Action Dribble, (ii) Pass Prediction and (iii) Marking Decision. The\nMulti Action Dribbling strategy enabled CYRUS to succeed more often and to be\nsafer when dribbling actions were performed during a game. The Pass Prediction\nenhanced our gameplay by predicting our teammate's passing behavior,\nanticipating and making our agents collaborate better towards scoring goals.\nFinally, the Marking Decision addressed the multi-agent matching problem to\nimprove CYRUS defensive strategy by finding an optimal solution to mark\nopponents' players.\n",
        "title": "Improving Dribbling, Passing, and Marking Actions in Soccer Simulation\n  2D Games Using Machine Learning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03407",
        "abstract_url": "http://arxiv.org/abs/2401.03407",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Peng"
            },
            {
                "last_name": "Gao",
                "first_name": "Dehong"
            },
            {
                "last_name": "Fan",
                "first_name": "Deng-Ping"
            },
            {
                "last_name": "Liu",
                "first_name": "Li"
            },
            {
                "last_name": "Laaksonen",
                "first_name": "Jorma"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Wanli"
            },
            {
                "last_name": "Sebe",
                "first_name": "Nicu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a novel bilateral reference framework (***BiRefNet***) for\nhigh-resolution dichotomous image segmentation (DIS). It comprises two\nessential components: the localization module (LM) and the reconstruction\nmodule (RM) with our proposed bilateral reference (BiRef). The LM aids in\nobject localization using global semantic information. Within the RM, we\nutilize BiRef for the reconstruction process, where hierarchical patches of\nimages provide the source reference and gradient maps serve as the target\nreference. These components collaborate to generate the final predicted maps.\nWe also introduce auxiliary gradient supervision to enhance focus on regions\nwith finer details. Furthermore, we outline practical training strategies\ntailored for DIS to improve map quality and training process. To validate the\ngeneral applicability of our approach, we conduct extensive experiments on four\ntasks to evince that *BiRefNet* exhibits remarkable performance, outperforming\ntask-specific cutting-edge methods across all benchmarks.\n",
        "title": "Bilateral Reference for High-Resolution Dichotomous Image Segmentation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03408",
        "abstract_url": "http://arxiv.org/abs/2401.03408",
        "authors": [
            {
                "last_name": "Rivera",
                "first_name": "Juan-Pablo"
            },
            {
                "last_name": "Mukobi",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Reuel",
                "first_name": "Anka"
            },
            {
                "last_name": "Lamparth",
                "first_name": "Max"
            },
            {
                "last_name": "Smith",
                "first_name": "Chandler"
            },
            {
                "last_name": "Schneider",
                "first_name": "Jacquelyn"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "CY",
            "MA"
        ],
        "abstract": "  Governments are increasingly considering integrating autonomous AI agents in\nhigh-stakes military and foreign-policy decision-making, especially with the\nemergence of advanced generative AI models like GPT-4. Our work aims to\nscrutinize the behavior of multiple AI agents in simulated wargames,\nspecifically focusing on their predilection to take escalatory actions that may\nexacerbate multilateral conflicts. Drawing on political science and\ninternational relations literature about escalation dynamics, we design a novel\nwargame simulation and scoring framework to assess the escalation risks of\nactions taken by these agents in different scenarios. Contrary to prior\nstudies, our research provides both qualitative and quantitative insights and\nfocuses on large language models (LLMs). We find that all five studied\noff-the-shelf LLMs show forms of escalation and difficult-to-predict escalation\npatterns. We observe that models tend to develop arms-race dynamics, leading to\ngreater conflict, and in rare cases, even to the deployment of nuclear weapons.\nQualitatively, we also collect the models' reported reasonings for chosen\nactions and observe worrying justifications based on deterrence and\nfirst-strike tactics. Given the high stakes of military and foreign-policy\ncontexts, we recommend further examination and cautious consideration before\ndeploying autonomous language model agents for strategic military or diplomatic\ndecision-making.\n",
        "title": "Escalation Risks from Language Models in Military and Diplomatic\n  Decision-Making",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03410",
        "abstract_url": "http://arxiv.org/abs/2401.03410",
        "authors": [
            {
                "last_name": "Zare",
                "first_name": "Nader"
            },
            {
                "last_name": "Sarvmaili",
                "first_name": "Mahtab"
            },
            {
                "last_name": "Sayareh",
                "first_name": "Aref"
            },
            {
                "last_name": "Amini",
                "first_name": "Omid"
            },
            {
                "last_name": "Soares",
                "first_name": "Stan Matwin Amilcar"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "RO"
        ],
        "abstract": "  Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two\ndimensions. In soccer, passing behavior is an essential action for keeping the\nball in possession of our team and creating goal opportunities. Similarly, for\nSS2D, predicting the passing behaviors of both opponents and our teammates\nhelps manage resources and score more goals. Therefore, in this research, we\nhave tried to address the modeling of passing behavior of soccer 2D players\nusing Deep Neural Networks (DNN) and Random Forest (RF). We propose an embedded\ndata extraction module that can record the decision-making of agents in an\nonline format. Afterward, we apply four data sorting techniques for training\ndata preparation. After, we evaluate the trained models' performance playing\nagainst 6 top teams of RoboCup 2019 that have distinctive playing strategies.\nFinally, we examine the importance of different feature groups on the\nprediction of a passing strategy. All results in each step of this work prove\nour suggested methodology's effectiveness and improve the performance of the\npass prediction in Soccer Simulation 2D games ranging from 5\\% (e.g., playing\nagainst the same team) to 10\\% (e.g., playing against Robocup top teams).\n",
        "title": "Engineering Features to Improve Pass Prediction in Soccer Simulation 2D\n  Games",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03411",
        "abstract_url": "http://arxiv.org/abs/2401.03411",
        "authors": [
            {
                "last_name": "Blau",
                "first_name": "Tsachi"
            },
            {
                "last_name": "Fogel",
                "first_name": "Sharon"
            },
            {
                "last_name": "Ronen",
                "first_name": "Roi"
            },
            {
                "last_name": "Golts",
                "first_name": "Alona"
            },
            {
                "last_name": "Ganz",
                "first_name": "Roy"
            },
            {
                "last_name": "Avraham",
                "first_name": "Elad Ben"
            },
            {
                "last_name": "Aberdam",
                "first_name": "Aviad"
            },
            {
                "last_name": "Tsiper",
                "first_name": "Shahar"
            },
            {
                "last_name": "Litman",
                "first_name": "Ron"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CV"
        ],
        "abstract": "  The increasing use of transformer-based large language models brings forward\nthe challenge of processing long sequences. In document visual question\nanswering (DocVQA), leading methods focus on the single-page setting, while\ndocuments can span hundreds of pages. We present GRAM, a method that seamlessly\nextends pre-trained single-page models to the multi-page setting, without\nrequiring computationally-heavy pretraining. To do so, we leverage a\nsingle-page encoder for local page-level understanding, and enhance it with\ndocument-level designated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce our model to utilize\nthe newly introduced document-level tokens, we propose a tailored bias\nadaptation method. For additional computational savings during decoding, we\nintroduce an optional compression stage using our C-Former model, which reduces\nthe encoded sequence length, thereby allowing a tradeoff between quality and\nlatency. Extensive experiments showcase GRAM's state-of-the-art performance on\nthe benchmarks for multi-page DocVQA, demonstrating the effectiveness of our\napproach.\n",
        "title": "GRAM: Global Reasoning for Multi-Page VQA",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03412",
        "abstract_url": "http://arxiv.org/abs/2401.03412",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Shuangfu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Junqiao"
            },
            {
                "last_name": "Huang",
                "first_name": "Kai"
            },
            {
                "last_name": "Lin",
                "first_name": "Jiaye"
            },
            {
                "last_name": "Ye",
                "first_name": "Chen"
            },
            {
                "last_name": "Feng",
                "first_name": "Tiantian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Accurate and dense mapping in large-scale environments is essential for\nvarious robot applications. Recently, implicit neural signed distance fields\n(SDFs) have shown promising advances in this task. However, most existing\napproaches employ projective distances from range data as SDF supervision,\nintroducing approximation errors and thus degrading the mapping quality. To\naddress this problem, we introduce N3-Mapping, an implicit neural mapping\nsystem featuring normal-guided neural non-projective signed distance fields.\nSpecifically, we directly sample points along the surface normal, instead of\nthe ray, to obtain more accurate non-projective distance values from range\ndata. Then these distance values are used as supervision to train the implicit\nmap. For large-scale mapping, we apply a voxel-oriented sliding window\nmechanism to alleviate the forgetting issue with a bounded memory footprint.\nBesides, considering the uneven distribution of measured point clouds, a\nhierarchical sampling strategy is designed to improve training efficiency.\nExperiments demonstrate that our method effectively mitigates SDF approximation\nerrors and achieves state-of-the-art mapping quality compared to existing\napproaches.\n",
        "title": "N$^{3}$-Mapping: Normal Guided Neural Non-Projective Signed Distance\n  Fields for Large-scale 3D Mapping",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03415",
        "abstract_url": "http://arxiv.org/abs/2401.03415",
        "authors": [
            {
                "last_name": "Agrawal",
                "first_name": "Akanksha"
            },
            {
                "last_name": "Jana",
                "first_name": "Satyabrata"
            },
            {
                "last_name": "Sahu",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            ""
        ],
        "abstract": "  A proper Helly circular-arc graph is an intersection graph of a set of arcs\non a circle such that none of the arcs properly contains any other arc and\nevery set of pairwise intersecting arcs has a common intersection. The Proper\nHelly Circular-arc Vertex Deletion problem takes as input a graph $G$ and an\ninteger $k$, and the goal is to check if we can remove at most $k$ vertices\nfrom the graph to obtain a proper Helly circular-arc graph; the parameter is\n$k$. Recently, Cao et al.~[MFCS 2023] obtained an FPT algorithm for this (and\nrelated) problem. In this work, we obtain a polynomial kernel for the problem.\n",
        "title": "A Polynomial Kernel for Proper Helly Circular-arc Vertex Deletion",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03424",
        "abstract_url": "http://arxiv.org/abs/2401.03424",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Guo",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Pan"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  While automatic speech recognition (ASR) systems degrade significantly in\nnoisy environments, audio-visual speech recognition (AVSR) systems aim to\ncomplement the audio stream with noise-invariant visual cues and improve the\nsystem's robustness. However, current studies mainly focus on fusing the\nwell-learned modality features, like the output of modality-specific encoders,\nwithout considering the contextual relationship during the modality feature\nlearning. In this study, we propose a multi-layer cross-attention fusion based\nAVSR (MLCA-AVSR) approach that promotes representation learning of each\nmodality by fusing them at different levels of audio/visual encoders.\nExperimental results on the MISP2022-AVSR Challenge dataset show the efficacy\nof our proposed system, achieving a concatenated minimum permutation character\nerror rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative\nimprovement compared with our previous system which ranked the second place in\nthe challenge. Following the fusion of multiple systems, our proposed approach\nsurpasses the first-place system, establishing a new SOTA cpCER of 29.13% on\nthis dataset.\n",
        "title": "MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech\n  Recognition",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03425",
        "abstract_url": "http://arxiv.org/abs/2401.03425",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Jikai"
            },
            {
                "last_name": "Chirikjian",
                "first_name": "Gregory S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We address the problem of uncertainty propagation and Bayesian fusion on\nunimodular Lie groups. Starting from a stochastic differential equation (SDE)\ndefined on Lie groups via Mckean-Gangolli injection, we first convert it to a\nparametric SDE in exponential coordinates. The coefficient transform method for\nthe conversion is stated for both Ito's and Stratonovich's interpretation of\nthe SDE. Then we derive a mean and covariance fitting formula for probability\ndistributions on Lie groups defined by a concentrated distribution on the\nexponential coordinate. It is used to derive the mean and covariance\npropagation equations for the SDE defined by injection, which coincides with\nthe result derived from a Fokker-Planck equation in previous work. We also\npropose a simple modification to the update step of Kalman filters using the\nfitting formula, which improves the fusion accuracy with moderate computation\ntime.\n",
        "title": "Uncertainty Propagation and Bayesian Fusion on Unimodular Lie Groups\n  from a Parametric Perspective",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03426",
        "abstract_url": "http://arxiv.org/abs/2401.03426",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Huahang"
            },
            {
                "last_name": "Feng",
                "first_name": "Longyu"
            },
            {
                "last_name": "Li",
                "first_name": "Shuangyin"
            },
            {
                "last_name": "Hao",
                "first_name": "Fei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chen Jason"
            },
            {
                "last_name": "Song",
                "first_name": "Yuanfeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Entity resolution, the task of identifying and consolidating records that\npertain to the same real-world entity, plays a pivotal role in various sectors\nsuch as e-commerce, healthcare, and law enforcement. The emergence of Large\nLanguage Models (LLMs) like GPT-4 has introduced a new dimension to this task,\nleveraging their advanced linguistic capabilities. This paper explores the\npotential of LLMs in the entity resolution process, shedding light on both\ntheir advantages and the computational complexities associated with large-scale\nmatching. We introduce strategies for the efficient utilization of LLMs,\nincluding the selection of an optimal set of matching questions, namely MQsSP,\nwhich is proved to be a NP-hard problem. Our approach optimally chooses the\nmost effective matching questions while keep consumption limited to your budget\n. Additionally, we propose a method to adjust the distribution of possible\npartitions after receiving responses from LLMs, with the goal of reducing the\nuncertainty of entity resolution. We evaluate the effectiveness of our approach\nusing entropy as a metric, and our experimental results demonstrate the\nefficiency and effectiveness of our proposed methods, offering promising\nprospects for real-world applications.\n",
        "title": "On Leveraging Large Language Models for Enhancing Entity Resolution",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03427",
        "abstract_url": "http://arxiv.org/abs/2401.03427",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Yangtao"
            },
            {
                "last_name": "He",
                "first_name": "Qiaolin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Efficient algorithms for solving high-dimensional partial differential\nequations (PDEs) has been an exceedingly difficult task for a long time, due to\nthe curse of dimensionality. We extend the forward-backward stochastic neural\nnetworks (FBSNNs) which depends on forward-backward stochastic differential\nequation (FBSDE) to solve incompressible Navier-Stokes equation. For\nCahn-Hilliard equation, we derive a modified Cahn-Hilliard equation from a\nwidely used stabilized scheme for original Cahn-Hilliard equation. This\nequation can be written as a continuous parabolic system, where FBSDE can be\napplied and the unknown solution is approximated by neural network. Also our\nmethod is successfully developed to Cahn-Hilliard-Navier-Stokes (CHNS)\nequation. The accuracy and stability of our methods are shown in many numerical\nexperiments, specially in high dimension.\n",
        "title": "Deep FBSDE Neural Networks for Solving Incompressible Navier-Stokes\n  Equation and Cahn-Hilliard Equation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03428",
        "abstract_url": "http://arxiv.org/abs/2401.03428",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Yuheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ceyao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhengwen"
            },
            {
                "last_name": "Meng",
                "first_name": "Xiangrui"
            },
            {
                "last_name": "Hong",
                "first_name": "Sirui"
            },
            {
                "last_name": "Li",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Wang",
                "first_name": "Zihao"
            },
            {
                "last_name": "Wang",
                "first_name": "Zekai"
            },
            {
                "last_name": "Yin",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Junhua"
            },
            {
                "last_name": "He",
                "first_name": "Xiuqiang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA"
        ],
        "abstract": "  Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.\n",
        "title": "Exploring Large Language Model based Intelligent Agents: Definitions,\n  Methods, and Prospects",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03431",
        "abstract_url": "http://arxiv.org/abs/2401.03431",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhi-Song"
            },
            {
                "last_name": "Cani",
                "first_name": "Marie-Paule"
            },
            {
                "last_name": "Siu",
                "first_name": "Wan-Chi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We present See360, which is a versatile and efficient framework for 360\npanoramic view interpolation using latent space viewpoint estimation. Most of\nthe existing view rendering approaches only focus on indoor or synthetic 3D\nenvironments and render new views of small objects. In contrast, we suggest to\ntackle camera-centered view synthesis as a 2D affine transformation without\nusing point clouds or depth maps, which enables an effective 360? panoramic\nscene exploration. Given a pair of reference images, the See360 model learns to\nrender novel views by a proposed novel Multi-Scale Affine Transformer (MSAT),\nenabling the coarse-to-fine feature rendering. We also propose a Conditional\nLatent space AutoEncoder (C-LAE) to achieve view interpolation at any arbitrary\nangle. To show the versatility of our method, we introduce four training\ndatasets, namely UrbanCity360, Archinterior360, HungHom360 and Lab360, which\nare collected from indoor and outdoor environments for both real and synthetic\nrendering. Experimental results show that the proposed method is generic enough\nto achieve real-time rendering of arbitrary views for all four datasets. In\naddition, our See360 model can be applied to view synthesis in the wild: with\nonly a short extra training time (approximately 10 mins), and is able to render\nunknown real-world scenes. The superior performance of See360 opens up a\npromising direction for camera-centered view rendering and 360 panoramic view\ninterpolation.\n",
        "title": "See360: Novel Panoramic View Interpolation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03433",
        "abstract_url": "http://arxiv.org/abs/2401.03433",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Songyan"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiancheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Text-conditional image editing based on large diffusion generative model has\nattracted the attention of both the industry and the research community. Most\nexisting methods are non-reference editing, with the user only able to provide\na source image and text prompt. However, it restricts user's control over the\ncharacteristics of editing outcome. To increase user freedom, we propose a new\ntask called Specific Reference Condition Real Image Editing, which allows user\nto provide a reference image to further control the outcome, such as replacing\nan object with a particular one. To accomplish this, we propose a fast baseline\nmethod named SpecRef. Specifically, we design a Specific Reference Attention\nController to incorporate features from the reference image, and adopt a mask\nmechanism to prevent interference between editing and non-editing regions. We\nevaluate SpecRef on typical editing tasks and show that it can achieve\nsatisfactory performance. The source code is available on\nhttps://github.com/jingjiqinggong/specp2p.\n",
        "title": "SpecRef: A Fast Training-free Baseline of Specific Reference-Condition\n  Real Image Editing",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03435",
        "abstract_url": "http://arxiv.org/abs/2401.03435",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Ruolin"
            },
            {
                "last_name": "Xu",
                "first_name": "Mengwei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ao"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yiran"
            },
            {
                "last_name": "Qian",
                "first_name": "Feng"
            },
            {
                "last_name": "Wang",
                "first_name": "Shangguang"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In the wake of the rapid deployment of large-scale low-Earth orbit satellite\nconstellations, exploiting the full computing potential of Commercial\nOff-The-Shelf (COTS) devices in these environments has become a pressing issue.\nHowever, understanding this problem is far from straightforward due to the\ninherent differences between the terrestrial infrastructure and the satellite\nplatform in space. In this paper, we take an important step towards closing\nthis knowledge gap by presenting the first measurement study on the thermal\ncontrol, power management, and performance of COTS devices on satellites. Our\nmeasurements reveal that the satellite platform and COTS devices significantly\ninterplay in terms of the temperature and energy consumption, forming the main\nconstraints on satellite computing. Further, we analyze the critical factors\nthat shape the characteristics of onboard COTS computing devices. We provide\nguidelines for future research on optimizing the use of such devices for\ncomputing purposes. Finally, we plan to release our datasets to facilitate the\nfuture study.\n",
        "title": "Deciphering the Enigma of Satellite Computing with COTS Devices:\n  Measurement and Analysis",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03441",
        "abstract_url": "http://arxiv.org/abs/2401.03441",
        "authors": [
            {
                "last_name": "Morgenstern",
                "first_name": "Hai"
            },
            {
                "last_name": "Rafaely",
                "first_name": "Boaz"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Methods are proposed for modifying the reverberation characteristics of sound\nfields in rooms by employing a loudspeaker with adjustable directivity,\nrealized with a compact spherical loudspeaker array (SLA). These methods are\nbased on minimization and maximization of clarity and direct-to-reverberant\nsound ratio. Significant modification of reverberation is achieved by these\nmethods, as shown in simulation studies. The system under investigation\nincludes a spherical microphone array and an SLA comprising a multiple-input\nmultiple-output system. The robustness of these methods to system\nidentification errors is also investigated. Finally, reverberation and\ndereverberation results are validated by a listening experiment.\n",
        "title": "Spatial Reverberation and Dereverberation using an Acoustic\n  Multiple-Input Multiple-Output System",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03444",
        "abstract_url": "http://arxiv.org/abs/2401.03444",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Meng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "SI"
        ],
        "abstract": "  Network dynamic (e.g., traffic burst in data center networks and channel\nfading in cellular WiFi networks) has a great impact on the performance of\ncommunication networks (e.g., throughput, capacity, delay, and jitter). This\narticle proposes a unified prediction-based method to handle the dynamic of\nvarious network systems. From the view of graph deep learning, I generally\nformulate the dynamic prediction of networks as a temporal link prediction task\nand analyze the possible challenges of the prediction of weighted networks,\nwhere link weights have the wide-value-range and sparsity issues. Inspired by\nthe high-resolution video frame prediction with generative adversarial network\n(GAN), I try to adopt adversarial learning to generate high-quality predicted\nsnapshots for network dynamic, which is expected to support the precise and\nfine-grained network control. A novel high-quality temporal link prediction\n(HQ-TLP) model with GAN is then developed to illustrate the potential of my\nbasic idea. Extensive experiments for various application scenarios further\ndemonstrate the powerful capability of HQ-TLP.\n",
        "title": "Towards a Unified Method for Network Dynamic via Adversarial Weighted\n  Link Prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03448",
        "abstract_url": "http://arxiv.org/abs/2401.03448",
        "authors": [
            {
                "last_name": "Opochinsky",
                "first_name": "Renana"
            },
            {
                "last_name": "Moradi",
                "first_name": "Mordehay"
            },
            {
                "last_name": "Gannot",
                "first_name": "Sharon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Speech separation involves extracting an individual speaker's voice from a\nmulti-speaker audio signal. The increasing complexity of real-world\nenvironments, where multiple speakers might converse simultaneously,\nunderscores the importance of effective speech separation techniques. This work\npresents a single-microphone speaker separation network with TF attention\naiming at noisy and reverberant environments. We dub this new architecture as\nSeparation TF Attention Network (Sep-TFAnet). In addition, we present a variant\nof the separation network, dubbed $ \\text{Sep-TFAnet}^{\\text{VAD}}$, which\nincorporates a voice activity detector (VAD) into the separation network.\n  The separation module is based on a temporal convolutional network (TCN)\nbackbone inspired by the Conv-Tasnet architecture with multiple modifications.\nRather than a learned encoder and decoder, we use short-time Fourier transform\n(STFT) and inverse short-time Fourier transform (iSTFT) for the analysis and\nsynthesis, respectively. Our system is specially developed for human-robotic\ninteractions and should support online mode. The separation capabilities of $\n\\text{Sep-TFAnet}^{\\text{VAD}}$ and Sep-TFAnet were evaluated and extensively\nanalyzed under several acoustic conditions, demonstrating their advantages over\ncompeting methods. Since separation networks trained on simulated data tend to\nperform poorly on real recordings, we also demonstrate the ability of the\nproposed scheme to better generalize to realistic examples recorded in our\nacoustic lab by a humanoid robot. Project page: https://Sep-TFAnet.github.io\n",
        "title": "Single-Microphone Speaker Separation and Voice Activity Detection in\n  Noisy and Reverberant Environments",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03450",
        "abstract_url": "http://arxiv.org/abs/2401.03450",
        "authors": [
            {
                "last_name": "Br\u00e5telund",
                "first_name": "Martin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Structure from motion is the process of recovering information about cameras\nand 3D scene from a set of images. Generally, in a noise-free setting, all\ninformation can be uniquely recovered if enough images and image points are\nprovided. There are, however, certain cases where unique recovery is\nimpossible, even in theory; these are called critical configurations. We use a\nrecently developed algebraic approach to classify all critical configurations\nfor any number of projective cameras. We show that they form well-known\nalgebraic varieties, such as quadric surfaces and curves of degree at most 4.\nThis paper also improves upon earlier results both by finding previously\nunknown critical configurations and by showing that some configurations\npreviously believed to be critical are in fact not.\n",
        "title": "A Classification of Critical Configurations for any Number of Projective\n  Views",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03451",
        "abstract_url": "http://arxiv.org/abs/2401.03451",
        "authors": [
            {
                "last_name": "Tong",
                "first_name": "Jiatai"
            },
            {
                "last_name": "Cai",
                "first_name": "Junyang"
            },
            {
                "last_name": "Serra",
                "first_name": "Thiago"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Besides training, mathematical optimization is also used in deep learning to\nmodel and solve formulations over trained neural networks for purposes such as\nverification, compression, and optimization with learned constraints. However,\nsolving these formulations soon becomes difficult as the network size grows due\nto the weak linear relaxation and dense constraint matrix. We have seen\nimprovements in recent years with cutting plane algorithms, reformulations, and\nan heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we\npropose a more scalable heuristic based on exploring global and local linear\nrelaxations of the neural network model. Our heuristic is competitive with a\nstate-of-the-art MILP solver and the prior heuristic while producing better\nsolutions with increases in input, depth, and number of neurons.\n",
        "title": "Optimization Over Trained Neural Networks: Taking a Relaxing Walk",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03454",
        "abstract_url": "http://arxiv.org/abs/2401.03454",
        "authors": [
            {
                "last_name": "Castagna",
                "first_name": "Federico"
            },
            {
                "last_name": "Kokciyan",
                "first_name": "Nadin"
            },
            {
                "last_name": "Sassoon",
                "first_name": "Isabel"
            },
            {
                "last_name": "Parsons",
                "first_name": "Simon"
            },
            {
                "last_name": "Sklar",
                "first_name": "Elizabeth"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Chatbots are conversational software applications designed to interact\ndialectically with users for a plethora of different purposes. Surprisingly,\nthese colloquial agents have only recently been coupled with computational\nmodels of arguments (i.e. computational argumentation), whose aim is to\nformalise, in a machine-readable format, the ordinary exchange of information\nthat characterises human communications. Chatbots may employ argumentation with\ndifferent degrees and in a variety of manners. The present survey sifts through\nthe literature to review papers concerning this kind of argumentation-based\nbot, drawing conclusions about the benefits and drawbacks that this approach\nentails in comparison with standard chatbots, while also envisaging possible\nfuture development and integration with the Transformer-based architecture and\nstate-of-the-art Large Language models.\n",
        "title": "Computational Argumentation-based Chatbots: a Survey",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03458",
        "abstract_url": "http://arxiv.org/abs/2401.03458",
        "authors": [
            {
                "last_name": "Morgenstern",
                "first_name": "Hai"
            },
            {
                "last_name": "Rafaely",
                "first_name": "Boaz"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Spatial analysis of room acoustics is an ongoing research topic. Microphone\narrays have been employed for spatial analyses with an important objective\nbeing the estimation of the direction-of-arrival (DOA) of direct sound and\nearly room reflections using room impulse responses (RIRs). An optimal method\nfor DOA estimation is the multiple signal classification algorithm. When RIRs\nare considered, this method typically fails due to the correlation of room\nreflections, which leads to rank deficiency of the cross-spectrum matrix.\nPreprocessing methods for rank restoration, which may involve averaging over\nfrequency, for example, have been proposed exclusively for spherical arrays.\nHowever, these methods fail in the case of reflections with equal time delays,\nwhich may arise in practice and could be of interest. In this paper, a method\nis proposed for systems that combine a spherical microphone array and a\nspherical loudspeaker array, referred to as multiple-input multiple-output\nsystems. This method, referred to as modal smoothing, exploits the additional\nspatial diversity for rank restoration and succeeds where previous methods\nfail, as demonstrated in a simulation study. Finally, combining modal smoothing\nwith a preprocessing method is proposed in order to increase the number of DOAs\nthat can be estimated using low-order spherical loudspeaker arrays.\n",
        "title": "Modal smoothing for analysis of room reflections measured with spherical\n  microphone and loudspeaker arrays",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03459",
        "abstract_url": "http://arxiv.org/abs/2401.03459",
        "authors": [
            {
                "last_name": "Miao",
                "first_name": "Xiangyang"
            },
            {
                "last_name": "Xiao",
                "first_name": "Guobao"
            },
            {
                "last_name": "Wang",
                "first_name": "Shiping"
            },
            {
                "last_name": "Yu",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Correspondence pruning aims to establish reliable correspondences between two\nrelated images and recover relative camera motion. Existing approaches often\nemploy a progressive strategy to handle the local and global contexts, with a\nprominent emphasis on transitioning from local to global, resulting in the\nneglect of interactions between different contexts. To tackle this issue, we\npropose a parallel context learning strategy that involves acquiring bilateral\nconsensus for the two-view correspondence pruning task. In our approach, we\ndesign a distinctive self-attention block to capture global context and\nparallel process it with the established local context learning module, which\nenables us to simultaneously capture both local and global consensuses. By\ncombining these local and global consensuses, we derive the required bilateral\nconsensus. We also design a recalibration block, reducing the influence of\nerroneous consensus information and enhancing the robustness of the model. The\nculmination of our efforts is the Bilateral Consensus Learning Network\n(BCLNet), which efficiently estimates camera pose and identifies inliers (true\ncorrespondences). Extensive experiments results demonstrate that our network\nnot only surpasses state-of-the-art methods on benchmark datasets but also\nshowcases robust generalization abilities across various feature extraction\ntechniques. Noteworthily, BCLNet obtains 3.98\\% mAP5$^{\\circ}$ gains over the\nsecond best method on unknown outdoor dataset, and obviously accelerates model\ntraining speed. The source code will be available at:\nhttps://github.com/guobaoxiao/BCLNet.\n",
        "title": "BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03461",
        "abstract_url": "http://arxiv.org/abs/2401.03461",
        "authors": [
            {
                "last_name": "Bojic",
                "first_name": "Ljubisa"
            },
            {
                "last_name": "Matthes",
                "first_name": "Joerg"
            },
            {
                "last_name": "Cabarkapa",
                "first_name": "Milan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  The emergence of the metaverse, envisioned as a hyperreal virtual universe\nfacilitating boundless human interaction, stands to revolutionize our\nconception of media, with significant impacts on addiction, creativity,\nrelationships, and social polarization. This paper aims to dissect the\naddictive potential of the metaverse due to its immersive and interactive\nfeatures, scrutinize the effects of its recommender systems on creativity and\nsocial polarization, and explore potential consequences stemming from the\nmetaverse development. We employed a literature review methodology, drawing\nparallels from the research on new media platforms and examining the\nprogression of reality-mimicking features in media from historical perspectives\nto understand this transformative digital frontier. The findings suggest that\nthese immersive and interactive features could potentially exacerbate media\naddiction. The designed recommender systems, while aiding personalization and\nuser engagement, might contribute to social polarization and affect the\ndiversity of creative output. However, our conclusions are based primarily on\ntheoretical propositions from studies conducted on existing media platforms and\nlack empirical support specific to the metaverse. Therefore, this paper\nidentifies a critical gap requiring further research, through empirical studies\nfocused on metaverse use and addiction and exploration of privacy, security,\nand ethical implications associated with this burgeoning digital universe. As\nthe development of the metaverse accelerates, it is incumbent on scholars,\ntechnologists, and policymakers to navigate its multilayered impacts\nthoughtfully to balance innovation with societal well-being.\n",
        "title": "Amplification of Addictive New Media Features in the Metaverse",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03462",
        "abstract_url": "http://arxiv.org/abs/2401.03462",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Peitian"
            },
            {
                "last_name": "Liu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Xiao",
                "first_name": "Shitao"
            },
            {
                "last_name": "Shao",
                "first_name": "Ninglu"
            },
            {
                "last_name": "Ye",
                "first_name": "Qiwei"
            },
            {
                "last_name": "Dou",
                "first_name": "Zhicheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The utilization of long contexts poses a big challenge for large language\nmodels due to their limited context window length. Although the context window\ncan be extended through fine-tuning, it will result in a considerable cost at\nboth training and inference time, and exert an unfavorable impact to the LLM's\noriginal capabilities. In this work, we propose Activation Beacon, which\ncondenses LLM's raw activations into more compact forms such that it can\nperceive a much longer context with a limited context window. Activation Beacon\nis introduced as a plug-and-play module for the LLM. It fully preserves the\nLLM's original capability on short contexts while extending the new capability\non processing longer contexts. Besides, it works with short sliding windows to\nprocess the long context, which achieves a competitive memory and time\nefficiency in both training and inference. Activation Beacon is learned by the\nauto-regression task conditioned on a mixture of beacons with diversified\ncondensing ratios. Thanks to such a treatment, it can be efficiently trained\npurely with short-sequence data in just 10K steps, which consumes less than 9\nhours on a single 8xA800 GPU machine. The experimental studies show that\nActivation Beacon is able to extend Llama-2-7B's context length by $\\times100$\ntimes (from 4K to 400K), meanwhile achieving a superior result on both\nlong-context generation and understanding tasks. Our model and code will be\navailable at the BGE repository.\n",
        "title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03467",
        "abstract_url": "http://arxiv.org/abs/2401.03467",
        "authors": [
            {
                "last_name": "Bojic",
                "first_name": "Ljubisa"
            },
            {
                "last_name": "Prodanovic",
                "first_name": "Nikola"
            },
            {
                "last_name": "Samala",
                "first_name": "Agariadne Dwinggo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The rapid growth of online news platforms has led to an increased need for\nreliable methods to evaluate the quality and credibility of news articles. This\npaper proposes a comprehensive framework to analyze online news texts using\nnatural language processing (NLP) techniques, particularly a language model\nspecifically trained for this purpose, alongside other well-established NLP\nmethods. The framework incorporates ten journalism standards-objectivity,\nbalance and fairness, readability and clarity, sensationalism and clickbait,\nethical considerations, public interest and value, source credibility,\nrelevance and timeliness, factual accuracy, and attribution and transparency-to\nassess the quality of news articles. By establishing these standards,\nresearchers, media organizations, and readers can better evaluate and\nunderstand the content they consume and produce. The proposed method has some\nlimitations, such as potential difficulty in detecting subtle biases and the\nneed for continuous updating of the language model to keep pace with evolving\nlanguage patterns.\n",
        "title": "Maintaining Journalistic Integrity in the Digital Age: A Comprehensive\n  NLP Framework for Evaluating Online News Content",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03468",
        "abstract_url": "http://arxiv.org/abs/2401.03468",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Qiushi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Gu",
                "first_name": "Yu"
            },
            {
                "last_name": "Hu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Dai",
                "first_name": "Lirong"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Self-supervised speech pre-training methods have developed rapidly in recent\nyears, which show to be very effective for many near-field single-channel\nspeech tasks. However, far-field multichannel speech processing is suffering\nfrom the scarcity of labeled multichannel data and complex ambient noises. The\nefficacy of self-supervised learning for far-field multichannel and multi-modal\nspeech processing has not been well explored. Considering that visual\ninformation helps to improve speech recognition performance in noisy scenes, in\nthis work we propose a multichannel multi-modal speech self-supervised learning\nframework AV-wav2vec2, which utilizes video and multichannel audio data as\ninputs. First, we propose a multi-path structure to process multichannel audio\nstreams and a visual stream in parallel, with intra- and inter-channel\ncontrastive losses as training targets to fully exploit the spatiotemporal\ninformation in multichannel speech data. Second, based on contrastive learning,\nwe use additional single-channel audio data, which is trained jointly to\nimprove the performance of speech representation. Finally, we use a Chinese\nmultichannel multi-modal dataset in real scenarios to validate the\neffectiveness of the proposed method on audio-visual speech recognition (AVSR),\nautomatic speech recognition (ASR), visual speech recognition (VSR) and\naudio-visual speaker diarization (AVSD) tasks.\n",
        "title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel\n  Multi-Modal Speech Representation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03469",
        "abstract_url": "http://arxiv.org/abs/2401.03469",
        "authors": [
            {
                "last_name": "Sartaj",
                "first_name": "Hassan"
            },
            {
                "last_name": "Iqbal",
                "first_name": "Muhammad Zohaib"
            },
            {
                "last_name": "Jilani",
                "first_name": "Atif Aftab Ahmed"
            },
            {
                "last_name": "Khan",
                "first_name": "Muhammad Uzair"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  System-level testing of avionics software systems requires compliance with\ndifferent international safety standards such as DO-178C. An important\nconsideration of the avionics industry is automated test data generation\naccording to the criteria suggested by safety standards. One of the recommended\ncriteria by DO-178C is the modified condition/decision coverage (MC/DC)\ncriterion. The current model-based test data generation approaches use\nconstraints written in Object Constraint Language (OCL), and apply search\ntechniques to generate test data. These approaches either do not support MC/DC\ncriterion or suffer from performance issues while generating test data for\nlarge-scale avionics systems. In this paper, we propose an effective way to\nautomate MC/DC test data generation during model-based testing. We develop a\nstrategy that utilizes case-based reasoning (CBR) and range reduction\nheuristics designed to solve MC/DC-tailored OCL constraints. We performed an\nempirical study to compare our proposed strategy for MC/DC test data generation\nusing CBR, range reduction, both CBR and range reduction, with an original\nsearch algorithm, and random search. We also empirically compared our strategy\nwith existing constraint-solving approaches. The results show that both CBR and\nrange reduction for MC/DC test data generation outperform the baseline\napproach. Moreover, the combination of both CBR and range reduction for MC/DC\ntest data generation is an effective approach compared to existing constraint\nsolvers.\n",
        "title": "Efficient Test Data Generation for MC/DC with OCL and Search",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03470",
        "abstract_url": "http://arxiv.org/abs/2401.03470",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Genghao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuxi"
            },
            {
                "last_name": "Luo",
                "first_name": "Chuanchen"
            },
            {
                "last_name": "Xu",
                "first_name": "Shibiao"
            },
            {
                "last_name": "Peng",
                "first_name": "Junran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Man"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Indoor scene generation has attracted significant attention recently as it is\ncrucial for applications of gaming, virtual reality, and interior design.\nCurrent indoor scene generation methods can produce reasonable room layouts but\noften lack diversity and realism. This is primarily due to the limited coverage\nof existing datasets, including only large furniture without tiny furnishings\nin daily life. To address these challenges, we propose FurniScene, a\nlarge-scale 3D room dataset with intricate furnishing scenes from interior\ndesign professionals. Specifically, the FurniScene consists of 11,698 rooms and\n39,691 unique furniture CAD models with 89 different types, covering things\nfrom large beds to small teacups on the coffee table. To better suit\nfine-grained indoor scene layout generation, we introduce a novel Two-Stage\nDiffusion Scene Model (TSDSM) and conduct an evaluation benchmark for various\nindoor scene generation based on FurniScene. Quantitative and qualitative\nevaluations demonstrate the capability of our method to generate highly\nrealistic indoor scenes. Our dataset and code will be publicly available soon.\n",
        "title": "FurniScene: A Large-scale 3D Room Dataset with Intricate Furnishing\n  Scenes",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03472",
        "abstract_url": "http://arxiv.org/abs/2401.03472",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Zening"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiapeng"
            },
            {
                "last_name": "Li",
                "first_name": "Teng"
            },
            {
                "last_name": "Liao",
                "first_name": "Wenhui"
            },
            {
                "last_name": "Huang",
                "first_name": "Dayi"
            },
            {
                "last_name": "Xiong",
                "first_name": "Longfei"
            },
            {
                "last_name": "Jin",
                "first_name": "Lianwen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Document pair extraction aims to identify key and value entities as well as\ntheir relationships from visually-rich documents. Most existing methods divide\nit into two separate tasks: semantic entity recognition (SER) and relation\nextraction (RE). However, simply concatenating SER and RE serially can lead to\nsevere error propagation, and it fails to handle cases like multi-line entities\nin real scenarios. To address these issues, this paper introduces a novel\nframework, PEneo (Pair Extraction new decoder option), which performs document\npair extraction in a unified pipeline, incorporating three concurrent\nsub-tasks: line extraction, line grouping, and entity linking. This approach\nalleviates the error accumulation problem and can handle the case of multi-line\nentities. Furthermore, to better evaluate the model's performance and to\nfacilitate future research on pair extraction, we introduce RFUND, a\nre-annotated version of the commonly used FUNSD and XFUND datasets, to make\nthem more accurate and cover realistic situations. Experiments on various\nbenchmarks demonstrate PEneo's superiority over previous pipelines, boosting\nthe performance by a large margin (e.g., 19.89%-22.91% F1 score on RFUND-EN)\nwhen combined with various backbones like LiLT and LayoutLMv3, showing its\neffectiveness and generality. Codes and the new annotations will be open to the\npublic.\n",
        "title": "PEneo: Unifying Line Extraction, Line Grouping, and Entity Linking for\n  End-to-end Document Pair Extraction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03473",
        "abstract_url": "http://arxiv.org/abs/2401.03473",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Guo",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Li",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ao"
            },
            {
                "last_name": "Sun",
                "first_name": "Jiayao"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Pan"
            },
            {
                "last_name": "Bu",
                "first_name": "Hui"
            },
            {
                "last_name": "Xu",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Binbin"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Wu",
                "first_name": "Jian"
            },
            {
                "last_name": "Wang",
                "first_name": "Longbiao"
            },
            {
                "last_name": "Chng",
                "first_name": "Eng Siong"
            },
            {
                "last_name": "Li",
                "first_name": "Sun"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  To promote speech processing and recognition research in driving scenarios,\nwe build on the success of the Intelligent Cockpit Speech Recognition Challenge\n(ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel\nAutomatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over\n100 hours of multi-channel speech data recorded inside a new energy vehicle and\n40 hours of noise for data augmentation. Two tracks, including automatic speech\nrecognition (ASR) and automatic speech diarization and recognition (ASDR) are\nset up, using character error rate (CER) and concatenated minimum permutation\ncharacter error rate (cpCER) as evaluation metrics, respectively. Overall, the\nICMC-ASR Challenge attracts 98 participating teams and receives 53 valid\nresults in both tracks. In the end, first-place team USTCiflytek achieves a CER\nof 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an\nabsolute improvement of 13.08% and 51.4% compared to our challenge baseline,\nrespectively.\n",
        "title": "ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech\n  Recognition Challenge",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03476",
        "abstract_url": "http://arxiv.org/abs/2401.03476",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Sicheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Zunnan"
            },
            {
                "last_name": "Xue",
                "first_name": "Haiwei"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yongkang"
            },
            {
                "last_name": "Huang",
                "first_name": "Shaoli"
            },
            {
                "last_name": "Gong",
                "first_name": "Mingming"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhiyong"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "",
            "HC",
            "SD",
            ""
        ],
        "abstract": "  Current talking avatars mostly generate co-speech gestures based on audio and\ntext of the utterance, without considering the non-speaking motion of the\nspeaker. Furthermore, previous works on co-speech gesture generation have\ndesigned network structures based on individual gesture datasets, which results\nin limited data volume, compromised generalizability, and restricted speaker\nmovements. To tackle these issues, we introduce FreeTalker, which, to the best\nof our knowledge, is the first framework for the generation of both spontaneous\n(e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium)\nspeaker motions. Specifically, we train a diffusion-based model for speaker\nmotion generation that employs unified representations of both speech-driven\ngestures and text-driven motions, utilizing heterogeneous data sourced from\nvarious motion datasets. During inference, we utilize classifier-free guidance\nto highly control the style in the clips. Additionally, to create smooth\ntransitions between clips, we utilize DoubleTake, a method that leverages a\ngenerative prior and ensures seamless motion blending. Extensive experiments\nshow that our method generates natural and controllable speaker movements. Our\ncode, model, and demo are are available at\n\\url{https://youngseng.github.io/FreeTalker/}.\n",
        "title": "Freetalker: Controllable Speech and Text-Driven Gesture Generation Based\n  on Diffusion Models for Enhanced Speaker Naturalness",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03477",
        "abstract_url": "http://arxiv.org/abs/2401.03477",
        "authors": [
            {
                "last_name": "Indurkhya",
                "first_name": "Bipin"
            },
            {
                "last_name": "Sienkiewicz",
                "first_name": "Barbara"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Sustainability is no longer a matter of choice but is invariably linked to\nthe survival of the entire ecosystem of our planet Earth. As robotics\ntechnology is growing at an exponential rate, it is crucial to examine its\nimplications for sustainability. Our focus is on social sustainability,\nspecifically analyzing the role of robotics technology in this domain by\nidentifying six distinct ways robots influence social sustainability.\n",
        "title": "Robots and Social Sustainability",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03481",
        "abstract_url": "http://arxiv.org/abs/2401.03481",
        "authors": [
            {
                "last_name": "Lancaster",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper reports on qualitative content analysis undertaken using ChatGPT,\na Large Language Model (LLM), to identify primary research themes in current\nacademic integrity research as well as the methodologies used to explore these\nareas. The analysis by the LLM identified 7 research themes and 13 key areas\nfor exploration. The outcomes from the analysis suggest that much contemporary\nresearch in the academic integrity field is guided by technology. Technology is\noften explored as potential way of preventing academic misconduct, but this\ncould also be a limiting factor when aiming to promote a culture of academic\nintegrity. The findings underscore that LLM led research may be option in the\nacademic integrity field, but that there is also a need for continued\ntraditional research. The findings also indicate that researchers and\neducational providers should continue to develop policy and operational\nframeworks for academic integrity. This will help to ensure that academic\nstandards are maintained across the wide range of settings that are present in\nmodern education.\n",
        "title": "A Large Language Model Supported Synthesis of Contemporary Academic\n  Integrity Research Trends",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03482",
        "abstract_url": "http://arxiv.org/abs/2401.03482",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Hao",
                "first_name": "Nan"
            },
            {
                "last_name": "Lu",
                "first_name": "Yingzhou"
            },
            {
                "last_name": "Van Rechem",
                "first_name": "Capucine"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The importance of uncertainty quantification is increasingly recognized in\nthe diverse field of machine learning. Accurately assessing model prediction\nuncertainty can help provide deeper understanding and confidence for\nresearchers and practitioners. This is especially critical in medical diagnosis\nand drug discovery areas, where reliable predictions directly impact research\nquality and patient health.\n  In this paper, we proposed incorporating uncertainty quantification into\nclinical trial outcome predictions. Our main goal is to enhance the model's\nability to discern nuanced differences, thereby significantly improving its\noverall performance.\n  We have adopted a selective classification approach to fulfill our objective,\nintegrating it seamlessly with the Hierarchical Interaction Network (HINT),\nwhich is at the forefront of clinical trial prediction modeling. Selective\nclassification, encompassing a spectrum of methods for uncertainty\nquantification, empowers the model to withhold decision-making in the face of\nsamples marked by ambiguity or low confidence, thereby amplifying the accuracy\nof predictions for the instances it chooses to classify. A series of\ncomprehensive experiments demonstrate that incorporating selective\nclassification into clinical trial predictions markedly enhances the model's\nperformance, as evidenced by significant upticks in pivotal metrics such as\nPR-AUC, F1, ROC-AUC, and overall accuracy.\n  Specifically, the proposed method achieved 32.37\\%, 21.43\\%, and 13.27\\%\nrelative improvement on PR-AUC over the base model (HINT) in phase I, II, and\nIII trial outcome prediction, respectively. When predicting phase III, our\nmethod reaches 0.9022 PR-AUC scores.\n  These findings illustrate the robustness and prospective utility of this\nstrategy within the area of clinical trial predictions, potentially setting a\nnew benchmark in the field.\n",
        "title": "Uncertainty Quantification on Clinical Trial Outcome Prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03488",
        "abstract_url": "http://arxiv.org/abs/2401.03488",
        "authors": [
            {
                "last_name": "Jameel",
                "first_name": "Abu Shafin Mohammad Mahdee"
            },
            {
                "last_name": "Mohamed",
                "first_name": "Ahmed P."
            },
            {
                "last_name": "Yi",
                "first_name": "Jinho"
            },
            {
                "last_name": "Gamal",
                "first_name": "Aly El"
            },
            {
                "last_name": "Malhotra",
                "first_name": "Akshay"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            ""
        ],
        "abstract": "  Deep learning based automatic modulation classification (AMC) has received\nsignificant attention owing to its potential applications in both military and\ncivilian use cases. Recently, data-driven subsampling techniques have been\nutilized to overcome the challenges associated with computational complexity\nand training time for AMC. Beyond these direct advantages of data-driven\nsubsampling, these methods also have regularizing properties that may improve\nthe adversarial robustness of the modulation classifier. In this paper, we\ninvestigate the effects of an adversarial attack on an AMC system that employs\ndeep learning models both for AMC and for subsampling. Our analysis shows that\nsubsampling itself is an effective deterrent to adversarial attacks. We also\nuncover the most efficient subsampling strategy when an adversarial attack on\nboth the classifier and the subsampler is anticipated.\n",
        "title": "Data-Driven Subsampling in the Presence of an Adversarial Actor",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03489",
        "abstract_url": "http://arxiv.org/abs/2401.03489",
        "authors": [
            {
                "last_name": "Jordan",
                "first_name": "Philip"
            },
            {
                "last_name": "Gr\u00f6tschla",
                "first_name": "Florian"
            },
            {
                "last_name": "Fan",
                "first_name": "Flint Xiaofeng"
            },
            {
                "last_name": "Wattenhofer",
                "first_name": "Roger"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DC",
            "MA"
        ],
        "abstract": "  In Federated Reinforcement Learning (FRL), agents aim to collaboratively\nlearn a common task, while each agent is acting in its local environment\nwithout exchanging raw trajectories. Existing approaches for FRL either (a) do\nnot provide any fault-tolerance guarantees (against misbehaving agents), or (b)\nrely on a trusted central agent (a single point of failure) for aggregating\nupdates. We provide the first decentralized Byzantine fault-tolerant FRL\nmethod. Towards this end, we first propose a new centralized Byzantine\nfault-tolerant policy gradient (PG) algorithm that improves over existing\nmethods by relying only on assumptions standard for non-fault-tolerant PG.\nThen, as our main contribution, we show how a combination of robust aggregation\nand Byzantine-resilient agreement methods can be leveraged in order to\neliminate the need for a trusted central entity. Since our results represent\nthe first sample complexity analysis for Byzantine fault-tolerant decentralized\nfederated non-convex optimization, our technical contributions may be of\nindependent interest. Finally, we corroborate our theoretical results\nexperimentally for common RL environments, demonstrating the speed-up of\ndecentralized federations w.r.t. the number of participating agents and\nresilience against various Byzantine attacks.\n",
        "title": "Decentralized Federated Policy Gradient with Byzantine Fault-Tolerance\n  and Provably Fast Convergence",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03491",
        "abstract_url": "http://arxiv.org/abs/2401.03491",
        "authors": [
            {
                "last_name": "Alharbi",
                "first_name": "Sarah"
            },
            {
                "last_name": "Khan",
                "first_name": "Arshiya"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Sophisticated cyber attacks present significant challenges for organizations\nin detecting and preventing such threats. To address this critical need for\nadvanced defense mechanisms, we propose an Ensemble Defense System (EDS). An\nEDS is a cybersecurity framework aggregating multiple security tools designed\nto monitor and alert an organization during cyber attacks. The proposed EDS\nleverages a comprehensive range of Intrusion Detection System (IDS)\ncapabilities by introducing a hybrid of signature-based IDS and anomaly-based\nIDS tools. It also incorporates Elasticsearch, an open-source Security\nInformation and Event Management (SIEM) tool, to facilitate data analysis and\ninteractive visualization of alerts generated from IDSs. The effectiveness of\nthe EDS is evaluated through a payload from a bash script that executes various\nattacks, including port scanning, privilege escalation, and Denial-of-Service\n(DoS). The evaluation demonstrates the EDS's ability to detect diverse cyber\nattacks.\n",
        "title": "Ensemble Defense System: A Hybrid IDS Approach for Effective Cyber\n  Threat Detection",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03492",
        "abstract_url": "http://arxiv.org/abs/2401.03492",
        "authors": [
            {
                "last_name": "Mora",
                "first_name": "Carlos"
            },
            {
                "last_name": "Yousefpour",
                "first_name": "Amin"
            },
            {
                "last_name": "Hosseinmardi",
                "first_name": "Shirin"
            },
            {
                "last_name": "Bostanabad",
                "first_name": "Ramin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Physics-informed machine learning (PIML) has emerged as a promising\nalternative to conventional numerical methods for solving partial differential\nequations (PDEs). PIML models are increasingly built via deep neural networks\n(NNs) whose architecture and training process are designed such that the\nnetwork satisfies the PDE system. While such PIML models have substantially\nadvanced over the past few years, their performance is still very sensitive to\nthe NN's architecture and loss function. Motivated by this limitation, we\nintroduce kernel-weighted Corrective Residuals (CoRes) to integrate the\nstrengths of kernel methods and deep NNs for solving nonlinear PDE systems. To\nachieve this integration, we design a modular and robust framework which\nconsistently outperforms competing methods in solving a broad range of\nbenchmark problems. This performance improvement has a theoretical\njustification and is particularly attractive since we simplify the training\nprocess while negligibly increasing the inference costs. Additionally, our\nstudies on solving multiple PDEs indicate that kernel-weighted CoRes\nconsiderably decrease the sensitivity of NNs to factors such as random\ninitialization, architecture type, and choice of optimizer. We believe our\nfindings have the potential to spark a renewed interest in leveraging kernel\nmethods for solving PDEs.\n",
        "title": "Neural Networks with Kernel-Weighted Corrective Residuals for Solving\n  Partial Differential Equations",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03493",
        "abstract_url": "http://arxiv.org/abs/2401.03493",
        "authors": [
            {
                "last_name": "Morgenstern",
                "first_name": "Hai"
            },
            {
                "last_name": "Rafaely",
                "first_name": "Boaz"
            },
            {
                "last_name": "Zotter",
                "first_name": "Franz"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Spatial attributes of room acoustics have been widely studied using\nmicrophone and loudspeaker arrays. However, systems that combine both arrays,\nreferred to as multiple-input multiple-output (MIMO) systems, have only been\nstudied to a limited degree in this context. These systems can potentially\nprovide a powerful tool for room acoustics analysis due to the ability to\nsimultaneously control both arrays. This paper offers a theoretical framework\nfor the spatial analysis of enclosed sound fields using a MIMO system\ncomprising spherical loudspeaker and microphone arrays. A system transfer\nfunction is formulated in matrix form for free-field conditions, and its\nproperties are studied using tools from linear algebra. The system is shown to\nhave unit-rank, regardless of the array types, and its singular vectors are\nrelated to the directions of arrival and radiation at the microphone and\nloudspeaker arrays, respectively. The formulation is then generalized to apply\nto rooms, using an image source method. In this case, the rank of the system is\nrelated to the number of significant reflections. The paper ends with\nsimulation studies, which support the developed theory, and with an extensive\nreflection analysis of a room impulse response, using the platform of a MIMO\nsystem.\n",
        "title": "Theory and investigation of acoustic multiple-input multiple-output\n  systems based on spherical arrays in a room",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03494",
        "abstract_url": "http://arxiv.org/abs/2401.03494",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Honghe"
            },
            {
                "last_name": "Mo",
                "first_name": "Site"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoxin"
            },
            {
                "last_name": "Yin",
                "first_name": "Nan"
            },
            {
                "last_name": "Fan",
                "first_name": "Songhai"
            },
            {
                "last_name": "Li",
                "first_name": "Bixiong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE",
            ""
        ],
        "abstract": "  The pre-insertion resistors (PIR) within high-voltage circuit breakers are\ncritical components and warm up by generating Joule heat when an electric\ncurrent flows through them. Elevated temperature can lead to temporary closure\nfailure and, in severe cases, the rupture of PIR. To accurately predict the\ntemperature of PIR, this study combines finite element simulation techniques\nwith Support Vector Regression (SVR) optimized by an Improved Whale\nOptimization Algorithm (IWOA) approach. The IWOA includes Tent mapping, a\nconvergence factor based on the sigmoid function, and the Ornstein-Uhlenbeck\nvariation strategy. The IWOA-SVR model is compared with the SSA-SVR and\nWOA-SVR. The results reveal that the prediction accuracies of the IWOA-SVR\nmodel were 90.2% and 81.5% (above 100$^\\circ$C) in the 3$^\\circ$C temperature\ndeviation range and 96.3% and 93.4% (above 100$^\\circ$C) in the 4$^\\circ$C\ntemperature deviation range, surpassing the performance of the comparative\nmodels. This research demonstrates the method proposed can realize the online\nmonitoring of the temperature of the PIR, which can effectively prevent thermal\nfaults PIR and provide a basis for the opening and closing of the circuit\nbreaker within a short period.\n",
        "title": "Pre-insertion resistors temperature prediction based on improved WOA-SVR",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03495",
        "abstract_url": "http://arxiv.org/abs/2401.03495",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yichi"
            },
            {
                "last_name": "Shen",
                "first_name": "Zhenrong"
            },
            {
                "last_name": "Jiao",
                "first_name": "Rushi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Due to the inherent flexibility of prompting, foundation models have emerged\nas the predominant force in the fields of natural language processing and\ncomputer vision. The recent introduction of the Segment Anything Model (SAM)\nsignifies a noteworthy expansion of the prompt-driven paradigm into the domain\nof image segmentation, thereby introducing a plethora of previously unexplored\ncapabilities. However, the viability of its application to medical image\nsegmentation remains uncertain, given the substantial distinctions between\nnatural and medical images. In this work, we provide a comprehensive overview\nof recent endeavors aimed at extending the efficacy of SAM to medical image\nsegmentation tasks, encompassing both empirical benchmarking and methodological\nadaptations. Additionally, we explore potential avenues for future research\ndirections in SAM's role within medical image segmentation. While direct\napplication of SAM to medical image segmentation does not yield satisfactory\nperformance on multi-modal and multi-target medical datasets so far, numerous\ninsights gleaned from these efforts serve as valuable guidance for shaping the\ntrajectory of foundational models in the realm of medical image analysis. To\nsupport ongoing research endeavors, we maintain an active repository that\ncontains an up-to-date paper list and a succinct summary of open-source\nprojects at https://github.com/YichiZhang98/SAM4MIS.\n",
        "title": "Segment Anything Model for Medical Image Segmentation: Current\n  Applications and Future Directions",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03497",
        "abstract_url": "http://arxiv.org/abs/2401.03497",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Wenxi"
            },
            {
                "last_name": "Liang",
                "first_name": "Yuzhe"
            },
            {
                "last_name": "Ma",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Zheng",
                "first_name": "Zhisheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xie"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  Audio self-supervised learning (SSL) pre-training, which aims to learn good\nrepresentations from unlabeled audio, has made remarkable progress. However,\nthe extensive computational demands during pre-training pose a significant\nbarrier to the potential application and optimization of audio SSL models. In\nthis paper, inspired by the success of data2vec 2.0 in image modality and\nAudio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to\nfurther improve the effectiveness and efficiency in audio SSL. The proposed EAT\nadopts the bootstrap self-supervised training paradigm to the audio domain. A\nnovel Utterance-Frame Objective (UFO) is designed to enhance the modeling\ncapability of acoustic events. Furthermore, we reveal that the masking strategy\nis critical in audio SSL pre-training, and superior audio representations can\nbe obtained with large inverse block masks. Experiment results demonstrate that\nEAT achieves state-of-the-art (SOTA) performance on a range of audio-related\ntasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a\nsignificant pre-training speedup up to ~15x compared to existing audio SSL\nmodels.\n",
        "title": "EAT: Self-Supervised Pre-Training with Efficient Audio Transformer",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03499",
        "abstract_url": "http://arxiv.org/abs/2401.03499",
        "authors": [
            {
                "last_name": "Cardoso",
                "first_name": "Joao Liborio"
            },
            {
                "last_name": "Banterle",
                "first_name": "Francesco"
            },
            {
                "last_name": "Cignoni",
                "first_name": "Paolo"
            },
            {
                "last_name": "Wimmer",
                "first_name": "Michael"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "GR",
            "MM",
            "",
            "",
            ""
        ],
        "abstract": "  We introduce context-aware translation, a novel method that combines the\nbenefits of inpainting and image-to-image translation, respecting\nsimultaneously the original input and contextual relevance -- where existing\nmethods fall short. By doing so, our method opens new avenues for the\ncontrollable use of AI within artistic creation, from animation to digital art.\n  As an use case, we apply our method to redraw any hand-drawn animated\ncharacter eyes based on any design specifications - eyes serve as a focal point\nthat captures viewer attention and conveys a range of emotions, however, the\nlabor-intensive nature of traditional animation often leads to compromises in\nthe complexity and consistency of eye design. Furthermore, we remove the need\nfor production data for training and introduce a new character recognition\nmethod that surpasses existing work by not requiring fine-tuning to specific\nproductions. This proposed use case could help maintain consistency throughout\nproduction and unlock bolder and more detailed design choices without the\nproduction cost drawbacks. A user study shows context-aware translation is\npreferred over existing work 95.16% of the time.\n",
        "title": "Re:Draw -- Context Aware Translation as a Controllable Method for\n  Artistic Production",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03500",
        "abstract_url": "http://arxiv.org/abs/2401.03500",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ming"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Weiland",
                "first_name": "Siep"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Safe stabilization is a significant challenge for quadrotors, which involves\nreaching a goal position while avoiding obstacles. Most of the existing\nsolutions for this problem rely on optimization-based methods, demanding\nsubstantial onboard computational resources. This paper introduces a novel\napproach to address this issue and provides a solution that offers fast\ncomputational capabilities tailored for onboard execution. Drawing inspiration\nfrom Sontag's universal formula, we propose an analytical control strategy that\nincorporates the conditions of control Lyapunov functions (CLFs) and control\nbarrier functions (CBFs), effectively avoiding the need for solving\noptimization problems onboard. Moreover, we extend our approach by\nincorporating the concepts of input-to-state stability (ISS) and input-to-state\nsafety (ISSf), enhancing the universal formula's capacity to effectively manage\ndisturbances. Furthermore, we present a projection-based approach to ensure\nthat the universal formula remains effective even when faced with control input\nconstraints. The basic idea of this approach is to project the control input\nderived from the universal formula onto the closest point within the control\ninput domain. Through comprehensive simulations and experimental results, we\nvalidate the efficacy and highlight the advantages of our methodology.\n",
        "title": "Quadrotor Stabilization with Safety Guarantees: A Universal Formula\n  Approach",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03504",
        "abstract_url": "http://arxiv.org/abs/2401.03504",
        "authors": [
            {
                "last_name": "M\u00fcller",
                "first_name": "Robert"
            },
            {
                "last_name": "Turalic",
                "first_name": "Hasan"
            },
            {
                "last_name": "Phan",
                "first_name": "Thomy"
            },
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "N\u00fc\u00dflein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In the realm of Multi-Agent Reinforcement Learning (MARL), prevailing\napproaches exhibit shortcomings in aligning with human learning, robustness,\nand scalability. Addressing this, we introduce ClusterComm, a fully\ndecentralized MARL framework where agents communicate discretely without a\ncentral control unit. ClusterComm utilizes Mini-Batch-K-Means clustering on the\nlast hidden layer's activations of an agent's policy network, translating them\ninto discrete messages. This approach outperforms no communication and competes\nfavorably with unbounded, continuous communication and hence poses a simple yet\neffective strategy for enhancing collaborative task-solving in MARL.\n",
        "title": "ClusterComm: Discrete Communication in Decentralized MARL using Internal\n  Representation Clustering",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03506",
        "abstract_url": "http://arxiv.org/abs/2401.03506",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Quan"
            },
            {
                "last_name": "Huang",
                "first_name": "Yiling"
            },
            {
                "last_name": "Zhao",
                "first_name": "Guanlong"
            },
            {
                "last_name": "Clark",
                "first_name": "Evan"
            },
            {
                "last_name": "Xia",
                "first_name": "Wei"
            },
            {
                "last_name": "Liao",
                "first_name": "Hank"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "SD"
        ],
        "abstract": "  In this paper, we introduce DiarizationLM, a framework to leverage large\nlanguage models (LLM) to post-process the outputs from a speaker diarization\nsystem. Various goals can be achieved with the proposed framework, such as\nimproving the readability of the diarized transcript, or reducing the word\ndiarization error rate (WDER). In this framework, the outputs of the automatic\nspeech recognition (ASR) and speaker diarization systems are represented as a\ncompact textual format, which is included in the prompt to an optionally\nfinetuned LLM. The outputs of the LLM can be used as the refined diarization\nresults with the desired enhancement. As a post-processing step, this framework\ncan be easily applied to any off-the-shelf ASR and speaker diarization systems\nwithout retraining existing components. Our experiments show that a finetuned\nPaLM 2-S model can reduce the WDER by rel. 25.9% on the Fisher telephone\nconversation dataset, and rel. 31% on the Callhome English dataset.\n",
        "title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language\n  Models",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03511",
        "abstract_url": "http://arxiv.org/abs/2401.03511",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xingjie Helen"
            },
            {
                "last_name": "Tao",
                "first_name": "Molei"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We introduce a novel approach for decomposing and learning every scale of a\ngiven multiscale objective function in $\\mathbb{R}^d$, where $d\\ge 1$. This\napproach leverages a recently demonstrated implicit bias of the optimization\nmethod of gradient descent by Kong and Tao, which enables the automatic\ngeneration of data that nearly follow Gibbs distribution with an effective\npotential at any desired scale. One application of this automated effective\npotential modeling is to construct reduced-order models. For instance, a\ndeterministic surrogate Hamiltonian model can be developed to substantially\nsoften the stiffness that bottlenecks the simulation, while maintaining the\naccuracy of phase portraits at the scale of interest. Similarly, a stochastic\nsurrogate model can be constructed at a desired scale, such that both its\nequilibrium and out-of-equilibrium behaviors (characterized by auto-correlation\nfunction and mean path) align with those of a damped mechanical system with the\noriginal multiscale function being its potential. The robustness and efficiency\nof our proposed approach in multi-dimensional scenarios have been demonstrated\nthrough a series of numerical experiments. A by-product of our development is a\nmethod for anisotropic noise estimation and calibration. More precisely,\nLangevin model of stochastic mechanical systems may not have isotropic noise in\npractice, and we provide a systematic algorithm to quantify its covariance\nmatrix without directly measuring the noise. In this case, the system may not\nadmit closed form expression of its invariant distribution either, but with\nthis tool, we can design friction matrix appropriately to calibrate the system\nso that its invariant distribution has a closed form expression of Gibbs.\n",
        "title": "Automated construction of effective potential via algorithmic implicit\n  bias",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03515",
        "abstract_url": "http://arxiv.org/abs/2401.03515",
        "authors": [
            {
                "last_name": "Tas",
                "first_name": "Nuri"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model\noutperforms BERTurk family models on the BOUN dataset for the POS task while\nresulting in underperformance on the IMST dataset for the same task and\nachieving competitive scores on the Turkish split of the XTREME dataset for the\nNER task - all while being pretrained on smaller data than its competitors. We\nrelease our pretrained model and tokenizer.\n",
        "title": "RoBERTurk: Adjusting RoBERTa for Turkish",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03522",
        "abstract_url": "http://arxiv.org/abs/2401.03522",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Rongqin"
            },
            {
                "last_name": "Li",
                "first_name": "Yuanman"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jiantao"
            },
            {
                "last_name": "Li",
                "first_name": "Xia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Traffic anomaly detection (TAD) in driving videos is critical for ensuring\nthe safety of autonomous driving and advanced driver assistance systems.\nPrevious single-stage TAD methods primarily rely on frame prediction, making\nthem vulnerable to interference from dynamic backgrounds induced by the rapid\nmovement of the dashboard camera. While two-stage TAD methods appear to be a\nnatural solution to mitigate such interference by pre-extracting\nbackground-independent features (such as bounding boxes and optical flow) using\nperceptual algorithms, they are susceptible to the performance of first-stage\nperceptual algorithms and may result in error propagation. In this paper, we\nintroduce TTHF, a novel single-stage method aligning video clips with text\nprompts, offering a new perspective on traffic anomaly detection. Unlike\nprevious approaches, the supervised signal of our method is derived from\nlanguages rather than orthogonal one-hot vectors, providing a more\ncomprehensive representation. Further, concerning visual representation, we\npropose to model the high frequency of driving videos in the temporal domain.\nThis modeling captures the dynamic changes of driving scenes, enhances the\nperception of driving behavior, and significantly improves the detection of\ntraffic anomalies. In addition, to better perceive various types of traffic\nanomalies, we carefully design an attentive anomaly focusing mechanism that\nvisually and linguistically guides the model to adaptively focus on the visual\ncontext of interest, thereby facilitating the detection of traffic anomalies.\nIt is shown that our proposed TTHF achieves promising performance,\noutperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and\nachieving high generalization on the DADA dataset.\n",
        "title": "Text-Driven Traffic Anomaly Detection with Temporal High-Frequency\n  Modeling in Driving Videos",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03523",
        "abstract_url": "http://arxiv.org/abs/2401.03523",
        "authors": [
            {
                "last_name": "Mansi",
                "first_name": "Mark"
            },
            {
                "last_name": "Swift",
                "first_name": "Michael M."
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS",
            "PF",
            ""
        ],
        "abstract": "  External fragmentation of physical memory occurs when adjacent differently\nsized regions of allocated physical memory are freed at different times,\ncausing free memory to be physically discontiguous. It can significantly\ndegrade system performance and efficiency, such as reducing the ability to use\nhuge pages, a critical optimization on modern large-memory system. For decades\nsystem developers have sought to avoid and mitigate fragmentation, but few\nprior studies quantify and characterize it in production settings.\n  Moreover, prior work often artificially fragments physical memory to create\nmore realistic performance evaluations, but their fragmentation methodologies\nare ad hoc and unvalidated. Out of 13 papers, we found 11 different\nmethodologies, some of which were subsequently found inadequate. The importance\nof addressing fragmentation necessitates a validated and principled\nmethodology.\n  Our work fills these gaps in knowledge and methodology. We conduct a study of\nmemory fragmentation in production by observing 248 machines in the Computer\nSciences Department at University of Wisconsin - Madison for a week. We\nidentify six key memory usage patterns, and find that Linux's file cache and\npage reclamation systems are major contributors to fragmentation because they\noften obliviously break up contiguous memory. Finally, we create and\\'uril, a\ntool to artificially fragment memory during experimental research evaluations.\nWhile and\\'uril ultimately fails as a scientific tool, we discuss its design\nideas, merits, and failings in hope that they may inspire future research.\n",
        "title": "Characterizing Physical Memory Fragmentation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03529",
        "abstract_url": "http://arxiv.org/abs/2401.03529",
        "authors": [
            {
                "last_name": "Gunter",
                "first_name": "Evan Ryan"
            },
            {
                "last_name": "Liokumovich",
                "first_name": "Yevgeny"
            },
            {
                "last_name": "Krakovna",
                "first_name": "Victoria"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We investigate the question: if an AI agent is known to be safe in one\nsetting, is it also safe in a new setting similar to the first? This is a core\nquestion of AI alignment--we train and test models in a certain environment,\nbut deploy them in another, and we need to guarantee that models that seem safe\nin testing remain so in deployment. Our notion of safety is based on\npower-seeking--an agent which seeks power is not safe. In particular, we focus\non a crucial type of power-seeking: resisting shutdown. We model agents as\npolicies for Markov decision processes, and show (in two cases of interest)\nthat not resisting shutdown is \"stable\": if an MDP has certain policies which\ndon't avoid shutdown, the corresponding policies for a similar MDP also don't\navoid shutdown. We also show that there are natural cases where safety is _not_\nstable--arbitrarily small perturbations may result in policies which never shut\ndown. In our first case of interest--near-optimal policies--we use a\nbisimulation metric on MDPs to prove that small perturbations won't make the\nagent take longer to shut down. Our second case of interest is policies for\nMDPs satisfying certain constraints which hold for various models (including\nlanguage models). Here, we demonstrate a quantitative bound on how fast the\nprobability of not shutting down can increase: by defining a metric on MDPs;\nproving that the probability of not shutting down, as a function on MDPs, is\nlower semicontinuous; and bounding how quickly this function decreases.\n",
        "title": "Quantifying stability of non-power-seeking in artificial agents",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03530",
        "abstract_url": "http://arxiv.org/abs/2401.03530",
        "authors": [
            {
                "last_name": "Hasan",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Rahman",
                "first_name": "Mohammad Shahriar"
            },
            {
                "last_name": "Janicke",
                "first_name": "Helge"
            },
            {
                "last_name": "Sarker",
                "first_name": "Iqbal H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  As the use of Blockchain for digital payments continues to rise in\npopularity, it also becomes susceptible to various malicious attacks.\nSuccessfully detecting anomalies within Blockchain transactions is essential\nfor bolstering trust in digital payments. However, the task of anomaly\ndetection in Blockchain transaction data is challenging due to the infrequent\noccurrence of illicit transactions. Although several studies have been\nconducted in the field, a limitation persists: the lack of explanations for the\nmodel's predictions. This study seeks to overcome this limitation by\nintegrating eXplainable Artificial Intelligence (XAI) techniques and anomaly\nrules into tree-based ensemble classifiers for detecting anomalous Bitcoin\ntransactions. The Shapley Additive exPlanation (SHAP) method is employed to\nmeasure the contribution of each feature, and it is compatible with ensemble\nmodels. Moreover, we present rules for interpreting whether a Bitcoin\ntransaction is anomalous or not. Additionally, we have introduced an\nunder-sampling algorithm named XGBCLUS, designed to balance anomalous and\nnon-anomalous transaction data. This algorithm is compared against other\ncommonly used under-sampling and over-sampling techniques. Finally, the\noutcomes of various tree-based single classifiers are compared with those of\nstacking and voting ensemble classifiers. Our experimental results demonstrate\nthat: (i) XGBCLUS enhances TPR and ROC-AUC scores compared to state-of-the-art\nunder-sampling and over-sampling techniques, and (ii) our proposed ensemble\nclassifiers outperform traditional single tree-based machine learning\nclassifiers in terms of accuracy, TPR, and FPR scores.\n",
        "title": "Detecting Anomalies in Blockchain Transactions using Machine Learning\n  Classifiers and Explainability Analysis",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03531",
        "abstract_url": "http://arxiv.org/abs/2401.03531",
        "authors": [
            {
                "last_name": "Valente",
                "first_name": "Luca"
            },
            {
                "last_name": "Nadalini",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Veeran",
                "first_name": "Asif"
            },
            {
                "last_name": "Sinigaglia",
                "first_name": "Mattia"
            },
            {
                "last_name": "Sa",
                "first_name": "Bruno"
            },
            {
                "last_name": "Wistoff",
                "first_name": "Nils"
            },
            {
                "last_name": "Tortorella",
                "first_name": "Yvan"
            },
            {
                "last_name": "Benatti",
                "first_name": "Simone"
            },
            {
                "last_name": "Psiakis",
                "first_name": "Rafail"
            },
            {
                "last_name": "Kulmala",
                "first_name": "Ari"
            },
            {
                "last_name": "Mohammad",
                "first_name": "Baker"
            },
            {
                "last_name": "Pinto",
                "first_name": "Sandro"
            },
            {
                "last_name": "Palossi",
                "first_name": "Daniele"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            },
            {
                "last_name": "Rossi",
                "first_name": "Davide"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            ""
        ],
        "abstract": "  The rapid advancement of energy-efficient parallel ultra-low-power (ULP)\nucontrollers units (MCUs) is enabling the development of autonomous nano-sized\nunmanned aerial vehicles (nano-UAVs). These sub-10cm drones represent the next\ngeneration of unobtrusive robotic helpers and ubiquitous smart sensors.\nHowever, nano-UAVs face significant power and payload constraints while\nrequiring advanced computing capabilities akin to standard drones, including\nreal-time Machine Learning (ML) performance and the safe co-existence of\ngeneral-purpose and real-time OSs. Although some advanced parallel ULP MCUs\noffer the necessary ML computing capabilities within the prescribed power\nlimits, they rely on small main memories (<1MB) and ucontroller-class CPUs with\nno virtualization or security features, and hence only support simple\nbare-metal runtimes. In this work, we present Shaheen, a 9mm2 200mW SoC\nimplemented in 22nm FDX technology. Differently from state-of-the-art MCUs,\nShaheen integrates a Linux-capable RV64 core, compliant with the v1.0 ratified\nHypervisor extension and equipped with timing channel protection, along with a\nlow-cost and low-power memory controller exposing up to 512MB of off-chip\nlow-cost low-power HyperRAM directly to the CPU. At the same time, it\nintegrates a fully programmable energy- and area-efficient multi-core cluster\nof RV32 cores optimized for general-purpose DSP as well as reduced- and\nmixed-precision ML. To the best of the authors' knowledge, it is the first\nsilicon prototype of a ULP SoC coupling the RV64 and RV32 cores in a\nheterogeneous host+accelerator architecture fully based on the RISC-V ISA. We\ndemonstrate the capabilities of the proposed SoC on a wide range of benchmarks\nrelevant to nano-UAV applications. The cluster can deliver up to 90GOp/s and up\nto 1.8TOp/s/W on 2-bit integer kernels and up to 7.9GFLOp/s and up to\n150GFLOp/s/W on 16-bit FP kernels.\n",
        "title": "A Heterogeneous RISC-V based SoC for Secure Nano-UAV Navigation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03533",
        "abstract_url": "http://arxiv.org/abs/2401.03533",
        "authors": [
            {
                "last_name": "Phadke",
                "first_name": "Shruti"
            },
            {
                "last_name": "Mitra",
                "first_name": "Tanushree"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "HC"
        ],
        "abstract": "  Increasingly online platforms are becoming popular arenas of political\namplification in India. With known instances of pre-organized coordinated\noperations, researchers are questioning the legitimacy of political expression\nand its consequences on the democratic processes in India. In this paper, we\nstudy an evolved form of political amplification by first identifying and then\ncharacterizing political campaigns with lexical mutations. By lexical mutation,\nwe mean content that is reframed, paraphrased, or altered while preserving the\nsame underlying message. Using multilingual embeddings and network analysis, we\ndetect over 3.8K political campaigns with text mutations spanning multiple\nlanguages and social media platforms in India. By further assessing the\npolitical leanings of accounts repeatedly involved in such amplification\ncampaigns, we contribute a broader understanding of how political amplification\nis used across various political parties in India. Moreover, our temporal\nanalysis of the largest amplification campaigns suggests that political\ncampaigning can evolve as temporally ordered arguments and counter-arguments\nbetween groups with competing political interests. Overall, our work\ncontributes insights into how lexical mutations can be leveraged to bypass the\nplatform manipulation policies and how such competing campaigning can provide\nan exaggerated sense of political divide on Indian social media.\n",
        "title": "Characterizing Political Campaigning with Lexical Mutants on Indian\n  Social Media",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03534",
        "abstract_url": "http://arxiv.org/abs/2401.03534",
        "authors": [
            {
                "last_name": "Alsharif",
                "first_name": "Hamza"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This dissertation investigates physics-informed neural networks (PINNs) as\ncandidate models for encoding governing equations, and assesses their\nperformance on experimental data from two different systems. The first system\nis a simple nonlinear pendulum, and the second is 2D heat diffusion across the\nsurface of a metal block. We show that for the pendulum system the PINNs\noutperformed equivalent uninformed neural networks (NNs) in the ideal data\ncase, with accuracy improvements of 18x and 6x for 10 linearly-spaced and 10\nuniformly-distributed random training points respectively. In similar test\ncases with real data collected from an experiment, PINNs outperformed NNs with\n9.3x and 9.1x accuracy improvements for 67 linearly-spaced and\nuniformly-distributed random points respectively. For the 2D heat diffusion, we\nshow that both PINNs and NNs do not fare very well in reconstructing the\nheating regime due to difficulties in optimizing the network parameters over a\nlarge domain in both time and space. We highlight that data denoising and\nsmoothing, reducing the size of the optimization problem, and using LBFGS as\nthe optimizer are all ways to improve the accuracy of the predicted solution\nfor both PINNs and NNs. Additionally, we address the viability of deploying\nphysics-informed models within physical systems, and we choose FPGAs as the\ncompute substrate for deployment. In light of this, we perform our experiments\nusing a PYNQ-Z1 FPGA and identify issues related to time-coherent sensing and\nspatial data alignment. We discuss the insights gained from this work and list\nfuture work items based on the proposed architecture for the system that our\nmethods work to develop.\n",
        "title": "Physics-informed Neural Networks for Encoding Dynamics in Real Physical\n  Systems",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03536",
        "abstract_url": "http://arxiv.org/abs/2401.03536",
        "authors": [
            {
                "last_name": "Bonato",
                "first_name": "Anthony"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhiyuan"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Counts of small subgraphs, or graphlet counts, are widely applicable to\nmeasure graph similarity. Computing graphlet counts can be computationally\nexpensive and may pose obstacles in network analysis. We study the role of\ncliques in graphlet counts as a method for graph similarity in social networks.\nHigher-order clustering coefficients and the Pivoter algorithm for exact clique\ncounts are employed\n",
        "title": "Clique counts for network similarity",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03538",
        "abstract_url": "http://arxiv.org/abs/2401.03538",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Pei",
                "first_name": "Jiakun"
            },
            {
                "last_name": "Xue",
                "first_name": "Liumeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Accent conversion aims to convert the accent of a source speech to a target\naccent, meanwhile preserving the speaker's identity. This paper introduces a\nnovel non-autoregressive framework for accent conversion that learns\naccent-agnostic linguistic representations and employs them to convert the\naccent in the source speech. Specifically, the proposed system aligns speech\nrepresentations with linguistic representations obtained from Text-to-Speech\n(TTS) systems, enabling training of the accent voice conversion model on\nnon-parallel data. Furthermore, we investigate the effectiveness of a\npretraining strategy on native data and different acoustic features within our\nproposed framework. We conduct a comprehensive evaluation using both subjective\nand objective metrics to assess the performance of our approach. The evaluation\nresults highlight the benefits of the pretraining strategy and the\nincorporation of richer semantic features, resulting in significantly enhanced\naudio quality and intelligibility.\n",
        "title": "Transfer the linguistic representations from TTS to accent conversion\n  with non-parallel data",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03540",
        "abstract_url": "http://arxiv.org/abs/2401.03540",
        "authors": [
            {
                "last_name": "Shamsolmoali",
                "first_name": "Pourya"
            },
            {
                "last_name": "Zareapoor",
                "first_name": "Masoumeh"
            },
            {
                "last_name": "Granger",
                "first_name": "Eric"
            },
            {
                "last_name": "Felsberg",
                "first_name": "Michael"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The dot product self-attention (DPSA) is a fundamental component of\ntransformers. However, scaling them to long sequences, like documents or\nhigh-resolution images, becomes prohibitively expensive due to quadratic time\nand memory complexities arising from the softmax operation. Kernel methods are\nemployed to simplify computations by approximating softmax but often lead to\nperformance drops compared to softmax attention. We propose SeTformer, a novel\ntransformer, where DPSA is purely replaced by Self-optimal Transport (SeT) for\nachieving better performance and computational efficiency. SeT is based on two\nessential softmax properties: maintaining a non-negative attention matrix and\nusing a nonlinear reweighting mechanism to emphasize important tokens in input\nsequences. By introducing a kernel cost function for optimal transport,\nSeTformer effectively satisfies these properties. In particular, with small and\nbasesized models, SeTformer achieves impressive top-1 accuracies of 84.7% and\n86.2% on ImageNet-1K. In object detection, SeTformer-base outperforms the\nFocalNet counterpart by +2.2 mAP, using 38% fewer parameters and 29% fewer\nFLOPs. In semantic segmentation, our base-size model surpasses NAT by +3.5 mIoU\nwith 33% fewer parameters. SeTformer also achieves state-of-the-art results in\nlanguage modeling on the GLUE benchmark. These findings highlight SeTformer's\napplicability in vision and language tasks.\n",
        "title": "SeTformer is What You Need for Vision and Language",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03545",
        "abstract_url": "http://arxiv.org/abs/2401.03545",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Hoa"
            },
            {
                "last_name": "Eger",
                "first_name": "Steffen"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL",
            "",
            "CL"
        ],
        "abstract": "  Citations are a key ingredient of scientific research to relate a paper to\nothers published in the community. Recently, it has been noted that there is a\ncitation age bias in the Natural Language Processing (NLP) community, one of\nthe currently fastest growing AI subfields, in that the mean age of the\nbibliography of NLP papers has become ever younger in the last few years,\nleading to `citation amnesia' in which older knowledge is increasingly\nforgotten. In this work, we put such claims into perspective by analyzing the\nbibliography of $\\sim$300k papers across 15 different scientific fields\nsubmitted to the popular preprint server Arxiv in the time period from 2013 to\n2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG)\nhave similar trends of citation amnesia, in which the age of the bibliography\nhas roughly halved in the last 10 years (from above 12 in 2013 to below 7 in\n2022), on average. Rather than diagnosing this as a citation age bias in the\nNLP community, we believe this pattern is an artefact of the dynamics of these\nresearch fields, in which new knowledge is produced in ever shorter time\nintervals.\n",
        "title": "Is there really a Citation Age Bias in NLP?",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03546",
        "abstract_url": "http://arxiv.org/abs/2401.03546",
        "authors": [
            {
                "last_name": "Goel",
                "first_name": "Shivam"
            },
            {
                "last_name": "Wei",
                "first_name": "Yichen"
            },
            {
                "last_name": "Lymperopoulos",
                "first_name": "Panagiotis"
            },
            {
                "last_name": "Scheutz",
                "first_name": "Matthias"
            },
            {
                "last_name": "Sinapov",
                "first_name": "Jivko"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  As AI agents leave the lab and venture into the real world as autonomous\nvehicles, delivery robots, and cooking robots, it is increasingly necessary to\ndesign and comprehensively evaluate algorithms that tackle the ``open-world''.\nTo this end, we introduce NovelGym, a flexible and adaptable ecosystem designed\nto simulate gridworld environments, serving as a robust platform for\nbenchmarking reinforcement learning (RL) and hybrid planning and learning\nagents in open-world contexts. The modular architecture of NovelGym facilitates\nrapid creation and modification of task environments, including multi-agent\nscenarios, with multiple environment transformations, thus providing a dynamic\ntestbed for researchers to develop open-world AI agents.\n",
        "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents\n  Designed for Open Worlds",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03547",
        "abstract_url": "http://arxiv.org/abs/2401.03547",
        "authors": [
            {
                "last_name": "Minato",
                "first_name": "Takashi"
            },
            {
                "last_name": "Higashinaka",
                "first_name": "Ryuichiro"
            },
            {
                "last_name": "Sakai",
                "first_name": "Kurima"
            },
            {
                "last_name": "Funayama",
                "first_name": "Tomo"
            },
            {
                "last_name": "Nishizaki",
                "first_name": "Hiromitsu"
            },
            {
                "last_name": "Naga",
                "first_name": "Takayuki"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  We have held dialogue robot competitions in 2020 and 2022 to compare the\nperformances of interactive robots using an android that closely resembles a\nhuman. In 2023, the third competition DRC2023 was held. The task of DRC2023 was\ndesigned to be more challenging than the previous travel agent dialogue tasks.\nSince anyone can now develop a dialogue system using LLMs, the participating\nteams are required to develop a system that effectively uses information about\nthe situation on the spot (real-time information), which is not handled by\nChatGPT and other systems. DRC2023 has two rounds, a preliminary round and the\nfinal round as well as the previous competitions. The preliminary round has\nheld on Oct.27 -- Nov.20, 2023 at real travel agency stores. The final round\nwill be held on December 23, 2023. This paper provides an overview of the task\nsettings and evaluation method of DRC2023 and the preliminary round results.\n",
        "title": "Overview of Dialogue Robot Competition 2023",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03551",
        "abstract_url": "http://arxiv.org/abs/2401.03551",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Chau"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Phuong"
            },
            {
                "last_name": "Tran",
                "first_name": "Thanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Dat"
            },
            {
                "last_name": "Trieu",
                "first_name": "An"
            },
            {
                "last_name": "Pham",
                "first_name": "Tin"
            },
            {
                "last_name": "Dang",
                "first_name": "Anh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Le-Minh"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  The Competition on Legal Information Extraction/Entailment (COLIEE) is held\nannually to encourage advancements in the automatic processing of legal texts.\nProcessing legal documents is challenging due to the intricate structure and\nmeaning of legal language. In this paper, we outline our strategies for\ntackling Task 2, Task 3, and Task 4 in the COLIEE 2023 competition. Our\napproach involved utilizing appropriate state-of-the-art deep learning methods,\ndesigning methods based on domain characteristics observation, and applying\nmeticulous engineering practices and methodologies to the competition. As a\nresult, our performance in these tasks has been outstanding, with first places\nin Task 2 and Task 3, and promising results in Task 4. Our source code is\navailable at https://github.com/Nguyen2015/CAPTAIN-COLIEE2023/tree/coliee2023.\n",
        "title": "CAPTAIN at COLIEE 2023: Efficient Methods for Legal Information\n  Retrieval and Entailment Tasks",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03552",
        "abstract_url": "http://arxiv.org/abs/2401.03552",
        "authors": [
            {
                "last_name": "M.",
                "first_name": "Sameera K."
            },
            {
                "last_name": "Nicolazzo",
                "first_name": "Serena"
            },
            {
                "last_name": "Arazzi",
                "first_name": "Marco"
            },
            {
                "last_name": "Nocera",
                "first_name": "Antonino"
            },
            {
                "last_name": "A.",
                "first_name": "Rafidha Rehiman K."
            },
            {
                "last_name": "P",
                "first_name": "Vinod"
            },
            {
                "last_name": "Conti",
                "first_name": "Mauro"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG"
        ],
        "abstract": "  Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.\n",
        "title": "Privacy-Preserving in Blockchain-based Federated Learning Systems",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03555",
        "abstract_url": "http://arxiv.org/abs/2401.03555",
        "authors": [
            {
                "last_name": "Wooding",
                "first_name": "Ben"
            },
            {
                "last_name": "Lavaei",
                "first_name": "Abolfazl"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper is concerned with developing a software tool, called IMPaCT, for\nthe parallelized verification and controller synthesis of large-scale\nstochastic systems using interval Markov chains (IMCs) and interval Markov\ndecision processes (IMDPs), respectively. The tool serves to (i) construct\nIMCs/IMDPs as finite abstractions of underlying original systems, and (ii)\nleverage interval iteration algorithms for formal verification and controller\nsynthesis over infinite-horizon properties, including safety, reachability, and\nreach-avoid, while offering convergence guarantees. IMPaCT is developed in C++\nand designed using AdaptiveCpp, an independent open-source implementation of\nSYCL, for adaptive parallelism over CPUs and GPUs of all hardware vendors,\nincluding Intel and NVIDIA. IMPaCT stands as the first software tool for the\nparallel construction of IMCs/IMDPs, empowered with the capability to leverage\nhigh-performance computing platforms and cloud computing services.\nSpecifically, parallelism offered by IMPaCT effectively addresses the\nchallenges arising from the state-explosion problem inherent in\ndiscretization-based techniques applied to large-scale stochastic systems. We\nbenchmark IMPaCT on several physical case studies, adopted from the ARCH tool\ncompetition for stochastic models, including a 2-dimensional robot, a\n3-dimensional autonomous vehicle, a 5-dimensional room temperature system, and\na 7-dimensional building automation system. To show the scalability of our\ntool, we also employ IMPaCT for the formal analysis of a 14-dimensional case\nstudy.\n",
        "title": "IMPaCT: Interval MDP Parallel Construction for Controller Synthesis of\n  Large-Scale Stochastic Systems",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03560",
        "abstract_url": "http://arxiv.org/abs/2401.03560",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Shreya"
            },
            {
                "last_name": "Jameel",
                "first_name": "Abu Shafin Mohammad Mahdee"
            },
            {
                "last_name": "Gamal",
                "first_name": "Aly El"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            ""
        ],
        "abstract": "  Network Intrusion Detection Systems (IDS) aim to detect the presence of an\nintruder by analyzing network packets arriving at an internet connected device.\nData-driven deep learning systems, popular due to their superior performance\ncompared to traditional IDS, depend on availability of high quality training\ndata for diverse intrusion classes. A way to overcome this limitation is\nthrough transferable learning, where training for one intrusion class can lead\nto detection of unseen intrusion classes after deployment. In this paper, we\nprovide a detailed study on the transferability of intrusion detection. We\ninvestigate practical federated learning configurations to enhance the\ntransferability of intrusion detection. We propose two techniques to\nsignificantly improve the transferability of a federated intrusion detection\nsystem. The code for this work can be found at\nhttps://github.com/ghosh64/transferability.\n",
        "title": "Improving Transferability of Network Intrusion Detection in a Federated\n  Learning Setup",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03561",
        "abstract_url": "http://arxiv.org/abs/2401.03561",
        "authors": [
            {
                "last_name": "Reusken",
                "first_name": "Arnold"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We consider the surface Stokes equation on a smooth closed hypersurface in\nthree-dimensional space. For discretization of this problem a generalization of\nthe surface finite element method (SFEM) of Dziuk-Elliott combined with a\nHood-Taylor pair of finite element spaces has been used in the literature. We\ncall this method Hood-Taylor-SFEM. This method uses a penalty technique to\nweakly satisfy the tangentiality constraint. In this paper we present a\ndiscretization error analysis of this method resulting in optimal\ndiscretization error bounds in an energy norm. We also address linear algebra\naspects related to (pre)conditioning of the system matrix.\n",
        "title": "Analysis of the Taylor-Hood Surface Finite Element Method for the\n  surface Stokes equation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03562",
        "abstract_url": "http://arxiv.org/abs/2401.03562",
        "authors": [
            {
                "last_name": "Meerza",
                "first_name": "Syed Irfan Ali"
            },
            {
                "last_name": "Liu",
                "first_name": "Luyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Jian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CY"
        ],
        "abstract": "  Federated learning (FL) has emerged as a prospective solution for\ncollaboratively learning a shared model across clients without sacrificing\ntheir data privacy. However, the federated learned model tends to be biased\nagainst certain demographic groups (e.g., racial and gender groups) due to the\ninherent FL properties, such as data heterogeneity and party selection. Unlike\ncentralized learning, mitigating bias in FL is particularly challenging as\nprivate training datasets and their sensitive attributes are typically not\ndirectly accessible. Most prior research in this field only focuses on global\nfairness while overlooking the local fairness of individual clients. Moreover,\nexisting methods often require sensitive information about the client's local\ndatasets to be shared, which is not desirable. To address these issues, we\npropose GLOCALFAIR, a client-server co-design fairness framework that can\njointly improve global and local group fairness in FL without the need for\nsensitive statistics about the client's private datasets. Specifically, we\nutilize constrained optimization to enforce local fairness on the client side\nand adopt a fairness-aware clustering-based aggregation on the server to\nfurther ensure the global model fairness across different sensitive groups\nwhile maintaining high utility. Experiments on two image datasets and one\ntabular dataset with various state-of-the-art fairness baselines show that\nGLOCALFAIR can achieve enhanced fairness under both global and local data\ndistributions while maintaining a good level of utility and client fairness.\n",
        "title": "GLOCALFAIR: Jointly Improving Global and Local Group Fairness in\n  Federated Learning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03563",
        "abstract_url": "http://arxiv.org/abs/2401.03563",
        "authors": [
            {
                "last_name": "Min",
                "first_name": "Yingqian"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kun"
            },
            {
                "last_name": "Gao",
                "first_name": "Dawei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Hu",
                "first_name": "He"
            },
            {
                "last_name": "Li",
                "first_name": "Yaliang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Recently, multi-task instruction tuning has been applied into sentence\nrepresentation learning, which endows the capability of generating specific\nrepresentations with the guidance of task instruction, exhibiting strong\ngeneralization ability on new tasks. However, these methods mostly neglect the\npotential interference problems across different tasks and instances, which may\naffect the training and convergence of the model. To address it, we propose a\ndata curriculum method, namely Data-CUBE, that arranges the orders of all the\nmulti-task data for training, to minimize the interference risks from the two\nviews. In the task level, we aim to find the optimal task order to minimize the\ntotal cross-task interference risk, which is exactly the traveling salesman\nproblem, hence we utilize a simulated annealing algorithm to find its solution.\nIn the instance level, we measure the difficulty of all instances per task,\nthen divide them into the easy-to-difficult mini-batches for training.\nExperiments on MTEB sentence representation evaluation tasks show that our\napproach can boost the performance of state-of-the-art methods. Our code and\ndata are publicly available at the link:\n\\url{https://github.com/RUCAIBox/Data-CUBE}.\n",
        "title": "Data-CUBE: Data Curriculum for Instruction-based Sentence Representation\n  Learning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03564",
        "abstract_url": "http://arxiv.org/abs/2401.03564",
        "authors": [
            {
                "last_name": "Srouji",
                "first_name": "Luis El"
            },
            {
                "last_name": "On",
                "first_name": "Mehmet Berkay"
            },
            {
                "last_name": "Lee",
                "first_name": "Yun-Jhu"
            },
            {
                "last_name": "Abdelghany",
                "first_name": "Mahmoud"
            },
            {
                "last_name": "Yoo",
                "first_name": "S. J. Ben"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Mach-Zehnder Interferometric meshes are attractive for low-loss photonic\nmatrix multiplication but are challenging to program. Using least-squares\noptimization of directional derivatives, we experimentally demonstrate that\ndesired matrix updates can be implemented agnostic to hardware imperfections.\n\\c{opyright} 2024 The Author(s)\n",
        "title": "Experimental Demonstration of Imperfection-Agnostic Local Learning Rules\n  on Photonic Neural Networks with Mach-Zehnder Interferometric Meshes",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03567",
        "abstract_url": "http://arxiv.org/abs/2401.03567",
        "authors": [
            {
                "last_name": "Petermann",
                "first_name": "Darius"
            },
            {
                "last_name": "Kim",
                "first_name": "Minje"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  In this work, we explore the task of hierarchical distance-based speech\nseparation defined on a hyperbolic manifold. Based on the recent advent of\naudio-related tasks performed in non-Euclidean spaces, we propose to make use\nof the Poincar\\'e ball to effectively unveil the inherent hierarchical\nstructure found in complex speaker mixtures. We design two sets of experiments\nin which the distance-based parent sound classes, namely \"near\" and \"far\", can\ncontain up to two or three speakers (i.e., children) each. We show that our\nhyperbolic approach is suitable for unveiling hierarchical structure from the\nproblem definition, resulting in improved child-level separation. We further\nshow that a clear correlation emerges between the notion of hyperbolic\ncertainty (i.e., the distance to the ball's origin) and acoustic semantics such\nas speaker density, inter-source location, and microphone-to-speaker distance.\n",
        "title": "Hyperbolic Distance-Based Speech Separation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03568",
        "abstract_url": "http://arxiv.org/abs/2401.03568",
        "authors": [
            {
                "last_name": "Durante",
                "first_name": "Zane"
            },
            {
                "last_name": "Huang",
                "first_name": "Qiuyuan"
            },
            {
                "last_name": "Wake",
                "first_name": "Naoki"
            },
            {
                "last_name": "Gong",
                "first_name": "Ran"
            },
            {
                "last_name": "Park",
                "first_name": "Jae Sung"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Bidipta"
            },
            {
                "last_name": "Taori",
                "first_name": "Rohan"
            },
            {
                "last_name": "Noda",
                "first_name": "Yusuke"
            },
            {
                "last_name": "Terzopoulos",
                "first_name": "Demetri"
            },
            {
                "last_name": "Choi",
                "first_name": "Yejin"
            },
            {
                "last_name": "Ikeuchi",
                "first_name": "Katsushi"
            },
            {
                "last_name": "Vo",
                "first_name": "Hoi"
            },
            {
                "last_name": "Fei-Fei",
                "first_name": "Li"
            },
            {
                "last_name": "Gao",
                "first_name": "Jianfeng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            "LG"
        ],
        "abstract": "  Multi-modal AI systems will likely become a ubiquitous presence in our\neveryday lives. A promising approach to making these systems more interactive\nis to embody them as agents within physical and virtual environments. At\npresent, systems leverage existing foundation models as the basic building\nblocks for the creation of embodied agents. Embedding agents within such\nenvironments facilitates the ability of models to process and interpret visual\nand contextual data, which is critical for the creation of more sophisticated\nand context-aware AI systems. For example, a system that can perceive user\nactions, human behavior, environmental objects, audio expressions, and the\ncollective sentiment of a scene can be used to inform and direct agent\nresponses within the given environment. To accelerate research on agent-based\nmultimodal intelligence, we define \"Agent AI\" as a class of interactive systems\nthat can perceive visual stimuli, language inputs, and other\nenvironmentally-grounded data, and can produce meaningful embodied action with\ninfinite agent. In particular, we explore systems that aim to improve agents\nbased on next-embodied action prediction by incorporating external knowledge,\nmulti-sensory inputs, and human feedback. We argue that by developing agentic\nAI systems in grounded environments, one can also mitigate the hallucinations\nof large foundation models and their tendency to generate environmentally\nincorrect outputs. The emerging field of Agent AI subsumes the broader embodied\nand agentic aspects of multimodal interactions. Beyond agents acting and\ninteracting in the physical world, we envision a future where people can easily\ncreate any virtual reality or simulated scene and interact with agents embodied\nwithin the virtual environment.\n",
        "title": "Agent AI: Surveying the Horizons of Multimodal Interaction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03570",
        "abstract_url": "http://arxiv.org/abs/2401.03570",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Qirun"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "DS"
        ],
        "abstract": "  Recently, Li et al. [2022] presented a dynamic Dyck-reachability algorithm\nfor bidirected graphs. The basic idea is based on updating edge weights in a\ndata structure called the merged graph $G_m$. As noted in Krishna et al.\n[2023], the edge deletion procedure described in the algorithm of Li et al.\n[2022] cannot properly update the weights in the presence of cycles in $G_m$.\nThis note discusses the cycle case and the time complexity.\n",
        "title": "A Note on Dynamic Bidirected Dyck-Reachability with Cycles",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03571",
        "abstract_url": "http://arxiv.org/abs/2401.03571",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Sixiang"
            },
            {
                "last_name": "Yang",
                "first_name": "Aaron J."
            },
            {
                "last_name": "Cai",
                "first_name": "Liming"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  RNA secondary structure is modeled with the novel arbitrary-order hidden\nMarkov model ({\\alpha}-HMM). The {\\alpha}-HMM extends over the traditional HMM\nwith capability to model stochastic events that may be in influenced by\nhistorically distant ones, making it suitable to account for long-range\ncanonical base pairings between nucleotides, which constitute the RNA secondary\nstructure. Unlike previous heavy-weight extensions over HMM, the {\\alpha}-HMM\nhas the flexibility to apply restrictions on how one event may influence\nanother in stochastic processes, enabling efficient prediction of RNA secondary\nstructure including pseudoknots.\n",
        "title": "{\\alpha}-HMM: A Graphical Model for RNA Folding",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03573",
        "abstract_url": "http://arxiv.org/abs/2401.03573",
        "authors": [
            {
                "last_name": "Jellen",
                "first_name": "Christopher"
            },
            {
                "last_name": "Nelson",
                "first_name": "Charles"
            },
            {
                "last_name": "Brownell",
                "first_name": "Cody"
            },
            {
                "last_name": "Burkhardt",
                "first_name": "John"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Optical turbulence presents a significant challenge for communication,\ndirected energy, and imaging systems, especially in the atmospheric boundary\nlayer. Effective modeling of optical turbulence strength is critical for the\ndevelopment and deployment of these systems. The lack of standard evaluation\ntools, especially long-term data sets, modeling tasks, metrics, and baseline\nmodels, prevent effective comparisons between approaches and models. This\nreduces the ease of reproducing results and contributes to over-fitting on\nlocal micro-climates. Performance characterized using evaluation metrics\nprovides some insight into the applicability of a model for predicting the\nstrength of optical turbulence. However, these metrics are not sufficient for\nunderstanding the relative quality of a model. We introduce the\n\\texttt{otbench} package, a Python package for rigorous development and\nevaluation of optical turbulence strength prediction models. The package\nprovides a consistent interface for evaluating optical turbulence models on a\nvariety of benchmark tasks and data sets. The \\texttt{otbench} package includes\na range of baseline models, including statistical, data-driven, and deep\nlearning models, to provide a sense of relative model quality. \\texttt{otbench}\nalso provides support for adding new data sets, tasks, and evaluation metrics.\nThe package is available at \\url{https://github.com/cdjellen/otbench}.\n",
        "title": "Effective Benchmarks for Optical Turbulence Modeling",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03575",
        "abstract_url": "http://arxiv.org/abs/2401.03575",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Md. Farhadul"
            },
            {
                "last_name": "Manab",
                "first_name": "Meem Arafat"
            },
            {
                "last_name": "Mondal",
                "first_name": "Joyanta Jyoti"
            },
            {
                "last_name": "Zabeen",
                "first_name": "Sarah"
            },
            {
                "last_name": "Rahman",
                "first_name": "Fardin Bin"
            },
            {
                "last_name": "Hasan",
                "first_name": "Md. Zahidul"
            },
            {
                "last_name": "Sadeque",
                "first_name": "Farig"
            },
            {
                "last_name": "Noor",
                "first_name": "Jannatun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Autism Spectrum Disorder (ASD) is a complicated neurological condition which\nis challenging to diagnose. Numerous studies demonstrate that children\ndiagnosed with autism struggle with maintaining attention spans and have less\nfocused vision. The eye-tracking technology has drawn special attention in the\ncontext of ASD since anomalies in gaze have long been acknowledged as a\ndefining feature of autism in general. Deep Learning (DL) approaches coupled\nwith eye-tracking sensors are exploiting additional capabilities to advance the\ndiagnostic and its applications. By learning intricate nonlinear input-output\nrelations, DL can accurately recognize the various gaze and eye-tracking\npatterns and adjust to the data. Convolutions alone are insufficient to capture\nthe important spatial information in gaze patterns or eye tracking. The dynamic\nkernel-based process known as involutions can improve the efficiency of\nclassifying gaze patterns or eye tracking data. In this paper, we utilise two\ndifferent image-processing operations to see how these processes learn\neye-tracking patterns. Since these patterns are primarily based on spatial\ninformation, we use involution with convolution making it a hybrid, which adds\nlocation-specific capability to a deep learning model. Our proposed model is\nimplemented in a simple yet effective approach, which makes it easier for\napplying in real life. We investigate the reasons why our approach works well\nfor classifying eye-tracking patterns. For comparative analysis, we experiment\nwith two separate datasets as well as a combined version of both. The results\nshow that IC with three involution layers outperforms the previous approaches.\n",
        "title": "Involution Fused ConvNet for Classifying Eye-Tracking Patterns of\n  Children with Autism Spectrum Disorder",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03580",
        "abstract_url": "http://arxiv.org/abs/2401.03580",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Qinwu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This study proposes a Newton based multiple objective optimization algorithm\nfor hyperparameter search. The first order differential (gradient) is\ncalculated using finite difference method and a gradient matrix with\nvectorization is formed for fast computation. The Newton Raphson iterative\nsolution is used to update model parameters with iterations, and a\nregularization term is included to eliminate the singularity issue. The\nalgorithm is applied to search the optimal probability threshold (a vector of\neight parameters) for a multiclass object detection problem of a convolutional\nneural network. The algorithm quickly finds the improved parameter values to\nproduce an overall higher true positive (TP) and lower false positive (FP)\nrates, as compared to using the default value of 0.5. In comparison, the\nBayesian optimization generates lower performance in the testing case. However,\nthe performance and parameter values may oscillate for some cases during\niterations, which may be due to the data driven stochastic nature of the\nsubject. Therefore, the optimal parameter value can be identified from a list\nof iteration steps according to the optimal TP and FP results.\n",
        "title": "A Multi-objective Newton Optimization Algorithm for Hyper-Parameter\n  Search",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03581",
        "abstract_url": "http://arxiv.org/abs/2401.03581",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Zhonghao"
            },
            {
                "last_name": "Chen",
                "first_name": "Han"
            },
            {
                "last_name": "Velentza",
                "first_name": "Anna-Maria"
            },
            {
                "last_name": "Liu",
                "first_name": "Siqi"
            },
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "O'Connell",
                "first_name": "Allison"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "RO"
        ],
        "abstract": "  Mindfulness-based therapies have been shown to be effective in improving\nmental health, and technology-based methods have the potential to expand the\naccessibility of these therapies. To enable real-time personalized content\ngeneration for mindfulness practice in these methods, high-quality\ncomputer-synthesized text-to-speech (TTS) voices are needed to provide verbal\nguidance and respond to user performance and preferences. However, the\nuser-perceived quality of state-of-the-art TTS voices has not yet been\nevaluated for administering mindfulness meditation, which requires emotional\nexpressiveness. In addition, work has not yet been done to study the effect of\nphysical embodiment and personalization on the user-perceived quality of TTS\nvoices for mindfulness. To that end, we designed a two-phase human subject\nstudy. In Phase 1, an online Mechanical Turk between-subject study (N=471)\nevaluated 3 (feminine, masculine, child-like) state-of-the-art TTS voices with\n2 (feminine, masculine) human therapists' voices in 3 different physical\nembodiment settings (no agent, conversational agent, socially assistive robot)\nwith remote participants. Building on findings from Phase 1, in Phase 2, an\nin-person within-subject study (N=94), we used a novel framework we developed\nfor personalizing TTS voices based on user preferences, and evaluated\nuser-perceived quality compared to best-rated non-personalized voices from\nPhase 1. We found that the best-rated human voice was perceived better than all\nTTS voices; the emotional expressiveness and naturalness of TTS voices were\npoorly rated, while users were satisfied with the clarity of TTS voices.\nSurprisingly, by allowing users to fine-tune TTS voice features, the\nuser-personalized TTS voices could perform almost as well as human voices,\nsuggesting user personalization could be a simple and very effective tool to\nimprove user-perceived quality of TTS voice.\n",
        "title": "Evaluating and Personalizing User-Perceived Quality of Text-to-Speech\n  Voices for Delivering Mindfulness Meditation with Different Physical\n  Embodiments",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03582",
        "abstract_url": "http://arxiv.org/abs/2401.03582",
        "authors": [
            {
                "last_name": "Sato",
                "first_name": "Takami"
            },
            {
                "last_name": "Bhupathiraju",
                "first_name": "Sri Hrushikesh Varma"
            },
            {
                "last_name": "Clifford",
                "first_name": "Michael"
            },
            {
                "last_name": "Sugawara",
                "first_name": "Takeshi"
            },
            {
                "last_name": "Chen",
                "first_name": "Qi Alfred"
            },
            {
                "last_name": "Rampazzi",
                "first_name": "Sara"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CV"
        ],
        "abstract": "  All vehicles must follow the rules that govern traffic behavior, regardless\nof whether the vehicles are human-driven or Connected Autonomous Vehicles\n(CAVs). Road signs indicate locally active rules, such as speed limits and\nrequirements to yield or stop. Recent research has demonstrated attacks, such\nas adding stickers or projected colored patches to signs, that cause CAV\nmisinterpretation, resulting in potential safety issues. Humans can see and\npotentially defend against these attacks. But humans can not detect what they\ncan not observe. We have developed an effective physical-world attack that\nleverages the sensitivity of filterless image sensors and the properties of\nInfrared Laser Reflections (ILRs), which are invisible to humans. The attack is\ndesigned to affect CAV cameras and perception, undermining traffic sign\nrecognition by inducing misclassification. In this work, we formulate the\nthreat model and requirements for an ILR-based traffic sign perception attack\nto succeed. We evaluate the effectiveness of the ILR attack with real-world\nexperiments against two major traffic sign recognition architectures on four\nIR-sensitive cameras. Our black-box optimization methodology allows the attack\nto achieve up to a 100% attack success rate in indoor, static scenarios and a\n>80.5% attack success rate in our outdoor, moving vehicle scenarios. We find\nthe latest state-of-the-art certifiable defense is ineffective against ILR\nattacks as it mis-certifies >33.5% of cases. To address this, we propose a\ndetection strategy based on the physical properties of IR laser reflections\nwhich can detect 96% of ILR attacks.\n",
        "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target\n  Traffic Sign Perception",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03585",
        "abstract_url": "http://arxiv.org/abs/2401.03585",
        "authors": [
            {
                "last_name": "Shukla",
                "first_name": "Prachi"
            },
            {
                "last_name": "Pavlidis",
                "first_name": "Vasilis F."
            },
            {
                "last_name": "Salman",
                "first_name": "Emre"
            },
            {
                "last_name": "Coskun",
                "first_name": "Ayse K."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Systolic arrays are popular for executing deep neural networks (DNNs) at the\nedge. Low latency and energy efficiency are key requirements in edge devices\nsuch as drones and autonomous vehicles. Monolithic 3D (MONO3D) is an emerging\n3D integration technique that offers ultra-high bandwidth among processing and\nmemory elements with a negligible area overhead. Such high bandwidth can help\nmeet the ever-growing latency and energy efficiency demands for DNNs. This\npaper presents a novel implementation for weight stationary (WS) dataflow in\nMONO3D systolic arrays, called WS-MONO3D. WS-MONO3D utilizes multiple resistive\nRAM layers and SRAM with high-density vertical interconnects to multicast\ninputs and perform high-bandwidth weight pre-loading while maintaining the same\norder of multiply-and-accumulate operations as in native WS dataflow.\nConsequently, WS-MONO3D eliminates input and weight forwarding cycles and,\nthus, provides up to 40% improvement in energy-delay-product (EDP) over the\nnative WS implementation in 2D at iso-configuration. WS-MONO3D also provides\n10X improvement in inference per second per watt per footprint due to multiple\nvertical tiers. Finally, we also show that temperature impacts the energy\nefficiency benefits in WS-MONO3D.\n",
        "title": "A New Dataflow Implementation to Improve Energy Efficiency of Monolithic\n  3D Systolic Arrays",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03587",
        "abstract_url": "http://arxiv.org/abs/2401.03587",
        "authors": [
            {
                "last_name": "Adewopo",
                "first_name": "Victor"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Zag"
            },
            {
                "last_name": "Ozer",
                "first_name": "Murat"
            },
            {
                "last_name": "Zekios",
                "first_name": "Constantinos"
            },
            {
                "last_name": "Abdelgawad",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Bayoumi",
                "first_name": "Magdy"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In the dynamic urban landscape, where the interplay of vehicles and\npedestrians defines the rhythm of life, integrating advanced technology for\nsafety and efficiency is increasingly crucial. This study delves into the\napplication of cutting-edge technological methods in smart cities, focusing on\nenhancing public safety through improved traffic accident detection. Action\nrecognition plays a pivotal role in interpreting visual data and tracking\nobject motion such as human pose estimation in video sequences. The challenges\nof action recognition include variability in rapid actions, limited dataset,\nand environmental factors such as (Weather, Illumination, and Occlusions). In\nthis paper, we present a novel comprehensive dataset for traffic accident\ndetection. This datasets is specifically designed to bolster computer vision\nand action recognition systems in predicting and detecting road traffic\naccidents. We integrated datasets from wide variety of data sources, road\nnetworks, weather conditions, and regions across the globe. This approach is\nunderpinned by empirical studies, aiming to contribute to the discourse on how\ntechnology can enhance the quality of life in densely populated areas. This\nresearch aims to bridge existing research gaps by introducing benchmark\ndatasets that leverage state-of-the-art algorithms tailored for traffic\naccident detection in smart cities. These dataset is expected to advance\nacademic research and also enhance real-time accident detection applications,\ncontributing significantly to the evolution of smart urban environments. Our\nstudy marks a pivotal step towards safer, more efficient smart cities,\nharnessing the power of AI and machine learning to transform urban living.\n",
        "title": "Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for\n  AI-Driven Traffic Accident Detection and Computer Vision Systems",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03588",
        "abstract_url": "http://arxiv.org/abs/2401.03588",
        "authors": [
            {
                "last_name": "Mishagli",
                "first_name": "Dmytro"
            },
            {
                "last_name": "Koskin",
                "first_name": "Eugene"
            },
            {
                "last_name": "Blokhina",
                "first_name": "Elena"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "AR"
        ],
        "abstract": "  In this paper, the Statistical Static Timing Analysis (SSTA) is considered\nwithin the block--based approach. The statistical model of the logic gate delay\npropagation is systematically studied and the exact analytical solution is\nobtained, which is strongly non-Gaussian. The procedure of handling such\n(non-Gaussian) distributions is described and the corresponding algorithm for\nthe critical path delay is outlined. Finally, the proposed approach is tested\nand compared with Monte Carlo simulations.\n",
        "title": "Gate--Level Statistical Timing Analysis: Exact Solutions, Approximations\n  and Algorithms",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03590",
        "abstract_url": "http://arxiv.org/abs/2401.03590",
        "authors": [
            {
                "last_name": "Budur",
                "first_name": "Emrah"
            },
            {
                "last_name": "\u00d6z\u00e7elik",
                "first_name": "R\u0131za"
            },
            {
                "last_name": "Soylu",
                "first_name": "Dilara"
            },
            {
                "last_name": "Khattab",
                "first_name": "Omar"
            },
            {
                "last_name": "G\u00fcng\u00f6r",
                "first_name": "Tunga"
            },
            {
                "last_name": "Potts",
                "first_name": "Christopher"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Question answering (QA) is the task of answering questions posed in natural\nlanguage with free-form natural language answers extracted from a given\npassage. In the OpenQA variant, only a question text is given, and the system\nmust retrieve relevant passages from an unstructured knowledge source and use\nthem to provide answers, which is the case in the mainstream QA systems on the\nWeb. QA systems currently are mostly limited to the English language due to the\nlack of large-scale labeled QA datasets in non-English languages. In this\npaper, we show that effective, low-cost OpenQA systems can be developed for\nlow-resource languages. The key ingredients are (1) weak supervision using\nmachine-translated labeled datasets and (2) a relevant unstructured knowledge\nsource in the target language. Furthermore, we show that only a few hundred\ngold assessment examples are needed to reliably evaluate these systems. We\napply our method to Turkish as a challenging case study, since English and\nTurkish are typologically very distinct. We present SQuAD-TR, a machine\ntranslation of SQuAD2.0, and we build our OpenQA system by adapting ColBERT-QA\nfor Turkish. We obtain a performance improvement of 9-34% in the EM score and\n13-33% in the F1 score compared to the BM25-based and DPR-based baseline QA\nreader models by using two versions of Wikipedia dumps spanning two years. Our\nresults show that SQuAD-TR makes OpenQA feasible for Turkish, which we hope\nencourages researchers to build OpenQA systems in other low-resource languages.\nWe make all the code, models, and the dataset publicly available.\n",
        "title": "Building Efficient and Effective OpenQA Systems for Low-Resource\n  Languages",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03591",
        "abstract_url": "http://arxiv.org/abs/2401.03591",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Siyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Lu"
            },
            {
                "last_name": "Song",
                "first_name": "Chenwei"
            },
            {
                "last_name": "Liu",
                "first_name": "Xinyi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To resolve the semantic ambiguity in texts, we propose a model, which\ninnovatively combines a knowledge graph with an improved attention mechanism.\nAn existing knowledge base is utilized to enrich the text with relevant\ncontextual concepts. The model operates at both character and word levels to\ndeepen its understanding by integrating the concepts. We first adopt\ninformation gain to select import words. Then an encoder-decoder framework is\nused to encode the text along with the related concepts. The local attention\nmechanism adjusts the weight of each concept, reducing the influence of\nirrelevant or noisy concepts during classification. We improve the calculation\nformula for attention scores in the local self-attention mechanism, ensuring\nthat words with different frequencies of occurrence in the text receive higher\nattention scores. Finally, the model employs a Bi-directional Gated Recurrent\nUnit (Bi-GRU), which is effective in feature extraction from texts for improved\nclassification accuracy. Its performance is demonstrated on datasets such as\nAGNews, Ohsumed, and TagMyNews, achieving accuracy of 75.1%, 58.7%, and 68.5%\nrespectively, showing its effectiveness in classifying tasks.\n",
        "title": "Text Classification Based on Knowledge Graphs and Improved Attention\n  Mechanism",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03597",
        "abstract_url": "http://arxiv.org/abs/2401.03597",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Liu",
                "first_name": "Guanfeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Nan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Heterogeneous graph few-shot learning (HGFL) has been developed to address\nthe label sparsity issue in heterogeneous graphs (HGs), which consist of\nvarious types of nodes and edges. The core concept of HGFL is to extract\nknowledge from rich-labeled classes in a source HG, transfer this knowledge to\na target HG to facilitate learning new classes with few-labeled training data,\nand finally make predictions on unlabeled testing data. Existing methods\ntypically assume that the source HG, training data, and testing data all share\nthe same distribution. However, in practice, distribution shifts among these\nthree types of data are inevitable due to two reasons: (1) the limited\navailability of the source HG that matches the target HG distribution, and (2)\nthe unpredictable data generation mechanism of the target HG. Such distribution\nshifts result in ineffective knowledge transfer and poor learning performance\nin existing methods, thereby leading to a novel problem of out-of-distribution\n(OOD) generalization in HGFL. To address this challenging problem, we propose a\nnovel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF. In\nCOHF, we first characterize distribution shifts in HGs with a structural causal\nmodel, establishing an invariance principle for OOD generalization in HGFL.\nThen, following this invariance principle, we propose a new variational\nautoencoder-based heterogeneous graph neural network to mitigate the impact of\ndistribution shifts. Finally, by integrating this network with a novel\nmeta-learning framework, COHF effectively transfers knowledge to the target HG\nto predict new classes with few-labeled data. Extensive experiments on seven\nreal-world datasets have demonstrated the superior performance of COHF over the\nstate-of-the-art methods.\n",
        "title": "Few-Shot Causal Representation Learning for Out-of-Distribution\n  Generalization on Heterogeneous Graphs",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03599",
        "abstract_url": "http://arxiv.org/abs/2401.03599",
        "authors": [
            {
                "last_name": "Dax",
                "first_name": "Victoria M."
            },
            {
                "last_name": "Li",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Sachdeva",
                "first_name": "Enna"
            },
            {
                "last_name": "Agarwal",
                "first_name": "Nakul"
            },
            {
                "last_name": "Kochenderfer",
                "first_name": "Mykel J."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Effective interaction modeling and behavior prediction of dynamic agents play\na significant role in interactive motion planning for autonomous robots.\nAlthough existing methods have improved prediction accuracy, few research\nefforts have been devoted to enhancing prediction model interpretability and\nout-of-distribution (OOD) generalizability. This work addresses these two\nchallenging aspects by designing a variational auto-encoder framework that\nintegrates graph-based representations and time-sequence models to efficiently\ncapture spatio-temporal relations between interactive agents and predict their\ndynamics. Our model infers dynamic interaction graphs in a latent space\naugmented with interpretable edge features that characterize the interactions.\nMoreover, we aim to enhance model interpretability and performance in OOD\nscenarios by disentangling the latent space of edge features, thereby\nstrengthening model versatility and robustness. We validate our approach\nthrough extensive experiments on both simulated and real-world datasets. The\nresults show superior performance compared to existing methods in modeling\nspatio-temporal relations, motion prediction, and identifying time-invariant\nlatent features.\n",
        "title": "Disentangled Neural Relational Inference for Interpretable Motion\n  Prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03601",
        "abstract_url": "http://arxiv.org/abs/2401.03601",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Song",
                "first_name": "Kaiqiang"
            },
            {
                "last_name": "Hu",
                "first_name": "Yebowen"
            },
            {
                "last_name": "Yao",
                "first_name": "Wenlin"
            },
            {
                "last_name": "Cho",
                "first_name": "Sangwoo"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaoyang"
            },
            {
                "last_name": "Wu",
                "first_name": "Xuansheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Fei"
            },
            {
                "last_name": "Liu",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Yu",
                "first_name": "Dong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This paper introduces the Decomposed Requirements Following Ratio (DRFR), a\nnew metric for evaluating Large Language Models' (LLMs) ability to follow\ninstructions. Addressing a gap in current methodologies, DRFR breaks down\ncomplex instructions into simpler criteria, facilitating a detailed analysis of\nLLMs' compliance with various aspects of tasks. Alongside this metric, we\npresent InFoBench, a benchmark comprising 500 diverse instructions and 2,250\ndecomposed questions across multiple constraint categories. Our experiments\ncompare DRFR with traditional scoring methods and explore annotation sources,\nincluding human experts, crowd-sourced workers, and GPT-4. The findings\ndemonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a\ncost-efficient annotator. The evaluation of several advanced LLMs using this\nframework reveals their strengths and areas needing improvement, particularly\nin complex instruction-following. This study contributes a novel metric and\nbenchmark, offering insights for future LLM development and evaluation.\n",
        "title": "InFoBench: Evaluating Instruction Following Ability in Large Language\n  Models",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03604",
        "abstract_url": "http://arxiv.org/abs/2401.03604",
        "authors": [
            {
                "last_name": "Samadzadeh",
                "first_name": "Ali"
            },
            {
                "last_name": "Mojab",
                "first_name": "Mohammad Hassan"
            },
            {
                "last_name": "Soudani",
                "first_name": "Heydar"
            },
            {
                "last_name": "Mireshghollah",
                "first_name": "Seyed Hesamoddin"
            },
            {
                "last_name": "Nickabadi",
                "first_name": "Ahmad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visual Inertial Odometry (VIO) algorithms estimate the accurate camera\ntrajectory by using camera and Inertial Measurement Unit (IMU) sensors. The\napplications of VIO span a diverse range, including augmented reality and\nindoor navigation. VIO algorithms hold the potential to facilitate navigation\nfor visually impaired individuals in both indoor and outdoor settings.\nNevertheless, state-of-the-art VIO algorithms encounter substantial challenges\nin dynamic environments, particularly in densely populated corridors. Existing\nVIO datasets, e.g., ADVIO, typically fail to effectively exploit these\nchallenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI)\nto address the mentioned problem and improve the navigation systems. AUT-VI is\na novel and super-challenging dataset with 126 diverse sequences in 17\ndifferent locations. This dataset contains dynamic objects, challenging\nloop-closure/map-reuse, different lighting conditions, reflections, and sudden\ncamera movements to cover all extreme navigation scenarios. Moreover, in\nsupport of ongoing development efforts, we have released the Android\napplication for data capture to the public. This allows fellow researchers to\neasily capture their customized VIO dataset variations. In addition, we\nevaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry\n(VO) methods on our dataset, emphasizing the essential need for this\nchallenging dataset.\n",
        "title": "Amirkabir campus dataset: Real-world challenges and scenarios of Visual\n  Inertial Odometry (VIO) for visually impaired people",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03605",
        "abstract_url": "http://arxiv.org/abs/2401.03605",
        "authors": [
            {
                "last_name": "Spurlock",
                "first_name": "Kyle Dylan"
            },
            {
                "last_name": "Acun",
                "first_name": "Cagla"
            },
            {
                "last_name": "Saka",
                "first_name": "Esin"
            },
            {
                "last_name": "Nasraoui",
                "first_name": "Olfa"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "CL",
            "LG",
            "",
            ""
        ],
        "abstract": "  Recommendation algorithms have been pivotal in handling the overwhelming\nvolume of online content. However, these algorithms seldom consider direct user\ninput, resulting in superficial interaction between them. Efforts have been\nmade to include the user directly in the recommendation process through\nconversation, but these systems too have had limited interactivity. Recently,\nLarge Language Models (LLMs) like ChatGPT have gained popularity due to their\nease of use and their ability to adapt dynamically to various tasks while\nresponding to feedback. In this paper, we investigate the effectiveness of\nChatGPT as a top-n conversational recommendation system. We build a rigorous\npipeline around ChatGPT to simulate how a user might realistically probe the\nmodel for recommendations: by first instructing and then reprompting with\nfeedback to refine a set of recommendations. We further explore the effect of\npopularity bias in ChatGPT's recommendations, and compare its performance to\nbaseline models. We find that reprompting ChatGPT with feedback is an effective\nstrategy to improve recommendation relevancy, and that popularity bias can be\nmitigated through prompt engineering.\n",
        "title": "ChatGPT for Conversational Recommendation: Refining Recommendations by\n  Reprompting with Feedback",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03608",
        "abstract_url": "http://arxiv.org/abs/2401.03608",
        "authors": [
            {
                "last_name": "Trogdon",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We develop the ultraspherical rectangular collocation (URC) method, a\ncollocation implementation of the sparse ultraspherical method of Olver \\&\nTownsend for two-point boundary-value problems. The URC method is provably\nconvergent, the implementation is simple and efficient, the convergence proof\nmotivates a preconditioner for iterative methods, and the modification of\ncollocation nodes is straightforward. The convergence theorem applies to all\nboundary-value problems when the coefficient functions are sufficiently smooth\nand when the roots of certain ultraspherical polynomials are used as\ncollocation nodes. We also adapt a theorem of Krasnolsel'skii et al.~to our\nsetting to prove convergence for the rectangular collocation method of Driscoll\n\\& Hale for a restricted class of boundary conditions.\n",
        "title": "The ultraspherical rectangular collocation method and its convergence",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03609",
        "abstract_url": "http://arxiv.org/abs/2401.03609",
        "authors": [
            {
                "last_name": "Borazjani",
                "first_name": "Kasra"
            },
            {
                "last_name": "Khosravan",
                "first_name": "Naji"
            },
            {
                "last_name": "Ying",
                "first_name": "Leslie"
            },
            {
                "last_name": "Hosseinalipour",
                "first_name": "Seyyedali"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The use of machine learning (ML) for cancer staging through medical image\nanalysis has gained substantial interest across medical disciplines. When\naccompanied by the innovative federated learning (FL) framework, ML techniques\ncan further overcome privacy concerns related to patient data exposure. Given\nthe frequent presence of diverse data modalities within patient records,\nleveraging FL in a multi-modal learning framework holds considerable promise\nfor cancer staging. However, existing works on multi-modal FL often presume\nthat all data-collecting institutions have access to all data modalities. This\noversimplified approach neglects institutions that have access to only a\nportion of data modalities within the system. In this work, we introduce a\nnovel FL architecture designed to accommodate not only the heterogeneity of\ndata samples, but also the inherent heterogeneity/non-uniformity of data\nmodalities across institutions. We shed light on the challenges associated with\nvarying convergence speeds observed across different data modalities within our\nFL system. Subsequently, we propose a solution to tackle these challenges by\ndevising a distributed gradient blending and proximity-aware client weighting\nstrategy tailored for multi-modal FL. To show the superiority of our method, we\nconduct experiments using The Cancer Genome Atlas program (TCGA) datalake\nconsidering different cancer types and three modalities of data: mRNA\nsequences, histopathological image data, and clinical information.\n",
        "title": "Multi-Modal Federated Learning for Cancer Staging over Non-IID Datasets\n  with Unbalanced Modalities",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03610",
        "abstract_url": "http://arxiv.org/abs/2401.03610",
        "authors": [
            {
                "last_name": "Hin",
                "first_name": "Shing"
            },
            {
                "last_name": "Yeung",
                "first_name": ""
            },
            {
                "last_name": "Piraveenan",
                "first_name": "Mahendra"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "MA",
            ""
        ],
        "abstract": "  We employ an agent-based contact network model to study the relationship\nbetween vaccine uptake and disease dynamics in a hypothetical country town from\nNew South Wales, Australia, undergoing a COVID-19 epidemic, over a period of\nthree years. We model the contact network in this hypothetical township of N =\n10000 people as a scale-free network, and simulate the spread of COVID-19 and\nvaccination program using disease and vaccination uptake parameters typically\nobserved in such a NSW town. We simulate the spread of the ancestral variant of\nCOVID-19 in this town, and study the disease dynamics while the town maintains\nlimited but non-negligible contact with the rest of the country which is\nassumed to be undergoing a severe COVID-19 epidemic. We also simulate a maximum\nthree doses of Pfizer Comirnaty vaccine being administered in this town, with\nlimited vaccine supply at first which gradually increases, and analyse how the\nvaccination uptake affects the disease dynamics in this town, which is captured\nusing an extended compartmental model with epidemic parameters typical for a\nCOVID-19 epidemic in Australia. Our results show that, in such a township,\nthree vaccination doses are sufficient to contain but not eradicate COVID-19,\nand the disease essentially becomes endemic. We also show that the average\ndegree of infected nodes (the average number of contacts for infected people)\npredicts the proportion of infected people. Therefore, if the hubs (people with\na relatively high number of contacts) are disproportionately infected, this\nindicates an oncoming peak of the infection, though the lag time thereof\ndepends on the maximum number of vaccines administered to the populace.\nOverall, our analysis provides interesting insights in understanding the\ninterplay between network topology, vaccination levels, and COVID-19 disease\ndynamics in a typical remote NSW country town.\n",
        "title": "Agent based network modelling of COVID-19 disease dynamics and\n  vaccination uptake in a New South Wales Country Township",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03615",
        "abstract_url": "http://arxiv.org/abs/2401.03615",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yihao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Philippe"
            },
            {
                "last_name": "Tan",
                "first_name": "Yubo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jing"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhihan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Weili"
            },
            {
                "last_name": "Conze",
                "first_name": "Pierre-Henri"
            },
            {
                "last_name": "Lamard",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Quellec",
                "first_name": "Gwenol\u00e9"
            },
            {
                "last_name": "Daho",
                "first_name": "Mostafa El Habib"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Myopic macular degeneration is the most common complication of myopia and the\nprimary cause of vision loss in individuals with pathological myopia. Early\ndetection and prompt treatment are crucial in preventing vision impairment due\nto myopic maculopathy. This was the focus of the Myopic Maculopathy Analysis\nChallenge (MMAC), in which we participated. In task 1, classification of myopic\nmaculopathy, we employed the contrastive learning framework, specifically\nSimCLR, to enhance classification accuracy by effectively capturing enriched\nfeatures from unlabeled data. This approach not only improved the intrinsic\nunderstanding of the data but also elevated the performance of our\nclassification model. For Task 2 (segmentation of myopic maculopathy plus\nlesions), we have developed independent segmentation models tailored for\ndifferent lesion segmentation tasks and implemented a test-time augmentation\nstrategy to further enhance the model's performance. As for Task 3 (prediction\nof spherical equivalent), we have designed a deep regression model based on the\ndata distribution of the dataset and employed an integration strategy to\nenhance the model's prediction accuracy. The results we obtained are promising\nand have allowed us to position ourselves in the Top 6 of the classification\ntask, the Top 2 of the segmentation task, and the Top 1 of the prediction task.\nThe code is available at\n\\url{https://github.com/liyihao76/MMAC_LaTIM_Solution}.\n",
        "title": "Automated Detection of Myopic Maculopathy in MMAC 2023: Achievements in\n  Classification, Segmentation, and Spherical Equivalent Prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03619",
        "abstract_url": "http://arxiv.org/abs/2401.03619",
        "authors": [
            {
                "last_name": "Ebrahimi",
                "first_name": "Zeinab"
            },
            {
                "last_name": "Batista",
                "first_name": "Gustavo"
            },
            {
                "last_name": "Deghat",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Stochastic gradient descent (SGD) and its many variants are the widespread\noptimization algorithms for training deep neural networks. However, SGD suffers\nfrom inevitable drawbacks, including vanishing gradients, lack of theoretical\nguarantees, and substantial sensitivity to input. The Alternating Direction\nMethod of Multipliers (ADMM) has been proposed to address these shortcomings as\nan effective alternative to the gradient-based methods. It has been\nsuccessfully employed for training deep neural networks. However, ADMM-based\noptimizers have a slow convergence rate. This paper proposes an Anderson\nAcceleration for Deep Learning ADMM (AA-DLADMM) algorithm to tackle this\ndrawback. The main intention of the AA-DLADMM algorithm is to employ Anderson\nacceleration to ADMM by considering it as a fixed-point iteration and attaining\na nearly quadratic convergence rate. We verify the effectiveness and efficiency\nof the proposed AA-DLADMM algorithm by conducting extensive experiments on four\nbenchmark datasets contrary to other state-of-the-art optimizers.\n",
        "title": "AA-DLADMM: An Accelerated ADMM-based Framework for Training Deep Neural\n  Networks",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03620",
        "abstract_url": "http://arxiv.org/abs/2401.03620",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Xingqiu"
            },
            {
                "last_name": "You",
                "first_name": "Chaoqun"
            },
            {
                "last_name": "Quek",
                "first_name": "Tony Q. S."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Collaborative Edge Computing (CEC) is a new edge computing paradigm that\nenables neighboring edge servers to share computational resources with each\nother. Although CEC can enhance the utilization of computational resources, it\nstill suffers from resource waste. The primary reason is that end-users from\nthe same area are likely to offload similar tasks to edge servers, thereby\nleading to duplicate computations. To improve system efficiency, the\ncomputation results of previously executed tasks can be cached and then reused\nby subsequent tasks. However, most existing computation reuse algorithms only\nconsider one edge server, which significantly limits the effectiveness of\ncomputation reuse. To address this issue, this paper applies computation reuse\nin CEC networks to exploit the collaboration among edge servers. We formulate\nan optimization problem that aims to minimize the overall task response time\nand decompose it into a caching subproblem and a scheduling subproblem. By\nanalyzing the properties of optimal solutions, we show that the optimal caching\ndecisions can be efficiently searched using the bisection method. For the\nscheduling subproblem, we utilize projected gradient descent and backtracking\nto find a local minimum. Numerical results show that our algorithm\nsignificantly reduces the response time in various situations.\n",
        "title": "Exploiting Storage for Computing: Computation Reuse in Collaborative\n  Edge Computing",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03621",
        "abstract_url": "http://arxiv.org/abs/2401.03621",
        "authors": [
            {
                "last_name": "Ellethy",
                "first_name": "Hanem"
            },
            {
                "last_name": "Chandra",
                "first_name": "Shekhar S."
            },
            {
                "last_name": "Vegh",
                "first_name": "Viktor"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Traumatic Brain Injury (TBI) poses a significant global public health\nchallenge, contributing to high morbidity and mortality rates and placing a\nsubstantial economic burden on healthcare systems worldwide. The diagnosis and\nprognosis of TBI relies on a combination of clinical and imaging data often\nacquired using a Computed Tomography (CT) scanner. Addressing the multifaceted\nchallenges posed by TBI requires innovative, data-driven approaches, for this\ncomplex condition. As such, we provide a summary of the state-of-the-art\nMachine Learning (ML) and Deep Learning (DL) techniques applied to clinical and\nimages in TBI, with a particular focus on mild TBI (mTBI). We explore the rich\nspectrum of ML and DL techniques used and highlight their impact in TBI . We\ncategorize ML and DL methods by TBI severity and showcase their application in\nmTBI and moderate-to-severe TBI scenarios. Finally, we emphasize the role of ML\nand DL in mTBI diagnosis, where conventional methods often fall short, and\ncomment on the potential of CT-based ML applications in TBI. This review may\nserve as a source of inspiration for future research endeavours aimed at\nimproving the diagnosis and prognosis of TBI.\n",
        "title": "Machine Learning Applications in Traumatic Brain Injury Diagnosis and\n  Prognosis: A Spotlight on Mild TBI and CT Imaging",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03629",
        "abstract_url": "http://arxiv.org/abs/2401.03629",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Hang",
                "first_name": "Peng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xiaocong"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianqiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Jian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Decision-making stands as a pivotal component in the realm of autonomous\nvehicles (AVs), playing a crucial role in navigating the intricacies of\nautonomous driving. Amidst the evolving landscape of data-driven methodologies,\nenhancing decision-making performance in complex scenarios has emerged as a\nprominent research focus. Despite considerable advancements, current\nlearning-based decision-making approaches exhibit potential for refinement,\nparticularly in aspects of policy articulation and safety assurance. To address\nthese challenges, we introduce DDM-Lag, a Diffusion Decision Model,augmented\nwith Lagrangian-based safety enhancements.In our approach, the autonomous\ndriving decision-making conundrum is conceptualized as a Constrained Markov\nDecision Process (CMDP). We have crafted an Actor-Critic framework, wherein the\ndiffusion model is employed as the actor,facilitating policy exploration and\nlearning. The integration of safety constraints in the CMDP and the adoption of\na Lagrangian relaxation-based policy optimization technique ensure enhanced\ndecision safety. A PID controller is employed for the stable updating of model\nparameters. The effectiveness of DDM-Lag is evaluated through different driving\ntasks, showcasing improvements in decision-making safety and overall\nperformance compared to baselines.\n",
        "title": "DDM-Lag : A Diffusion-based Decision-making Model for Autonomous\n  Vehicles with Lagrangian Safety Enhancement",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03630",
        "abstract_url": "http://arxiv.org/abs/2401.03630",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Weizhe"
            },
            {
                "last_name": "Koenig",
                "first_name": "Sven"
            },
            {
                "last_name": "Dilkina",
                "first_name": "Bistra"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "CL"
        ],
        "abstract": "  With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study how to solve MAPF with LLMs. We first\nshow the motivating success on an empty room map without obstacles, then the\nfailure to plan on a slightly harder room map. We present our hypothesis of why\ndirectly solving MAPF with LLMs has not been successful yet, and we use various\nexperiments to support our hypothesis.\n",
        "title": "Why Solving Multi-agent Path Finding with Large Language Model has not\n  Succeeded Yet",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03631",
        "abstract_url": "http://arxiv.org/abs/2401.03631",
        "authors": [
            {
                "last_name": "Kearns",
                "first_name": "William R."
            },
            {
                "last_name": "Bertram",
                "first_name": "Jessica"
            },
            {
                "last_name": "Divina",
                "first_name": "Myra"
            },
            {
                "last_name": "Kemp",
                "first_name": "Lauren"
            },
            {
                "last_name": "Wang",
                "first_name": "Yinzhou"
            },
            {
                "last_name": "Marin",
                "first_name": "Alex"
            },
            {
                "last_name": "Cohen",
                "first_name": "Trevor"
            },
            {
                "last_name": "Yuwen",
                "first_name": "Weichao"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL",
            "IR",
            "LG"
        ],
        "abstract": "  Despite the high prevalence and burden of mental health conditions, there is\na global shortage of mental health providers. Artificial Intelligence (AI)\nmethods have been proposed as a way to address this shortage, by supporting\nproviders with less extensive training as they deliver care. To this end, we\ndeveloped the AI-Assisted Provider Platform (A2P2), a text-based virtual\ntherapy interface that includes a response suggestion feature, which supports\nproviders in delivering protocolized therapies empathetically. We studied\nproviders with and without expertise in mental health treatment delivering a\ntherapy session using the platform with (intervention) and without (control)\nAI-assistance features. Upon evaluation, the AI-assisted system significantly\ndecreased response times by 29.34% (p=0.002), tripled empathic response\naccuracy (p=0.0001), and increased goal recommendation accuracy by 66.67%\n(p=0.001) across both user groups compared to the control. Both groups rated\nthe system as having excellent usability.\n",
        "title": "Bridging the Skills Gap: Evaluating an AI-Assisted Provider Platform to\n  Support Care Providers with Empathetic Delivery of Protocolized Therapy",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03637",
        "abstract_url": "http://arxiv.org/abs/2401.03637",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shi-Xue"
            },
            {
                "last_name": "Yang",
                "first_name": "Chun"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiaobin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Hongyang"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongfa"
            },
            {
                "last_name": "Yin",
                "first_name": "Xu-Cheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Scene text spotting is a challenging task, especially for inverse-like scene\ntext, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed.\nIn this paper, we propose a unified end-to-end trainable inverse-like\nantagonistic text spotting framework dubbed IATS, which can effectively spot\ninverse-like scene texts without sacrificing general ones. Specifically, we\npropose an innovative reading-order estimation module (REM) that extracts\nreading-order information from the initial text boundary generated by an\ninitial boundary module (IBM). To optimize and train REM, we propose a joint\nreading-order estimation loss consisting of a classification loss, an\northogonality loss, and a distribution loss. With the help of IBM, we can\ndivide the initial text boundary into two symmetric control points and\niteratively refine the new text boundary using a lightweight boundary\nrefinement module (BRM) for adapting to various shapes and scales. To alleviate\nthe incompatibility between text detection and recognition, we propose a\ndynamic sampling module (DSM) with a thin-plate spline that can dynamically\nsample appropriate features for recognition in the detected text region.\nWithout extra supervision, the DSM can proactively learn to sample appropriate\nfeatures for text recognition through the gradient returned by the recognition\nmodule. Extensive experiments on both challenging scene text and inverse-like\nscene text datasets demonstrate that our method achieves superior performance\nboth on irregular and inverse-like text spotting.\n",
        "title": "Inverse-like Antagonistic Scene Text Spotting via Reading-Order\n  Estimation and Dynamic Sampling",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03638",
        "abstract_url": "http://arxiv.org/abs/2401.03638",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Ziyan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bo"
            },
            {
                "last_name": "Tang",
                "first_name": "Jin"
            },
            {
                "last_name": "Luo",
                "first_name": "Bin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Graph contrastive learning is usually performed by first conducting Graph\nData Augmentation (GDA) and then employing a contrastive learning pipeline to\ntrain GNNs. As we know that GDA is an important issue for graph contrastive\nlearning. Various GDAs have been developed recently which mainly involve\ndropping or perturbing edges, nodes, node attributes and edge attributes.\nHowever, to our knowledge, it still lacks a universal and effective augmentor\nthat is suitable for different types of graph data. To address this issue, in\nthis paper, we first introduce the graph message representation of graph data.\nBased on it, we then propose a novel Graph Message Augmentation (GMA), a\nuniversal scheme for reformulating many existing GDAs. The proposed unified GMA\nnot only gives a new perspective to understand many existing GDAs but also\nprovides a universal and more effective graph data augmentation for graph\nself-supervised learning tasks. Moreover, GMA introduces an easy way to\nimplement the mixup augmentor which is natural for images but usually\nchallengeable for graphs. Based on the proposed GMA, we then propose a unified\ngraph contrastive learning, termed Graph Message Contrastive Learning (GMCL),\nthat employs attribution-guided universal GMA for graph contrastive learning.\nExperiments on many graph learning tasks demonstrate the effectiveness and\nbenefits of the proposed GMA and GMCL approaches.\n",
        "title": "Unifying Graph Contrastive Learning via Graph Message Augmentation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03639",
        "abstract_url": "http://arxiv.org/abs/2401.03639",
        "authors": [
            {
                "last_name": "Beech",
                "first_name": "Peter"
            },
            {
                "last_name": "Jia",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhaofei"
            },
            {
                "last_name": "Liu",
                "first_name": "Jian K."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV",
            "LG"
        ],
        "abstract": "  The visual pathway involves complex networks of cells and regions which\ncontribute to the encoding and processing of visual information. While some\naspects of visual perception are understood, there are still many unanswered\nquestions regarding the exact mechanisms of visual encoding and the\norganization of visual information along the pathway. This chapter discusses\nthe importance of visual perception and the challenges associated with\nunderstanding how visual information is encoded and represented in the brain.\nFurthermore, this chapter introduces the concept of neuroprostheses: devices\ndesigned to enhance or replace bodily functions, and highlights the importance\nof constructing computational models of the visual pathway in the\nimplementation of such devices. A number of such models, employing the use of\ndeep learning models, are outlined, and their value to understanding visual\ncoding and natural vision is discussed.\n",
        "title": "Deep Learning for Visual Neuroprosthesis",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03641",
        "abstract_url": "http://arxiv.org/abs/2401.03641",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Wencheng"
            },
            {
                "last_name": "Guo",
                "first_name": "Dongqian"
            },
            {
                "last_name": "Xu",
                "first_name": "Cheng-Zhong"
            },
            {
                "last_name": "Shen",
                "first_name": "Jianbing"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  In the field of autonomous driving, two important features of autonomous\ndriving car systems are the explainability of decision logic and the accuracy\nof environmental perception. This paper introduces DME-Driver, a new autonomous\ndriving system that enhances the performance and reliability of autonomous\ndriving system. DME-Driver utilizes a powerful vision language model as the\ndecision-maker and a planning-oriented perception model as the control signal\ngenerator. To ensure explainable and reliable driving decisions, the logical\ndecision-maker is constructed based on a large vision language model. This\nmodel follows the logic employed by experienced human drivers and makes\ndecisions in a similar manner. On the other hand, the generation of accurate\ncontrol signals relies on precise and detailed environmental perception, which\nis where 3D scene perception models excel. Therefore, a planning oriented\nperception model is employed as the signal generator. It translates the logical\ndecisions made by the decision-maker into accurate control signals for the\nself-driving cars. To effectively train the proposed model, a new dataset for\nautonomous driving was created. This dataset encompasses a diverse range of\nhuman driver behaviors and their underlying motivations. By leveraging this\ndataset, our model achieves high-precision planning accuracy through a logical\nthinking process.\n",
        "title": "DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in\n  Autonomous Driving",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03642",
        "abstract_url": "http://arxiv.org/abs/2401.03642",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haining"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "DL"
        ],
        "abstract": "  Novelty, akin to gene mutation in evolution, opens possibilities for\nscientific advancement. Despite peer review being the gold standard for\nevaluating novelty in scholarly communication and resource allocation, the vast\nvolume of submissions necessitates an automated measure of scientific novelty.\nAdopting a perspective that views novelty as the atypical combination of\nexisting knowledge, we introduce an information-theoretic measure of novelty in\nscholarly publications. This measure is quantified by the degree of `surprise'\nperceived by a language model that represents the distribution of scientific\ndiscourse. The proposed measure is accompanied by face and construct validity\nevidence; the former demonstrates correspondence to scientific common sense,\nand the latter is endorsed through alignments with novelty evaluations from a\nselect panel of domain experts. Additionally, characterized by its\ninterpretability, fine granularity, and accessibility, this measure addresses\ngaps prevalent in existing methods. We believe this measure holds great\npotential to benefit editors, stakeholders, and policymakers, and it provides a\nconfident lens for examining the relationship between novelty and scientific\ndynamics such as creativity, interdisciplinarity, scientific advances, and\nmore.\n",
        "title": "A Content-Based Novelty Measure for Scholarly Publications: A Proof of\n  Concept",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03643",
        "abstract_url": "http://arxiv.org/abs/2401.03643",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Lin"
            },
            {
                "last_name": "Wang",
                "first_name": "Fajie"
            },
            {
                "last_name": "Qu",
                "first_name": "Wenzhen"
            },
            {
                "last_name": "Gu",
                "first_name": "Yan"
            },
            {
                "last_name": "Qin",
                "first_name": "Qing-Hua"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper proposes a novel neural network framework, denoted as spectral\nintegrated neural networks (SINNs), for resolving three-dimensional forward and\ninverse dynamic problems. In the SINNs, the spectral integration method is\napplied to perform temporal discretization, and then a fully connected neural\nnetwork is adopted to solve resulting partial differential equations (PDEs) in\nthe spatial domain. Specifically, spatial coordinates are employed as inputs in\nthe network architecture, and the output layer is configured with multiple\noutputs, each dedicated to approximating solutions at different time instances\ncharacterized by Gaussian points used in the spectral method. By leveraging the\nautomatic differentiation technique and spectral integration scheme, the SINNs\nminimize the loss function, constructed based on the governing PDEs and\nboundary conditions, to obtain solutions for dynamic problems. Additionally, we\nutilize polynomial basis functions to expand the unknown function, aiming to\nenhance the performance of SINNs in addressing inverse problems. The conceived\nframework is tested on six forward and inverse dynamic problems, involving\nnonlinear PDEs. Numerical results demonstrate the superior performance of SINNs\nover the popularly used physics-informed neural networks in terms of\nconvergence speed, computational accuracy and efficiency. It is also noteworthy\nthat the SINNs exhibit the capability to deliver accurate and stable solutions\nfor long-time dynamic problems.\n",
        "title": "Spectral integrated neural networks (SINNs) for solving forward and\n  inverse dynamic problems",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03646",
        "abstract_url": "http://arxiv.org/abs/2401.03646",
        "authors": [
            {
                "last_name": "Nainani",
                "first_name": "Jatin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE",
            ""
        ],
        "abstract": "  Large Language Models (LLMs) have experienced a rapid rise in AI, changing a\nwide range of applications with their advanced capabilities. As these models\nbecome increasingly integral to decision-making, the need for thorough\ninterpretability has never been more critical. Mechanistic Interpretability\noffers a pathway to this understanding by identifying and analyzing specific\nsub-networks or 'circuits' within these complex systems. A crucial aspect of\nthis approach is Automated Circuit Discovery, which facilitates the study of\nlarge models like GPT4 or LLAMA in a feasible manner. In this context, our\nresearch evaluates a recent method, Brain-Inspired Modular Training (BIMT),\ndesigned to enhance the interpretability of neural networks. We demonstrate how\nBIMT significantly improves the efficiency and quality of Automated Circuit\nDiscovery, overcoming the limitations of manual methods. Our comparative\nanalysis further reveals that BIMT outperforms existing models in terms of\ncircuit quality, discovery time, and sparsity. Additionally, we provide a\ncomprehensive computational analysis of BIMT, including aspects such as\ntraining duration, memory allocation requirements, and inference speed. This\nstudy advances the larger objective of creating trustworthy and transparent AI\nsystems in addition to demonstrating how well BIMT works to make neural\nnetworks easier to understand.\n",
        "title": "Evaluating Brain-Inspired Modular Training in Automated Circuit\n  Discovery for Mechanistic Interpretability",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03648",
        "abstract_url": "http://arxiv.org/abs/2401.03648",
        "authors": [
            {
                "last_name": "Bi",
                "first_name": "Keping"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiaojie"
            },
            {
                "last_name": "Guo",
                "first_name": "Jiafeng"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xueqi"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Multi-aspect dense retrieval aims to incorporate aspect information (e.g.,\nbrand and category) into dual encoders to facilitate relevance matching. As an\nearly and representative multi-aspect dense retriever, MADRAL learns several\nextra aspect embeddings and fuses the explicit aspects with an implicit aspect\n\"OTHER\" for final representation. MADRAL was evaluated on proprietary data and\nits code was not released, making it challenging to validate its effectiveness\non other datasets. We failed to reproduce its effectiveness on the public\nMA-Amazon data, motivating us to probe the reasons and re-examine its\ncomponents. We propose several component alternatives for comparisons,\nincluding replacing \"OTHER\" with \"CLS\" and representing aspects with the first\nseveral content tokens. Through extensive experiments, we confirm that learning\n\"OTHER\" from scratch in aspect fusion is harmful. In contrast, our proposed\nvariants can greatly enhance the retrieval performance. Our research not only\nsheds light on the limitations of MADRAL but also provides valuable insights\nfor future studies on more powerful multi-aspect dense retrieval models. Code\nwill be released at:\nhttps://github.com/sunxiaojie99/Reproducibility-for-MADRAL.\n",
        "title": "Reproducibility Analysis and Enhancements for Multi-Aspect Dense\n  Retriever with Aspect Learning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03650",
        "abstract_url": "http://arxiv.org/abs/2401.03650",
        "authors": [
            {
                "last_name": "Yi",
                "first_name": "Jayeon"
            },
            {
                "last_name": "Koo",
                "first_name": "Junghyun"
            },
            {
                "last_name": "Lee",
                "first_name": "Kyogu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD",
            ""
        ],
        "abstract": "  Clipping is a common nonlinear distortion that occurs whenever the input or\noutput of an audio system exceeds the supported range. This phenomenon\nundermines not only the perception of speech quality but also downstream\nprocesses utilizing the disrupted signal. Therefore, a real-time-capable,\nrobust, and low-response-time method for speech declipping (SD) is desired. In\nthis work, we introduce DDD (Demucs-Discriminator-Declipper), a\nreal-time-capable speech-declipping deep neural network (DNN) that requires\nless response time by design. We first observe that a previously untested\nreal-time-capable DNN model, Demucs, exhibits a reasonable declipping\nperformance. Then we utilize adversarial learning objectives to increase the\nperceptual quality of output speech without additional inference overhead.\nSubjective evaluations on harshly clipped speech shows that DDD outperforms the\nbaselines by a wide margin in terms of speech quality. We perform detailed\nwaveform and spectral analyses to gain an insight into the output behavior of\nDDD in comparison to the baselines. Finally, our streaming simulations also\nshow that DDD is capable of sub-decisecond mean response times, outperforming\nthe state-of-the-art DNN approach by a factor of six.\n",
        "title": "DDD: A Perceptually Superior Low-Response-Time DNN-based Declipper",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03652",
        "abstract_url": "http://arxiv.org/abs/2401.03652",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Shaoxuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guofeng"
            },
            {
                "last_name": "Jard\u00f3n-Kojakhmetov",
                "first_name": "Hildeberto"
            },
            {
                "last_name": "Cao",
                "first_name": "Ming"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In graph-theoretical terms, an edge in a graph connects two vertices while a\nhyperedge of a hypergraph connects any more than one vertices. If the\nhypergraph's hyperedges further connect the same number of vertices, it is said\nto be uniform. In algebraic graph theory, a graph can be characterized by an\nadjacency matrix, and similarly, a uniform hypergraph can be characterized by\nan adjacency tensor. This similarity enables us to extend existing tools of\nmatrix analysis for studying dynamical systems evolving on graphs to the study\nof a class of polynomial dynamical systems evolving on hypergraphs utilizing\nthe properties of tensors. To be more precise, in this paper, we first extend\nthe concept of a Metzler matrix to a Metzler tensor and then describe some\nuseful properties of such tensors. Next, we focus on positive systems on\nhypergraphs associated with Metzler tensors. More importantly, we design\ncontrol laws to stabilize the origin of this class of Metzler positive systems\non hypergraphs. In the end, we apply our findings to two classic dynamical\nsystems: a higher-order Lotka-Volterra population dynamics system and a\nhigher-order SIS epidemic dynamic process. The corresponding novel stability\nresults are accompanied by ample numerical examples.\n",
        "title": "On Metzler positive systems on hypergraphs",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03659",
        "abstract_url": "http://arxiv.org/abs/2401.03659",
        "authors": [
            {
                "last_name": "Xiang",
                "first_name": "Shuhuang"
            },
            {
                "last_name": "Yang",
                "first_name": "Shunfeng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper builds further rigorous analysis on the root-exponential\nconvergence for lightning schemes approximating corner singularity problems. By\nutilizing Poisson summation formula, Runge's approximation theorem and Cauchy's\nintegral theorem, the optimal rate is obtained for efficient lightning plus\npolynomial schemes, newly developed by Herremans, Huybrechs and Trefethen\n\\cite{Herremans2023}, for approximation of $g(z)z^\\alpha$ or $g(z)z^\\alpha\\log\nz$ in a sector-shaped domain with tapered exponentially clustering poles, where\n$g(z)$ is analytic on the sector domain. From these results, Conjecture 5.3 in\n\\cite{Herremans2023} on the root-exponential convergence rate is confirmed and\nthe choice of the parameter\n$\\sigma_{opt}=\\frac{\\sqrt{2(2-\\beta)}\\pi}{\\sqrt{\\alpha}}$ may achieve the\nfastest convergence rate among all $\\sigma>0$. Furthermore, based on Lehman and\nWasow's study of corner singularities \\cite{Lehman1954DevelopmentsIT, Wasow},\ntogether with the decomposition of Gopal and Trefethen \\cite{Gopal2019},\nroot-exponential rates for lightning plus polynomial schemes in corner domains\n$\\Omega$ are validated, and the best choice of lightning clustering parameter\n$\\sigma$ for $\\Omega$ is also obtained explicitly. The thorough analysis\nprovides a solid foundation for lightning schemes.\n",
        "title": "The root-exponential convergence of lightning plus polynomial\n  approximation on corner domains",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03661",
        "abstract_url": "http://arxiv.org/abs/2401.03661",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Yigong"
            },
            {
                "last_name": "DeWitt",
                "first_name": "Stephen"
            },
            {
                "last_name": "Radhakrishnan",
                "first_name": "Balasubramanian"
            },
            {
                "last_name": "Biros",
                "first_name": "George"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  We propose GrainGNN, a surrogate model for polycrystalline grain formation\nunder rapid solidification conditions in metal additive manufacturing. Such\ngrain formation problems are modeled by a multicomponent partial differential\nequation PDE with moving interfaces. The inherent randomness of the PDE initial\nconditions (grain seeds) necessitates ensemble simulations to predict\nmicrostructure statistics, e.g., grain size, aspect ratio, and crystallographic\norientation. Currently such ensemble simulations are prohibitively expensive\nand surrogates are necessary.\n  In GrainGNN, we use a dynamic graph to represent interface motion and\ntopological changes due to grain coarsening. We use a reduced representation of\nthe microstructure using hand-crafted features; we combine pattern finding and\naltering graph algorithms with two neural networks, a classifier (for\ntopological changes) and a regressor (for interface motion). Both networks have\nan encoder-decoder architecture; the encoder has a multi-layer transformer\nlong-short-term-memory architecture; the decoder is a single layer perceptron.\n  We evaluate GrainGNN by comparing it to high-fidelity phase field simulations\nfor in-distribution and out-of-distribution grain configurations for\nsolidification under laser power bed fusion conditions. GrainGNN results in\n80\\%--90\\% pointwise accuracy; and nearly identical distributions of scalar\nquantities of interest (QoI) between phase field and GrainGNN simulations\ncompared using Kolmogorov-Smirnov test. GrainGNN's inference speedup (PyTorch\non single x86 node) over a high-fidelity phase-field simulation (CUDA on a\nsingle NVIDIA A100 GPU) is 300$\\times$--2000$\\times$ for 100-initial grain\nproblem. Further, using GrainGNN, we model the formation of 11,600 grains in\n220 seconds on a single CPU core.\n",
        "title": "GrainGNN: A dynamic graph neural network for predicting 3D grain\n  microstructure",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03664",
        "abstract_url": "http://arxiv.org/abs/2401.03664",
        "authors": [
            {
                "last_name": "Lei",
                "first_name": "Shuge"
            },
            {
                "last_name": "Hu",
                "first_name": "Haonan"
            },
            {
                "last_name": "Sun",
                "first_name": "Dasheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Huabin"
            },
            {
                "last_name": "Yuan",
                "first_name": "Kehong"
            },
            {
                "last_name": "Dai",
                "first_name": "Jian"
            },
            {
                "last_name": "Tang",
                "first_name": "Jijun"
            },
            {
                "last_name": "Tong",
                "first_name": "Yan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  This paper focuses on the classification task of breast ultrasound images and\nresearches on the reliability measurement of classification results. We\nproposed a dual-channel evaluation framework based on the proposed inference\nreliability and predictive reliability scores. For the inference reliability\nevaluation, human-aligned and doctor-agreed inference rationales based on the\nimproved feature attribution algorithm SP-RISA are gracefully applied.\nUncertainty quantification is used to evaluate the predictive reliability via\nthe Test Time Enhancement. The effectiveness of this reliability evaluation\nframework has been verified on our breast ultrasound clinical dataset YBUS, and\nits robustness is verified on the public dataset BUSI. The expected calibration\nerrors on both datasets are significantly lower than traditional evaluation\nmethods, which proves the effectiveness of our proposed reliability\nmeasurement.\n",
        "title": "Dual-Channel Reliable Breast Ultrasound Image Classification Based on\n  Explainable Attribution and Uncertainty Quantification",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03665",
        "abstract_url": "http://arxiv.org/abs/2401.03665",
        "authors": [
            {
                "last_name": "Tadokoro",
                "first_name": "Ryu"
            },
            {
                "last_name": "Yamada",
                "first_name": "Ryosuke"
            },
            {
                "last_name": "Nakashima",
                "first_name": "Kodai"
            },
            {
                "last_name": "Nakamura",
                "first_name": "Ryo"
            },
            {
                "last_name": "Kataoka",
                "first_name": "Hirokatsu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The construction of 3D medical image datasets presents several issues,\nincluding requiring significant financial costs in data collection and\nspecialized expertise for annotation, as well as strict privacy concerns for\npatient confidentiality compared to natural image datasets. Therefore, it has\nbecome a pressing issue in 3D medical image segmentation to enable\ndata-efficient learning with limited 3D medical data and supervision. A\npromising approach is pre-training, but improving its performance in 3D medical\nimage segmentation is difficult due to the small size of existing 3D medical\nimage datasets. We thus present the Primitive Geometry Segment Pre-training\n(PrimGeoSeg) method to enable the learning of 3D semantic features by\npre-training segmentation tasks using only primitive geometric objects for 3D\nmedical image segmentation. PrimGeoSeg performs more accurate and efficient 3D\nmedical image segmentation without manual data collection and annotation.\nFurther, experimental results show that PrimGeoSeg on SwinUNETR improves\nperformance over learning from scratch on BTCV, MSD (Task06), and BraTS\ndatasets by 3.7%, 4.4%, and 0.3%, respectively. Remarkably, the performance was\nequal to or better than state-of-the-art self-supervised learning despite the\nequal number of pre-training data. From experimental results, we conclude that\neffective pre-training can be achieved by looking at primitive geometric\nobjects only. Code and dataset are available at\nhttps://github.com/SUPER-TADORY/PrimGeoSeg.\n",
        "title": "Primitive Geometry Segment Pre-training for 3D Medical Image\n  Segmentation",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03668",
        "abstract_url": "http://arxiv.org/abs/2401.03668",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Tingzhou"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  We explore Langevin dynamics in the spherical Sherrington-Kirkpatrick model,\ndelving into the asymptotic energy limit. Our approach involves\nintegro-differential equations, incorporating the\nCrisanti-Horner-Sommers-Cugliandolo-Kurchan equation from spin glass\nliterature, to analyze the system's size and its temperature-dependent phase\ntransition. Additionally, we conduct an average case complexity analysis,\nestablishing hitting time bounds for the bottom eigenvector of a Wigner matrix.\nOur investigation also includes the power iteration algorithm, examining its\naverage case complexity in identifying the top eigenvector overlap, with\ncomprehensive complexity bounds.\n",
        "title": "Analyzing dynamics and average case complexity in the spherical\n  Sherrington-Kirkpatrick model: a focus on extreme eigenvectors",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03671",
        "abstract_url": "http://arxiv.org/abs/2401.03671",
        "authors": [
            {
                "last_name": "Arieli",
                "first_name": "Itai"
            },
            {
                "last_name": "Geffner",
                "first_name": "Ivan"
            },
            {
                "last_name": "Tennenholtz",
                "first_name": "Moshe"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "DS",
            ""
        ],
        "abstract": "  This paper considers the dynamics of cheap talk interactions between a sender\nand receiver, departing from conventional models by focusing on the receiver's\nperspective. We study two models, one with transparent motives and another one\nin which the receiver can \\emph{filter} the information that is accessible by\nthe sender. We give a geometric characterization of the best receiver\nequilibrium under transparent motives and prove that the receiver does not\nbenefit from filtering information in this case. However, in general, we show\nthat the receiver can strictly benefit from filtering and provide efficient\nalgorithms for computing optimal equilibria. This innovative analysis aligns\nwith user-based platforms where receivers (users) control information\naccessible to senders (sellers). Our findings provide insights into\ncommunication dynamics, leveling the sender's inherent advantage, and offering\nstrategic interaction predictions.\n",
        "title": "Receiver-Oriented Cheap Talk Design",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03672",
        "abstract_url": "http://arxiv.org/abs/2401.03672",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Yachen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Lina"
            },
            {
                "last_name": "Zheng",
                "first_name": "Haibiao"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper explores the application of the multiscale finite element method\n(MsFEM) to address steady-state Stokes-Darcy problems with BJS interface\nconditions in highly heterogeneous porous media. We assume the existence of\nmultiscale features in the Darcy region and propose an algorithm for the\nmultiscale Stokes-Darcy model. During the offline phase, we employ MsFEM to\nconstruct permeability-dependent offline bases for efficient coarse-grid\nsimulation, with this process conducted in parallel to enhance its efficiency.\nIn the online phase, we use the Robin-Robin algorithm to derive the model's\nsolution. Subsequently, we conduct error analysis based on $L^2$ and $H^1$\nnorms, assuming certain periodic coefficients in the Darcy region. To validate\nour approach, we present extensive numerical tests on highly heterogeneous\nmedia, illustrating the results of the error analysis.\n",
        "title": "Multiscale finite element method for Stokes-Darcy model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03673",
        "abstract_url": "http://arxiv.org/abs/2401.03673",
        "authors": [
            {
                "last_name": "Jiao",
                "first_name": "Xinshan"
            },
            {
                "last_name": "Wan",
                "first_name": "Shuyan"
            },
            {
                "last_name": "Liu",
                "first_name": "Qian"
            },
            {
                "last_name": "Bi",
                "first_name": "Yilin"
            },
            {
                "last_name": "Lee",
                "first_name": "Yan-Li"
            },
            {
                "last_name": "Xu",
                "first_name": "En"
            },
            {
                "last_name": "Hao",
                "first_name": "Dong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Tao"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            ""
        ],
        "abstract": "  Link prediction aims to predict the potential existence of links between two\nunconnected nodes within a network based on the known topological\ncharacteristics. Evaluation metrics are used to assess the effectiveness of\nalgorithms in link prediction. The discriminating ability of these evaluation\nmetrics is vitally important for accurately evaluating link prediction\nalgorithms. In this study, we propose an artificial network model, based on\nwhich one can adjust a single parameter to monotonically and continuously turn\nthe prediction accuracy of the specifically designed link prediction algorithm.\nBuilding upon this foundation, we show a framework to depict the effectiveness\nof evaluating metrics by focusing on their discriminating ability.\nSpecifically, a quantitative comparison in the abilities of correctly\ndiscerning varying prediction accuracies was conducted encompassing nine\nevaluation metrics: Precision, Recall, F1-Measure, Matthews Correlation\nCoefficient (MCC), Balanced Precision (BP), the Area Under the receiver\noperating characteristic Curve (AUC), the Area Under the Precision-Recall curve\n(AUPR), Normalized Discounted Cumulative Gain (NDCG), and the Area Under the\nmagnified ROC (AUC-mROC). The results indicate that the discriminating\nabilities of the three metrics, AUC, AUPR, and NDCG, are significantly higher\nthan those of other metrics.\n",
        "title": "Comparing discriminating abilities of evaluation metrics in link\n  prediction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03675",
        "abstract_url": "http://arxiv.org/abs/2401.03675",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Haena"
            },
            {
                "last_name": "Kim",
                "first_name": "Yejun"
            },
            {
                "last_name": "Kim",
                "first_name": "Seungjoo"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Recently, the usage of cloud services has been increasing annually, and with\nremote work becoming one of the new forms of employment within enterprises, the\nsecurity of cloud-based remote work environments has become important. The\nexisting work environment relies on a perimeter security model, where accessing\none's resources is based on the assumption that everything within the internal\nnetwork is secure. However, due to the limitations of the perimeter security\nmodel, which assumes the safety of everything within the internal network, the\nadoption of Zero Trust is now being demanded. Accordingly, NIST and DoD have\npublished guidelines related to Zero Trust architecture. However, these\nguidelines describe security requirements at an abstract level, focusing on\nlogical architecture. In this paper, we conduct a threat modeling for OpenStack\ncloud to propose more detailed security requirements compared to NIST and DoD\nguidelines. Subsequently, we perform a security analysis of commercial cloud\nservices such as Microsoft Azure, Amazon Web Service, and Google Cloud to\nvalidate these requirements. The security analysis results identify security\nrequirements that each cloud service fails to satisfy, indicating potential\nexposure to threats. This paper proposes detailed security requirements based\non the Zero Trust model and conducts security analyses of various cloud\nservices accordingly. As a result of the security analysis, we proposed\npotential threats and countermeasures for cloud services with Zero Trust, and\nthis is intended to help build a secure Zero Trust-based remote work\nenvironment.\n",
        "title": "A Study on the Security Requirements Analysis to build a Zero\n  Trust-based Remote Work Environment",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03676",
        "abstract_url": "http://arxiv.org/abs/2401.03676",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Wei Hung"
            },
            {
                "last_name": "Chok",
                "first_name": "Ming Jie"
            },
            {
                "last_name": "Wong",
                "first_name": "Jonathan Leong Shan"
            },
            {
                "last_name": "Shin",
                "first_name": "Yung Xin"
            },
            {
                "last_name": "Poon",
                "first_name": "Yeong Shian"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhou"
            },
            {
                "last_name": "Chong",
                "first_name": "Chun Yong"
            },
            {
                "last_name": "Lo",
                "first_name": "David"
            },
            {
                "last_name": "Lim",
                "first_name": "Mei Kuan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  Educators are increasingly concerned about the usage of Large Language Models\n(LLMs) such as ChatGPT in programming education, particularly regarding the\npotential exploitation of imperfections in Artificial Intelligence Generated\nContent (AIGC) Detectors for academic misconduct. In this paper, we present an\nempirical study where the LLM is examined for its attempts to bypass detection\nby AIGC Detectors. This is achieved by generating code in response to a given\nquestion using different variants. We collected a dataset comprising 5,069\nsamples, with each sample consisting of a textual description of a coding\nproblem and its corresponding human-written Python solution codes. These\nsamples were obtained from various sources, including 80 from Quescol, 3,264\nfrom Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of\ncode problem variant prompts, which were used to instruct ChatGPT to generate\nthe outputs. Subsequently, we assessed the performance of five AIGC detectors.\nOur results demonstrate that existing AIGC Detectors perform poorly in\ndistinguishing between human-written code and AI-generated code.\n",
        "title": "Assessing AI Detectors in Identifying AI-Generated Code: Implications\n  for Education",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03677",
        "abstract_url": "http://arxiv.org/abs/2401.03677",
        "authors": [
            {
                "last_name": "Vaidya",
                "first_name": "Aatman"
            },
            {
                "last_name": "Arora",
                "first_name": "Arnav"
            },
            {
                "last_name": "Joshi",
                "first_name": "Aditya"
            },
            {
                "last_name": "Prabhakar",
                "first_name": "Tarunima"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  This paper reports the findings of the ICON 2023 on Gendered Abuse Detection\nin Indic Languages. The shared task deals with the detection of gendered abuse\nin online text. The shared task was conducted as a part of ICON 2023, based on\na novel dataset in Hindi, Tamil and the Indian dialect of English. The\nparticipants were given three subtasks with the train dataset consisting of\napproximately 6500 posts sourced from Twitter. For the test set, approximately\n1200 posts were provided. The shared task received a total of 9 registrations.\nThe best F-1 scores are 0.616 for subtask 1, 0.572 for subtask 2 and, 0.616 and\n0.582 for subtask 3. The paper contains examples of hateful content owing to\nits topic.\n",
        "title": "Overview of the 2023 ICON Shared Task on Gendered Abuse Detection in\n  Indic Languages",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03680",
        "abstract_url": "http://arxiv.org/abs/2401.03680",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Haipeng"
            },
            {
                "last_name": "Li",
                "first_name": "Ran"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Better forecasts may not lead to better decision-making. To address this\nchallenge, decision-oriented learning (DOL) has been proposed as a new branch\nof machine learning that replaces traditional statistical loss with a decision\nloss to form an end-to-end model. Applications of DOL in power systems have\nbeen developed in recent years. For renewable-rich power systems, uncertainties\npropagate through sequential tasks, where traditional statistical-based\napproaches focus on minimizing statistical errors at intermediate stages but\nmay fail to provide optimal decisions at the final stage. This paper first\nelaborates on the mismatch between more accurate forecasts and more optimal\ndecisions in the power system caused by statistical-based learning (SBL) and\nexplains how DOL resolves this problem. Secondly, this paper extensively\nreviews DOL techniques and their applications in power systems while\nhighlighting their pros and cons in relation to SBL. Finally, this paper\nidentifies the challenges to adopt DOL in the energy sector and presents future\nresearch directions.\n",
        "title": "Decision-Oriented Learning for Future Power System Decision-Making under\n  Uncertainty",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03685",
        "abstract_url": "http://arxiv.org/abs/2401.03685",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Bo"
            },
            {
                "last_name": "Wen",
                "first_name": "Tian"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuwei"
            },
            {
                "last_name": "Sun",
                "first_name": "Sheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated Distillation (FD) is a novel and promising distributed machine\nlearning paradigm, where knowledge distillation is leveraged to facilitate a\nmore efficient and flexible cross-device knowledge transfer in federated\nlearning. By optimizing local models with knowledge distillation, FD\ncircumvents the necessity of uploading large-scale model parameters to the\ncentral server, simultaneously preserving the raw data on local clients.\nDespite the growing popularity of FD, there is a noticeable gap in previous\nworks concerning the exploration of poisoning attacks within this framework.\nThis can lead to a scant understanding of the vulnerabilities to potential\nadversarial actions. To this end, we introduce FDLA, a poisoning attack method\ntailored for FD. FDLA manipulates logit communications in FD, aiming to\nsignificantly degrade model performance on clients through misleading the\ndiscrimination of private samples. Through extensive simulation experiments\nacross a variety of datasets, attack scenarios, and FD configurations, we\ndemonstrate that LPA effectively compromises client model accuracy,\noutperforming established baseline algorithms in this regard. Our findings\nunderscore the critical need for robust defense mechanisms in FD settings to\nmitigate such adversarial threats.\n",
        "title": "Logits Poisoning Attack in Federated Distillation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03687",
        "abstract_url": "http://arxiv.org/abs/2401.03687",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Sun",
                "first_name": "Jiayao"
            },
            {
                "last_name": "Xia",
                "first_name": "Xianjun"
            },
            {
                "last_name": "Huang",
                "first_name": "Chuanzeng"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yijian"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Packet loss is a common and unavoidable problem in voice over internet phone\n(VoIP) systems. To deal with the problem, we propose a band-split packet loss\nconcealment network (BS-PLCNet). Specifically, we split the full-band signal\ninto wide-band (0-8kHz) and high-band (8-24kHz). The wide-band signals are\nprocessed by a gated convolutional recurrent network (GCRN), while the\nhigh-band counterpart is processed by a simple GRU network. To ensure high\nspeech quality and automatic speech recognition (ASR) compatibility, multi-task\nlearning (MTL) framework including fundamental frequency (f0) prediction,\nlinguistic awareness, and multi-discriminators are used. The proposed approach\ntied for 1st place in the ICASSP 2024 PLC Challenge.\n",
        "title": "BS-PLCNet: Band-split Packet Loss Concealment Network with Multi-task\n  Learning Framework and Multi-discriminators",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03689",
        "abstract_url": "http://arxiv.org/abs/2401.03689",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Wei"
            },
            {
                "last_name": "Hou",
                "first_name": "Jingyong"
            },
            {
                "last_name": "Yang",
                "first_name": "Dong"
            },
            {
                "last_name": "Cao",
                "first_name": "Muyong"
            },
            {
                "last_name": "Lee",
                "first_name": "Tan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Many factors have separately shown their effectiveness on improving\nmultilingual ASR. They include language identity (LID) and phoneme information,\nlanguage-specific processing modules and cross-lingual self-supervised speech\nrepresentation, etc. However, few studies work on synergistically combining\nthem to contribute a unified solution, which still remains an open question. To\nthis end, a novel view to incorporate hierarchical information path LUPET into\nmultilingual ASR is proposed. The LUPET is a path encoding multiple information\nin different granularity from shallow to deep encoder layers. Early information\nin this path is beneficial for deriving later occurred information.\nSpecifically, the input goes from LID prediction to acoustic unit discovery\nfollowed by phoneme sharing, and then dynamically routed by mixture-of-expert\nfor final token recognition. Experiments on 10 languages of Common Voice\nexamined the superior performance of LUPET. Importantly, LUPET significantly\nboosts the recognition on high-resource languages, thus mitigating the\ncompromised phenomenon towards low-resource languages in a multilingual\nsetting.\n",
        "title": "LUPET: Incorporating Hierarchical Information Path into Multilingual ASR",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03692",
        "abstract_url": "http://arxiv.org/abs/2401.03692",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Ye",
                "first_name": "Tinghan"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Van Hentenryck",
                "first_name": "Pascal"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Optimizing service schedules is pivotal to the reliable, efficient, and\ninclusive on-demand mobility. This pressing challenge is further exacerbated by\nthe increasing needs of an aging population, the over-subscription of existing\nservices, and the lack of effective solution methods. This study addresses the\nintricacies of service scheduling, by jointly optimizing rider trip planning\nand crew scheduling for a complex dynamic mobility service. The resulting\noptimization problems are extremely challenging computationally for\nstate-of-the-art methods. To address this fundamental gap, this paper\nintroduces the Joint Rider Trip Planning and Crew Shift Scheduling Problem\n(JRTPCSSP) and a novel solution method, called AGGNNI-CG (Attention and Gated\nGNN- Informed Column Generation), that hybridizes column generation and machine\nlearning to obtain near-optimal solutions to the JRTPCSSP with the real-time\nconstraints of the application. The key idea of the machine-learning component\nis to dramatically reduce the number of paths to explore in the pricing\ncomponent, accelerating the most time-consuming component of the column\ngeneration. The machine learning component is a graph neural network with an\nattention mechanism and a gated architecture, that is particularly suited to\ncater for the different input sizes coming from daily operations. AGGNNI-CG has\nbeen applied to a challenging, real-world dataset from the Paratransit system\nof Chatham County in Georgia. It produces dramatic improvements compared to the\nbaseline column generation approach, which typically cannot produce feasible\nsolutions in reasonable time on both medium-sized and large-scale complex\ninstances. AGGNNI-CG also produces significant improvements in service compared\nto the existing system.\n",
        "title": "Boosting Column Generation with Graph Neural Networks for Joint Rider\n  Trip Planning and Crew Shift Scheduling",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03694",
        "abstract_url": "http://arxiv.org/abs/2401.03694",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Han"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanjie"
            },
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Huang",
                "first_name": "Can"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Video Text Spotting (VTS) is a fundamental visual task that aims to predict\nthe trajectories and content of texts in a video. Previous works usually\nconduct local associations and apply IoU-based distance and complex\npost-processing procedures to boost performance, ignoring the abundant temporal\ninformation and the morphological characteristics in VTS. In this paper, we\npropose a novel Global Video Text Spotting Transformer GloTSFormer to model the\ntracking problem as global associations and utilize the Gaussian Wasserstein\ndistance to guide the morphological correlation between frames. Our main\ncontributions can be summarized as three folds. 1). We propose a\nTransformer-based global tracking method GloTSFormer for VTS and associate\nmultiple frames simultaneously. 2). We introduce a Wasserstein distance-based\nmethod to conduct positional associations between frames. 3). We conduct\nextensive experiments on public datasets. On the ICDAR2015 video dataset,\nGloTSFormer achieves 56.0 MOTA with 4.6 absolute improvement compared with the\nprevious SOTA method and outperforms the previous Transformer-based method by a\nsignificant 8.3 MOTA.\n",
        "title": "GloTSFormer: Global Video Text Spotting Transformer",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03695",
        "abstract_url": "http://arxiv.org/abs/2401.03695",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Jiang",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Sun",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Junjie"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Fairness has been a critical issue that affects the adoption of deep learning\nmodels in real practice. To improve model fairness, many existing methods have\nbeen proposed and evaluated to be effective in their own contexts. However,\nthere is still no systematic evaluation among them for a comprehensive\ncomparison under the same context, which makes it hard to understand the\nperformance distinction among them, hindering the research progress and\npractical adoption of them. To fill this gap, this paper endeavours to conduct\nthe first large-scale empirical study to comprehensively compare the\nperformance of existing state-of-the-art fairness improving techniques.\nSpecifically, we target the widely-used application scenario of image\nclassification, and utilized three different datasets and five commonly-used\nperformance metrics to assess in total 13 methods from diverse categories. Our\nfindings reveal substantial variations in the performance of each method across\ndifferent datasets and sensitive attributes, indicating over-fitting on\nspecific datasets by many existing methods. Furthermore, different fairness\nevaluation metrics, due to their distinct focuses, yield significantly\ndifferent assessment results. Overall, we observe that pre-processing methods\nand in-processing methods outperform post-processing methods, with\npre-processing methods exhibiting the best performance. Our empirical study\noffers comprehensive recommendations for enhancing fairness in deep learning\nmodels. We approach the problem from multiple dimensions, aiming to provide a\nuniform evaluation platform and inspire researchers to explore more effective\nfairness solutions via a set of implications.\n",
        "title": "A Large-scale Empirical Study on Improving the Fairness of Deep Learning\n  Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03697",
        "abstract_url": "http://arxiv.org/abs/2401.03697",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Runduo"
            },
            {
                "last_name": "Yan",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Xu",
                "first_name": "Weiming"
            },
            {
                "last_name": "Guo",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Sun",
                "first_name": "Jiayao"
            },
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Lu",
                "first_name": "Quan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Ning"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  This paper describes our audio-quality-based multi-strategy approach for the\naudio-visual target speaker extraction (AVTSE) task in the Multi-modal\nInformation based Speech Processing (MISP) 2023 Challenge. Specifically, our\napproach adopts different extraction strategies based on the audio quality,\nstriking a balance between interference removal and speech preservation, which\nbenifits the back-end automatic speech recognition (ASR) systems. Experiments\nshow that our approach achieves a character error rate (CER) of 24.2% and 33.2%\non the Dev and Eval set, respectively, obtaining the second place in the\nchallenge.\n",
        "title": "An audio-quality-based multi-strategy approach for target speaker\n  extraction in the MISP 2023 Challenge",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03700",
        "abstract_url": "http://arxiv.org/abs/2401.03700",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Hsiao-Ying"
            },
            {
                "last_name": "Li",
                "first_name": "Yiran"
            },
            {
                "last_name": "Ma",
                "first_name": "Kwan-Liu"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "HC",
            "LG"
        ],
        "abstract": "  Communication among healthcare professionals (HCPs) is crucial for the\nquality of patient treatment. Surrounding each patient's treatment,\ncommunication among HCPs can be examined as temporal networks, constructed from\nElectronic Health Record (EHR) access logs. This paper introduces a visual\nanalytics system designed to study the effectiveness and efficiency of temporal\ncommunication networks mediated by the EHR system. We present a method that\nassociates network measures with patient survival outcomes and devises\neffectiveness metrics based on these associations. To analyze communication\nefficiency, we extract the latencies and frequencies of EHR accesses. Our\nvisual analytics system is designed to assist in inspecting and understanding\nthe composed communication effectiveness metrics and to enable the exploration\nof communication efficiency by encoding latencies and frequencies in an\ninformation flow diagram. We demonstrate and evaluate our system through\nmultiple case studies and an expert review.\n",
        "title": "A Visual Analytics Design for Connecting Healthcare Team Communication\n  to Patient Outcomes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03701",
        "abstract_url": "http://arxiv.org/abs/2401.03701",
        "authors": [
            {
                "last_name": "Yow",
                "first_name": "J-Anne"
            },
            {
                "last_name": "Garg",
                "first_name": "Neha Priyadarshini"
            },
            {
                "last_name": "Ramanathan",
                "first_name": "Manoj"
            },
            {
                "last_name": "Ang",
                "first_name": "Wei Tech"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Natural language provides an intuitive and expressive way of conveying human\nintent to robots. Prior works employed end-to-end methods for learning\ntrajectory deformations from language corrections. However, such methods do not\ngeneralize to new initial trajectories or object configurations. This work\npresents ExTraCT, a modular framework for trajectory corrections using natural\nlanguage that combines Large Language Models (LLMs) for natural language\nunderstanding and trajectory deformation functions. Given a scene, ExTraCT\ngenerates the trajectory modification features (scene-specific and\nscene-independent) and their corresponding natural language textual\ndescriptions for the objects in the scene online based on a template. We use\nLLMs for semantic matching of user utterances to the textual descriptions of\nfeatures. Based on the feature matched, a trajectory modification function is\napplied to the initial trajectory, allowing generalization to unseen\ntrajectories and object configurations. Through user studies conducted both in\nsimulation and with a physical robot arm, we demonstrate that trajectories\ndeformed using our method were more accurate and were preferred in about 80\\%\nof cases, outperforming the baseline. We also showcase the versatility of our\nsystem in a manipulation task and an assistive feeding task.\n",
        "title": "ExTraCT -- Explainable Trajectory Corrections from language inputs using\n  Textual description of features",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03703",
        "abstract_url": "http://arxiv.org/abs/2401.03703",
        "authors": [
            {
                "last_name": "Regev",
                "first_name": "Oded"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CC",
            ""
        ],
        "abstract": "  Our main result is a reduction from worst-case lattice problems such as\nGapSVP and SIVP to a certain learning problem. This learning problem is a\nnatural extension of the `learning from parity with error' problem to higher\nmoduli. It can also be viewed as the problem of decoding from a random linear\ncode. This, we believe, gives a strong indication that these problems are hard.\nOur reduction, however, is quantum. Hence, an efficient solution to the\nlearning problem implies a quantum algorithm for GapSVP and SIVP. A main open\nquestion is whether this reduction can be made classical (i.e., non-quantum).\n  We also present a (classical) public-key cryptosystem whose security is based\non the hardness of the learning problem. By the main result, its security is\nalso based on the worst-case quantum hardness of GapSVP and SIVP. The new\ncryptosystem is much more efficient than previous lattice-based cryptosystems:\nthe public key is of size $\\tilde{O}(n^2)$ and encrypting a message increases\nits size by a factor of $\\tilde{O}(n)$ (in previous cryptosystems these values\nare $\\tilde{O}(n^4)$ and $\\tilde{O}(n^2)$, respectively). In fact, under the\nassumption that all parties share a random bit string of length\n$\\tilde{O}(n^2)$, the size of the public key can be reduced to $\\tilde{O}(n)$.\n",
        "title": "On Lattices, Learning with Errors, Random Linear Codes, and Cryptography",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03704",
        "abstract_url": "http://arxiv.org/abs/2401.03704",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Zhangjin"
            },
            {
                "last_name": "Liang",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haojie"
            },
            {
                "last_name": "Lin",
                "first_name": "Yangkai"
            },
            {
                "last_name": "Jia",
                "first_name": "Kui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Multi-view surface reconstruction is an ill-posed, inverse problem in 3D\nvision research. It involves modeling the geometry and appearance with\nappropriate surface representations. Most of the existing methods rely either\non explicit meshes, using surface rendering of meshes for reconstruction, or on\nimplicit field functions, using volume rendering of the fields for\nreconstruction. The two types of representations in fact have their respective\nmerits. In this work, we propose a new hybrid representation, termed Sur2f,\naiming to better benefit from both representations in a complementary manner.\nTechnically, we learn two parallel streams of an implicit signed distance field\nand an explicit surrogate surface Sur2f mesh, and unify volume rendering of the\nimplicit signed distance function (SDF) and surface rendering of the surrogate\nmesh with a shared, neural shader; the unified shading promotes their\nconvergence to the same, underlying surface. We synchronize learning of the\nsurrogate mesh by driving its deformation with functions induced from the\nimplicit SDF. In addition, the synchronized surrogate mesh enables\nsurface-guided volume sampling, which greatly improves the sampling efficiency\nper ray in volume rendering. We conduct thorough experiments showing that\nSur$^2$f outperforms existing reconstruction methods and surface\nrepresentations, including hybrid ones, in terms of both recovery quality and\nrecovery efficiency.\n",
        "title": "Sur2f: A Hybrid Representation for High-Quality and Efficient Surface\n  Reconstruction from Multi-view Images",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03707",
        "abstract_url": "http://arxiv.org/abs/2401.03707",
        "authors": [
            {
                "last_name": "Youk",
                "first_name": "Geunhyuk"
            },
            {
                "last_name": "Oh",
                "first_name": "Jihyong"
            },
            {
                "last_name": "Kim",
                "first_name": "Munchurl"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a joint learning scheme of video super-resolution and deblurring,\ncalled VSRDB, to restore clean high-resolution (HR) videos from blurry\nlow-resolution (LR) ones. This joint restoration problem has drawn much less\nattention compared to single restoration problems. In this paper, we propose a\nnovel flow-guided dynamic filtering (FGDF) and iterative feature refinement\nwith multi-attention (FRMA), which constitutes our VSRDB framework, denoted as\nFMA-Net. Specifically, our proposed FGDF enables precise estimation of both\nspatio-temporally-variant degradation and restoration kernels that are aware of\nmotion trajectories through sophisticated motion representation learning.\nCompared to conventional dynamic filtering, the FGDF enables the FMA-Net to\neffectively handle large motions into the VSRDB. Additionally, the stacked FRMA\nblocks trained with our novel temporal anchor (TA) loss, which temporally\nanchors and sharpens features, refine features in a course-to-fine manner\nthrough iterative updates. Extensive experiments demonstrate the superiority of\nthe proposed FMA-Net over state-of-the-art methods in terms of both\nquantitative and qualitative quality. Codes and pre-trained models are\navailable at: https://kaist-viclab.github.io/fmanet-site\n",
        "title": "FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement\n  with Multi-Attention for Joint Video Super-Resolution and Deblurring",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03708",
        "abstract_url": "http://arxiv.org/abs/2401.03708",
        "authors": [
            {
                "last_name": "Ehlers",
                "first_name": "Svenja"
            },
            {
                "last_name": "Wagner",
                "first_name": "Niklas A."
            },
            {
                "last_name": "Scherzl",
                "first_name": "Annamaria"
            },
            {
                "last_name": "Klein",
                "first_name": "Marco"
            },
            {
                "last_name": "Hoffmann",
                "first_name": "Norbert"
            },
            {
                "last_name": "Stender",
                "first_name": "Merten"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The measurement of deep water gravity wave elevations using in-situ devices,\nsuch as wave gauges, typically yields spatially sparse data. This sparsity\narises from the deployment of a limited number of gauges due to their\ninstallation effort and high operational costs. The reconstruction of the\nspatio-temporal extent of surface elevation poses an ill-posed data\nassimilation problem, challenging to solve with conventional numerical\ntechniques. To address this issue, we propose the application of a\nphysics-informed neural network (PINN), aiming to reconstruct physically\nconsistent wave fields between two designated measurement locations several\nmeters apart.\n  Our method ensures this physical consistency by integrating residuals of the\nhydrodynamic nonlinear Schr\\\"{o}dinger equation (NLSE) into the PINN's loss\nfunction. Using synthetic wave elevation time series from distinct locations\nwithin a wave tank, we initially achieve successful reconstruction quality by\nemploying constant, predetermined NLSE coefficients. However, the\nreconstruction quality is further improved by introducing NLSE coefficients as\nadditional identifiable variables during PINN training. The results not only\nshowcase a technically relevant application of the PINN method but also\nrepresent a pioneering step towards improving the initialization of\ndeterministic wave prediction methods.\n",
        "title": "Data assimilation and parameter identification for water waves using the\n  nonlinear Schr\\\"{o}dinger equation and physics-informed neural networks",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03711",
        "abstract_url": "http://arxiv.org/abs/2401.03711",
        "authors": [
            {
                "last_name": "Amat",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Zilio",
                "first_name": "Silvano Dal"
            },
            {
                "last_name": "Botlan",
                "first_name": "Didier Le"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We propose a method for checking generalized reachability properties in Petri\nnets that takes advantage of structural reductions and that can be used,\ntransparently, as a pre-processing step of existing model-checkers. Our\napproach is based on a new procedure that can project a property, about an\ninitial Petri net, into an equivalent formula that only refers to the reduced\nversion of this net. Our projection is defined as a variable elimination\nprocedure for linear integer arithmetic tailored to the specific kind of\nconstraints we handle. It has linear complexity, is guaranteed to return a\nsound property, and makes use of a simple condition to detect when the result\nis exact. Experimental results show that our approach works well in practice\nand that it can be useful even when there is only a limited amount of\nreductions.\n",
        "title": "Project and Conquer: Fast Quantifier Elimination for Checking Petri Net\n  Reachability",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03714",
        "abstract_url": "http://arxiv.org/abs/2401.03714",
        "authors": [
            {
                "last_name": "Lukacova-Medvidova",
                "first_name": "Maria"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuhuan"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper we study the convergence of a second order finite volume\napproximation of the scalar conservation law. This scheme is based on the\ngeneralized Riemann problem (GRP) solver. We firstly investigate the stability\nof the GRP scheme and find that it might be entropy unstable when the shock\nwave is generated. By adding an artificial viscosity we propose a new\nstabilized GRP scheme. Under the assumption that numerical solutions are\nuniformly bounded, we prove consistency and convergence of this new GRP method.\n",
        "title": "Convergence of a generalized Riemann problem scheme for the Burgers\n  equation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03717",
        "abstract_url": "http://arxiv.org/abs/2401.03717",
        "authors": [
            {
                "last_name": "Trirat",
                "first_name": "Patara"
            },
            {
                "last_name": "Shin",
                "first_name": "Yooju"
            },
            {
                "last_name": "Kang",
                "first_name": "Junhyeok"
            },
            {
                "last_name": "Nam",
                "first_name": "Youngeun"
            },
            {
                "last_name": "Na",
                "first_name": "Jihye"
            },
            {
                "last_name": "Bae",
                "first_name": "Minyoung"
            },
            {
                "last_name": "Kim",
                "first_name": "Joeun"
            },
            {
                "last_name": "Kim",
                "first_name": "Byunghyun"
            },
            {
                "last_name": "Lee",
                "first_name": "Jae-Gil"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations.\n",
        "title": "Universal Time-Series Representation Learning: A Survey",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03719",
        "abstract_url": "http://arxiv.org/abs/2401.03719",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Qi"
            },
            {
                "last_name": "Gao",
                "first_name": "Yuyuan"
            },
            {
                "last_name": "Shen",
                "first_name": "Jiangrong"
            },
            {
                "last_name": "Li",
                "first_name": "Yaxin"
            },
            {
                "last_name": "Ran",
                "first_name": "Xuming"
            },
            {
                "last_name": "Tang",
                "first_name": "Huajin"
            },
            {
                "last_name": "Pan",
                "first_name": "Gang"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Spiking neural networks (SNNs) serve as one type of efficient model to\nprocess spatio-temporal patterns in time series, such as the Address-Event\nRepresentation data collected from Dynamic Vision Sensor (DVS). Although\nconvolutional SNNs have achieved remarkable performance on these AER datasets,\nbenefiting from the predominant spatial feature extraction ability of\nconvolutional structure, they ignore temporal features related to sequential\ntime points. In this paper, we develop a recurrent spiking neural network\n(RSNN) model embedded with an advanced spiking convolutional block attention\nmodule (SCBAM) component to combine both spatial and temporal features of\nspatio-temporal patterns. It invokes the history information in spatial and\ntemporal channels adaptively through SCBAM, which brings the advantages of\nefficient memory calling and history redundancy elimination. The performance of\nour model was evaluated in DVS128-Gesture dataset and other time-series\ndatasets. The experimental results show that the proposed SRNN-SCBAM model\nmakes better use of the history information in spatial and temporal dimensions\nwith less memory space, and achieves higher accuracy compared to other models.\n",
        "title": "Enhancing Adaptive History Reserving by Spiking Convolutional Block\n  Attention Module in Recurrent Neural Networks",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03722",
        "abstract_url": "http://arxiv.org/abs/2401.03722",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Lee",
                "first_name": "Vincent CS"
            },
            {
                "last_name": "Liu",
                "first_name": "Feng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Thyroid cancer, the most prevalent endocrine cancer, has gained significant\nglobal attention due to its impact on public health. Extensive research efforts\nhave been dedicated to leveraging artificial intelligence (AI) methods for the\nearly detection of this disease, aiming to reduce its morbidity rates. However,\na comprehensive understanding of the structured organization of research\napplications in this particular field remains elusive. To address this\nknowledge gap, we conducted a systematic review and developed a comprehensive\ntaxonomy of machine learning-based applications in thyroid cancer pathogenesis,\ndiagnosis, and prognosis. Our primary objective was to facilitate the research\ncommunity's ability to stay abreast of technological advancements and\npotentially lead the emerging trends in this field. This survey presents a\ncoherent literature review framework for interpreting the advanced techniques\nused in thyroid cancer research. A total of 758 related studies were identified\nand scrutinized. To the best of our knowledge, this is the first review that\nprovides an in-depth analysis of the various aspects of AI applications\nemployed in the context of thyroid cancer. Furthermore, we highlight key\nchallenges encountered in this domain and propose future research opportunities\nfor those interested in studying the latest trends or exploring\nless-investigated aspects of thyroid cancer research. By presenting this\ncomprehensive review and taxonomy, we contribute to the existing knowledge in\nthe field, while providing valuable insights for researchers, clinicians, and\nstakeholders in advancing the understanding and management of this disease.\n",
        "title": "From Data to Insights: A Comprehensive Survey on Advanced Applications\n  in Thyroid Cancer Research",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03723",
        "abstract_url": "http://arxiv.org/abs/2401.03723",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Hanxian"
            },
            {
                "last_name": "Siddiqui",
                "first_name": "Tarique"
            },
            {
                "last_name": "Alotaibi",
                "first_name": "Rana"
            },
            {
                "last_name": "Curino",
                "first_name": "Carlo"
            },
            {
                "last_name": "Leeka",
                "first_name": "Jyoti"
            },
            {
                "last_name": "Jindal",
                "first_name": "Alekh"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jishen"
            },
            {
                "last_name": "Camacho-Rodriguez",
                "first_name": "Jesus"
            },
            {
                "last_name": "Tian",
                "first_name": "Yuanyuan"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  Database systems often rely on historical query traces to perform\nworkload-based performance tuning. However, real production workloads are\ntime-evolving, making historical queries ineffective for optimizing future\nworkloads. To address this challenge, we propose SIBYL, an end-to-end machine\nlearning-based framework that accurately forecasts a sequence of future\nqueries, with the entire query statements, in various prediction windows.\nDrawing insights from real-workloads, we propose template-based featurization\ntechniques and develop a stacked-LSTM with an encoder-decoder architecture for\naccurate forecasting of query workloads. We also develop techniques to improve\nforecasting accuracy over large prediction windows and achieve high scalability\nover large workloads with high variability in arrival rates of queries.\nFinally, we propose techniques to handle workload drifts. Our evaluation on\nfour real workloads demonstrates that SIBYL can forecast workloads with an\n$87.3\\%$ median F1 score, and can result in $1.7\\times$ and $1.3\\times$\nperformance improvement when applied to materialized view selection and index\nselection applications, respectively.\n",
        "title": "Sibyl: Forecasting Time-Evolving Query Workloads",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03726",
        "abstract_url": "http://arxiv.org/abs/2401.03726",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Yifan"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingqing"
            },
            {
                "last_name": "Chen",
                "first_name": "Wen"
            },
            {
                "last_name": "Meng",
                "first_name": "Kaitao"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            ""
        ],
        "abstract": "  Integrated sensing and communications (ISAC) enabled by unmanned aerial\nvehicles (UAVs) is a promising technology to facilitate target tracking\napplications. In contrast to conventional UAV-based ISAC system designs that\nmainly focus on estimating the target position, the target velocity estimation\nalso needs to be considered due to its crucial impacts on link maintenance and\nreal-time response, which requires new designs on resource allocation and\ntracking scheme. In this paper, we propose an extended Kalman filtering-based\ntracking scheme for a UAV-enabled ISAC system where a UAV tracks a moving\nobject and also communicates with a device attached to the object.\nSpecifically, a weighted sum of predicted posterior Cram\\'er-Rao bound (PCRB)\nfor object relative position and velocity estimation is minimized by optimizing\nthe UAV trajectory, where an efficient solution is obtained based on the\nsuccessive convex approximation method. Furthermore, under a special case with\nthe measurement mean square error (MSE), the optimal relative motion state is\nobtained and proved to keep a fixed elevation angle and zero relative velocity.\nNumerical results validate that the obtained solution to the predicted PCRB\nminimization can be approximated by the optimal relative motion state when\npredicted measurement MSE dominates the predicted PCRBs, as well as the\neffectiveness of the proposed tracking scheme. Moreover, three interesting\ntrade-offs on system performance resulted from the fixed elevation angle are\nillustrated.\n",
        "title": "UAV-enabled Integrated Sensing and Communication: Tracking Design and\n  Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03735",
        "abstract_url": "http://arxiv.org/abs/2401.03735",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Fangwei"
            },
            {
                "last_name": "Dai",
                "first_name": "Damai"
            },
            {
                "last_name": "Sui",
                "first_name": "Zhifang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have exhibited impressive competency in various\ntext-related tasks. However, their opaque internal mechanisms become a\nhindrance to leveraging them in mathematical problems. In this paper, we study\na fundamental question: whether language models understand numbers, which play\na basic element in mathematical problems. We assume that to solve mathematical\nproblems, language models should be capable of understanding numbers and\ncompressing these numbers in their hidden states. We construct a synthetic\ndataset comprising addition problems and utilize linear probes to read out\ninput numbers from the hidden states of models. Experimental results\ndemonstrate evidence supporting the existence of compressed numbers in the\nLLaMA-2 model family from early layers. However, the compression process seems\nto be not lossless, presenting difficulty in precisely reconstructing the\noriginal numbers. Further experiments show that language models can utilize the\nencoded numbers to perform arithmetic computations, and the computational\nability scales up with the model size. Our preliminary research suggests that\nlanguage models exhibit a partial understanding of numbers, offering insights\ninto future investigations about the models' capability of solving mathematical\nproblems.\n",
        "title": "Language Models Understand Numbers, at Least Partially",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03737",
        "abstract_url": "http://arxiv.org/abs/2401.03737",
        "authors": [
            {
                "last_name": "Fatouros",
                "first_name": "Georgios"
            },
            {
                "last_name": "Metaxas",
                "first_name": "Konstantinos"
            },
            {
                "last_name": "Soldatos",
                "first_name": "John"
            },
            {
                "last_name": "Kyriazis",
                "first_name": "Dimosthenis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CE",
            "CL",
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In the dynamic and data-driven landscape of financial markets, this paper\nintroduces MarketSenseAI, a novel AI-driven framework leveraging the advanced\nreasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI\nincorporates Chain of Thought and In-Context Learning methodologies to analyze\na wide array of data sources, including market price dynamics, financial news,\ncompany fundamentals, and macroeconomic reports emulating the decision making\nprocess of prominent financial investment teams. The development,\nimplementation, and empirical validation of MarketSenseAI are detailed, with a\nfocus on its ability to provide actionable investment signals (buy, hold, sell)\nbacked by cogent explanations. A notable aspect of this study is the use of\nGPT-4 not only as a predictive tool but also as an evaluator, revealing the\nsignificant impact of the AI-generated explanations on the reliability and\nacceptance of the suggested investment signals. In an extensive empirical\nevaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index\nby 13%, achieving returns up to 40%, while maintaining a risk profile\ncomparable to the market. These results demonstrate the efficacy of Large\nLanguage Models in complex financial decision-making and mark a significant\nadvancement in the integration of AI into financial analysis and investment\nstrategies. This research contributes to the financial AI field, presenting an\ninnovative approach and underscoring the transformative potential of AI in\nrevolutionizing traditional financial analysis investment methodologies.\n",
        "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of\n  AI in Stock Selection",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03741",
        "abstract_url": "http://arxiv.org/abs/2401.03741",
        "authors": [
            {
                "last_name": "de-Fitero-Dominguez",
                "first_name": "David"
            },
            {
                "last_name": "Garcia-Lopez",
                "first_name": "Eva"
            },
            {
                "last_name": "Garcia-Cabot",
                "first_name": "Antonio"
            },
            {
                "last_name": "Martinez-Herraiz",
                "first_name": "Jose-Javier"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL"
        ],
        "abstract": "  This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.\n",
        "title": "Enhanced Automated Code Vulnerability Repair using Large Language Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03742",
        "abstract_url": "http://arxiv.org/abs/2401.03742",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Huanyu"
            },
            {
                "last_name": "Cai",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tingjia"
            },
            {
                "last_name": "Li",
                "first_name": "Hongsheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Shah",
                "first_name": "Syed Afaq Ali"
            },
            {
                "last_name": "Bennamoun",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Flowcharts and mind maps, collectively known as flowmind, are vital in daily\nactivities, with hand-drawn versions facilitating real-time collaboration.\nHowever, there's a growing need to digitize them for efficient processing.\nAutomated conversion methods are essential to overcome manual conversion\nchallenges. Existing sketch recognition methods face limitations in practical\nsituations, being field-specific and lacking digital conversion steps. Our\npaper introduces the Flowmind2digital method and hdFlowmind dataset to address\nthese challenges. Flowmind2digital, utilizing neural networks and keypoint\ndetection, achieves a record 87.3% accuracy on our dataset, surpassing previous\nmethods by 11.9%. The hdFlowmind dataset, comprising 1,776 annotated flowminds\nacross 22 scenarios, outperforms existing datasets. Additionally, our\nexperiments emphasize the importance of simple graphics, enhancing accuracy by\n9.3%.\n",
        "title": "Flowmind2Digital: The First Comprehensive Flowmind Recognition and\n  Conversion Approach",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03748",
        "abstract_url": "http://arxiv.org/abs/2401.03748",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Ngoc-Hieu"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Tuan-Anh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Tuan"
            },
            {
                "last_name": "Hoang",
                "first_name": "Vu Tien"
            },
            {
                "last_name": "Le",
                "first_name": "Dung D."
            },
            {
                "last_name": "Wong",
                "first_name": "Kok-Seng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "DC",
            "IR"
        ],
        "abstract": "  In Federated Recommendation (FedRec) systems, communication costs are a\ncritical bottleneck that arises from the need to transmit neural network models\nbetween user devices and a central server. Prior approaches to these challenges\noften lead to issues such as computational overheads, model specificity\nconstraints, and compatibility issues with secure aggregation protocols. In\nresponse, we propose a novel framework, called Correlated Low-rank Structure\n(CoLR), which leverages the concept of adjusting lightweight trainable\nparameters while keeping most parameters frozen. Our approach substantially\nreduces communication overheads without introducing additional computational\nburdens. Critically, our framework remains fully compatible with secure\naggregation protocols, including the robust use of Homomorphic Encryption. Our\napproach resulted in a reduction of up to 93.75% in payload size, with only an\napproximate 8% decrease in recommendation performance across datasets. Code for\nreproducing our experiments can be found at\nhttps://github.com/NNHieu/CoLR-FedRec.\n",
        "title": "Towards Efficient Communication Federated Recommendation System via\n  Low-rank Training",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03749",
        "abstract_url": "http://arxiv.org/abs/2401.03749",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Hua",
                "first_name": "Zexi"
            },
            {
                "last_name": "Li",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Li",
                "first_name": "Yan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Aiming at the characteristics of the flying bird object in surveillance\nvideo, such as the single frame image feature is not obvious, the size is small\nin most cases, and asymmetric, this paper proposes a Flying Bird Object\nDetection method for Surveillance Video (FBOD-SV). Firstly, a new feature\naggregation module, the Correlation Attention Feature Aggregation\n(Co-Attention-FA) module, is designed to aggregate the features of the flying\nbird object according to the bird object's correlation on multiple consecutive\nframes of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net)\nwith down-sampling and then up-sampling is designed, which uses a large feature\nlayer that fuses fine spatial information and large receptive field information\nto detect special multi-scale (mostly small-scale) bird objects. Finally, the\nSimOTA dynamic label allocation method is applied to One-Category object\ndetection, and the SimOTA-OC dynamic label strategy is proposed to solve the\ndifficult problem of label allocation caused by irregular flying bird objects.\nIn this paper, the algorithm's performance is verified by the experimental data\nset of the surveillance video of the flying bird object of the traction\nsubstation. The experimental results show that the surveillance video flying\nbird object detection method proposed in this paper effectively improves the\ndetection performance of flying bird objects.\n",
        "title": "Flying Bird Object Detection Algorithm in Surveillance Video",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03751",
        "abstract_url": "http://arxiv.org/abs/2401.03751",
        "authors": [
            {
                "last_name": "Scholz",
                "first_name": "D\u00e9sir\u00e9e"
            },
            {
                "last_name": "Graefe",
                "first_name": "Linda"
            },
            {
                "last_name": "Prinz",
                "first_name": "Thomas M."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Reaction time studies with computers investigate how and how quickly\nparticipants respond to changing sensory input. They promise simple and precise\nmeasurement of time and inputs and offer interesting insights into human\nbehavior. However, several previous studies have discovered imprecisions in\ntiming appearing as delays, depending on the browser, software and programming\nused for conducting such studies. Since the accuaracy of the collected data is\nwidely discussed, we aim to provide new results on the effect of unintended\ndelays on participants' behavior. For this purpose, a new reaction time study\nwas conducted. Computer delays were added to the experiment to investigate\ntheir effects on participants' performance and repulsion. Minimal changes in\nparticipants' behavior did occur and should be furtherly investigated, as the\npower of this study was rather low and might not have uncovered all underlying\neffects. The following report details our study design and results and offers\nseveral suggestions for improvements in further studies.\n",
        "title": "How Participants Respond to Computer Delays",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03752",
        "abstract_url": "http://arxiv.org/abs/2401.03752",
        "authors": [
            {
                "last_name": "Iwase",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Beynier",
                "first_name": "Aur\u00e9lie"
            },
            {
                "last_name": "Bredeche",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Maudet",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Marden",
                "first_name": "Jason R."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Conventional distributed approaches to coverage control may suffer from lack\nof convergence and poor performance, due to the fact that agents have limited\ninformation, especially in non-convex discrete environments. To address this\nissue, we extend the approach of [Marden 2016] which demonstrates how a limited\ndegree of inter-agent communication can be exploited to overcome such pitfalls\nin one-dimensional discrete environments. The focus of this paper is on\nextending such results to general dimensional settings. We show that the\nextension is convergent and keeps the approximation ratio of 2, meaning that\nany stable solution is guaranteed to have a performance within 50% of the\noptimal one. We also show that the computational complexity and communication\ncomplexity are both polynomial in the size of the problem. The experimental\nresults exhibit that our algorithm outperforms several state-of-the-art\nalgorithms, and also that the runtime is scalable as per theory.\n",
        "title": "Is Limited Information Enough? An Approximate Multi-agent Coverage\n  Control in Non-Convex Discrete Environments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03753",
        "abstract_url": "http://arxiv.org/abs/2401.03753",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hanxiao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This work addresses the problem of semi-supervised image classification tasks\nwith the integration of several effective self-supervised pretext tasks.\nDifferent from widely-used consistency regularization within semi-supervised\nlearning, we explored a novel self-supervised semi-supervised learning\nframework (Color-$S^{4}L$) especially with image colorization proxy task and\ndeeply evaluate performances of various network architectures in such special\npipeline. Also, we demonstrated its effectiveness and optimal performance on\nCIFAR-10, SVHN and CIFAR-100 datasets in comparison to previous supervised and\nsemi-supervised optimal methods.\n",
        "title": "Color-$S^{4}L$: Self-supervised Semi-supervised Learning with Image\n  Colorization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03754",
        "abstract_url": "http://arxiv.org/abs/2401.03754",
        "authors": [
            {
                "last_name": "Van Chien",
                "first_name": "Trinh"
            },
            {
                "last_name": "Le",
                "first_name": "Ha An"
            },
            {
                "last_name": "Tung",
                "first_name": "Ta Hai"
            },
            {
                "last_name": "Ngo",
                "first_name": "Hien Quoc"
            },
            {
                "last_name": "Chatzinotas",
                "first_name": "Symeon"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Both space and ground communications have been proven effective solutions\nunder different perspectives in Internet of Things (IoT) networks. This paper\ninvestigates multiple-access scenarios, where plenty of IoT users are\ncooperatively served by a satellite in space and access points (APs) on the\nground. Available users in each coherence interval are split into scheduled and\nunscheduled subsets to optimize limited radio resources. We compute the uplink\nergodic throughput of each scheduled user under imperfect channel state\ninformation (CSI) and non-orthogonal pilot signals. As maximum-radio combining\nis deployed locally at the ground gateway and the APs, the uplink ergodic\nthroughput is obtained in a closed-form expression. The analytical results\nexplicitly unveil the effects of channel conditions and pilot contamination on\neach scheduled user. By maximizing the sum throughput, the system can\nsimultaneously determine scheduled users and perform power allocation based on\neither a model-based approach with alternating optimization or a learning-based\napproach with the graph neural network. Numerical results manifest that\nintegrated satellite-terrestrial cell-free massive multiple-input\nmultiple-output systems can significantly improve the sum ergodic throughput\nover coherence intervals. The integrated systems can schedule the vast majority\nof users; some might be out of service due to the limited power budget.\n",
        "title": "Joint Power Allocation and User Scheduling in Integrated\n  Satellite-Terrestrial Cell-Free Massive MIMO IoT Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03755",
        "abstract_url": "http://arxiv.org/abs/2401.03755",
        "authors": [
            {
                "last_name": "Carson",
                "first_name": "Erin"
            },
            {
                "last_name": "Oktay",
                "first_name": "Eda"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  With the recent emergence of mixed precision hardware, there has been a\nrenewed interest in its use for solving numerical linear algebra problems fast\nand accurately. The solution of least squares (LS) problems $\\min_x\\|b-Ax\\|_2$,\nwhere $A \\in \\mathbb{R}^{m\\times n}$, arise in numerous application areas.\nOverdetermined standard least squares problems can be solved by using mixed\nprecision within the iterative refinement method of Bj\\\"{o}rck, which\ntransforms the least squares problem into an $(m+n)\\times(m+n)$ ''augmented''\nsystem. It has recently been shown that mixed precision GMRES-based iterative\nrefinement can also be used, in an approach termed GMRES-LSIR. In practice, we\noften encounter types of least squares problems beyond standard least squares,\nincluding weighted least squares (WLS), $\\min_x\\|D^{1/2}(b-Ax)\\|_2$, where\n$D^{1/2}$ is a diagonal matrix of weights. In this paper, we discuss a mixed\nprecision FGMRES-WLSIR algorithm for solving WLS problems using two different\npreconditioners.\n",
        "title": "Mixed Precision FGMRES-Based Iterative Refinement for Weighted Least\n  Squares",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03761",
        "abstract_url": "http://arxiv.org/abs/2401.03761",
        "authors": [
            {
                "last_name": "Gagner\u00e9",
                "first_name": "Georges"
            },
            {
                "last_name": "Ternova",
                "first_name": "Anastasiia"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  AvatarStaging framework consists in directing avatars on a mixed theatrical\nstage, enabling a co-presence between the materiality of the physical actor and\nthe virtuality of avatars controlled in real time by motion capture or specific\nanimation players. It led to the implementation of the AKN_Regie authoring\ntool, programmed with the Blueprint visual language as a plugin for the Unreal\nEngine (UE) video game engine. The paper describes AKN_Regie main\nfunctionalities as a tool for non-programmer theatrical people. It gives\ninsights of its implementation in the Blueprint visual language specific to UE.\nIt details how the tool evolved along with its use in around ten theater\nproductions. A circulation process between a nonprogramming point of view on\nAKN_Regie called Plugin Perspective and a programming acculturation to its\ndevelopment called Blueprint Perspective is discussed. Finally, a C++\nPerspective is suggested to enhance the cultural appropriation of technological\nissues, bridging the gap between performing arts deeply involved in human\nmateriality and avatars inviting to discover new worlds.\n",
        "title": "AKN_Regie: bridging digital and performing arts",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03762",
        "abstract_url": "http://arxiv.org/abs/2401.03762",
        "authors": [
            {
                "last_name": "Blank",
                "first_name": "Lotte"
            },
            {
                "last_name": "Driemel",
                "first_name": "Anne"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  We study the Fr\\'echet queries problem. It is a data structure problem, where\nwe are given a set $S$ of $n$ polygonal curves and a distance threshold $\\rho$.\nThe data structure should support queries with a polygonal curve $q$ for the\nelements of $S$, for which the continuous Fr\\'echet distance to $q$ is at most\n$\\rho$. Afshani and Driemel in 2018 studied this problem for two-dimensional\npolygonal curves and gave upper and lower bounds on the space-query time\ntradeoff. We study the case that the ambient space of the curves is\none-dimensional and show an intimate connection to the well-studied rectangle\nstabbing problem. Here, we are given a set of hyperrectangles as input and a\nquery with a point $q$ should return all input rectangles that contain this\npoint. Using known data structures for rectangle stabbing or orthogonal range\nsearching this directly leads to a data structure with $\\mathcal{O}(n \\log\n^{t-1} n)$ storage and $\\mathcal{O}(\\log^{t-1} n+k)$ query time, where $k$\ndenotes the output size and $t$ can be chosen as the maximum number of vertices\nof either (a) the stored curves or (b) the query curves. The resulting bounds\nimprove upon the bounds by Afshani and Driemel in both the storage and query\ntime. In addition, we show that known lower bounds for rectangle stabbing and\northogonal range reporting with dimension parameter $d= \\lfloor t/2 \\rfloor$\ncan be applied to our problem via reduction. .\n",
        "title": "Range Reporting for Time Series via Rectangle Stabbing",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03764",
        "abstract_url": "http://arxiv.org/abs/2401.03764",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Ruiqi"
            },
            {
                "last_name": "Zheng",
                "first_name": "Peng"
            },
            {
                "last_name": "Wang",
                "first_name": "Ye"
            },
            {
                "last_name": "Ma",
                "first_name": "Rui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Existing 3D-aware portrait synthesis methods can generate impressive\nhigh-quality images while preserving strong 3D consistency. However, most of\nthem cannot support the fine-grained part-level control over synthesized\nimages. Conversely, some GAN-based 2D portrait synthesis methods can achieve\nclear disentanglement of facial regions, but they cannot preserve view\nconsistency due to a lack of 3D modeling abilities. To address these issues, we\npropose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image\nsynthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module\nmaps the generated 2D part features and semantics to 3D. Then, a volume\nrenderer with a novel 3D-aware semantic mask renderer is utilized to produce\nthe composed face features and corresponding masks. The whole framework is\ntrained end-to-end by discriminating between real and synthesized 2D images and\ntheir semantic masks. Quantitative and qualitative evaluations demonstrate the\nsuperiority of 3D-SSGAN in controllable part-level synthesis while preserving\n3D view consistency.\n",
        "title": "3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait\n  Synthesis",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03765",
        "abstract_url": "http://arxiv.org/abs/2401.03765",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhimin"
            },
            {
                "last_name": "Gao",
                "first_name": "Xiang"
            },
            {
                "last_name": "Hu",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The convenience of 3D sensors has led to an increase in the use of 3D point\nclouds in various applications. However, the differences in acquisition devices\nor scenarios lead to divergence in the data distribution of point clouds, which\nrequires good generalization of point cloud representation learning methods.\nWhile most previous methods rely on domain adaptation, which involves\nfine-tuning pre-trained models on target domain data, this may not always be\nfeasible in real-world scenarios where target domain data may be unavailable.\nTo address this issue, we propose InvariantOODG, which learns invariability\nbetween point clouds with different distributions using a two-branch network to\nextract local-to-global features from original and augmented point clouds.\nSpecifically, to enhance local feature learning of point clouds, we define a\nset of learnable anchor points that locate the most useful local regions and\ntwo types of transformations to augment the input point clouds. The\nexperimental results demonstrate the effectiveness of the proposed model on 3D\ndomain generalization benchmarks.\n",
        "title": "InvariantOODG: Learning Invariant Features of Point Clouds for\n  Out-of-Distribution Generalization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03768",
        "abstract_url": "http://arxiv.org/abs/2401.03768",
        "authors": [
            {
                "last_name": "Olisah",
                "first_name": "Chollette"
            },
            {
                "last_name": "Smith",
                "first_name": "Lyndon"
            },
            {
                "last_name": "Smith",
                "first_name": "Melvyn"
            },
            {
                "last_name": "Morolake",
                "first_name": "Lawrence"
            },
            {
                "last_name": "Ojukwu",
                "first_name": "Osi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CY",
            "HC"
        ],
        "abstract": "  Given the nonlinearity of the interaction between weather and soil variables,\na novel deep neural network regressor (DNNR) was carefully designed with\nconsiderations to the depth, number of neurons of the hidden layers, and the\nhyperparameters with their optimizations. Additionally, a new metric, the\naverage of absolute root squared error (ARSE) was proposed to address the\nshortcomings of root mean square error (RMSE) and mean absolute error (MAE)\nwhile combining their strengths. Using the ARSE metric, the random forest\nregressor (RFR) and the extreme gradient boosting regressor (XGBR), were\ncompared with DNNR. The RFR and XGBR achieved yield errors of 0.0000294 t/ha,\nand 0.000792 t/ha, respectively, compared to the DNNR(s) which achieved 0.0146\nt/ha and 0.0209 t/ha, respectively. All errors were impressively small.\nHowever, with changes to the explanatory variables to ensure generalizability\nto unforeseen data, DNNR(s) performed best. The unforeseen data, different from\nunseen data, is coined to represent sudden and unexplainable change to weather\nand soil variables due to climate change. Further analysis reveals that a\nstrong interaction does exist between weather and soil variables. Using\nprecipitation and silt, which are strong-negatively and strong-positively\ncorrelated with yield, respectively, yield was observed to increase when\nprecipitation was reduced and silt increased, and vice-versa.\n",
        "title": "Corn Yield Prediction Model with Deep Neural Networks for Smallholder\n  Farmer Decision Support System",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03770",
        "abstract_url": "http://arxiv.org/abs/2401.03770",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Ngoc Luyen"
            },
            {
                "last_name": "Abel",
                "first_name": "Marie-H\u00e9l\u00e8ne"
            },
            {
                "last_name": "Negre",
                "first_name": "Elsa"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recognizing and learning from similar crisis situations is crucial for the\ndevelopment of effective response strategies. This study addresses the\nchallenge of identifying similarities within a wide range of crisis-related\ninformation. To overcome this challenge, we employed an ontology-based crisis\nsituation knowledge base enriched with crisis-related information.\nAdditionally, we implemented a semantic similarity measure to assess the degree\nof similarity between crisis situations. Our investigation specifically focuses\non recognizing similar crises through the application of ontology-based\nknowledge mining. Through our experiments, we demonstrate the accuracy and\nefficiency of our approach to recognizing similar crises. These findings\nhighlight the potential of ontology-based knowledge mining for enhancing crisis\nrecognition processes and improving overall crisis management strategies.\n",
        "title": "Recognizing Similar Crises through the Application of Ontology-based\n  Knowledge Mining",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03771",
        "abstract_url": "http://arxiv.org/abs/2401.03771",
        "authors": [
            {
                "last_name": "Feldmann",
                "first_name": "Casimir"
            },
            {
                "last_name": "Siegenheim",
                "first_name": "Niall"
            },
            {
                "last_name": "Hars",
                "first_name": "Nikolas"
            },
            {
                "last_name": "Rabuzin",
                "first_name": "Lovro"
            },
            {
                "last_name": "Ertugrul",
                "first_name": "Mert"
            },
            {
                "last_name": "Wolfart",
                "first_name": "Luca"
            },
            {
                "last_name": "Pollefeys",
                "first_name": "Marc"
            },
            {
                "last_name": "Bauer",
                "first_name": "Zuria"
            },
            {
                "last_name": "Oswald",
                "first_name": "Martin R."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The capabilities of monocular depth estimation (MDE) models are limited by\nthe availability of sufficient and diverse datasets. In the case of MDE models\nfor autonomous driving, this issue is exacerbated by the linearity of the\ncaptured data trajectories. We propose a NeRF-based data augmentation pipeline\nto introduce synthetic data with more diverse viewing directions into training\ndatasets and demonstrate the benefits of our approach to model performance and\nrobustness. Our data augmentation pipeline, which we call \"NeRFmentation\",\ntrains NeRFs on each scene in the dataset, filters out subpar NeRFs based on\nrelevant metrics, and uses them to generate synthetic RGB-D images captured\nfrom new viewing directions. In this work, we apply our technique in\nconjunction with three state-of-the-art MDE architectures on the popular\nautonomous driving dataset KITTI, augmenting its training set of the Eigen\nsplit. We evaluate the resulting performance gain on the original test set, a\nseparate popular driving set, and our own synthetic test set.\n",
        "title": "NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03780",
        "abstract_url": "http://arxiv.org/abs/2401.03780",
        "authors": [
            {
                "last_name": "del Moral",
                "first_name": "Javier Oliva"
            },
            {
                "last_name": "iOlius",
                "first_name": "Antonio deMarti"
            },
            {
                "last_name": "Vidal",
                "first_name": "Gerard"
            },
            {
                "last_name": "Crespo",
                "first_name": "Pedro M."
            },
            {
                "last_name": "Martinez",
                "first_name": "Josu Etxezarreta"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The machinery of industrial environments was connected to the Internet years\nago with the scope of increasing their performance. However, this made such\nenvironments vulnerable against cyber-attacks that can compromise their correct\nfunctioning resulting in economic or social problems. Lately, an increase of\ncyberattacks to industrial environments has been experienced. Moreover,\nimplementing cryptosystems in the communications between OT devices is a more\nchallenging task than for IT environments since the OT are generally composed\nof legacy elements, characterized by low-computational capabilities.\nConsequently, implementing cryptosystems in industrial communication networks\nfaces a trade-off between the security of the communications and the\namortization of the industrial infrastructure. Critical Infrastructure (CI)\nrefers to the industries which provide key resources for the daily social and\neconomical development, e.g. electricity or water, and their communications are\na very exposed target to cyberattacks. Furthermore, a new threat to\ncybersecurity has arisen with the theoretical proposal of quantum computers,\ndue to their potential ability of breaking state-of-the-art cryptography\nprotocols, such as RSA or ECC. The chase of functional quantum computers has\nresulted in a technological race involving many global agents. Those agents\nhave become aware that transitioning their secure communications to a quantum\nsecure paradigm is a priority that should be established before the arrival of\nfault-tolerance. In this sense, two main cryptographic solutions have been\nproposed: QKD and PQC. Nevertheless, quantum secure solutions have been mainly\ncentered from the perspective of IT environments. In this paper, we provide a\nperspective of the problem of applying PQC solutions to CI and analyze which\ncould be the most suitable cryptography schemes for these scenarios.\n",
        "title": "Cybersecurity in Critical Infrastructures: A Post-Quantum Cryptography\n  Perspective",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03785",
        "abstract_url": "http://arxiv.org/abs/2401.03785",
        "authors": [
            {
                "last_name": "Sumiyasu",
                "first_name": "Kosuke"
            },
            {
                "last_name": "Kawamoto",
                "first_name": "Kazuhiko"
            },
            {
                "last_name": "Kera",
                "first_name": "Hiroshi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  To better understand the behavior of image classifiers, it is useful to\nvisualize the contribution of individual pixels to the model prediction. In\nthis study, we propose a method, MoXI~($\\textbf{Mo}$del e$\\textbf{X}$planation\nby $\\textbf{I}$nteractions), that efficiently and accurately identifies a group\nof pixels with high prediction confidence. The proposed method employs\ngame-theoretic concepts, Shapley values and interactions, taking into account\nthe effects of individual pixels and the cooperative influence of pixels on\nmodel confidence. Theoretical analysis and experiments demonstrate that our\nmethod better identifies the pixels that are highly contributing to the model\noutputs than widely-used visualization methods using Grad-CAM, Attention\nrollout, and Shapley value. While prior studies have suffered from the\nexponential computational cost in the computation of Shapley value and\ninteractions, we show that this can be reduced to linear cost for our task.\n",
        "title": "Identifying Important Group of Pixels using Interactions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03786",
        "abstract_url": "http://arxiv.org/abs/2401.03786",
        "authors": [
            {
                "last_name": "Wachi",
                "first_name": "Akifumi"
            },
            {
                "last_name": "Hashimoto",
                "first_name": "Wataru"
            },
            {
                "last_name": "Hashimoto",
                "first_name": "Kazumune"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "RO"
        ],
        "abstract": "  Safety is an indispensable requirement for applying reinforcement learning\n(RL) to real problems. Although there has been a surge of safe RL algorithms\nproposed in recent years, most existing work typically 1) relies on receiving\nnumeric safety feedback; 2) does not guarantee safety during the learning\nprocess; 3) limits the problem to a priori known, deterministic transition\ndynamics; and/or 4) assume the existence of a known safe policy for any states.\nAddressing the issues mentioned above, we thus propose Long-term Binaryfeedback\nSafe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision\nprocesses (CMDPs) with binary safety feedback and an unknown, stochastic state\ntransition function. LoBiSaRL optimizes a policy to maximize rewards while\nguaranteeing a long-term safety that an agent executes only safe state-action\npairs throughout each episode with high probability. Specifically, LoBiSaRL\nmodels the binary safety function via a generalized linear model (GLM) and\nconservatively takes only a safe action at every time step while inferring its\neffect on future safety under proper assumptions. Our theoretical results show\nthat LoBiSaRL guarantees the long-term safety constraint, with high\nprobability. Finally, our empirical results demonstrate that our algorithm is\nsafer than existing methods without significantly compromising performance in\nterms of reward.\n",
        "title": "Long-term Safe Reinforcement Learning with Binary Feedback",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03788",
        "abstract_url": "http://arxiv.org/abs/2401.03788",
        "authors": [
            {
                "last_name": "Xue",
                "first_name": "Minglong"
            },
            {
                "last_name": "He",
                "first_name": "Jinhong"
            },
            {
                "last_name": "He",
                "first_name": "Yanyi"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhipu"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenhai"
            },
            {
                "last_name": "Zhou",
                "first_name": "Mingliang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Low-light image enhancement techniques have significantly progressed, but\nunstable image quality recovery and unsatisfactory visual perception are still\nsignificant challenges. To solve these problems, we propose a novel and robust\nlow-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion,\nabbreviated as CFWD. Specifically, we design a guided network with a multiscale\nvisual language in the frequency domain based on the wavelet transform to\nachieve effective image enhancement iteratively. In addition, we combine the\nadvantages of Fourier transform in detail perception to construct a hybrid\nfrequency domain space with significant perceptual capabilities(HFDPM). This\noperation guides wavelet diffusion to recover the fine-grained structure of the\nimage and avoid diversity confusion. Extensive quantitative and qualitative\nexperiments on publicly available real-world benchmarks show that our method\noutperforms existing state-of-the-art methods and better reproduces images\nsimilar to normal images. Code is available at\nhttps://github.com/He-Jinhong/CFWD.\n",
        "title": "Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03790",
        "abstract_url": "http://arxiv.org/abs/2401.03790",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Dat"
            },
            {
                "last_name": "Vu",
                "first_name": "Hieu M."
            },
            {
                "last_name": "Le",
                "first_name": "Cong-Thanh"
            },
            {
                "last_name": "Le",
                "first_name": "Bach"
            },
            {
                "last_name": "Lo",
                "first_name": "David"
            },
            {
                "last_name": "Pasareanu",
                "first_name": "Corina"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "PL",
            "SE"
        ],
        "abstract": "  We propose GNNInfer, the first automatic property inference technique for\nGNNs. To tackle the challenge of varying input structures in GNNs, GNNInfer\nfirst identifies a set of representative influential structures that contribute\nsignificantly towards the prediction of a GNN. Using these structures, GNNInfer\nconverts each pair of an influential structure and the GNN to their equivalent\nFNN and then leverages existing property inference techniques to effectively\ncapture properties of the GNN that are specific to the influential structures.\nGNNINfer then generalizes the captured properties to any input graphs that\ncontain the influential structures. Finally, GNNInfer improves the correctness\nof the inferred properties by building a model (either a decision tree or\nlinear regression) that estimates the deviation of GNN output from the inferred\nproperties given full input graphs. The learned model helps GNNInfer extend the\ninferred properties with constraints to the input and output of the GNN,\nobtaining stronger properties that hold on full input graphs.\n  Our experiments show that GNNInfer is effective in inferring likely\nproperties of popular real-world GNNs, and more importantly, these inferred\nproperties help effectively defend against GNNs' backdoor attacks. In\nparticular, out of the 13 ground truth properties, GNNInfer re-discovered 8\ncorrect properties and discovered likely correct properties that approximate\nthe remaining 5 ground truth properties. Using properties inferred by GNNInfer\nto defend against the state-of-the-art backdoor attack technique on GNNs,\nnamely UGBA, experiments show that GNNInfer's defense success rate is up to 30\ntimes better than existing baselines.\n",
        "title": "Inferring Properties of Graph Neural Networks",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03792",
        "abstract_url": "http://arxiv.org/abs/2401.03792",
        "authors": [
            {
                "last_name": "Razzano",
                "first_name": "Francesca"
            },
            {
                "last_name": "Mauro",
                "first_name": "Francesco"
            },
            {
                "last_name": "Di Stasio",
                "first_name": "Pietro"
            },
            {
                "last_name": "Meoni",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Esposito",
                "first_name": "Marco"
            },
            {
                "last_name": "Schirinzi",
                "first_name": "Gilda"
            },
            {
                "last_name": "Ullo",
                "first_name": "Silvia Liberata"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Monitoring water contaminants is of paramount importance, ensuring public\nhealth and environmental well-being. Turbidity, a key parameter, poses a\nsignificant problem, affecting water quality. Its accurate assessment is\ncrucial for safeguarding ecosystems and human consumption, demanding meticulous\nattention and action. For this, our study pioneers a novel approach to monitor\nthe Turbidity contaminant, integrating CatBoost Machine Learning (ML) with\nhigh-resolution data from Sentinel-2 Level-2A. Traditional methods are\nlabor-intensive while CatBoost offers an efficient solution, excelling in\npredictive accuracy. Leveraging atmospherically corrected Sentinel-2 data\nthrough the Google Earth Engine (GEE), our study contributes to scalable and\nprecise Turbidity monitoring. A specific tabular dataset derived from Hong Kong\ncontaminants monitoring stations enriches our study, providing region-specific\ninsights. Results showcase the viability of this integrated approach, laying\nthe foundation for adopting advanced techniques in global water quality\nmanagement.\n",
        "title": "Monitoring water contaminants in coastal areas through ML algorithms\n  leveraging atmospherically corrected Sentinel-2 data",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03794",
        "abstract_url": "http://arxiv.org/abs/2401.03794",
        "authors": [
            {
                "last_name": "Axelsson",
                "first_name": "Minja"
            },
            {
                "last_name": "Spitale",
                "first_name": "Micol"
            },
            {
                "last_name": "Gunes",
                "first_name": "Hatice"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robotic well-being coaches have been shown to successfully promote people's\nmental well-being. To provide successful coaching, a robotic coach should have\nthe capability to repair the mistakes it makes. Past investigations of robot\nmistakes are limited to game or task-based, one-off and in-lab studies. This\npaper presents a 4-phase design process to design repair strategies for robotic\nlongitudinal well-being coaching with the involvement of real-world\nstakeholders: 1) designing repair strategies with a professional well-being\ncoach; 2) a longitudinal study with the involvement of experienced users (i.e.,\nwho had already interacted with a robotic coach) to investigate the repair\nstrategies defined in (1); 3) a design workshop with users from the study in\n(2) to gather their perspectives on the robotic coach's repair strategies; 4)\ndiscussing the results obtained in (2) and (3) with the mental well-being\nprofessional to reflect on how to design repair strategies for robotic\ncoaching. Our results show that users have different expectations for a robotic\ncoach than a human coach, which influences how repair strategies should be\ndesigned. We show that different repair strategies (e.g., apologizing,\nexplaining, or repairing empathically) are appropriate in different scenarios,\nand that preferences for repair strategies change during longitudinal\ninteractions with the robotic coach.\n",
        "title": "\"Oh, Sorry, I Think I Interrupted You'': Designing Repair Strategies for\n  Robotic Longitudinal Well-being Coaching",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03797",
        "abstract_url": "http://arxiv.org/abs/2401.03797",
        "authors": [
            {
                "last_name": "Saleh",
                "first_name": "Majd"
            },
            {
                "last_name": "Paquelet",
                "first_name": "St\u00e9phane"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Generative AI and transfer learning fields have experienced remarkable\nadvancements in recent years especially in the domain of Natural Language\nProcessing (NLP). Transformers were at the heart of these advancements where\nthe cutting-edge transformer-based Language Models (LMs) enabled new\nstate-of-the-art results in a wide spectrum of applications. While the number\nof research works involving neural LMs is exponentially increasing, their vast\nmajority are high-level and far from self-contained. Consequently, a deep\nunderstanding of the literature in this area is a tough task especially at the\nabsence of a unified mathematical framework explaining the main types of neural\nLMs. We address the aforementioned problem in this tutorial where the objective\nis to explain neural LMs in a detailed, simplified and unambiguous mathematical\nframework accompanied with clear graphical illustrations. Concrete examples on\nwidely used models like BERT and GPT2 are explored. Finally, since transformers\npretrained on language-modeling-like tasks have been widely adopted in computer\nvision and time series applications, we briefly explore some examples of such\nsolutions in order to enable readers understand how transformers work in the\naforementioned domains and compare this use with the original one in NLP.\n",
        "title": "Anatomy of Neural Language Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03799",
        "abstract_url": "http://arxiv.org/abs/2401.03799",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Kai"
            },
            {
                "last_name": "Chen",
                "first_name": "Colin"
            },
            {
                "last_name": "Sung",
                "first_name": "Hyeontae"
            },
            {
                "last_name": "Ahn",
                "first_name": "Heejin"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Ian"
            },
            {
                "last_name": "Kamgarpour",
                "first_name": "Maryam"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We present a chance-constrained model predictive control (MPC) framework\nunder Gaussian mixture model (GMM) uncertainty. Specifically, we consider the\nuncertainty that arises from predicting future behaviors of moving obstacles,\nwhich may exhibit multiple modes (for example, turning left or right). To\naddress the multi-modal uncertainty distribution, we propose three MPC\nformulations: nominal chance-constrained planning, robust chance-constrained\nplanning, and contingency planning. We prove that closed-loop trajectories\ngenerated by the three planners are safe. The approaches differ in\nconservativeness and performance guarantee. In particular, the robust\nchance-constrained planner is recursively feasible under certain assumptions on\nthe propagation of prediction uncertainty. On the other hand, the contingency\nplanner generates a less conservative closed-loop trajectory than the nominal\nplanner. We validate our planners using state-of-the-art trajectory prediction\nalgorithms in autonomous driving simulators.\n",
        "title": "Safe Chance-constrained Model Predictive Control under Gaussian Mixture\n  Model Uncertainty",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03800",
        "abstract_url": "http://arxiv.org/abs/2401.03800",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Dong"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Gao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Lu",
                "first_name": "Yuxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingming"
            },
            {
                "last_name": "Guo",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  High-quality imaging is crucial for ensuring safety supervision and\nintelligent deployment in fields like transportation and industry. It enables\nprecise and detailed monitoring of operations, facilitating timely detection of\npotential hazards and efficient management. However, adverse weather\nconditions, such as atmospheric haziness and precipitation, can have a\nsignificant impact on image quality. When the atmosphere contains dense haze or\nwater droplets, the incident light scatters, leading to degraded captured\nimages. This degradation is evident in the form of image blur and reduced\ncontrast, increasing the likelihood of incorrect assessments and\ninterpretations by intelligent imaging systems (IIS). To address the challenge\nof restoring degraded images in hazy and rainy conditions, this paper proposes\na novel multi-view knowledge-guided scene recovery network (termed MvKSR).\nSpecifically, guided filtering is performed on the degraded image to separate\nhigh/low-frequency components. Subsequently, an en-decoder-based multi-view\nfeature coarse extraction module (MCE) is used to coarsely extract features\nfrom different views of the degraded image. The multi-view feature fine fusion\nmodule (MFF) will learn and infer the restoration of degraded images through\nmixed supervision under different views. Additionally, we suggest an atrous\nresidual block to handle global restoration and local repair in\nhazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR\noutperforms other state-of-the-art methods in terms of efficiency and stability\nfor restoring degraded scenarios in IIS.\n",
        "title": "MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy\n  Degradation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03804",
        "abstract_url": "http://arxiv.org/abs/2401.03804",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xinzhang"
            },
            {
                "last_name": "Liu",
                "first_name": "Shixuan"
            },
            {
                "last_name": "Yao",
                "first_name": "Yitong"
            },
            {
                "last_name": "Huang",
                "first_name": "Yuyao"
            },
            {
                "last_name": "He",
                "first_name": "Zhongjiang"
            },
            {
                "last_name": "Li",
                "first_name": "Xuelong"
            },
            {
                "last_name": "Li",
                "first_name": "Yongxiang"
            },
            {
                "last_name": "Che",
                "first_name": "Zhonghao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Wang",
                "first_name": "Xin"
            },
            {
                "last_name": "Pu",
                "first_name": "Luwen"
            },
            {
                "last_name": "Xu",
                "first_name": "Huihan"
            },
            {
                "last_name": "Fang",
                "first_name": "Ruiyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Huang",
                "first_name": "Xiaomeng"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhilong"
            },
            {
                "last_name": "Peng",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zheng",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Wang",
                "first_name": "Shiquan"
            },
            {
                "last_name": "Yang",
                "first_name": "Bingkai"
            },
            {
                "last_name": "he",
                "first_name": "Xuewei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Zhuoru"
            },
            {
                "last_name": "Xie",
                "first_name": "Qiyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yanhan"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongqiu"
            },
            {
                "last_name": "Shi",
                "first_name": "Lingling"
            },
            {
                "last_name": "Fu",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yin"
            },
            {
                "last_name": "Huang",
                "first_name": "Zilu"
            },
            {
                "last_name": "Xiong",
                "first_name": "Sishi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Wang",
                "first_name": "Chao"
            },
            {
                "last_name": "Song",
                "first_name": "Shuangyong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            ""
        ],
        "abstract": "  In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.\n",
        "title": "TeleChat Technical Report",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03806",
        "abstract_url": "http://arxiv.org/abs/2401.03806",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Canzong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Can"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongqiu"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianhao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Zinc electrolysis is one of the key processes in zinc smelting, and\nmaintaining stable operation of zinc electrolysis is an important factor in\nensuring production efficiency and product quality. However, poor contact\nbetween the zinc electrolysis cathode and the anode is a common problem that\nleads to reduced production efficiency and damage to the electrolysis cell.\nTherefore, online monitoring of the contact status of the plates is crucial for\nensuring production quality and efficiency. To address this issue, we propose\nan end-to-end network, the Frequency-masked Multimodal Autoencoder (FM-AE).\nThis method takes the cell voltage signal and infrared image information as\ninput, and through automatic encoding, fuses the two features together and\npredicts the poor contact status of the plates through a cascaded detector.\nExperimental results show that the proposed method maintains high accuracy\n(86.2%) while having good robustness and generalization ability, effectively\ndetecting poor contact status of the zinc electrolysis cell, providing strong\nsupport for production practice.\n",
        "title": "FM-AE: Frequency-masked Multimodal Autoencoder for Zinc Electrolysis\n  Plate Contact Abnormality Detection",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03807",
        "abstract_url": "http://arxiv.org/abs/2401.03807",
        "authors": [
            {
                "last_name": "Debris-Alazard",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fallahpour",
                "first_name": "Pouria"
            },
            {
                "last_name": "Stehl\u00e9",
                "first_name": "Damien"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The Learning With Errors ($\\mathsf{LWE}$) problem asks to find $\\mathbf{s}$\nfrom an input of the form $(\\mathbf{A}, \\mathbf{b} =\n\\mathbf{A}\\mathbf{s}+\\mathbf{e}) \\in (\\mathbb{Z}/q\\mathbb{Z})^{m \\times n}\n\\times (\\mathbb{Z}/q\\mathbb{Z})^{m}$, for a vector $\\mathbf{e}$ that has\nsmall-magnitude entries. In this work, we do not focus on solving\n$\\mathsf{LWE}$ but on the task of sampling instances. As these are extremely\nsparse in their range, it may seem plausible that the only way to proceed is to\nfirst create $\\mathbf{s}$ and $\\mathbf{e}$ and then set $\\mathbf{b} =\n\\mathbf{A}\\mathbf{s}+\\mathbf{e}$. In particular, such an instance sampler knows\nthe solution. This raises the question whether it is possible to obliviously\nsample $(\\mathbf{A}, \\mathbf{A}\\mathbf{s}+\\mathbf{e})$, namely, without knowing\nthe underlying $\\mathbf{s}$. A variant of the assumption that oblivious\n$\\mathsf{LWE}$ sampling is hard has been used in a series of works constructing\nSuccinct Non-interactive Arguments of Knowledge (SNARKs) in the standard model.\nAs the assumption is related to $\\mathsf{LWE}$, these SNARKs have been\nconjectured to be secure in the presence of quantum adversaries.\n  Our main result is a quantum polynomial-time algorithm that samples\nwell-distributed $\\mathsf{LWE}$ instances while provably not knowing the\nsolution, under the assumption that $\\mathsf{LWE}$ is hard. Moreover, the\napproach works for a vast range of $\\mathsf{LWE}$ parametrizations, including\nthose used in the above-mentioned SNARKs.\n",
        "title": "Quantum Oblivious LWE Sampling and Insecurity of Standard Model\n  Lattice-Based SNARKs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03811",
        "abstract_url": "http://arxiv.org/abs/2401.03811",
        "authors": [
            {
                "last_name": "Casares",
                "first_name": "Antonio"
            },
            {
                "last_name": "Mascle",
                "first_name": "Corto"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "LO",
            "",
            ""
        ],
        "abstract": "  In 2021, Casares, Colcombet and Fijalkow introduced the Alternating Cycle\nDecomposition (ACD), a structure used to define optimal transformations of\nMuller into parity automata and to obtain theoretical results about the\npossibility of relabelling automata with different acceptance conditions. In\nthis work, we study the complexity of computing the ACD and its DAG-version,\nproving that this can be done in polynomial time for suitable representations\nof the acceptance condition of the Muller automaton. As corollaries, we obtain\nthat we can decide typeness of Muller automata in polynomial time, as well as\nthe parity index of the languages they recognise.\n  Furthermore, we show that we can minimise in polynomial time the number of\ncolours (resp. Rabin pairs) defining a Muller (resp. Rabin) acceptance\ncondition, but that these problems become NP-hard when taking into account the\nstructure of an automaton using such a condition.\n",
        "title": "The Complexity of Simplifying $\\omega$-Automata through the Alternating\n  Cycle Decomposition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03812",
        "abstract_url": "http://arxiv.org/abs/2401.03812",
        "authors": [
            {
                "last_name": "Adamuz-Hinojosa",
                "first_name": "Oscar"
            },
            {
                "last_name": "Zanzi",
                "first_name": "Lanfranco"
            },
            {
                "last_name": "Sciancalepore",
                "first_name": "Vincenzo"
            },
            {
                "last_name": "Garcia-Saavedra",
                "first_name": "Andres"
            },
            {
                "last_name": "Costa-P\u00e9rez",
                "first_name": "Xavier"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The Open Radio Access Network (O-RAN)-compliant solutions lack crucial\ndetails to perform effective control loops at multiple time scales. In this\nvein, we propose ORANUS, an O-RAN-compliant mathematical framework to allocate\nradio resources to multiple ultra Reliable Low Latency Communication (uRLLC)\nservices. In the near-RT control loop, ORANUS relies on a novel Stochastic\nNetwork Calculus (SNC)-based model to compute the amount of guaranteed radio\nresources for each uRLLC service. Unlike traditional approaches as queueing\ntheory, the SNC-based model allows ORANUS to ensure the probability the packet\ntransmission delay exceeds a budget, i.e., the violation probability, is below\na target tolerance. ORANUS also utilizes a RT control loop to monitor service\ntransmission queues, dynamically adjusting the guaranteed radio resources based\non detected traffic anomalies. To the best of our knowledge, ORANUS is the\nfirst O-RAN-compliant solution which benefits from SNC to carry out near-RT and\nRT control loops. Simulation results show that ORANUS significantly improves\nover reference solutions, with an average violation probability 10x lower.\n",
        "title": "ORANUS: Latency-tailored Orchestration via Stochastic Network Calculus\n  in 6G O-RAN",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03814",
        "abstract_url": "http://arxiv.org/abs/2401.03814",
        "authors": [
            {
                "last_name": "Infante-Sainz",
                "first_name": "Ra\u00fal"
            },
            {
                "last_name": "Akhlaghi",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV"
        ],
        "abstract": "  Color plays a crucial role in the visualization, interpretation, and analysis\nof multi-wavelength astronomical images. However, generating color images that\naccurately represent the full dynamic range of astronomical sources is\nchallenging. In response, Gnuastro v0.22 introduces the program\n'astscript-color-faint-gray', which is extensively documented in the Gnuastro\nmanual. It employs a non-linear transformation to assign an 8-bit RGB\n(Red-Green-Blue) value to brighter pixels, while the fainter ones are shown in\nan inverse grayscale. This approach enables the simultaneous visualization of\nlow surface brightness features within the same image. This research note is\nreproducible with Maneage, on the Git commit 48f5408.\n",
        "title": "Gnuastro: visualizing the full dynamic range in color images",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03816",
        "abstract_url": "http://arxiv.org/abs/2401.03816",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Yusheng"
            },
            {
                "last_name": "Li",
                "first_name": "Jingyu"
            },
            {
                "last_name": "Lee",
                "first_name": "Tan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  This research is about the creation of personalized synthetic voices for head\nand neck cancer survivors. It is focused particularly on tongue cancer patients\nwhose speech might exhibit severe articulation impairment. Our goal is to\nrestore normal articulation in the synthesized speech, while maximally\npreserving the target speaker's individuality in terms of both the voice timbre\nand speaking style. This is formulated as a task of learning from noisy labels.\nWe propose to augment the commonly used speech reconstruction loss with two\nadditional terms. The first term constitutes a regularization loss that\nmitigates the impact of distorted articulation in the training speech. The\nsecond term is a consistency loss that encourages correct articulation in the\ngenerated speech. These additional loss terms are obtained from frame-level\narticulation scores of original and generated speech, which are derived using a\nseparately trained phone classifier. Experimental results on a real case of\ntongue cancer patient confirm that the synthetic voice achieves comparable\narticulation quality to unimpaired natural speech, while effectively\nmaintaining the target speaker's individuality. Audio samples are available at\nhttps://myspeechproject.github.io/ArticulationRepair/.\n",
        "title": "Creating Personalized Synthetic Voices from Articulation Impaired Speech\n  Using Augmented Reconstruction Loss",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03820",
        "abstract_url": "http://arxiv.org/abs/2401.03820",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "T. Tony"
            },
            {
                "last_name": "Xia",
                "first_name": "Dong"
            },
            {
                "last_name": "Zha",
                "first_name": "Mengyue"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "",
            ""
        ],
        "abstract": "  Estimating a covariance matrix and its associated principal components is a\nfundamental problem in contemporary statistics. While optimal estimation\nprocedures have been developed with well-understood properties, the increasing\ndemand for privacy preservation introduces new complexities to this classical\nproblem. In this paper, we study optimal differentially private Principal\nComponent Analysis (PCA) and covariance estimation within the spiked covariance\nmodel.\n  We precisely characterize the sensitivity of eigenvalues and eigenvectors\nunder this model and establish the minimax rates of convergence for estimating\nboth the principal components and covariance matrix. These rates hold up to\nlogarithmic factors and encompass general Schatten norms, including spectral\nnorm, Frobenius norm, and nuclear norm as special cases.\n  We introduce computationally efficient differentially private estimators and\nprove their minimax optimality, up to logarithmic factors. Additionally,\nmatching minimax lower bounds are established. Notably, in comparison with\nexisting literature, our results accommodate a diverging rank, necessitate no\neigengap condition between distinct principal components, and remain valid even\nif the sample size is much smaller than the dimension.\n",
        "title": "Optimal Differentially Private PCA and Estimation for Spiked Covariance\n  Matrices",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03824",
        "abstract_url": "http://arxiv.org/abs/2401.03824",
        "authors": [
            {
                "last_name": "Bucarelli",
                "first_name": "Maria Sofia"
            },
            {
                "last_name": "D'Inverno",
                "first_name": "Giuseppe Alessio"
            },
            {
                "last_name": "Bianchini",
                "first_name": "Monica"
            },
            {
                "last_name": "Scarselli",
                "first_name": "Franco"
            },
            {
                "last_name": "Silvestri",
                "first_name": "Fabrizio"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In the context of deep learning models, attention has recently been paid to\nstudying the surface of the loss function in order to better understand\ntraining with methods based on gradient descent. This search for an appropriate\ndescription, both analytical and topological, has led to numerous efforts to\nidentify spurious minima and characterize gradient dynamics. Our work aims to\ncontribute to this field by providing a topological measure to evaluate loss\ncomplexity in the case of multilayer neural networks. We compare deep and\nshallow architectures with common sigmoidal activation functions by deriving\nupper and lower bounds on the complexity of their loss function and revealing\nhow that complexity is influenced by the number of hidden units, training\nmodels, and the activation function used. Additionally, we found that certain\nvariations in the loss function or model architecture, such as adding an\n$\\ell_2$ regularization term or implementing skip connections in a feedforward\nnetwork, do not affect loss topology in specific cases.\n",
        "title": "A topological description of loss surfaces based on Betti Numbers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03828",
        "abstract_url": "http://arxiv.org/abs/2401.03828",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Fengchao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            },
            {
                "last_name": "Gao",
                "first_name": "Eryang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Gesture recognition is an indispensable component of natural and efficient\nhuman-computer interaction technology, particularly in desktop-level\napplications, where it can significantly enhance people's productivity.\nHowever, the current gesture recognition community lacks a suitable\ndesktop-level (top-view perspective) dataset for lightweight gesture capture\ndevices. In this study, we have established a dataset named GR4DHCI. What\ndistinguishes this dataset is its inherent naturalness, intuitive\ncharacteristics, and diversity. Its primary purpose is to serve as a valuable\nresource for the development of desktop-level portable applications. GR4DHCI\ncomprises over 7,000 gesture samples and a total of 382,447 frames for both\nStereo IR and skeletal modalities. We also address the variances in hand\npositioning during desktop interactions by incorporating 27 different hand\npositions into the dataset. Building upon the GR4DHCI dataset, we conducted a\nseries of experimental studies, the results of which demonstrate that the\nfine-grained classification blocks proposed in this paper can enhance the\nmodel's recognition accuracy. Our dataset and experimental findings presented\nin this paper are anticipated to propel advancements in desktop-level gesture\nrecognition research.\n",
        "title": "A multimodal gesture recognition dataset for desktop human-computer\n  interaction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03830",
        "abstract_url": "http://arxiv.org/abs/2401.03830",
        "authors": [
            {
                "last_name": "Aouad",
                "first_name": "Theodore"
            },
            {
                "last_name": "Talbot",
                "first_name": "Hugues"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Training and running deep neural networks (NNs) often demands a lot of\ncomputation and energy-intensive specialized hardware (e.g. GPU, TPU...). One\nway to reduce the computation and power cost is to use binary weight NNs, but\nthese are hard to train because the sign function has a non-smooth gradient. We\npresent a model based on Mathematical Morphology (MM), which can binarize\nConvNets without losing performance under certain conditions, but these\nconditions may not be easy to satisfy in real-world scenarios. To solve this,\nwe propose two new approximation methods and develop a robust theoretical\nframework for ConvNets binarization using MM. We propose as well regularization\nlosses to improve the optimization. We empirically show that our model can\nlearn a complex morphological network, and explore its performance on a\nclassification task.\n",
        "title": "A foundation for exact binarized morphological neural networks",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03831",
        "abstract_url": "http://arxiv.org/abs/2401.03831",
        "authors": [
            {
                "last_name": "Vickers",
                "first_name": "Peter"
            },
            {
                "last_name": "Barrault",
                "first_name": "Lo\u00efc"
            },
            {
                "last_name": "Monti",
                "first_name": "Emilio"
            },
            {
                "last_name": "Aletras",
                "first_name": "Nikolaos"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  In Natural Language Processing (NLP) classification tasks such as topic\ncategorisation and sentiment analysis, model generalizability is generally\nmeasured with standard metrics such as Accuracy, F-Measure, or AUC-ROC. The\ndiversity of metrics, and the arbitrariness of their application suggest that\nthere is no agreement within NLP on a single best metric to use. This lack\nsuggests there has not been sufficient examination of the underlying heuristics\nwhich each metric encodes. To address this we compare several standard\nclassification metrics with more 'exotic' metrics and demonstrate that a\nrandom-guess normalised Informedness metric is a parsimonious baseline for task\nperformance. To show how important the choice of metric is, we perform\nextensive experiments on a wide range of NLP tasks including a synthetic\nscenario, natural language understanding, question answering and machine\ntranslation. Across these tasks we use a superset of metrics to rank models and\nfind that Informedness best captures the ideal model characteristics. Finally,\nwe release a Python implementation of Informedness following the SciKitLearn\nclassifier format.\n",
        "title": "We Need to Talk About Classification Evaluation Metrics in NLP",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03833",
        "abstract_url": "http://arxiv.org/abs/2401.03833",
        "authors": [
            {
                "last_name": "Motger",
                "first_name": "Quim"
            },
            {
                "last_name": "Miaschi",
                "first_name": "Alessio"
            },
            {
                "last_name": "Dell'Orletta",
                "first_name": "Felice"
            },
            {
                "last_name": "Franch",
                "first_name": "Xavier"
            },
            {
                "last_name": "Marco",
                "first_name": "Jordi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Mobile app reviews are a large-scale data source for software-related\nknowledge generation activities, including software maintenance, evolution and\nfeedback analysis. Effective extraction of features (i.e., functionalities or\ncharacteristics) from these reviews is key to support analysis on the\nacceptance of these features, identification of relevant new feature requests\nand prioritization of feature development, among others. Traditional methods\nfocus on syntactic pattern-based approaches, typically context-agnostic,\nevaluated on a closed set of apps, difficult to replicate and limited to a\nreduced set and domain of apps. Meanwhile, the pervasiveness of Large Language\nModels (LLMs) based on the Transformer architecture in software engineering\ntasks lays the groundwork for empirical evaluation of the performance of these\nmodels to support feature extraction. In this study, we present T-FREX, a\nTransformer-based, fully automatic approach for mobile app review feature\nextraction. First, we collect a set of ground truth features from users in a\nreal crowdsourced software recommendation platform and transfer them\nautomatically into a dataset of app reviews. Then, we use this newly created\ndataset to fine-tune multiple LLMs on a named entity recognition task under\ndifferent data configurations. We assess the performance of T-FREX with respect\nto this ground truth, and we complement our analysis by comparing T-FREX with a\nbaseline method from the field. Finally, we assess the quality of new features\npredicted by T-FREX through an external human evaluation. Results show that\nT-FREX outperforms on average the traditional syntactic-based method,\nespecially when discovering new features from a domain for which the model has\nbeen fine-tuned.\n",
        "title": "T-FREX: A Transformer-based Feature Extraction Method from Mobile App\n  Reviews",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03835",
        "abstract_url": "http://arxiv.org/abs/2401.03835",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Qiang"
            },
            {
                "last_name": "Souza",
                "first_name": "Matheus"
            },
            {
                "last_name": "Choi",
                "first_name": "Eunsue"
            },
            {
                "last_name": "Shin",
                "first_name": "Suhyun"
            },
            {
                "last_name": "Baek",
                "first_name": "Seung-Hwan"
            },
            {
                "last_name": "Heidrich",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Hyperspectral imaging empowers computer vision systems with the distinct\ncapability of identifying materials through recording their spectral\nsignatures. Recent efforts in data-driven spectral reconstruction aim at\nextracting spectral information from RGB images captured by cost-effective RGB\ncameras, instead of dedicated hardware.\n  In this paper we systematically analyze the performance of such methods,\nevaluating both the practical limitations with respect to current datasets and\noverfitting, as well as fundamental limits with respect to the nature of the\ninformation encoded in the RGB images, and the dependency of this information\non the optical system of the camera.\n  We find that the current models are not robust under slight variations, e.g.,\nin noise level or compression of the RGB file. Both the methods and the\ndatasets are also limited in their ability to cope with metameric colors. This\nissue can in part be overcome with metameric data augmentation. Moreover,\noptical lens aberrations can help to improve the encoding of the metameric\ninformation into the RGB image, which paves the road towards higher performing\nspectral imaging and reconstruction approaches.\n",
        "title": "Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware\n  Analysis",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03844",
        "abstract_url": "http://arxiv.org/abs/2401.03844",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Bingyin"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhiding"
            },
            {
                "last_name": "Lan",
                "first_name": "Shiyi"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yutao"
            },
            {
                "last_name": "Anandkumar",
                "first_name": "Anima"
            },
            {
                "last_name": "Lao",
                "first_name": "Yingjie"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Jose M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent studies indicate that Vision Transformers (ViTs) are robust against\nout-of-distribution scenarios. In particular, the Fully Attentional Network\n(FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In\nthis paper, we revisit the FAN models and improve their pre-training with a\nself-emerging token labeling (STL) framework. Our method contains a two-stage\ntraining framework. Specifically, we first train a FAN token labeler (FAN-TL)\nto generate semantically meaningful patch token labels, followed by a FAN\nstudent model training stage that uses both the token labels and the original\nclass label. With the proposed STL framework, our best model based on\nFAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on\nImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A\n(46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the\noriginal FAN counterpart by significant margins. The proposed framework also\ndemonstrates significantly enhanced performance on downstream tasks such as\nsemantic segmentation, with up to 1.7% improvement in robustness over the\ncounterpart model. Code is available at https://github.com/NVlabs/STL.\n",
        "title": "Fully Attentional Networks with Self-emerging Token Labeling",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03846",
        "abstract_url": "http://arxiv.org/abs/2401.03846",
        "authors": [
            {
                "last_name": "Choi",
                "first_name": "Hyunjun"
            },
            {
                "last_name": "Jeong",
                "first_name": "Hawook"
            },
            {
                "last_name": "Choi",
                "first_name": "Jin Young"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  In this paper, we raise a new issue on Unidentified Foreground Object (UFO)\ndetection in 3D point clouds, which is a crucial technology in autonomous\ndriving in the wild. UFO detection is challenging in that existing 3D object\ndetectors encounter extremely hard challenges in both 3D localization and\nOut-of-Distribution (OOD) detection. To tackle these challenges, we suggest a\nnew UFO detection framework including three tasks: evaluation protocol,\nmethodology, and benchmark. The evaluation includes a new approach to measure\nthe performance on our goal, i.e. both localization and OOD detection of UFOs.\nThe methodology includes practical techniques to enhance the performance of our\ngoal. The benchmark is composed of the KITTI Misc benchmark and our additional\nsynthetic benchmark for modeling a more diverse range of UFOs. The proposed\nframework consistently enhances performance by a large margin across all four\nbaseline detectors: SECOND, PointPillars, PV-RCNN, and PartA2, giving insight\nfor future work on UFO detection in the wild.\n",
        "title": "UFO: Unidentified Foreground Object Detection in 3D Point Cloud",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03848",
        "abstract_url": "http://arxiv.org/abs/2401.03848",
        "authors": [
            {
                "last_name": "Ouaguid",
                "first_name": "Abdellah"
            },
            {
                "last_name": "Hanine",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Chiba",
                "first_name": "Zouhair"
            },
            {
                "last_name": "Abghour",
                "first_name": "Noreddine"
            },
            {
                "last_name": "Ghazal",
                "first_name": "Hassan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CY"
        ],
        "abstract": "  No one can dispute the disruptive impact of blockchain technology, which has\nlong been considered one of the major revolutions of contemporary times. Its\nintegration into the healthcare ecosystem has helped overcome numerous\ndifficulties and constraints faced by healthcare systems. This has been notably\ndemonstrated in the meticulous management of electronic health records (EHR)\nand their access rights, as well as in its capabilities in terms of security,\nscalability, flexibility, and interoperability with other systems. This article\nundertakes the study and analysis of the most commonly adopted approaches in\nhealthcare data management systems using blockchain technology. An evaluation\nis then conducted based on a set of observed common characteristics,\ndistinguishing one approach from the others. The results of this analysis\nhighlight the advantages and limitations of each approach, thus facilitating\nthe choice of the method best suited to the readers' specific case study.\nFurthermore, for effective implementation in the context of e-health, we\nemphasize the existence of crucial challenges, such as the incomplete\nrepresentation of major stakeholders in the blockchain network, the lack of\nregulatory flexibility to ensure legal interoperability by country, and the\ninsufficient integration of an official regulatory authority ensuring\ncompliance with ethical and legal standards. To address these challenges, it is\nnecessary to establish close collaboration between regulators, technology\ndevelopers, and healthcare stakeholders.\n",
        "title": "Analysis of Blockchain Integration in the e-Healthcare Ecosystem",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03850",
        "abstract_url": "http://arxiv.org/abs/2401.03850",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Jin Woo"
            },
            {
                "last_name": "An",
                "first_name": "Gwang Seok"
            },
            {
                "last_name": "Sun",
                "first_name": "Jeong-Yun"
            },
            {
                "last_name": "Lee",
                "first_name": "Kyogu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  This paper delves into the analysis of nonlinear deformation induced by\ndielectric actuation in pre-stressed ideal dielectric elastomers. It formulates\na nonlinear ordinary differential equation governing this deformation based on\nthe hyperelastic model under dielectric stress. Through numerical integration\nand neural network approximations, the relationship between voltage and stretch\nis established. Neural networks are employed to approximate solutions for\nvoltage-to-stretch and stretch-to-voltage transformations obtained via an\nexplicit Runge-Kutta method. The effectiveness of these approximations is\ndemonstrated by leveraging them for compensating nonlinearity through the\nwaveshaping of the input signal. The comparative analysis highlights the\nsuperior accuracy of the approximated solutions over baseline methods,\nresulting in minimized harmonic distortions when utilizing dielectric\nelastomers as acoustic actuators. This study underscores the efficacy of the\nproposed approach in mitigating nonlinearities and enhancing the performance of\ndielectric elastomers in acoustic actuation applications.\n",
        "title": "Inverse Nonlinearity Compensation of Hyperelastic Deformation in\n  Dielectric Elastomer for Acoustic Actuation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03851",
        "abstract_url": "http://arxiv.org/abs/2401.03851",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Shuxiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Linyuan"
            },
            {
                "last_name": "Hou",
                "first_name": "Senbao"
            },
            {
                "last_name": "Yan",
                "first_name": "Bin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recently, there has been a surge in the popularity of pre trained large\nlanguage models (LLMs) (such as GPT-4), sweeping across the entire Natural\nLanguage Processing (NLP) and Computer Vision (CV) communities. These LLMs have\ndemonstrated advanced multi-modal understanding capabilities and showcased\nstrong performance across various benchmarks. The LLM has started to embody\ntraits of artificial general intelligence, which holds vital guidance for\nenhancing brain-like characteristics within visual encoding models. Hence, This\npaper proposes a new multi-modal training paradigm, aligning with LLM, for\nencoding fMRI activity in visual cortex. Based on this paradigm, we trained an\nencoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).\nSpecifically, we utilize LLM (miniGPT4) to generate descriptive text for all\nstimulus images, forming a high-quality textual description set. Moreover, we\nuse the pre-trained text encoder (CLIP) to process these detailed descriptions,\nobtaining the text embedding features. Next, we use the contrast loss function\nto minimize the distance between the image embedding features and the text\nembedding features to complete the alignment operation of the stimulus image\nand text information. With the assistance of the pre-trained LLM, this\nalignment process facilitates better learning of the visual encoding model,\nresulting in higher precision. The final experimental results indicate that our\ntraining paradigm has significantly aided in enhancing the performance of the\nvisual encoding model.\n",
        "title": "Aligned with LLM: a new multi-modal training paradigm for encoding fMRI\n  activity in visual cortex",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03852",
        "abstract_url": "http://arxiv.org/abs/2401.03852",
        "authors": [
            {
                "last_name": "Ghazalian",
                "first_name": "Reza"
            },
            {
                "last_name": "Alexandropoulos",
                "first_name": "George C."
            },
            {
                "last_name": "Seco-Granados",
                "first_name": "Gonzalo"
            },
            {
                "last_name": "Wymeersch",
                "first_name": "Henk"
            },
            {
                "last_name": "J\u00e4ntti",
                "first_name": "Riku"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "ET"
        ],
        "abstract": "  The latest assessments of the emerging technologies for reconfigurable\nintelligent surfaces (RISs) have indicated the concept's significant potential\nfor localization and sensing, either as individual or simultaneously realized\ntasks. However, in the vast majority of those studies, the RIS state (i.e., its\nposition and rotation angles) is required to be known a priori. In this paper,\nwe address the problem of the joint three-dimensional (3D) localization of a\nhybrid RIS (HRIS) and a user. The most cost- and power-efficient hybrid version\nof an RIS is equipped with a single reception radio-frequency chain and\nmeta-atoms capable of simultaneous reconfigurable reflection and sensing. This\ndual functionality is controlled by adjustable power splitters embedded at each\nhybrid meta-atom. Focusing on a downlink scenario where a multi-antenna base\nstation transmits multicarrier signals to a user via an HRIS, we propose a\nmultistage approach to jointly estimate the metasurface's 3D position and 3D\nrotation matrix (i.e., 6D parameter estimation) as well as the user's 3D\nposition. Our simulation results verify the validity of the proposed estimator\nvia extensive comparisons of the root-mean-square error of the state\nestimations with the Cram\\'{e}r-Rao lower bound (CRB), which is analytically\nderived. Furthermore, it is showcased that there exists an optimal hybrid\nreconfigurable intelligent surface (HRIS) power splitting ratio for the desired\nmulti-parameter estimation problem. We also study the robustness of the\nproposed method in the presence of scattering points in the wireless\npropagation environment.\n",
        "title": "Joint 3D User and 6D Hybrid Reconfigurable Intelligent Surface\n  Localization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03854",
        "abstract_url": "http://arxiv.org/abs/2401.03854",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Jiquan"
            },
            {
                "last_name": "Cao",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Che",
                "first_name": "Jinming"
            },
            {
                "last_name": "Wang",
                "first_name": "Qinyuan"
            },
            {
                "last_name": "Liang",
                "first_name": "Sen"
            },
            {
                "last_name": "Ren",
                "first_name": "Wei"
            },
            {
                "last_name": "Lin",
                "first_name": "Jinlong"
            },
            {
                "last_name": "Cao",
                "first_name": "Xixin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the\nquality of AI-generated images from a human perception perspective, has emerged\nas a new topic in computer vision. Unlike common image quality assessment tasks\nwhere images are derived from original ones distorted by noise, blur, and\ncompression, in AIGCIQA tasks, images are typically generated by generative\nmodels using text prompts. Considerable efforts have been made in the past\nyears to advance AIGCIQA. However, most existing AIGCIQA methods regress\npredicted scores directly from individual generated images, overlooking the\ninformation contained in the text prompts of these images. This oversight\npartially limits the performance of these AIGCIQA methods. To address this\nissue, we propose a text and image encoder-based regression (TIER) framework.\nSpecifically, we process the generated images and their corresponding text\nprompts as inputs, utilizing a text encoder and an image encoder to extract\nfeatures from these text prompts and generated images, respectively. To\ndemonstrate the effectiveness of our proposed TIER method, we conduct extensive\nexperiments on several mainstream AIGCIQA databases, including AGIQA-1K,\nAGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed\nTIER method generally demonstrates superior performance compared to baseline in\nmost cases.\n",
        "title": "TIER: Text and Image Encoder-based Regression for AIGC Image Quality\n  Assessment",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03855",
        "abstract_url": "http://arxiv.org/abs/2401.03855",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Singh",
                "first_name": "Mayank"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Motivated by the increasing popularity of code generation from human\ndescriptions using large language models (LLMs), several benchmarks have been\nproposed to assess the capabilities of existing and emerging models. This study\npresents a large-scale human evaluation of HumanEval and MBPP, two widely used\nbenchmarks for Python code generation, focusing on their diversity and\ndifficulty. Our findings reveal a significant bias towards a limited number of\nprogramming concepts, with negligible or no representation of most concepts.\nAdditionally, we identify a concerningly high proportion of easy programming\nquestions, potentially leading to an overestimation of model performance on\ncode generation tasks.\n",
        "title": "Boldly Going Where No Benchmark Has Gone Before: Exposing Bias and\n  Shortcomings in Code Generation Evaluation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03856",
        "abstract_url": "http://arxiv.org/abs/2401.03856",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Haojie"
            },
            {
                "last_name": "Cao",
                "first_name": "Xueke"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Dan",
                "first_name": "Tangpeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Yueguo"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  Singing voice separation and vocal pitch estimation are pivotal tasks in\nmusic information retrieval. Existing methods for simultaneous extraction of\nclean vocals and vocal pitches can be classified into two categories: pipeline\nmethods and naive joint learning methods. However, the efficacy of these\nmethods is limited by the following problems: On the one hand, pipeline methods\ntrain models for each task independently, resulting a mismatch between the data\ndistributions at the training and testing time. On the other hand, naive joint\nlearning methods simply add the losses of both tasks, possibly leading to a\nmisalignment between the distinct objectives of each task. To solve these\nproblems, we propose a Deep Joint Cascade Model (DJCM) for singing voice\nseparation and vocal pitch estimation. DJCM employs a novel joint cascade model\nstructure to concurrently train both tasks. Moreover, task-specific weights are\nused to align different objectives of both tasks. Experimental results show\nthat DJCM achieves state-of-the-art performance on both tasks, with great\nimprovements of 0.45 in terms of Signal-to-Distortion Ratio (SDR) for singing\nvoice separation and 2.86% in terms of Overall Accuracy (OA) for vocal pitch\nestimation. Furthermore, extensive ablation studies validate the effectiveness\nof each design of our proposed model. The code of DJCM is available at\nhttps://github.com/Dream-High/DJCM .\n",
        "title": "DJCM: A Deep Joint Cascade Model for Singing Voice Separation and Vocal\n  Pitch Estimation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03857",
        "abstract_url": "http://arxiv.org/abs/2401.03857",
        "authors": [
            {
                "last_name": "Poiani",
                "first_name": "Riccardo"
            },
            {
                "last_name": "Curti",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Metelli",
                "first_name": "Alberto Maria"
            },
            {
                "last_name": "Restelli",
                "first_name": "Marcello"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Inverse Reinforcement Learning (IRL) techniques deal with the problem of\ndeducing a reward function that explains the behavior of an expert agent who is\nassumed to act optimally in an underlying unknown task. In several problems of\ninterest, however, it is possible to observe the behavior of multiple experts\nwith different degree of optimality (e.g., racing drivers whose skills ranges\nfrom amateurs to professionals). For this reason, in this work, we extend the\nIRL formulation to problems where, in addition to demonstrations from the\noptimal agent, we can observe the behavior of multiple sub-optimal experts.\nGiven this problem, we first study the theoretical properties of the class of\nreward functions that are compatible with a given set of experts, i.e., the\nfeasible reward set. Our results show that the presence of multiple sub-optimal\nexperts can significantly shrink the set of compatible rewards. Furthermore, we\nstudy the statistical complexity of estimating the feasible reward set with a\ngenerative model. To this end, we analyze a uniform sampling algorithm that\nresults in being minimax optimal whenever the sub-optimal experts' performance\nlevel is sufficiently close to the one of the optimal agent.\n",
        "title": "Inverse Reinforcement Learning with Sub-optimal Experts",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03861",
        "abstract_url": "http://arxiv.org/abs/2401.03861",
        "authors": [
            {
                "last_name": "Takazawa",
                "first_name": "Kenjiro"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "DM",
            ""
        ],
        "abstract": "  Congestion games offer a primary model in the study of pure Nash equilibria\nin non-cooperative games, and a number of generalized models have been proposed\nin the literature. One line of generalization includes weighted congestion\ngames, in which the cost of a resource is a function of the total weight of the\nplayers choosing that resource. Another line includes congestion games with\nmixed costs, in which the cost imposed on a player is a convex combination of\nthe total cost and the maximum cost of the resources in her strategy. This\nmodel is further generalized to that of congestion games with\ncomplementarities. For the above models, the existence of a pure Nash\nequilibrium is proved under some assumptions, including that the strategy space\nof each player is the base family of a matroid and that the cost functions have\na certain kind of monotonicity. In this paper, we deal with common\ngeneralizations of these two lines, namely weighted matroid congestion games\nwith complementarities, and its further generalization. Our main technical\ncontribution is a proof of the existence of pure Nash equilibria in these\ngeneralized models under a simplified assumption on the monotonicity, which\nprovide a common extension of the previous results. We also present some\nextensions on the existence of pure Nash equilibria in player-specific and\nweighted matroid congestion games with mixed costs.\n",
        "title": "Pure Nash Equilibria in Weighted Congestion Games with Complementarities\n  and Beyond",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03862",
        "abstract_url": "http://arxiv.org/abs/2401.03862",
        "authors": [
            {
                "last_name": "Lai",
                "first_name": "Qingsi"
            },
            {
                "last_name": "Yao",
                "first_name": "Lin"
            },
            {
                "last_name": "Gao",
                "first_name": "Zhifeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongshuai"
            },
            {
                "last_name": "Lu",
                "first_name": "Shuqi"
            },
            {
                "last_name": "He",
                "first_name": "Di"
            },
            {
                "last_name": "Wang",
                "first_name": "Liwei"
            },
            {
                "last_name": "Wang",
                "first_name": "Cheng"
            },
            {
                "last_name": "Ke",
                "first_name": "Guolin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Powder X-ray diffraction (PXRD) is a crucial means for crystal structure\ndetermination. Such determination often involves external database matching to\nfind a structural analogue and Rietveld refinement to obtain finer structure.\nHowever, databases may be incomplete and Rietveld refinement often requires\nintensive trial-and-error efforts from trained experimentalists, which remains\nineffective in practice. To settle these issues, we propose XtalNet, the first\nend-to-end deep learning-based framework capable of ab initio generation of\ncrystal structures that accurately match given PXRD patterns. The model employs\ncontrastive learning and Diffusion-based conditional generation to enable the\nsimultaneous execution of two tasks: crystal structure retrieval based on PXRD\npatterns and conditional structure generations. To validate the effectiveness\nof XtalNet, we curate a much more challenging and practical dataset hMOF-100,\nXtalNet performs well on this dataset, reaching 96.3\\% top-10 hit ratio on the\ndatabase retrieval task and 95.0\\% top-10 match rate on the ranked structure\ngeneration task.\n",
        "title": "End-to-End Crystal Structure Prediction from Powder X-Ray Diffraction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03864",
        "abstract_url": "http://arxiv.org/abs/2401.03864",
        "authors": [
            {
                "last_name": "Magnusson",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  The Domain Name System (DNS) comprises name servers translating domain names\ninto, commonly, IP addresses. Authoritative name servers hosts the resource\nrecords (RR) for certain zones, and resolver name servers are responsible for\nquerying and answering DNS queries on behalf of their clients. Unfortunately,\ncybercriminals often use DNS for malicious purposes, such as phishing, malware\ndistribution, and botnet communication. To combat these threats, filtering\nresolvers have become increasingly popular, employing various techniques to\nidentify and block malicious requests. In this paper, we survey several\ntechniques to implement and enhance the capabilities of filtering resolvers\nincluding response policy zones, threat intelligence feeds, and detection of\nalgorithmically generated domains. We identify the current trends of each area\nand find missing intersections in the literature, which could be used to\nimprove the effectiveness of filtering resolvers. In addition, we propose\nfuture work designing a framework for filtering resolvers using\nstate-of-the-art approaches identified in this study.\n",
        "title": "Survey and Analysis of DNS Filtering Components",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03865",
        "abstract_url": "http://arxiv.org/abs/2401.03865",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Shiluo"
            },
            {
                "last_name": "Liu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Deng",
                "first_name": "Ye"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Forecasting the trend of stock prices is an enduring topic at the\nintersection of finance and computer science. Incremental updates to\nforecasters have proven effective in alleviating the impacts of concept drift\narising from non-stationary stock markets. However, there is a need for\nrefinement in the incremental learning of stock trends, as existing methods\ndisregard recurring patterns. To address this issue, we propose meta-learning\nwith dynamic adaptation (MetaDA) for the incremental learning of stock trends,\nwhich performs dynamic model adaptation considering both the recurring and\nemerging patterns. We initially organize the stock trend forecasting into\nmeta-learning tasks and train a forecasting model following meta-learning\nprotocols. During model adaptation, MetaDA adapts the forecasting model with\nthe latest data and a selected portion of historical data, which is dynamically\nidentified by a task inference module. The task inference module first extracts\ntask-level embeddings from the historical tasks, and then identifies the\ninformative data with a task inference network. MetaDA has been evaluated on\nreal-world stock datasets, achieving state-of-the-art performance with\nsatisfactory efficiency.\n",
        "title": "Incremental Learning of Stock Trends via Meta-Learning with Dynamic\n  Adaptation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03870",
        "abstract_url": "http://arxiv.org/abs/2401.03870",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Hui"
            },
            {
                "last_name": "Ma",
                "first_name": "Zhiheng"
            },
            {
                "last_name": "Hong",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Shangguan",
                "first_name": "Qinnan"
            },
            {
                "last_name": "Meng",
                "first_name": "Deyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Transformer has been popular in recent crowd counting work since it breaks\nthe limited receptive field of traditional CNNs. However, since crowd images\nalways contain a large number of similar patches, the self-attention mechanism\nin Transformer tends to find a homogenized solution where the attention maps of\nalmost all patches are identical. In this paper, we address this problem by\nproposing Gramformer: a graph-modulated transformer to enhance the network by\nadjusting the attention and input node features respectively on the basis of\ntwo different types of graphs. Firstly, an attention graph is proposed to\ndiverse attention maps to attend to complementary information. The graph is\nbuilding upon the dissimilarities between patches, modulating the attention in\nan anti-similarity fashion. Secondly, a feature-based centrality encoding is\nproposed to discover the centrality positions or importance of nodes. We encode\nthem with a proposed centrality indices scheme to modulate the node features\nand similarity relationships. Extensive experiments on four challenging crowd\ncounting datasets have validated the competitiveness of the proposed method.\nCode is available at {https://github.com/LoraLinH/Gramformer}.\n",
        "title": "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03872",
        "abstract_url": "http://arxiv.org/abs/2401.03872",
        "authors": [
            {
                "last_name": "Lukezic",
                "first_name": "Alan"
            },
            {
                "last_name": "Trojer",
                "first_name": "Ziga"
            },
            {
                "last_name": "Matas",
                "first_name": "Jiri"
            },
            {
                "last_name": "Kristan",
                "first_name": "Matej"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Performance of modern trackers degrades substantially on transparent objects\ncompared to opaque objects. This is largely due to two distinct reasons.\nTransparent objects are unique in that their appearance is directly affected by\nthe background. Furthermore, transparent object scenes often contain many\nvisually similar objects (distractors), which often lead to tracking failure.\nHowever, development of modern tracking architectures requires large training\nsets, which do not exist in transparent object tracking. We present two\ncontributions addressing the aforementioned issues. We propose the first\ntransparent object tracking training dataset Trans2k that consists of over 2k\nsequences with 104,343 images overall, annotated by bounding boxes and\nsegmentation masks. Standard trackers trained on this dataset consistently\nimprove by up to 16%. Our second contribution is a new distractor-aware\ntransparent object tracker (DiTra) that treats localization accuracy and target\nidentification as separate tasks and implements them by a novel architecture.\nDiTra sets a new state-of-the-art in transparent object tracking and\ngeneralizes well to opaque objects.\n",
        "title": "A New Dataset and a Distractor-Aware Architecture for Transparent Object\n  Tracking",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03878",
        "abstract_url": "http://arxiv.org/abs/2401.03878",
        "authors": [
            {
                "last_name": "Parra-Ullauri",
                "first_name": "Juan Marcelo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xunzheng"
            },
            {
                "last_name": "Bravalheri",
                "first_name": "Anderson"
            },
            {
                "last_name": "Wu",
                "first_name": "Yulei"
            },
            {
                "last_name": "Nejabati",
                "first_name": "Reza"
            },
            {
                "last_name": "Simeonidou",
                "first_name": "Dimitra"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Extensive research is underway to meet the hyper-connectivity demands of 6G\nnetworks, driven by applications like XR/VR and holographic communications,\nwhich generate substantial data requiring network-based processing,\ntransmission, and analysis. However, adhering to diverse data privacy and\nsecurity policies in the anticipated multi-domain, multi-tenancy scenarios of\n6G presents a significant challenge. Federated Analytics (FA) emerges as a\npromising distributed computing paradigm, enabling collaborative data value\ngeneration while preserving privacy and reducing communication overhead. FA\napplies big data principles to manage and secure distributed heterogeneous\nnetworks, improving performance, reliability, visibility, and security without\ncompromising data confidentiality. This paper provides a comprehensive overview\nof potential FA applications, domains, and types in 6G networks, elucidating\nanalysis methods, techniques, and queries. It explores complementary approaches\nto enhance privacy and security in 6G networks alongside FA and discusses the\nchallenges and prerequisites for successful FA implementation. Additionally,\ndistinctions between FA and Federated Learning are drawn, highlighting their\nsynergistic potential through a network orchestration scenario.\n",
        "title": "Federated Analytics for 6G Networks: Applications, Challenges, and\n  Opportunities",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03880",
        "abstract_url": "http://arxiv.org/abs/2401.03880",
        "authors": [
            {
                "last_name": "Talbi",
                "first_name": "Prof. El-Ghazali"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            "NE"
        ],
        "abstract": "  Many real world optimization problems are formulated as mixed-variable\noptimization problems (MVOPs) which involve both continuous and discrete\nvariables. MVOPs including dimensional variables are characterized by a\nvariable-size search space. Depending on the values of dimensional variables,\nthe number and type of the variables of the problem can vary dynamically. MVOPs\nand variable-size MVOPs (VMVOPs) are difficult to solve and raise a number of\nscientific challenges in the design of metaheuristics. Standard metaheuristics\nhave been first designed to address continuous or discrete optimization\nproblems, and are not able to tackle (V)MVOPs in an efficient way. The\ndevelopment of metaheuristics for solving such problems has attracted the\nattention of many researchers and is increasingly popular. However, to our\nknowledge there is no well established taxonomy and comprehensive survey for\nhandling this important family of optimization problems.\n  This paper presents a unified taxonomy for metaheuristic solutions for\nsolving (V)MVOPs in an attempt to provide a common terminology and\nclassification mechanisms. It provides a general mathematical formulation and\nconcepts of (V)MVOPs, and identifies the various solving methodologies than can\nbe applied in metaheuristics. The advantages, the weaknesses and the\nlimitations of the presented methodologies are discussed. The proposed taxonomy\nalso allows to identify some open research issues which needs further in-depth\ninvestigations.\n",
        "title": "Metaheuristics for (Variable-Size) Mixed Optimization Problems: A\n  Unified Taxonomy and Survey",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03883",
        "abstract_url": "http://arxiv.org/abs/2401.03883",
        "authors": [
            {
                "last_name": "M\u00fcllner",
                "first_name": "Peter"
            },
            {
                "last_name": "Lex",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Schedl",
                "first_name": "Markus"
            },
            {
                "last_name": "Kowald",
                "first_name": "Dominik"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.\n",
        "title": "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03888",
        "abstract_url": "http://arxiv.org/abs/2401.03888",
        "authors": [
            {
                "last_name": "Clausen",
                "first_name": "Christian Skafte Beck"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng Grace"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  Facing economic challenges due to the diverse objectives of businesses, and\nconsumers, commercial greenhouses strive to minimize energy costs while\naddressing CO2 emissions. This scenario is intensified by rising energy costs\nand the global imperative to curtail CO2 emissions. To address these dynamic\neconomic challenges, this paper proposes an architectural design for an energy\neconomic dispatch testbed for commercial greenhouses. Utilizing the\nAttribute-Driven De-sign method, core architectural components of a\nsoftware-in-the-loop testbed are proposed which emphasizes modularity and\ncareful consideration of the multi-objective optimization problem. This\napproach extends prior research by implementing a modular multi-objective\noptimization framework in Java. The results demonstrate the successful\nintegration of the CO2 reduction objective within the modular architecture with\nminimal effort. The multi-objective optimization output can also be employed to\nexamine cost and CO2 objectives, ultimately serving as a valuable\ndecision-support tool. The novel testbed architecture and a modular approach\ncan tackle the multi-objective optimization problem and enable commercial\ngreenhouses to navigate the intricate landscape of energy cost and CO2\nemissions management.\n",
        "title": "A Modifiable Architectural Design for Commercial Greenhouses Energy\n  Economic Dispatch Testbed",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03890",
        "abstract_url": "http://arxiv.org/abs/2401.03890",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Guikun"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenguan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "GR",
            "MM"
        ],
        "abstract": "  3D Gaussian splatting (3D GS) has recently emerged as a transformative\ntechnique in the explicit radiance field and computer graphics landscape. This\ninnovative approach, characterized by the utilization of millions of 3D\nGaussians, represents a significant departure from the neural radiance field\n(NeRF) methodologies, which predominantly use implicit, coordinate-based models\nto map spatial coordinates to pixel values. 3D GS, with its explicit scene\nrepresentations and differentiable rendering algorithms, not only promises\nreal-time rendering capabilities but also introduces unprecedented levels of\ncontrol and editability. This positions 3D GS as a potential game-changer for\nthe next generation of 3D reconstruction and representation. In the present\npaper, we provide the first systematic overview of the recent developments and\ncritical contributions in the domain of 3D GS. We begin with a detailed\nexploration of the underlying principles and the driving forces behind the\nadvent of 3D GS, setting the stage for understanding its significance. A focal\npoint of our discussion is the practical applicability of 3D GS. By\nfacilitating real-time performance, 3D GS opens up a plethora of applications,\nranging from virtual reality to interactive media and beyond. This is\ncomplemented by a comparative analysis of leading 3D GS models, evaluated\nacross various benchmark tasks to highlight their performance and practical\nutility. The survey concludes by identifying current challenges and suggesting\npotential avenues for future research in this domain. Through this survey, we\naim to provide a valuable resource for both newcomers and seasoned researchers,\nfostering further exploration and advancement in applicable and explicit\nradiance field representation.\n",
        "title": "A Survey on 3D Gaussian Splatting",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03892",
        "abstract_url": "http://arxiv.org/abs/2401.03892",
        "authors": [
            {
                "last_name": "Maurais",
                "first_name": "Aimee"
            },
            {
                "last_name": "Marzouk",
                "first_name": "Youssef"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  We introduce a new mean-field ODE and corresponding interacting particle\nsystems for sampling from an unnormalized target density or Bayesian posterior.\nThe interacting particle systems are gradient-free, available in closed form,\nand only require the ability to sample from the reference density and compute\nthe (unnormalized) target-to-reference density ratio. The mean-field ODE is\nobtained by solving a Poisson equation for a velocity field that transports\nsamples along the geometric mixture of the two densities, which is the path of\na particular Fisher-Rao gradient flow. We employ a reproducing kernel Hilbert\nspace ansatz for the velocity field, which makes the Poisson equation tractable\nand enables us to discretize the resulting mean-field ODE over finite samples,\nas a simple interacting particle system. The mean-field ODE can be additionally\nbe derived from a discrete-time perspective as the limit of successive\nlinearizations of the Monge-Amp\\`ere equations within a framework known as\nsample-driven optimal transport. We demonstrate empirically that our\ninteracting particle systems can produce high-quality samples from\ndistributions with varying characteristics.\n",
        "title": "Sampling in Unit Time with Kernel Fisher-Rao Flow",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03896",
        "abstract_url": "http://arxiv.org/abs/2401.03896",
        "authors": [
            {
                "last_name": "Howard",
                "first_name": "Sunny"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "MA"
        ],
        "abstract": "  Recently it has been shown that tensor networks (TNs) have the ability to\nrepresent the expected return of a single-agent finite Markov decision process\n(FMDP). The TN represents a distribution model, where all possible trajectories\nare considered. When extending these ideas to a multi-agent setting,\ndistribution models suffer from the curse of dimensionality: the exponential\nrelation between the number of possible trajectories and the number of agents.\nThe key advantage of using TNs in this setting is that there exists a large\nnumber of established optimisation and decomposition techniques that are\nspecific to TNs, that one can apply to ensure the most efficient representation\nis found. In this report, these methods are used to form a TN that represents\nthe expected return of a multi-agent reinforcement learning (MARL) task. This\nmodel is then applied to a 2 agent random walker example, where it was shown\nthat the policy is correctly optimised using a DMRG technique. Finally, I\ndemonstrate the use of an exact decomposition technique, reducing the number of\nelements in the tensors by 97.5%, without experiencing any loss of information.\n",
        "title": "A Tensor Network Implementation of Multi Agent Reinforcement Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03898",
        "abstract_url": "http://arxiv.org/abs/2401.03898",
        "authors": [
            {
                "last_name": "Ngo",
                "first_name": "Hien Quoc"
            },
            {
                "last_name": "Interdonato",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Larsson",
                "first_name": "Erik G."
            },
            {
                "last_name": "Caire",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Andrews",
                "first_name": "Jeffrey G."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Ultra-dense cell-free massive multiple-input multiple-output (CF-MMIMO) has\nemerged as a promising technology expected to meet the future ubiquitous\nconnectivity requirements and ever-growing data traffic demands in 6G. This\narticle provides a contemporary overview of ultra-dense CF-MMIMO networks, and\naddresses important unresolved questions on their future deployment. We first\npresent a comprehensive survey of state-of-the-art research on CF-MMIMO and\nultra-dense networks. Then, we discuss the key challenges of CF-MMIMO under\nultra-dense scenarios such as low-complexity architecture and processing,\nlow-complexity/scalable resource allocation, fronthaul limitation, massive\naccess, synchronization, and channel acquisition. Finally, we answer key open\nquestions, considering different design comparisons and discussing suitable\nmethods dealing with the key challenges of ultra-dense CF-MMIMO. The discussion\naims to provide a valuable roadmap for interesting future research directions\nin this area, facilitating the development of CF-MMIMO MIMO for 6G.\n",
        "title": "Ultra-Dense Cell-Free Massive MIMO for 6G: Technical Overview and Open\n  Questions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03901",
        "abstract_url": "http://arxiv.org/abs/2401.03901",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yueqian"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Chen",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhao",
                "first_name": "Dongyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Recently we have witnessed the rapid development of video question answering\nmodels. However, most models can only handle simple videos in terms of temporal\nreasoning, and their performance tends to drop when answering\ntemporal-reasoning questions on long and informative videos. To tackle this\nproblem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable\nIntermediate Results for video question answering. STAIR is a neural module\nnetwork, which contains a program generator to decompose a given question into\na hierarchical combination of several sub-tasks, and a set of lightweight\nneural modules to complete each of these sub-tasks. Though neural module\nnetworks are already widely studied on image-text tasks, applying them to\nvideos is a non-trivial task, as reasoning on videos requires different\nabilities. In this paper, we define a set of basic video-text sub-tasks for\nvideo question answering and design a set of lightweight modules to complete\nthem. Different from most prior works, modules of STAIR return intermediate\noutputs specific to their intentions instead of always returning attention\nmaps, which makes it easier to interpret and collaborate with pre-trained\nmodels. We also introduce intermediate supervision to make these intermediate\noutputs more accurate. We conduct extensive experiments on several video\nquestion answering datasets under various settings to show STAIR's performance,\nexplainability, compatibility with pre-trained models, and applicability when\nprogram annotations are not available. Code:\nhttps://github.com/yellow-binary-tree/STAIR\n",
        "title": "STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results\n  for Video Question Answering",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03903",
        "abstract_url": "http://arxiv.org/abs/2401.03903",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Yang",
                "first_name": "Liying"
            },
            {
                "last_name": "Huang",
                "first_name": "Chaoxiong"
            },
            {
                "last_name": "Chang",
                "first_name": "Yanchun"
            },
            {
                "last_name": "Li",
                "first_name": "Siliang"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Torch relay is an important tradition of the Olympics and heralds the start\nof the Games. Robots applied in the torch relay activity can not only\ndemonstrate the technological capability of humans to the world but also\nprovide a sight of human lives with robots in the future. This article presents\nan aerial manipulator designed for the robot-to-robot torch relay task of the\nBeijing 2022 Winter Olympics. This aerial manipulator system is composed of a\nquadrotor, a 3 DoF (Degree of Freedom) manipulator, and a monocular camera.\nThis article primarily describes the system design and system control scheme of\nthe aerial manipulator. The experimental results demonstrate that it can\ncomplete robot-to-robot torch relay task under the guidance of vision in the\nice and snow field.\n",
        "title": "An Aerial Manipulator for Robot-to-robot Torch Relay Task: System Design\n  and Control Scheme",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03904",
        "abstract_url": "http://arxiv.org/abs/2401.03904",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yongjie"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Yang",
                "first_name": "Liying"
            },
            {
                "last_name": "Nie",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Chaoxiong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yiwen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Time-optimal control of a multi-rotor remains an open problem due to the\nunder-actuation and nonlinearity of its dynamics, which make it difficult to\nsolve this problem directly. In this paper, the time-optimal control problem of\nthe multi-rotor is studied. Firstly, a thrust limit optimal decomposition\nmethod is proposed, which can reasonably decompose the limited thrust into\nthree directions according to the current state and the target state. As a\nresult, the thrust limit constraint is decomposed as a linear constraint. With\nthe linear constraint and decoupled dynamics, a time-optimal guidance\ntrajectory can be obtained. Then, a cost function is defined based on the\ntime-optimal guidance trajectory, which has a quadratic form and can be used to\nevaluate the time-optimal performance of the system outputs. Finally, based on\nthe cost function, the time-optimal control problem is reformulated as an MPC\n(Model Predictive Control) problem. The experimental results demonstrate the\nfeasibility and validity of the proposed methods.\n",
        "title": "Guided Time-optimal Model Predictive Control of a Multi-rotor",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03905",
        "abstract_url": "http://arxiv.org/abs/2401.03905",
        "authors": [
            {
                "last_name": "Veyret",
                "first_name": "Morgan"
            },
            {
                "last_name": "Duchene",
                "first_name": "Jean-Baptiste"
            },
            {
                "last_name": "Afonouvi",
                "first_name": "Kekeli"
            },
            {
                "last_name": "Brabant",
                "first_name": "Quentin"
            },
            {
                "last_name": "Lecorve",
                "first_name": "Gwenole"
            },
            {
                "last_name": "Rojas-Barahona",
                "first_name": "Lina M."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Typically available dialogue frameworks have adopted a semantic\nrepresentation based on dialogue-acts and slot-value pairs. Despite its\nsimplicity, this representation has disadvantages such as the lack of\nexpressivity, scalability and explainability. We present WEBDial: a dialogue\nframework that relies on a graph formalism by using RDF triples instead of\nslot-value pairs. We describe its overall architecture and the graph-based\nsemantic representation. We show its applicability from simple to complex\napplications, by varying the complexity of domains and tasks: from single\ndomain and tasks to multiple domains and complex tasks.\n",
        "title": "WEBDial, a Multi-domain, Multitask Statistical Dialogue Framework with\n  RDF",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03907",
        "abstract_url": "http://arxiv.org/abs/2401.03907",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guoxing"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            },
            {
                "last_name": "Xu",
                "first_name": "Shaoqing"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            },
            {
                "last_name": "Jia",
                "first_name": "Feiyang"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Multi-modal 3D object detectors are dedicated to exploring secure and\nreliable perception systems for autonomous driving (AD). However, while\nachieving state-of-the-art (SOTA) performance on clean benchmark datasets, they\ntend to overlook the complexity and harsh conditions of real-world\nenvironments. Meanwhile, with the emergence of visual foundation models (VFMs),\nopportunities and challenges are presented for improving the robustness and\ngeneralization of multi-modal 3D object detection in autonomous driving.\nTherefore, we propose RoboFusion, a robust framework that leverages VFMs like\nSAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the\noriginal SAM for autonomous driving scenarios named SAM-AD. To align SAM or\nSAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the\nimage features extracted by SAM. We employ wavelet decomposition to denoise the\ndepth-guided images for further noise reduction and weather interference.\nLastly, we employ self-attention mechanisms to adaptively reweight the fused\nfeatures, enhancing informative features while suppressing excess noise. In\nsummary, our RoboFusion gradually reduces noise by leveraging the\ngeneralization and robustness of VFMs, thereby enhancing the resilience of\nmulti-modal 3D object detection. Consequently, our RoboFusion achieves\nstate-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C\nand nuScenes-C benchmarks.\n",
        "title": "RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03910",
        "abstract_url": "http://arxiv.org/abs/2401.03910",
        "authors": [
            {
                "last_name": "Milli\u00e8re",
                "first_name": "Rapha\u00ebl"
            },
            {
                "last_name": "Buckner",
                "first_name": "Cameron"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Large language models like GPT-4 have achieved remarkable proficiency in a\nbroad spectrum of language-based tasks, some of which are traditionally\nassociated with hallmarks of human intelligence. This has prompted ongoing\ndisagreements about the extent to which we can meaningfully ascribe any kind of\nlinguistic or cognitive competence to language models. Such questions have deep\nphilosophical roots, echoing longstanding debates about the status of\nartificial neural networks as cognitive models. This article -- the first part\nof two companion papers -- serves both as a primer on language models for\nphilosophers, and as an opinionated survey of their significance in relation to\nclassic debates in the philosophy cognitive science, artificial intelligence,\nand linguistics. We cover topics such as compositionality, language\nacquisition, semantic competence, grounding, world models, and the transmission\nof cultural knowledge. We argue that the success of language models challenges\nseveral long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better\nunderstand their internal mechanisms. This sets the stage for the companion\npaper (Part II), which turns to novel empirical methods for probing the inner\nworkings of language models, and new philosophical questions prompted by their\nlatest developments.\n",
        "title": "A Philosophical Introduction to Language Models -- Part I: Continuity\n  With Classic Debates",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03912",
        "abstract_url": "http://arxiv.org/abs/2401.03912",
        "authors": [
            {
                "last_name": "Panambur",
                "first_name": "Adarsh Bhandary"
            },
            {
                "last_name": "Yu",
                "first_name": "Hui"
            },
            {
                "last_name": "Bhat",
                "first_name": "Sheethal"
            },
            {
                "last_name": "Madhu",
                "first_name": "Prathmesh"
            },
            {
                "last_name": "Bayer",
                "first_name": "Siming"
            },
            {
                "last_name": "Maier",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  The assessment of breast density is crucial in the context of breast cancer\nscreening, especially in populations with a higher percentage of dense breast\ntissues. This study introduces a novel data augmentation technique termed\nAttention-Guided Erasing (AGE), devised to enhance the downstream\nclassification of four distinct breast density categories in mammography\nfollowing the BI-RADS recommendation in the Vietnamese cohort. The proposed\nmethod integrates supplementary information during transfer learning, utilizing\nvisual attention maps derived from a vision transformer backbone trained using\nthe self-supervised DINO method. These maps are utilized to erase background\nregions in the mammogram images, unveiling only the potential areas of dense\nbreast tissues to the network. Through the incorporation of AGE during transfer\nlearning with varying random probabilities, we consistently surpass\nclassification performance compared to scenarios without AGE and the\ntraditional random erasing transformation. We validate our methodology using\nthe publicly available VinDr-Mammo dataset. Specifically, we attain a mean\nF1-score of 0.5910, outperforming values of 0.5594 and 0.5691 corresponding to\nscenarios without AGE and with random erasing (RE), respectively. This\nsuperiority is further substantiated by t-tests, revealing a p-value of\np<0.0001, underscoring the statistical significance of our approach.\n",
        "title": "Attention-Guided Erasing: A Novel Augmentation Method for Enhancing\n  Downstream Breast Density Classification",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03914",
        "abstract_url": "http://arxiv.org/abs/2401.03914",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Danqi"
            },
            {
                "last_name": "Gao",
                "first_name": "Qing"
            },
            {
                "last_name": "Qian",
                "first_name": "Yuepeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xinxing"
            },
            {
                "last_name": "Fu",
                "first_name": "Chenglong"
            },
            {
                "last_name": "Leng",
                "first_name": "Yuquan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Three-dimensional (3D) human pose estimation using a monocular camera has\ngained increasing attention due to its ease of implementation and the abundance\nof data available from daily life. However, owing to the inherent depth\nambiguity in images, the accuracy of existing monocular camera-based 3D pose\nestimation methods remains unsatisfactory, and the estimated 3D poses usually\ninclude much noise. By observing the histogram of this noise, we find each\ndimension of the noise follows a certain distribution, which indicates the\npossibility for a neural network to learn the mapping between noisy poses and\nground truth poses. In this work, in order to obtain more accurate 3D poses, a\nDiffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output\nof any existing 3D pose estimator. We first introduce a conditional\nmultivariate Gaussian distribution to model the distribution of noisy 3D poses,\nusing paired 2D poses and noisy 3D poses as conditions to achieve greater\naccuracy. Additionally, we leverage the architecture of current diffusion\nmodels to convert the distribution of noisy 3D poses into ground truth 3D\nposes. To evaluate the effectiveness of the proposed method, two\nstate-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D\npose estimation models, and the proposed method is evaluated on different types\nof 2D poses and different lengths of the input sequence. Experimental results\ndemonstrate the proposed architecture can significantly improve the performance\nof current sequence-to-sequence 3D pose estimators, with a reduction of at\nleast 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in\nthe Procrustes MPJPE (P-MPJPE).\n",
        "title": "D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose\n  Refinement",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03915",
        "abstract_url": "http://arxiv.org/abs/2401.03915",
        "authors": [
            {
                "last_name": "Daas",
                "first_name": "Hussam Al"
            },
            {
                "last_name": "Jolivet",
                "first_name": "Pierre"
            },
            {
                "last_name": "Nataf",
                "first_name": "Fr\u00e9d\u00e9ric"
            },
            {
                "last_name": "Tournier",
                "first_name": "Pierre-Henri"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper introduces a fully algebraic two-level additive Schwarz\npreconditioner for general sparse large-scale matrices. The preconditioner is\nanalyzed for symmetric positive definite (SPD) matrices. For those matrices,\nthe coarse space is constructed based on approximating two local subspaces in\neach subdomain. These subspaces are obtained by approximating a number of\neigenvectors corresponding to dominant eigenvalues of two judiciously posed\ngeneralized eigenvalue problems. The number of eigenvectors can be chosen to\ncontrol the condition number. For general sparse matrices, the coarse space is\nconstructed by approximating the image of a local operator that can be defined\nfrom information in the coefficient matrix. The connection between the coarse\nspaces for SPD and general matrices is also discussed. Numerical experiments\nshow the great effectiveness of the proposed preconditioners on matrices\narising from a wide range of applications. The set of matrices includes SPD,\nsymmetric indefinite, nonsymmetric, and saddle-point matrices. In addition, we\ncompare the proposed preconditioners to the state-of-the-art domain\ndecomposition preconditioners.\n",
        "title": "A Robust Two-Level Schwarz Preconditioner For Sparse Matrices",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03917",
        "abstract_url": "http://arxiv.org/abs/2401.03917",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Quoc Chuong"
            },
            {
                "last_name": "Le",
                "first_name": "Trung Kien"
            }
        ],
        "primary_category": "MS",
        "categories": [
            "MS"
        ],
        "abstract": "  Hypergraphs, or generalization of graphs such that edges can contain more\nthan two nodes, have become increasingly prominent in understanding complex\nnetwork analysis. Unlike graphs, hypergraphs have relatively few supporting\nplatforms, and such dearth presents a barrier to more widespread adaptation of\nhypergraph computational toolboxes that could enable further research in\nseveral areas. Here, we introduce HyperRD, a Python package for hypergraph\ncomputation, simulation, and interoperability with other powerful Python\npackages in graph and hypergraph research. Then, we will introduce two models\non hypergraph, the general Schelling's model and the SIR model, and simulate\nthem with HyperRD.\n",
        "title": "Toward a comprehensive simulation framework for hypergraphs: a\n  Python-base approach",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03921",
        "abstract_url": "http://arxiv.org/abs/2401.03921",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hau-Tieng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  In this manuscript, we propose an efficient manifold denoiser based on\nlandmark diffusion and optimal shrinkage under the complicated high dimensional\nnoise and compact manifold setup. It is flexible to handle several setups,\nincluding the high ambient space dimension with a manifold embedding that\noccupies a subspace of high or low dimensions, and the noise could be colored\nand dependent. A systematic comparison with other existing algorithms on both\nsimulated and real datasets is provided. This manuscript is mainly algorithmic\nand we report several existing tools and numerical results. Theoretical\nguarantees and more comparisons will be reported in the official paper of this\nmanuscript.\n",
        "title": "Design a Metric Robust to Complicated High Dimensional Noise for\n  Efficient Manifold Denoising",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03923",
        "abstract_url": "http://arxiv.org/abs/2401.03923",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Gen"
            },
            {
                "last_name": "Wei",
                "first_name": "Yuting"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "LG",
            "",
            ""
        ],
        "abstract": "  Characterizing the distribution of high-dimensional statistical estimators is\na challenging task, due to the breakdown of classical asymptotic theory in high\ndimension. This paper makes progress towards this by developing non-asymptotic\ndistributional characterizations for approximate message passing (AMP) -- a\nfamily of iterative algorithms that prove effective as both fast estimators and\npowerful theoretical machinery -- for both sparse and robust regression. Prior\nAMP theory, which focused on high-dimensional asymptotics for the most part,\nfailed to describe the behavior of AMP when the number of iterations exceeds\n$o\\big({\\log n}/{\\log \\log n}\\big)$ (with $n$ the sample size). We establish\nthe first finite-sample non-asymptotic distributional theory of AMP for both\nsparse and robust regression that accommodates a polynomial number of\niterations. Our results derive approximate accuracy of Gaussian approximation\nof the AMP iterates, which improves upon all prior results and implies enhanced\ndistributional characterizations for both optimally tuned Lasso and robust\nM-estimator.\n",
        "title": "A non-asymptotic distributional theory of approximate message passing\n  for sparse and robust regression",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03924",
        "abstract_url": "http://arxiv.org/abs/2401.03924",
        "authors": [
            {
                "last_name": "Maalouly",
                "first_name": "Nicolas El"
            },
            {
                "last_name": "Haslebacher",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Wulf",
                "first_name": "Lasse"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  In the Exact Matching problem, we are given a graph whose edges are colored\nred or blue and the task is to decide for a given integer k, if there is a\nperfect matching with exactly k red edges. Since 1987 it is known that the\nExact Matching Problem can be solved in randomized polynomial time. Despite\nnumerous efforts, it is still not known today whether a deterministic\npolynomial-time algorithm exists as well. In this paper, we make substantial\nprogress by solving the problem for a multitude of different classes of dense\ngraphs. We solve the Exact Matching problem in deterministic polynomial time\nfor complete r-partite graphs, for unit interval graphs, for bipartite unit\ninterval graphs, for graphs of bounded neighborhood diversity, for chain\ngraphs, and for graphs without a complete bipartite t-hole. We solve the\nproblem in quasi-polynomial time for Erd\\H{o}s-R\\'enyi random graphs G(n, 1/2).\nWe also reprove an earlier result for bounded independence number/bipartite\nindependence number. We use two main tools to obtain these results: A local\nsearch algorithm as well as a generalization of an earlier result by Karzanov.\n",
        "title": "On the Exact Matching Problem in Dense Graphs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03925",
        "abstract_url": "http://arxiv.org/abs/2401.03925",
        "authors": [
            {
                "last_name": "de Castro",
                "first_name": "Marcus Vinicius Borela"
            },
            {
                "last_name": "Balaniuk",
                "first_name": "Remis"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "",
            "LG"
        ],
        "abstract": "  This paper proposes a methodology for documenting data mining (DM) projects,\nRastro-DM (Trail Data Mining), with a focus not on the model that is generated,\nbut on the processes behind its construction, in order to leave a trail (Rastro\nin Portuguese) of planned actions, training completed, results obtained, and\nlessons learned. The proposed practices are complementary to structuring\nmethodologies of DM, such as CRISP-DM, which establish a methodological and\nparadigmatic framework for the DM process. The application of best practices\nand their benefits is illustrated in a project called 'Cladop' that was created\nfor the classification of PDF documents associated with the investigative\nprocess of damages to the Brazilian Federal Public Treasury. Building the\nRastro-DM kit in the context of a project is a small step that can lead to an\ninstitutional leap to be achieved by sharing and using the trail across the\nenterprise.\n",
        "title": "Rastro-DM: data mining with a trail",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03932",
        "abstract_url": "http://arxiv.org/abs/2401.03932",
        "authors": [
            {
                "last_name": "van Hove",
                "first_name": "Alouette"
            },
            {
                "last_name": "Aalstad",
                "first_name": "Kristoffer"
            },
            {
                "last_name": "Pirk",
                "first_name": "Norbert"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO",
            ""
        ],
        "abstract": "  Accurate mapping of greenhouse gas fluxes at the Earth's surface is essential\nfor the validation and calibration of climate models. In this study, we present\na framework for surface flux estimation with drones. Our approach uses data\nassimilation (DA) to infer fluxes from drone-based observations, and\nreinforcement learning (RL) to optimize the drone's sampling strategy. Herein,\nwe demonstrate that a RL-trained drone can quantify a CO2 hotspot more\naccurately than a drone sampling along a predefined flight path that traverses\nthe emission plume. We find that information-based reward functions can match\nthe performance of an error-based reward function that quantifies the\ndifference between the estimated surface flux and the true value. Reward\nfunctions based on information gain and information entropy can motivate\nactions that increase the drone's confidence in its updated belief, without\nrequiring knowledge of the true surface flux. These findings provide valuable\ninsights for further development of the framework for the mapping of more\ncomplex surface flux fields.\n",
        "title": "Using reinforcement learning to improve drone-based inference of\n  greenhouse gas fluxes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03936",
        "abstract_url": "http://arxiv.org/abs/2401.03936",
        "authors": [
            {
                "last_name": "Williams",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Pizzi",
                "first_name": "Karla"
            },
            {
                "last_name": "Noe",
                "first_name": "Paul-Gauthier"
            },
            {
                "last_name": "Das",
                "first_name": "Sneha"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CR",
            "LG",
            "SD"
        ],
        "abstract": "  Most recent speech privacy efforts have focused on anonymizing acoustic\nspeaker attributes but there has not been as much research into protecting\ninformation from speech content. We introduce a toy problem that explores an\nemerging type of privacy called \"content masking\" which conceals selected words\nand phrases in speech. In our efforts to define this problem space, we evaluate\nan introductory baseline masking technique based on modifying sequences of\ndiscrete phone representations (phone codes) produced from a pre-trained\nvector-quantized variational autoencoder (VQ-VAE) and re-synthesized using\nWaveRNN. We investigate three different masking locations and three types of\nmasking strategies: noise substitution, word deletion, and phone sequence\nreversal. Our work attempts to characterize how masking affects two downstream\ntasks: automatic speech recognition (ASR) and automatic speaker verification\n(ASV). We observe how the different masks types and locations impact these\ndownstream tasks and discuss how these issues may influence privacy goals.\n",
        "title": "Exploratory Evaluation of Speech Content Masking",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03938",
        "abstract_url": "http://arxiv.org/abs/2401.03938",
        "authors": [
            {
                "last_name": "\u0110ura\u0161",
                "first_name": "Antun"
            },
            {
                "last_name": "Sukno",
                "first_name": "Matija"
            },
            {
                "last_name": "Palunko",
                "first_name": "Ivana"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper we propose a novel approach aimed at recovering the 3D position\nof an UUV from UAV imagery in shallow-water environments. Through combination\nof UAV and UUV measurements, we show that our method can be utilized as an\naccurate and cost-effective alternative when compared to acoustic sensing\nmethods, typically required to obtain ground truth information in underwater\nlocalization problems. Furthermore, our approach allows for a seamless\nconversion to geo-referenced coordinates which can be utilized for navigation\npurposes. To validate our method, we present the results with data collected\nthrough a simulation environment and field experiments, demonstrating the\nability to successfully recover the UUV position with sub-meter accuracy.\n",
        "title": "Recovering the 3D UUV Position using UAV Imagery in Shallow-Water\n  Environments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03939",
        "abstract_url": "http://arxiv.org/abs/2401.03939",
        "authors": [
            {
                "last_name": "Neubauer",
                "first_name": "Theresa"
            },
            {
                "last_name": "Berg",
                "first_name": "Astrid"
            },
            {
                "last_name": "Wimmer",
                "first_name": "Maria"
            },
            {
                "last_name": "Lenis",
                "first_name": "Dimitrios"
            },
            {
                "last_name": "Major",
                "first_name": "David"
            },
            {
                "last_name": "Winter",
                "first_name": "Philip Matthias"
            },
            {
                "last_name": "De Paolis",
                "first_name": "Gaia Romana"
            },
            {
                "last_name": "Novotny",
                "first_name": "Johannes"
            },
            {
                "last_name": "L\u00fcftner",
                "first_name": "Daniel"
            },
            {
                "last_name": "Reinharter",
                "first_name": "Katja"
            },
            {
                "last_name": "B\u00fchler",
                "first_name": "Katja"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            ""
        ],
        "abstract": "  Quantitative measurement of crystals in high-resolution images allows for\nimportant insights into underlying material characteristics. Deep learning has\nshown great progress in vision-based automatic crystal size measurement, but\ncurrent instance segmentation methods reach their limits with images that have\nlarge variation in crystal size or hard to detect crystal boundaries. Even\nsmall image segmentation errors, such as incorrectly fused or separated\nsegments, can significantly lower the accuracy of the measured results. Instead\nof improving the existing pixel-wise boundary segmentation methods, we propose\nto use an instance-based segmentation method, which gives more robust\nsegmentation results to improve measurement accuracy. Our novel method enhances\nflow maps with a size-aware multi-scale attention module. The attention module\nadaptively fuses information from multiple scales and focuses on the most\nrelevant scale for each segmented image area. We demonstrate that our proposed\nattention fusion strategy outperforms state-of-the-art instance and boundary\nsegmentation methods, as well as simple average fusion of multi-scale\npredictions. We evaluate our method on a refractory raw material dataset of\nhigh-resolution images with large variation in crystal size and show that our\nmodel can be used to calculate the crystal size more accurately than existing\nmethods.\n",
        "title": "Multi-scale attention-based instance segmentation for measuring crystals\n  with large size variation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03943",
        "abstract_url": "http://arxiv.org/abs/2401.03943",
        "authors": [
            {
                "last_name": "Gharibian",
                "first_name": "Sevag"
            },
            {
                "last_name": "Kamminga",
                "first_name": "Jonas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CC"
        ],
        "abstract": "  What is the power of polynomial-time quantum computation with access to an NP\noracle? In this work, we focus on two fundamental tasks from the study of\nBoolean satisfiability (SAT) problems: search-to-decision reductions, and\napproximate counting. We first show that, in strong contrast to the classical\nsetting where a poly-time Turing machine requires $\\Theta(n)$ queries to an NP\noracle to compute a witness to a given SAT formula, quantumly $\\Theta(\\log n)$\nqueries suffice. We then show this is tight in the black-box model - any\nquantum algorithm with \"NP-like\" query access to a formula requires\n$\\Omega(\\log n)$ queries to extract a solution with constant probability.\nMoving to approximate counting of SAT solutions, by exploiting a quantum link\nbetween search-to-decision reductions and approximate counting, we show that\nexisting classical approximate counting algorithms are likely optimal. First,\nwe give a lower bound in the \"NP-like\" black-box query setting: Approximate\ncounting requires $\\Omega(\\log n)$ queries, even on a quantum computer. We then\ngive a \"white-box\" lower bound (i.e. where the input formula is not hidden in\nthe oracle) - if there exists a randomized poly-time classical or quantum\nalgorithm for approximate counting making $o(log n)$ NP queries, then\n$\\text{BPP}^{\\text{NP}[o(n)]}$ contains a $\\text{P}^{\\text{NP}}$-complete\nproblem if the algorithm is classical and $\\text{FBQP}^{\\text{NP}[o(n)]}$\ncontains an $\\text{FP}^{\\text{NP}}$-complete problem if the algorithm is\nquantum.\n",
        "title": "BQP, meet NP: Search-to-decision reductions and approximate counting",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03944",
        "abstract_url": "http://arxiv.org/abs/2401.03944",
        "authors": [
            {
                "last_name": "Sardinha",
                "first_name": "Emanuel Nunez"
            },
            {
                "last_name": "Munera",
                "first_name": "Marcela"
            },
            {
                "last_name": "Zook",
                "first_name": "Nancy"
            },
            {
                "last_name": "Western",
                "first_name": "David"
            },
            {
                "last_name": "Garate",
                "first_name": "Virginia Ruiz"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Individuals with tetraplegia and similar forms of paralysis suffer physically\nand emotionally due to a lack of autonomy. To help regain part of this\nautonomy, assistive robotic arms have been shown to increase living\nindependence. However, users with paralysis pose unique challenging conditions\nfor the control of these devices. In this article, we present the use of\nDiegetic Graphical User Interfaces, a novel, intuitive, and computationally\ninexpensive approach for gaze-controlled interfaces applied to robots. By using\nsymbols paired with fiducial markers, interactive buttons can be defined in the\nreal world which the user can trigger via gaze, and which can be embedded\neasily into the environment. We apply this system to pilot a\n3-degree-of-freedom robotic arm for precision pick-and-place tasks. The\ninterface is placed directly on the robot to allow intuitive and direct\ninteraction, eliminating the need for context-switching between external\nscreens, menus, and the robot. After calibration and a brief habituation\nperiod, twenty-one participants from multiple backgrounds, ages and eye-sight\nconditions completed the Yale-CMU-Berkeley (YCB) Block Pick and Place Protocol\nto benchmark the system, achieving a mean score of 13.71 out of the maximum\n16.00 points. Good usability and user experience were reported (System\nUsability Score of 75.36) while achieving a low task workload measure (NASA-TLX\nof 44.76). Results show that users can employ multiple interface elements to\nperform actions with minimal practice and with a small cognitive load. To our\nknowledge, this is the first easily reconfigurable screenless system that\nenables robot control entirely via gaze for Cartesian robot control without the\nneed for eye or face gestures.\n",
        "title": "Diegetic Graphical User Interfaces and Intuitive Control of Assistive\n  Robots via Eye-gaze",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03945",
        "abstract_url": "http://arxiv.org/abs/2401.03945",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dong"
            },
            {
                "last_name": "Li",
                "first_name": "Zhaowei"
            },
            {
                "last_name": "Wang",
                "first_name": "Pengyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yaqian"
            },
            {
                "last_name": "Qiu",
                "first_name": "Xipeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Human communication is a complex and diverse process that not only involves\nmultiple factors such as language, commonsense, and cultural backgrounds but\nalso requires the participation of multimodal information, such as speech.\nLarge Language Model (LLM)-based multi-agent systems have demonstrated\npromising performance in simulating human society. Can we leverage LLM-based\nmulti-agent systems to simulate human communication? However, current LLM-based\nmulti-agent systems mainly rely on text as the primary medium. In this paper,\nwe propose SpeechAgents, a multi-modal LLM based multi-agent system designed\nfor simulating human communication. SpeechAgents utilizes multi-modal LLM as\nthe control center for individual agent and employes multi-modal signals as the\nmedium for exchanged messages among agents. Additionally, we propose\nMulti-Agent Tuning to enhance the multi-agent capabilities of LLM without\ncompromising general abilities. To strengthen and evaluate the effectiveness of\nhuman communication simulation, we build the Human-Communication Simulation\nBenchmark. Experimental results demonstrate that SpeechAgents can simulate\nhuman communication dialogues with consistent content, authentic rhythm, and\nrich emotions and demonstrate excellent scalability even with up to 25 agents,\nwhich can apply to tasks such as drama creation and audio novels generation.\nCode and models will be open-sourced at https://github.\ncom/0nutation/SpeechAgents\n",
        "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal\n  Multi-Agent Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03946",
        "abstract_url": "http://arxiv.org/abs/2401.03946",
        "authors": [
            {
                "last_name": "Sarvazyan",
                "first_name": "Areg Mikael"
            },
            {
                "last_name": "Gonz\u00e1lez",
                "first_name": "Jos\u00e9 \u00c1ngel"
            },
            {
                "last_name": "Franco-Salvador",
                "first_name": "Marc"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancements in Large Language Models (LLMs) have led to high-quality\nMachine-Generated Text (MGT), giving rise to countless new use cases and\napplications. However, easy access to LLMs is posing new challenges due to\nmisuse. To address malicious usage, researchers have released datasets to\neffectively train models on MGT-related tasks. Similar strategies are used to\ncompile these datasets, but no tool currently unifies them. In this scenario,\nwe introduce TextMachina, a modular and extensible Python framework, designed\nto aid in the creation of high-quality, unbiased datasets to build robust\nmodels for MGT-related tasks such as detection, attribution, or boundary\ndetection. It provides a user-friendly pipeline that abstracts away the\ninherent intricacies of building MGT datasets, such as LLM integrations, prompt\ntemplating, and bias mitigation. The quality of the datasets generated by\nTextMachina has been assessed in previous works, including shared tasks where\nmore than one hundred teams trained robust MGT detectors.\n",
        "title": "TextMachina: Seamless Generation of Machine-Generated Text Datasets",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03947",
        "abstract_url": "http://arxiv.org/abs/2401.03947",
        "authors": [
            {
                "last_name": "van Hove",
                "first_name": "Alouette"
            },
            {
                "last_name": "Aalstad",
                "first_name": "Kristoffer"
            },
            {
                "last_name": "Pirk",
                "first_name": "Norbert"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The accurate estimation of locations and emission rates of gas sources is\ncrucial across various domains, including environmental monitoring and\ngreenhouse gas emission analysis. This study investigates two drone sampling\nstrategies for inferring source term parameters of gas plumes from atmospheric\nmeasurements. Both strategies are guided by the goal of maximizing information\ngain attained from observations at sequential locations. Our research compares\nthe myopic approach of infotaxis to a far-sighted navigation strategy trained\nthrough deep reinforcement learning. We demonstrate the superior performance of\ndeep reinforcement learning over infotaxis in environments with non-isotropic\ngas plumes.\n",
        "title": "Guiding drones by information gain",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03951",
        "abstract_url": "http://arxiv.org/abs/2401.03951",
        "authors": [
            {
                "last_name": "Henke",
                "first_name": "Dorothee"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DS"
        ],
        "abstract": "  In bilevel optimization problems, a leader and a follower make their\ndecisions in a hierarchy, and both decisions influence each other. Usually one\nassumes that both players have full knowledge also of the other player's data.\nIn a more realistic model, uncertainty can be quantified, e.g., using the\nrobust optimization approach: We assume that the leader does not know the\nfollower's objective precisely, but only up to some uncertainty set, and her\naim is to optimize the worst case of the corresponding scenarios. Now the\nquestion arises how the complexity of bilevel optimization changes under the\nadditional complications of this uncertainty.\n  We make a further step towards answering this question by examining an easy\nbilevel problem. In the Bilevel Selection Problem (BSP), the leader and the\nfollower each select some items, while a common number of items to select in\ntotal is given, and each player minimizes the total costs of the selected\nitems, according to different sets of item costs. We show that the BSP can be\nsolved in polynomial time and then investigate its robust version. If the item\nsets controlled by the players are disjoint, it can still be solved in\npolynomial time for several types of uncertainty sets. Otherwise, we show that\nthe Robust BSP is NP-hard and present a 2-approximation algorithm and exact\nexponential-time approaches.\n  Furthermore, we investigate variants of the BSP where one or both of the two\nplayers take a continuous decision. One variant leads to an example of a\nbilevel optimization problem whose optimum value may not be attained. For the\nRobust Continuous BSP, where all variables are continuous, we also develop a\nnew approach for the setting of discrete uncorrelated uncertainty, which gives\na polynomial-time algorithm for the Robust Continuous BSP and a\npseudopolynomial-time algorithm for the Robust Bilevel Continuous Knapsack\nProblem.\n",
        "title": "The Robust Bilevel Selection Problem",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03952",
        "abstract_url": "http://arxiv.org/abs/2401.03952",
        "authors": [
            {
                "last_name": "Anandan",
                "first_name": "Megala"
            },
            {
                "last_name": "Rao",
                "first_name": "S. V. Raghurama"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper, we are concerned about the lattice Boltzmann methods (LBMs)\nbased on vector-kinetic models for hyperbolic partial differential equations.\nIn addition to usual lattice Boltzmann equation (LBE) derived by explicit\ndiscretisation of vector-kinetic equation (VKE), we also consider LBE derived\nby semi-implicit discretisation of VKE and compare the relaxation factors of\nboth. We study the properties such as H-inequality, total variation boundedness\nand positivity of both the LBEs, and infer that the LBE due to semi-implicit\ndiscretisation naturally satisfies all the properties while the LBE due to\nexplicit discretisation requires more restrictive condition on relaxation\nfactor compared to the usual condition obtained from Chapman-Enskog expansion.\nWe also derive the macroscopic finite difference form of the LBEs, and utilise\nit to establish the consistency of LBEs with the hyperbolic system. Further, we\nextend this LBM framework to hyperbolic conservation laws with source terms,\nsuch that there is no spurious numerical convection due to imbalance between\nconvection and source terms. We also present a D$2$Q$9$ model that allows\nupwinding even along diagonal directions in addition to the usual upwinding\nalong coordinate directions. The different aspects of the results are validated\nnumerically on standard benchmark problems.\n",
        "title": "On Lattice Boltzmann Methods based on vector-kinetic models for\n  hyperbolic partial differential equations",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03955",
        "abstract_url": "http://arxiv.org/abs/2401.03955",
        "authors": [
            {
                "last_name": "Ekambaram",
                "first_name": "Vijay"
            },
            {
                "last_name": "Jati",
                "first_name": "Arindam"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Nam H."
            },
            {
                "last_name": "Dayama",
                "first_name": "Pankaj"
            },
            {
                "last_name": "Reddy",
                "first_name": "Chandra"
            },
            {
                "last_name": "Gifford",
                "first_name": "Wesley M."
            },
            {
                "last_name": "Kalagnanam",
                "first_name": "Jayant"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Large Pretrained models for Zero/Few-shot learning excel in language and\nvision domains but encounter challenges in multivariate time series (TS) due to\nthe diverse nature and scarcity of publicly available pretraining data.\nConsequently, there has been a recent surge in utilizing pretrained large\nlanguage models (LLMs) with various adaptations for time series forecasting.\nThese approaches employ cross-domain transfer learning, yielding highly\nimpressive results. However, these models are typically very large ($\\sim$\nbillion parameters), exhibit slow execution, and do not consider cross-channel\ncorrelations. To address this, we present Multi-level Tiny Time Mixers (TTM), a\nsignificantly smaller model based on the lightweight TSMixer architecture. TTM\nmarks the first success in developing tiny pretrained models ($\\le$1 million\nparameters), exclusively trained on public TS data with effective transfer\nlearning capabilities. To tackle the complexity of pretraining on multiple\ndatasets with varied temporal resolutions, we introduce several novel\nenhancements such as adaptive patching, dataset augmentation via downsampling,\nand resolution prefix tuning. Moreover, we employ a multi-level modeling\nstrategy to effectively model channel correlations and incorporate exogenous\nsignals during finetuning, a crucial capability lacking in existing benchmarks.\nTTM excels in few/zero-shot forecasting, demonstrating significant accuracy\ngains (12-38%) over existing benchmarks. Further, it achieves a remarkable\n14-106X reduction in model parameters, enabling 54-65X faster\ntraining/inference as compared to the LLM-TS benchmarks. In fact, TTM's\nzero-shot results often surpass the few-shot results in many benchmarks,\nhighlighting the efficacy of our approach. Code and Pretrained Models will be\nopen-sourced.\n",
        "title": "TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and\n  Few-shot Forecasting of Multivariate Time Series",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03956",
        "abstract_url": "http://arxiv.org/abs/2401.03956",
        "authors": [
            {
                "last_name": "Lai",
                "first_name": "Ming-Jun"
            },
            {
                "last_name": "Shen",
                "first_name": "Zhaiming"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We propose a new approach for approximating functions in $C([0,1]^d)$ via\nKolmogorov superposition theorem (KST) based on the linear spline approximation\nof the K-outer function in Kolmogorov superposition representation. We improve\nthe results in \\cite{LaiShenKST21} by showing that the optimal approximation\nrate based on our proposed approach is $\\mathcal{O}(\\frac{1}{n^2})$, with $n$\nbeing the number of knots over $[0,1]$, and the approximation constant\nincreases linearly in $d$. We show that there is a dense subclass in\n$C([0,1]^d)$ whose approximation can achieve such optimal rate, and the number\nof parameters needed in such approximation is at most $\\mathcal{O}(nd)$.\nMoreover, for $d\\geq 4$, we apply the tensor product spline denoising technique\nto smooth the KB-splines and get the corresponding LKB-splines. We use those\nLKB-splines as the basis to approximate functions for the cases when $d=4$ and\n$d=6$, which extends the results in \\cite{LaiShenKST21} for $d=2$ and $d=3$.\nBased on the idea of pivotal data locations introduced in \\cite{LaiShenKST21},\nwe validate via numerical experiments that fewer than $\\mathcal{O}(nd)$\nfunction values are enough to achieve the approximation rates such as\n$\\mathcal{O}(\\frac{1}{n})$ or $\\mathcal{O}(\\frac{1}{n^2})$ based on the\nsmoothness of the K-outer function. Finally, we demonstrate that our approach\ncan be applied to numerically solving partial differential equation such as the\nPoisson equation with accurate approximation results.\n",
        "title": "The Optimal Rate for Linear KB-splines and LKB-splines Approximation of\n  High Dimensional Continuous Functions and its Application",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03960",
        "abstract_url": "http://arxiv.org/abs/2401.03960",
        "authors": [
            {
                "last_name": "Reimers",
                "first_name": "Christian"
            },
            {
                "last_name": "Rachti",
                "first_name": "David Hafezi"
            },
            {
                "last_name": "Liu",
                "first_name": "Guahua"
            },
            {
                "last_name": "Winkler",
                "first_name": "Alexander J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Understanding the future climate is crucial for informed policy decisions on\nclimate change prevention and mitigation. Earth system models play an important\nrole in predicting future climate, requiring accurate representation of complex\nsub-processes that span multiple time scales and spatial scales. One such\nprocess that links seasonal and interannual climate variability to cyclical\nbiological events is tree phenology in deciduous broadleaf forests.\nPhenological dates, such as the start and end of the growing season, are\ncritical for understanding the exchange of carbon and water between the\nbiosphere and the atmosphere. Mechanistic prediction of these dates is\nchallenging. Hybrid modelling, which integrates data-driven approaches into\ncomplex models, offers a solution. In this work, as a first step towards this\ngoal, train a deep neural network to predict a phenological index from\nmeteorological time series. We find that this approach outperforms traditional\nprocess-based models. This highlights the potential of data-driven methods to\nimprove climate predictions. We also analyze which variables and aspects of the\ntime series influence the predicted onset of the season, in order to gain a\nbetter understanding of the advantages and limitations of our model.\n",
        "title": "Comparing Data-Driven and Mechanistic Models for Predicting Phenology in\n  Deciduous Broadleaf Forests",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03964",
        "abstract_url": "http://arxiv.org/abs/2401.03964",
        "authors": [
            {
                "last_name": "Knobloch",
                "first_name": "Petr"
            },
            {
                "last_name": "Kuzmin",
                "first_name": "Dmitri"
            },
            {
                "last_name": "Jha",
                "first_name": "Abhinav"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We address the numerical treatment of source terms in algebraic flux\ncorrection schemes for steady convection-diffusion-reaction (CDR) equations.\nThe proposed algorithm constrains a continuous piecewise-linear finite element\napproximation using a monolithic convex limiting (MCL) strategy. Failure to\ndiscretize the convective derivatives and source terms in a compatible manner\nproduces spurious ripples, e.g., in regions where the coefficients of the\ncontinuous problem are constant and the exact solution is linear. We cure this\ndeficiency by incorporating source term components into the fluxes and\nintermediate states of the MCL procedure. The design of our new limiter is\nmotivated by the desire to preserve simple steady-state equilibria exactly, as\nin well-balanced schemes for the shallow water equations. The results of our\nnumerical experiments for two-dimensional CDR problems illustrate potential\nbenefits of well-balanced flux limiting in the scalar case.\n",
        "title": "Well-balanced convex limiting for finite element discretizations of\n  steady convection-diffusion-reaction equations",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03965",
        "abstract_url": "http://arxiv.org/abs/2401.03965",
        "authors": [
            {
                "last_name": "Ruthotto",
                "first_name": "Lars"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  This short, self-contained article seeks to introduce and survey\ncontinuous-time deep learning approaches that are based on neural ordinary\ndifferential equations (neural ODEs). It primarily targets readers familiar\nwith ordinary and partial differential equations and their analysis who are\ncurious to see their role in machine learning. Using three examples from\nmachine learning and applied mathematics, we will see how neural ODEs can\nprovide new insights into deep learning and a foundation for more efficient\nalgorithms.\n",
        "title": "Differential Equations for Continuous-Time Deep Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03968",
        "abstract_url": "http://arxiv.org/abs/2401.03968",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Erpai"
            },
            {
                "last_name": "Hao",
                "first_name": "Minsheng"
            },
            {
                "last_name": "Wei",
                "first_name": "Lei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xuegong"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Single-cell RNA sequencing (scRNA-seq) data are important for studying the\nbiology of development or diseases at single-cell level. To better understand\nthe properties of the data, to build controlled benchmark data for testing\ndownstream methods, and to augment data when collecting sufficient real data is\nchallenging, generative models have been proposed to computationally generate\nsynthetic scRNA-seq data. However, the data generated with current models are\nnot very realistic yet, especially when we need to generate data with\ncontrolled conditions. In the meantime, the Diffusion models have shown their\npower in generating data in computer vision at high fidelity, providing a new\nopportunity for scRNA-seq generation.\n  In this study, we developed scDiffusion, a diffusion-based model to generate\nhigh-quality scRNA-seq data with controlled conditions. We designed multiple\nclassifiers to guide the diffusion process simultaneously, enabling scDiffusion\nto generate data under multiple condition combinations. We also proposed a new\ncontrol strategy called Gradient Interpolation. This strategy allows the model\nto generate continuous trajectories of cell development from a given cell\nstate.\n  Experiments showed that scDiffusion can generate single-cell gene expression\ndata closely resembling real scRNA-seq data, surpassing state-of-the-art models\nin multiple metrics. Also, scDiffusion can conditionally produce data on\nspecific cell types including rare cell types. Furthermore, we could use the\nmultiple-condition generation of scDiffusion to generate cell type that was out\nof the training data. Leveraging the Gradient Interpolation strategy, we\ngenerated a continuous developmental trajectory of mouse embryonic cells. These\nexperiments demonstrate that scDiffusion is a powerful tool for augmenting the\nreal scRNA-seq data and can provide insights into cell fate research.\n",
        "title": "scDiffusion: conditional generation of high-quality single-cell data\n  using diffusion model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03977",
        "abstract_url": "http://arxiv.org/abs/2401.03977",
        "authors": [
            {
                "last_name": "Tran",
                "first_name": "Ngoc Khue"
            },
            {
                "last_name": "Kieu",
                "first_name": "Trung-Thuy"
            },
            {
                "last_name": "Luong",
                "first_name": "Duc-Trong"
            },
            {
                "last_name": "Ngo",
                "first_name": "Hoang-Long"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  This paper studies the numerical approximation for McKean-Vlasov stochastic\ndifferential equations driven by L\\'evy processes. We propose a tamed-adaptive\nEuler-Maruyama scheme and consider its strong convergence in both finite and\ninfinite time horizons when applying for some classes of L\\'evy-driven\nMcKean-Vlasov stochastic differential equations with non-globally Lipschitz\ncontinuous and super-linearly growth drift and diffusion coefficients.\n",
        "title": "On the infinite time horizon approximation for L\\'evy-driven\n  McKean-Vlasov SDEs with non-globally Lipschitz continuous and super-linearly\n  growth drift and diffusion coefficients",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03981",
        "abstract_url": "http://arxiv.org/abs/2401.03981",
        "authors": [
            {
                "last_name": "Ashby",
                "first_name": "Ben S."
            },
            {
                "last_name": "Pryer",
                "first_name": "Tristan"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this article we present a numerical method for the Stokes flow of an\nOldroyd-B fluid. The viscoelastic stress evolves according to a constitutive\nlaw formulated in terms of the upper convected time derivative. A finite\ndifference method is used to discretise along fluid trajectories to approximate\nthe advection and deformation terms of the upper convected derivative in a\nsimple, cheap and cohesive manner, as well as ensuring that the discrete\nconformation tensor is positive definite. A full implementation with coupling\nto the fluid flow is presented, along with detailed discussion of the issues\nthat arise with such schemes. We demonstrate the performance of this method\nwith detailed numerical experiments in a lid-driven cavity setup. Numerical\nresults are benchmarked against published data, and the method is shown to\nperform well in this challenging case.\n",
        "title": "Discretisation of an Oldroyd-B viscoelastic fluid flow using a Lie\n  derivative formulation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03989",
        "abstract_url": "http://arxiv.org/abs/2401.03989",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Chuyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Yifan"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Chen",
                "first_name": "Qiang"
            },
            {
                "last_name": "Ding",
                "first_name": "Errui"
            },
            {
                "last_name": "Yang",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Jingdong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  DETR accomplishes end-to-end object detection through iteratively generating\nmultiple object candidates based on image features and promoting one candidate\nfor each ground-truth object. The traditional training procedure using\none-to-one supervision in the original DETR lacks direct supervision for the\nobject detection candidates.\n  We aim at improving the DETR training efficiency by explicitly supervising\nthe candidate generation procedure through mixing one-to-one supervision and\none-to-many supervision. Our approach, namely MS-DETR, is simple, and places\none-to-many supervision to the object queries of the primary decoder that is\nused for inference. In comparison to existing DETR variants with one-to-many\nsupervision, such as Group DETR and Hybrid DETR, our approach does not need\nadditional decoder branches or object queries. The object queries of the\nprimary decoder in our approach directly benefit from one-to-many supervision\nand thus are superior in object candidate prediction. Experimental results show\nthat our approach outperforms related DETR variants, such as DN-DETR, Hybrid\nDETR, and Group DETR, and the combination with related DETR variants further\nimproves the performance.\n",
        "title": "MS-DETR: Efficient DETR Training with Mixed Supervision",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03991",
        "abstract_url": "http://arxiv.org/abs/2401.03991",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Fangjun"
            },
            {
                "last_name": "Hogg",
                "first_name": "David C."
            },
            {
                "last_name": "Cohn",
                "first_name": "Anthony G."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "DB",
            "LO"
        ],
        "abstract": "  Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT's spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT's ``cognitive process\", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.\n",
        "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth\n  Evaluation and Enhancement Using the StepGame Benchmark",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03993",
        "abstract_url": "http://arxiv.org/abs/2401.03993",
        "authors": [
            {
                "last_name": "Spick",
                "first_name": "Ryan"
            },
            {
                "last_name": "Bradley",
                "first_name": "Timothy"
            },
            {
                "last_name": "Raina",
                "first_name": "Ayush"
            },
            {
                "last_name": "Amadori",
                "first_name": "Pierluigi Vito"
            },
            {
                "last_name": "Moss",
                "first_name": "Guy"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  This paper describes methods for training autonomous agents to play the game\n\"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We\nalso explore how Reinforcement Learning (RL) compares to IL for humanness by\ncomparing camera movement and trajectory data. Through behavioural cloning, we\nexamine the ability of individual models to learn varying behavioural traits.\nWe attempt to mimic the behaviour of real players with different play styles,\nand find we can train agents that behave aggressively, passively, or simply\nmore human-like than traditional AIs. We propose these methods of introducing\nmore depth and human-like behaviour to agents in video games. The trained IL\nagents perform on par with the average players in our dataset, whilst\noutperforming the worst players. While performance was not as strong as common\nRL approaches, it provides much stronger human-like behavioural traits to the\nagent.\n",
        "title": "Behavioural Cloning in VizDoom",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03996",
        "abstract_url": "http://arxiv.org/abs/2401.03996",
        "authors": [
            {
                "last_name": "Agbeyangi",
                "first_name": "Abayomi"
            },
            {
                "last_name": "Makinde",
                "first_name": "Ayodeji"
            },
            {
                "last_name": "Odun-Ayo",
                "first_name": "Isaac"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Nigeria's remarkable information and communication technology (ICT) journey\nspans decades, playing a pivotal role in economic sustainability, especially as\nthe nation celebrates its Republic at Sixty. This paper provides an overview of\nNigeria's ICT journey, underscoring its central role in sustainable economic\nprosperity. We explore the potential of artificial intelligence, blockchain,\nand the Internet of Things (IoT), revealing the remarkable opportunities on the\nhorizon. We stress the urgency of achieving digital inclusivity, bridging the\nurban-rural gap, and reducing the technological divide, all of which are\ncritical as Nigeria marks its sixtieth year. We intend to prove the invaluable\nopportunities of ICT for policymakers, business leaders, and educational\ninstitutes as Nigeria looks towards enduring economic development in this\ndigital age. Specifically, we envision a dynamic landscape where emerging\ntechnologies are set to redefine industries, supercharge economic growth, and\nenhance the quality of life for every Nigerian.\n",
        "title": "Nigeria's ICT and Economic Sustainability in the Digital Age",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03997",
        "abstract_url": "http://arxiv.org/abs/2401.03997",
        "authors": [
            {
                "last_name": "Mehdifar",
                "first_name": "Farhad"
            },
            {
                "last_name": "Lindemann",
                "first_name": "Lars"
            },
            {
                "last_name": "Bechlioulis",
                "first_name": "Charalampos P."
            },
            {
                "last_name": "Dimarogonas",
                "first_name": "Dimos V."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper introduces a novel control framework for addressing the\nsatisfaction of multiple time-varying output constraints in uncertain\nhigh-order MIMO nonlinear control systems. Unlike existing methods, which often\nassume that the constraints are always decoupled and feasible, our approach can\nhandle coupled time-varying constraints even in the presence of potential\ninfeasibilities. First, it is shown that satisfying multiple constraints\nessentially boils down to ensuring the positivity of a scalar variable,\nrepresenting the signed distance from the boundary of the time-varying\noutput-constrained set. To achieve this, a single consolidating constraint is\ndesigned that, when satisfied, guarantees convergence to and invariance of the\ntime-varying output-constrained set within a user-defined finite time. Next, a\nnovel robust and low-complexity feedback controller is proposed to ensure the\nsatisfaction of the consolidating constraint. Additionally, we provide a\nmechanism for online modification of the consolidating constraint to find a\nleast violating solution when the constraints become mutually infeasible for\nsome time. Finally, simulation examples of trajectory and region tracking for a\nmobile robot validate the proposed approach.\n",
        "title": "Low-Complexity Control for a Class of Uncertain MIMO Nonlinear Systems\n  under Generalized Time-Varying Output Constraints",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03999",
        "abstract_url": "http://arxiv.org/abs/2401.03999",
        "authors": [
            {
                "last_name": "Angell",
                "first_name": "Rico"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  We present solutions to the matrix completion problems proposed by the\nAlignment Research Center that have a polynomial dependence on the precision\n$\\varepsilon$. The motivation for these problems is to enable efficient\ncomputation of heuristic estimators to formally evaluate and reason about\ndifferent quantities of deep neural networks in the interest of AI alignment.\nOur solutions involve reframing the matrix completion problems as a\nsemidefinite program (SDP) and using recent advances in spectral bundle methods\nfor fast, efficient, and scalable SDP solving.\n",
        "title": "Polynomial Precision Dependence Solutions to Alignment Research Center\n  Matrix Completion Problems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04003",
        "abstract_url": "http://arxiv.org/abs/2401.04003",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Xusheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Changliu"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "FL"
        ],
        "abstract": "  Past research into robotic planning with temporal logic specifications,\nnotably Linear Temporal Logic (LTL), was largely based on singular formulas for\nindividual or groups of robots. But with increasing task complexity, LTL\nformulas unavoidably grow lengthy, complicating interpretation and\nspecification generation, and straining the computational capacities of the\nplanners. In order to maximize the potential of LTL specifications, we\ncapitalized on the intrinsic structure of tasks and introduced a hierarchical\nstructure to LTL specifications, and designed an algorithm to ascertain whether\nthey are satisfied given an input sequence. Second, we employ a search-based\napproach to synthesize plans for a multi-robot system, accomplishing\nsimultaneous task allocation and planning. The search space is approximated by\nloosely interconnected sub-spaces, with each sub-space corresponding to one LTL\nspecification. The search is predominantly confined to a single sub-space,\ntransitioning to another sub-space under certain conditions, determined by the\ndecomposition of automatons. Moreover, multiple heuristics are formulated to\nexpedite the search significantly. A theoretical analysis concerning\ncompleteness and optimality is conducted under mild assumptions. When compared\nwith existing methods on service tasks, our method outperforms in terms of\nexecution times with comparable solution quality. Finally, scalability is\nevaluated by testing a group of 30 robots and achieving reasonable runtimes.\n",
        "title": "Simultaneous Task Allocation and Planning for Multi-Robots under\n  Hierarchical Temporal Logic Specifications",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04004",
        "abstract_url": "http://arxiv.org/abs/2401.04004",
        "authors": [
            {
                "last_name": "Rani",
                "first_name": "Jyoti"
            },
            {
                "last_name": "Tripura",
                "first_name": "Tapas"
            },
            {
                "last_name": "Kodamana",
                "first_name": "Hariprasad"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Souvik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Fault detection and isolation in complex systems are critical to ensure\nreliable and efficient operation. However, traditional fault detection methods\noften struggle with issues such as nonlinearity and multivariate\ncharacteristics of the time series variables. This article proposes a\ngenerative adversarial wavelet neural operator (GAWNO) as a novel unsupervised\ndeep learning approach for fault detection and isolation of multivariate time\nseries processes.The GAWNO combines the strengths of wavelet neural operators\nand generative adversarial networks (GANs) to effectively capture both the\ntemporal distributions and the spatial dependencies among different variables\nof an underlying system. The approach of fault detection and isolation using\nGAWNO consists of two main stages. In the first stage, the GAWNO is trained on\na dataset of normal operating conditions to learn the underlying data\ndistribution. In the second stage, a reconstruction error-based threshold\napproach using the trained GAWNO is employed to detect and isolate faults based\non the discrepancy values. We validate the proposed approach using the\nTennessee Eastman Process (TEP) dataset and Avedore wastewater treatment plant\n(WWTP) and N2O emissions named as WWTPN2O datasets. Overall, we showcase that\nthe idea of harnessing the power of wavelet analysis, neural operators, and\ngenerative models in a single framework to detect and isolate faults has shown\npromising results compared to various well-established baselines in the\nliterature.\n",
        "title": "Generative adversarial wavelet neural operator: Application to fault\n  detection and isolation of multivariate time series data",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04007",
        "abstract_url": "http://arxiv.org/abs/2401.04007",
        "authors": [
            {
                "last_name": "LaGrassa",
                "first_name": "Alex"
            },
            {
                "last_name": "Lee",
                "first_name": "Moonyoung"
            },
            {
                "last_name": "Kroemer",
                "first_name": "Oliver"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  When planning with an inaccurate dynamics model, a practical strategy is to\nrestrict planning to regions of state-action space where the model is accurate:\nalso known as a model precondition. Empirical real-world trajectory data is\nvaluable for defining data-driven model preconditions regardless of the model\nform (analytical, simulator, learned, etc...). However, real-world data is\noften expensive and dangerous to collect. In order to achieve data efficiency,\nthis paper presents an algorithm for actively selecting trajectories to learn a\nmodel precondition for an inaccurate pre-specified dynamics model. Our proposed\ntechniques address challenges arising from the sequential nature of\ntrajectories, and potential benefit of prioritizing task-relevant data. The\nexperimental analysis shows how algorithmic properties affect performance in\nthree planning scenarios: icy gridworld, simulated plant watering, and\nreal-world plant watering. Results demonstrate an improvement of approximately\n80% after only four real-world trajectories when using our proposed techniques.\n",
        "title": "Task-Oriented Active Learning of Model Preconditions for Inaccurate\n  Dynamics Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04012",
        "abstract_url": "http://arxiv.org/abs/2401.04012",
        "authors": [
            {
                "last_name": "Perotti",
                "first_name": "Matteo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yichao"
            },
            {
                "last_name": "Cavalcante",
                "first_name": "Matheus"
            },
            {
                "last_name": "Mustafa",
                "first_name": "Enis"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Dense Matrix Multiplication (MatMul) is arguably one of the most ubiquitous\ncompute-intensive kernels, spanning linear algebra, DSP, graphics, and machine\nlearning applications. Thus, MatMul optimization is crucial not only in\nhigh-performance processors but also in embedded low-power platforms. Several\nInstruction Set Architectures (ISAs) have recently included matrix extensions\nto improve MatMul performance and efficiency at the cost of added matrix\nregister files and units. In this paper, we propose Matrix eXtension (MX), a\nlightweight approach that builds upon the open-source RISC-V Vector (RVV) ISA\nto boost MatMul energy efficiency. Instead of adding expensive dedicated\nhardware, MX uses the pre-existing vector register file and functional units to\ncreate a hybrid vector/matrix engine at a negligible area cost (< 3%), which\ncomes from a compact near-FPU tile buffer for higher data reuse, and no clock\nfrequency overhead. We implement MX on a compact and highly energy-optimized\nRVV processor and evaluate it in both a Dual- and 64-Core cluster in a 12-nm\ntechnology node. MX boosts the Dual-Core's energy efficiency by 10% for a\ndouble-precision 64x64x64 matrix multiplication with the same FPU utilization\n(~97%) and by 25% on the 64-Core cluster for the same benchmark on 32-bit data,\nwith a 56% performance gain.\n",
        "title": "MX: Enhancing RISC-V's Vector ISA for Ultra-Low Overhead,\n  Energy-Efficient Matrix Multiplication",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04013",
        "abstract_url": "http://arxiv.org/abs/2401.04013",
        "authors": [
            {
                "last_name": "Shem-Ur",
                "first_name": "Ori"
            },
            {
                "last_name": "Oz",
                "first_name": "Yaron"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Deep learning models, such as wide neural networks, can be conceptualized as\nnonlinear dynamical physical systems characterized by a multitude of\ninteracting degrees of freedom. Such systems in the infinite limit, tend to\nexhibit simplified dynamics. This paper delves into gradient descent-based\nlearning algorithms, that display a linear structure in their parameter\ndynamics, reminiscent of the neural tangent kernel. We establish this apparent\nlinearity arises due to weak correlations between the first and higher-order\nderivatives of the hypothesis function, concerning the parameters, taken around\ntheir initial values. This insight suggests that these weak correlations could\nbe the underlying reason for the observed linearization in such systems. As a\ncase in point, we showcase this weak correlations structure within neural\nnetworks in the large width limit. Exploiting the relationship between\nlinearity and weak correlations, we derive a bound on deviations from linearity\nobserved during the training trajectory of stochastic gradient descent. To\nfacilitate our proof, we introduce a novel method to characterise the\nasymptotic behavior of random tensors.\n",
        "title": "Weak Correlations as the Underlying Principle for Linearization of\n  Gradient-Based Learning Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04016",
        "abstract_url": "http://arxiv.org/abs/2401.04016",
        "authors": [
            {
                "last_name": "Galante",
                "first_name": "Nicola"
            },
            {
                "last_name": "Moiola",
                "first_name": "Andrea"
            },
            {
                "last_name": "Parolin",
                "first_name": "Emile"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The goal of this paper is to show that evanescent plane waves are much better\nat numerically approximating Helmholtz solutions than classical propagative\nplane waves. By generalizing the Jacobi$\\unicode{x2013}$Anger identity to\ncomplex-valued directions, we first prove that any solution of the Helmholtz\nequation on a three-dimensional ball can be written as a continuous\nsuperposition of evanescent plane waves in a stable way. We then propose a\npractical numerical recipe to select discrete approximation sets of evanescent\nplane waves, which exhibits considerable improvements over standard propagative\nplane wave schemes in numerical experiments. We show that all this is not\npossible for propagative plane waves: they cannot stably represent general\nHelmholtz solutions, and any approximation based on discrete sets of\npropagative plane waves is doomed to have exponentially large coefficients and\nthus to be numerically unstable. This paper is motivated by applications to\nTrefftz-type Galerkin schemes and extends the recent results in [Parolin,\nHuybrechs and Moiola, M2AN, 2023] from two to three space dimensions.\n",
        "title": "Stable approximation of Helmholtz solutions in the ball using evanescent\n  plane waves",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04020",
        "abstract_url": "http://arxiv.org/abs/2401.04020",
        "authors": [
            {
                "last_name": "Elishco",
                "first_name": "Ohad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  In response to the evolving landscape of data storage, researchers have\nincreasingly explored non-traditional platforms, with DNA-based storage\nemerging as a cutting-edge solution. Our work is motivated by the potential of\nin-vivo DNA storage, known for its capacity to store vast amounts of\ninformation efficiently and confidentially within an organism's native DNA.\nWhile promising, in-vivo DNA storage faces challenges, including susceptibility\nto errors introduced by mutations. To understand the long-term behavior of such\nmutation systems, we investigate the frequency of $k$-tuples after multiple\nmutation applications.\n  Drawing inspiration from related works, we generalize results from the study\nof mutation systems, particularly focusing on the frequency of $k$-tuples. In\nthis work, we provide a broad analysis through the construction of a\nspecialized matrix and the identification of its eigenvectors. In the context\nof substitution and duplication systems, we leverage previous results on almost\nsure convergence, equating the expected frequency to the limiting frequency.\nMoreover, we demonstrate convergence in probability under certain assumptions.\n",
        "title": "On the Long-Term behavior of $k$-tuples Frequencies in Mutation Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04022",
        "abstract_url": "http://arxiv.org/abs/2401.04022",
        "authors": [
            {
                "last_name": "Porter",
                "first_name": "Simon J."
            },
            {
                "last_name": "McIntosh",
                "first_name": "Leslie D."
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  Fabricated papers do not just need text, images, and data, they also require\na fabricated or partially fabricated network of authors. Most `authors' on a\nfabricated paper have not been associated with the research, but rather are\nadded through a transaction. This lack of deeper connection means that there is\na low likelihood that co-authors on fabricated papers will ever appear together\non the same paper more than once. This paper constructs a model that encodes\nsome of the key characteristics of this activity in an `authorship-for-sale'\nnetwork with the aim to create a robust method to detect this type of activity.\nA characteristic network fingerprint arises from this model that provides a\nrobust statistical approach to the detection of paper-mill networks. The model\nsuggested in this paper detects networks that have a statistically significant\noverlap with other approaches that principally rely on textual analysis for the\ndetection of fraudulent papers. Researchers connected to networks identified\nusing the methodology outlined in this paper are shown to be connected with 37%\nof papers identified through the tortured-phrase and clay-feet methods deployed\nin the Problematic Paper Screener website. Finally, methods to limit the\nexpansion and propagation of these networks is discussed both in technological\nand social terms.\n",
        "title": "Identifying Fabricated Networks within Authorship-for-Sale Enterprises",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04023",
        "abstract_url": "http://arxiv.org/abs/2401.04023",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wentao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  In recent years, researchers combine both audio and video signals to deal\nwith challenges where actions are not well represented or captured by visual\ncues. However, how to effectively leverage the two modalities is still under\ndevelopment. In this work, we develop a multiscale multimodal Transformer (MMT)\nthat leverages hierarchical representation learning. Particularly, MMT is\ncomposed of a novel multiscale audio Transformer (MAT) and a multiscale video\nTransformer [43]. To learn a discriminative cross-modality fusion, we further\ndesign multimodal supervised contrastive objectives called audio-video\ncontrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly\nalign the two modalities. MMT surpasses previous state-of-the-art approaches by\n7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy\nwithout external training data. Moreover, the proposed MAT significantly\noutperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark\ndatasets, and is about 3% more efficient based on the number of FLOPs and 9.8%\nmore efficient based on GPU memory usage.\n",
        "title": "Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video\n  Classification",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04025",
        "abstract_url": "http://arxiv.org/abs/2401.04025",
        "authors": [
            {
                "last_name": "Alsuhaibani",
                "first_name": "Abdullah"
            },
            {
                "last_name": "Zogan",
                "first_name": "Hamad"
            },
            {
                "last_name": "Razzak",
                "first_name": "Imran"
            },
            {
                "last_name": "Jameel",
                "first_name": "Shoaib"
            },
            {
                "last_name": "Xu",
                "first_name": "Guandong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have been very effective in various Natural Language\nProcessing (NLP) and text mining tasks including text classification. However,\nsome tasks still pose challenges for these models, including text\nclassification with limited labels. This can result in a cold-start problem.\nAlthough some approaches have attempted to address this problem through\nsingle-stage clustering as an intermediate training step coupled with a\npre-trained language model, which generates pseudo-labels to improve\nclassification, these methods are often error-prone due to the limitations of\nthe clustering algorithms. To overcome this, we have developed a novel\ntwo-stage intermediate clustering with subsequent fine-tuning that models the\npseudo-labels reliably, resulting in reduced prediction errors. The key novelty\nin our model, IDoFew, is that the two-stage clustering coupled with two\ndifferent clustering algorithms helps exploit the advantages of the\ncomplementary algorithms that reduce the errors in generating reliable\npseudo-labels for fine-tuning. Our approach has shown significant improvements\ncompared to strong comparative models.\n",
        "title": "IDoFew: Intermediate Training Using Dual-Clustering in Language Models\n  for Few Labels Text Classification",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04032",
        "abstract_url": "http://arxiv.org/abs/2401.04032",
        "authors": [
            {
                "last_name": "Menges",
                "first_name": "Daniel"
            },
            {
                "last_name": "Von Brandis",
                "first_name": "Andreas"
            },
            {
                "last_name": "Rasheed",
                "first_name": "Adil"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Autonomous surface vessels (ASVs) play an increasingly important role in the\nsafety and sustainability of open sea operations. Since most maritime accidents\nare related to human failure, intelligent algorithms for autonomous collision\navoidance and path following can drastically reduce the risk in the maritime\nsector. A DT is a virtual representative of a real physical system and can\nenhance the situational awareness (SITAW) of such an ASV to generate optimal\ndecisions. This work builds on an existing DT framework for ASVs and\ndemonstrates foundations for enabling predictive, prescriptive, and autonomous\ncapabilities. In this context, sophisticated target tracking approaches are\ncrucial for estimating and predicting the position and motion of other dynamic\nobjects. The applied tracking method is enabled by real-time automatic\nidentification system (AIS) data and synthetic light detection and ranging\n(Lidar) measurements. To guarantee safety during autonomous operations, we\napplied a predictive safety filter, based on the concept of nonlinear model\npredictive control (NMPC). The approaches are implemented into a DT built with\nthe Unity game engine. As a result, this work demonstrates the potential of a\nDT capable of making predictions, playing through various what-if scenarios,\nand providing optimal control decisions according to its enhanced SITAW.\n",
        "title": "Digital Twin for Autonomous Surface Vessels for Safe Maritime Navigation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04039",
        "abstract_url": "http://arxiv.org/abs/2401.04039",
        "authors": [
            {
                "last_name": "Barman",
                "first_name": "Nabajeet"
            },
            {
                "last_name": "Martini",
                "first_name": "Maria G."
            },
            {
                "last_name": "Reznik",
                "first_name": "Yuriy"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "IT",
            ""
        ],
        "abstract": "  The Bj{\\o}ntegaard Delta (BD) method proposed in 2001 has become a popular\ntool for comparing video codec compression efficiency. It was initially\nproposed to compute bitrate and quality differences between two Rate-Distortion\ncurves using PSNR as a distortion metric. Over the years, many works have\ncalculated and reported BD results using other objective quality metrics such\nas SSIM, VMAF and, in some cases, even subjective ratings (mean opinion\nscores). However, the lack of consolidated literature explaining the metric,\nits evolution over the years, and a systematic evaluation of the same under\ndifferent test conditions can result in a wrong interpretation of the BD\nresults thus obtained.\n  Towards this end, this paper presents a detailed tutorial describing the BD\nmethod and example cases where the metric might fail. We also provide a\ndetailed history of its evolution, including a discussion of various proposed\nimprovements and variations over the last 20 years. In addition, we evaluate\nthe various BD methods and their open-source implementations, considering\ndifferent objective quality metrics and subjective ratings taking into account\ndifferent RD characteristics. Based on our results, we present a set of\nrecommendations on using existing BD metrics and various insights for possible\nexploration towards developing more effective tools for codec compression\nefficiency evaluation and comparison.\n",
        "title": "Bj{\\o}ntegaard Delta (BD): A Tutorial Overview of the Metric, Evolution,\n  Challenges, and Recommendations",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04040",
        "abstract_url": "http://arxiv.org/abs/2401.04040",
        "authors": [
            {
                "last_name": "Blauth",
                "first_name": "Jannis"
            },
            {
                "last_name": "Ellerbrock",
                "first_name": "Antonia"
            },
            {
                "last_name": "Traub",
                "first_name": "Vera"
            },
            {
                "last_name": "Vygen",
                "first_name": "Jens"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GT"
        ],
        "abstract": "  We consider cost allocation for set covering problems. We allocate as much\ncost to the elements (players) as possible without violating the group\nrationality condition (no subset of players pays more than covering this subset\nwould cost), and so that the excess vector is lexicographically maximized. This\nis identical to the well-known nucleolus if the core of the corresponding\ncooperative game is nonempty, i.e., if some optimum fractional cover is\nintegral. In general, we call this the 'happy nucleolus'. Like for the\nnucleolus, the excess vector contains an entry for every subset of players, not\nonly for the sets in the given set covering instance. Moreover, it is NP-hard\nto compute a single entry because this requires solving a set covering problem.\nNevertheless, we give an explicit family of at most $mn$ subsets, each with a\ntrivial cover (by a single set), such that the happy nucleolus is always\ncompletely determined by this proxy excess vector; here $m$ and $n$ denote the\nnumber of sets and the number of players in our set covering instance. We show\nthat this is the unique minimal such family in a natural sense. While computing\nthe nucleolus for set covering is NP-hard, our results imply that the happy\nnucleolus can be computed in polynomial time.\n",
        "title": "Cost Allocation for Set Covering: the Happy Nucleolus",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04044",
        "abstract_url": "http://arxiv.org/abs/2401.04044",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zirui"
            },
            {
                "last_name": "Song",
                "first_name": "Qingquan"
            },
            {
                "last_name": "Xiao",
                "first_name": "Qiang Charles"
            },
            {
                "last_name": "Selvaraj",
                "first_name": "Sathiya Keerthi"
            },
            {
                "last_name": "Mazumder",
                "first_name": "Rahul"
            },
            {
                "last_name": "Gupta",
                "first_name": "Aman"
            },
            {
                "last_name": "Hu",
                "first_name": "Xia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The large number of parameters in Pretrained Language Models enhance their\nperformance, but also make them resource-intensive, making it challenging to\ndeploy them on commodity hardware like a single GPU. Due to the memory and\npower limitations of these devices, model compression techniques are often used\nto decrease both the model's size and its inference latency. This usually\nresults in a trade-off between model accuracy and efficiency. Therefore,\noptimizing this balance is essential for effectively deploying LLMs on\ncommodity hardware. A significant portion of the efficiency challenge is the\nFeed-forward network (FFN) component, which accounts for roughly $\\frac{2}{3}$\ntotal parameters and inference latency. In this paper, we first observe that\nonly a few neurons of FFN module have large output norm for any input tokens,\na.k.a. heavy hitters, while the others are sparsely triggered by different\ntokens. Based on this observation, we explicitly split the FFN into two parts\naccording to the heavy hitters. We improve the efficiency-accuracy trade-off of\nexisting compression methods by allocating more resource to FFN parts with\nheavy hitters. In practice, our method can reduce model size by 43.1\\% and\nbring $1.25\\sim1.56\\times$ wall clock time speedup on different hardware with\nnegligible accuracy drop.\n",
        "title": "FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency\n  Trade-off in Language Model Inference",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04051",
        "abstract_url": "http://arxiv.org/abs/2401.04051",
        "authors": [
            {
                "last_name": "Doering",
                "first_name": "Nigel"
            },
            {
                "last_name": "Gorlla",
                "first_name": "Cyril"
            },
            {
                "last_name": "Tuttle",
                "first_name": "Trevor"
            },
            {
                "last_name": "Vijay",
                "first_name": "Adhvaith"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Fine-tuning large pre-trained language models for downstream tasks remains a\ncritical challenge in natural language processing. This paper presents an\nempirical analysis comparing two efficient fine-tuning methods - BitFit and\nadapter modules - to standard full model fine-tuning. Experiments conducted on\nGLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The\nBitFit approach, which trains only bias terms and task heads, matches full\nfine-tuning performance across varying amounts of training data and time\nconstraints. It demonstrates remarkable stability even with only 30\\% of data,\noutperforming full fine-tuning at intermediate data levels. Adapter modules\nexhibit high variability, with inconsistent gains over default models. The\nfindings indicate BitFit offers an attractive balance between performance and\nparameter efficiency. Our work provides valuable perspectives on model tuning,\nemphasizing robustness and highlighting BitFit as a promising alternative for\nresource-constrained or streaming task settings. The analysis offers actionable\nguidelines for efficient adaptation of large pre-trained models, while\nillustrating open challenges in stabilizing techniques like adapter modules.\n",
        "title": "Empirical Analysis of Efficient Fine-Tuning Methods for Large\n  Pre-Trained Language Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04052",
        "abstract_url": "http://arxiv.org/abs/2401.04052",
        "authors": [
            {
                "last_name": "Stokes",
                "first_name": "Chase"
            },
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            },
            {
                "last_name": "Hearst",
                "first_name": "Marti A."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  This paper investigates the role of text in visualizations, specifically the\nimpact of text position, semantic content, and biased wording. Two empirical\nstudies were conducted based on two tasks (predicting data trends and\nappraising bias) using two visualization types (bar and line charts). While the\naddition of text had a minimal effect on how people perceive data trends, there\nwas a significant impact on how biased they perceive the authors to be. This\nfinding revealed a relationship between the degree of bias in textual\ninformation and the perception of the authors' bias. Exploratory analyses\nsupport an interaction between a person's prediction and the degree of bias\nthey perceived. This paper also develops a crowdsourced method for creating\nchart annotations that range from neutral to highly biased. This research\nhighlights the need for designers to mitigate potential polarization of\nreaders' opinions based on how authors' ideas are expressed.\n",
        "title": "The Role of Text in Visualizations: How Annotations Shape Perceptions of\n  Bias and Influence Predictions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04053",
        "abstract_url": "http://arxiv.org/abs/2401.04053",
        "authors": [
            {
                "last_name": "Sagtani",
                "first_name": "Hitesh"
            },
            {
                "last_name": "Jeunen",
                "first_name": "Olivier"
            },
            {
                "last_name": "Ustimenko",
                "first_name": "Aleksei"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Many platforms on the web present ranked lists of content to users, typically\noptimized for engagement-, satisfaction- or retention- driven metrics. Advances\nin the Learning-to-Rank (LTR) research literature have enabled rapid growth in\nthis application area. Several popular interfaces now include nested lists,\nwhere users can enter a 2nd-level feed via any given 1st-level item. Naturally,\nthis has implications for evaluation metrics, objective functions, and the\nranking policies we wish to learn. We propose a theoretically grounded method\nto incorporate 2nd-level feedback into any 1st-level ranking model. Online\nexperiments on a large-scale recommendation system confirm our theoretical\nfindings.\n",
        "title": "Learning-to-Rank with Nested Feedback",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04055",
        "abstract_url": "http://arxiv.org/abs/2401.04055",
        "authors": [
            {
                "last_name": "Mandikal",
                "first_name": "Priyanka"
            },
            {
                "last_name": "Mooney",
                "first_name": "Raymond"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Traditional information retrieval is based on sparse bag-of-words vector\nrepresentations of documents and queries. More recent deep-learning approaches\nhave used dense embeddings learned using a transformer-based large language\nmodel. We show that on a classic benchmark on scientific document retrieval in\nthe medical domain of cystic fibrosis, that both of these models perform\nroughly equivalently. Notably, dense vectors from the state-of-the-art SPECTER2\nmodel do not significantly enhance performance. However, a hybrid model that we\npropose combining these methods yields significantly better results,\nunderscoring the merits of integrating classical and contemporary deep learning\ntechniques in information retrieval in the domain of specialized scientific\ndocuments.\n",
        "title": "Sparse Meets Dense: A Hybrid Approach to Enhance Scientific Document\n  Retrieval",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04056",
        "abstract_url": "http://arxiv.org/abs/2401.04056",
        "authors": [
            {
                "last_name": "Swamy",
                "first_name": "Gokul"
            },
            {
                "last_name": "Dann",
                "first_name": "Christoph"
            },
            {
                "last_name": "Kidambi",
                "first_name": "Rahul"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhiwei Steven"
            },
            {
                "last_name": "Agarwal",
                "first_name": "Alekh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present Self-Play Preference Optimization (SPO), an algorithm for\nreinforcement learning from human feedback. Our approach is minimalist in that\nit does not require training a reward model nor unstable adversarial training\nand is therefore rather simple to implement. Our approach is maximalist in that\nit provably handles non-Markovian, intransitive, and stochastic preferences\nwhile being robust to the compounding errors that plague offline approaches to\nsequential prediction. To achieve the preceding qualities, we build upon the\nconcept of a Minimax Winner (MW), a notion of preference aggregation from the\nsocial choice theory literature that frames learning from preferences as a\nzero-sum game between two policies. By leveraging the symmetry of this game, we\nprove that rather than using the traditional technique of dueling two policies\nto compute the MW, we can simply have a single agent play against itself while\nmaintaining strong convergence guarantees. Practically, this corresponds to\nsampling multiple trajectories from a policy, asking a rater or preference\nmodel to compare them, and then using the proportion of wins as the reward for\na particular trajectory. We demonstrate that on a suite of continuous control\ntasks, we are able to learn significantly more efficiently than reward-model\nbased approaches while maintaining robustness to the intransitive and\nstochastic preferences that frequently occur in practice when aggregating human\njudgments.\n",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04057",
        "abstract_url": "http://arxiv.org/abs/2401.04057",
        "authors": [
            {
                "last_name": "Sah",
                "first_name": "Chandan Kumar"
            },
            {
                "last_name": "Xiaoli",
                "first_name": "Dr. Lian"
            },
            {
                "last_name": "Islam",
                "first_name": "Muhammad Mirajul"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "SE"
        ],
        "abstract": "  The rise of generative artificial intelligence, particularly Large Language\nModels (LLMs), has intensified the imperative to scrutinize fairness alongside\naccuracy. Recent studies have begun to investigate fairness evaluations for\nLLMs within domains such as recommendations. Given that personalization is an\nintrinsic aspect of recommendation systems, its incorporation into fairness\nassessments is paramount. Yet, the degree to which current fairness evaluation\nframeworks account for personalization remains unclear. Our comprehensive\nliterature review aims to fill this gap by examining how existing frameworks\nhandle fairness evaluations of LLMs, with a focus on the integration of\npersonalization factors. Despite an exhaustive collection and analysis of\nrelevant works, we discovered that most evaluations overlook personalization, a\ncritical facet of recommendation systems, thereby inadvertently perpetuating\nunfair practices. Our findings shed light on this oversight and underscore the\nurgent need for more nuanced fairness evaluations that acknowledge\npersonalization. Such improvements are vital for fostering equitable\ndevelopment within the AI community.\n",
        "title": "Unveiling Bias in Fairness Evaluations of Large Language Models: A\n  Critical Literature Review of Music and Movie Recommendation Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04059",
        "abstract_url": "http://arxiv.org/abs/2401.04059",
        "authors": [
            {
                "last_name": "Ghadi",
                "first_name": "Farshad Rostami"
            },
            {
                "last_name": "Kaveh",
                "first_name": "Masoud"
            },
            {
                "last_name": "Wong",
                "first_name": "Kai-Kit"
            },
            {
                "last_name": "Martin",
                "first_name": "Diego"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper investigates the performance of physical layer security (PLS) in a\nvehicle-to-vehicle (V2V) communication system, where a transmitter vehicle\nexploits a dual reconfigurable intelligent surface (RIS) to send confidential\ninformation to legitimate receiver vehicles under the non-orthogonal multiple\naccess (NOMA) scheme in the presence of an eavesdropper vehicle. In particular,\nit is assumed that an RIS is near the transmitter vehicle and another RIS is\nclose to the receiver vehicles to provide a wider smart radio environment.\nBesides, we suppose that the channels between two RISs suffer from the\nFisher-Snedecor F fading model. Under this scenario, we first provide the\nmarginal distributions of equivalent channels at the legitimate receiver\nvehicles by exploiting the central limit theorem (CLT). Then, in order to\nevaluate the PLS performance of the considered secure communication system, we\nderive analytical expressions of the average secrecy capacity (ASC), secrecy\noutage probability (SOP), and secrecy energy efficiency (SEE) by using the\nGauss-Laguerre quadrature and the Gaussian quadrature techniques. Moreover, to\ngain more insights into the secrecy performance, the asymptotic expression of\nthe ASC is obtained. The numerical results indicate that incorporating the dual\nRIS in the secure V2V communication under the NOMA scheme can significantly\nprovide ultra-reliable transmission and guarantee more secure communication for\nintelligent transportation systems (ITS).\n",
        "title": "Physical Layer Security Performance of Dual RIS-aided V2V NOMA\n  Communications",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04062",
        "abstract_url": "http://arxiv.org/abs/2401.04062",
        "authors": [
            {
                "last_name": "Baweja",
                "first_name": "Shubham"
            },
            {
                "last_name": "Pokharna",
                "first_name": "Neeti"
            },
            {
                "last_name": "Ustimenko",
                "first_name": "Aleksei"
            },
            {
                "last_name": "Jeunen",
                "first_name": "Olivier"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR",
            ""
        ],
        "abstract": "  Online controlled experiments, such as A/B-tests, are commonly used by modern\ntech companies to enable continuous system improvements. Despite their\nparamount importance, A/B-tests are expensive: by their very definition, a\npercentage of traffic is assigned an inferior system variant. To ensure\nstatistical significance on top-level metrics, online experiments typically run\nfor several weeks. Even then, a considerable amount of experiments will lead to\ninconclusive results (i.e. false negatives, or type-II error). The main culprit\nfor this inefficiency is the variance of the online metrics. Variance reduction\ntechniques have been proposed in the literature, but their direct applicability\nto commonly used ratio metrics (e.g. click-through rate or user retention) is\nlimited.\n  In this work, we successfully apply variance reduction techniques to ratio\nmetrics on a large-scale short-video platform: ShareChat. Our empirical results\nshow that we can either improve A/B-test confidence in 77% of cases, or can\nretain the same level of confidence with 30% fewer data points. Importantly, we\nshow that the common approach of including as many covariates as possible in\nregression is counter-productive, highlighting that control variates based on\nGradient-Boosted Decision Tree predictors are most effective. We discuss the\npracticalities of implementing these methods at scale and showcase the cost\nreduction they beget.\n",
        "title": "Variance Reduction in Ratio Metrics for Efficient Online Experiments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04063",
        "abstract_url": "http://arxiv.org/abs/2401.04063",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Yue"
            },
            {
                "last_name": "Guan",
                "first_name": "Zhijin"
            },
            {
                "last_name": "Xie",
                "first_name": "Hehu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Chenguang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This study proposes a class of augmented subspace schemes for the weak\nGalerkin (WG) finite element method used to solve eigenvalue problems. The\naugmented subspace is built with the conforming linear finite element space\ndefined on the coarse mesh and the eigenfunction approximations in the WG\nfinite element space defined on the fine mesh. Based on this augmented\nsubspace, solving the eigenvalue problem in the fine WG finite element space\ncan be reduced to the solution of the linear boundary value problem in the same\nWG finite element space and a low dimensional eigenvalue problem in the\naugmented subspace. The proposed augmented subspace techniques have the second\norder convergence rate with respect to the coarse mesh size, as demonstrated by\nthe accompanying error estimates. Finally, a few numerical examples are\nprovided to validate the proposed numerical techniques.\n",
        "title": "Augmented Subspace Scheme for Eigenvalue Problem by Weak Galerkin Finite\n  Element Method",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04067",
        "abstract_url": "http://arxiv.org/abs/2401.04067",
        "authors": [
            {
                "last_name": "Hendrickx",
                "first_name": "Julien"
            },
            {
                "last_name": "Olshevsky",
                "first_name": "Alex"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  We consider the generalization error associated with stochastic gradient\ndescent on a smooth convex function over a compact set. We show the first bound\non the generalization error that vanishes when the number of iterations $T$ and\nthe dataset size $n$ go to zero at arbitrary rates; our bound scales as\n$\\tilde{O}(1/\\sqrt{T} + 1/\\sqrt{n})$ with step-size $\\alpha_t = 1/\\sqrt{t}$. In\nparticular, strong convexity is not needed for stochastic gradient descent to\ngeneralize well.\n",
        "title": "Convex SGD: Generalization Without Early Stopping",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04068",
        "abstract_url": "http://arxiv.org/abs/2401.04068",
        "authors": [
            {
                "last_name": "Mathiesen",
                "first_name": "Frederik Baymler"
            },
            {
                "last_name": "Lahijanian",
                "first_name": "Morteza"
            },
            {
                "last_name": "Laurenti",
                "first_name": "Luca"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper, we present IntervalMDP.jl, a Julia package for probabilistic\nanalysis of interval Markov Decision Processes (IMDPs). IntervalMDP.jl\nfacilitates the synthesis of optimal strategies and verification of IMDPs\nagainst reachability specifications and discounted reward properties. The\nlibrary supports sparse matrices and is compatible with common tools for\nanalysis of probabilistic models, such as PRISM. A key feature of\nIntervalMDP.jl is that it presents both a multi-threaded CPU and a\nGPU-accelerated implementation of value iteration algorithms for IMDPs. In\nparticular, IntervalMDP.jl takes advantage of the Julia type system and the\ninherently parallelizable nature of value iteration to improve the efficiency\nof performing analysis of IMDPs. On a set of examples, we show that\nIntervalMDP.jl substantially outperforms existing tools for verification and\nstrategy synthesis for IMDPs in both computation time and memory consumption.\n",
        "title": "IntervalMDP.jl: Accelerated Value Iteration for Interval Markov Decision\n  Processes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04071",
        "abstract_url": "http://arxiv.org/abs/2401.04071",
        "authors": [
            {
                "last_name": "Mankovich",
                "first_name": "Nathan"
            },
            {
                "last_name": "Camps-Valls",
                "first_name": "Gustau"
            },
            {
                "last_name": "Birdal",
                "first_name": "Tolga"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "",
            "",
            ""
        ],
        "abstract": "  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, \\ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n",
        "title": "Fun with Flags: Robust Principal Directions via Flag Manifolds",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04076",
        "abstract_url": "http://arxiv.org/abs/2401.04076",
        "authors": [
            {
                "last_name": "Asiri",
                "first_name": "Norah"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  Even with the vast potential that cloud computing has, so far, it has not\nbeen adopted by the consumers with the enthusiasm and pace that it be worthy;\nthis is a very reason statement why consumers still hesitated of using cloud\ncomputing for their sensitive data and the threats that prevent the consumers\nfrom shifting to use cloud computing in general and cloud storage in\nparticular. The cloud computing inherits the traditional potential security and\nprivacy threats besides its own issues due to its unique structures. Some\nthreats related to cloud computing are the insider malicious attacks from the\nemployees that even sometime the provider unconscious about, the lack of\ntransparency of agreement between consumer and provider, data loss, traffic\nhijacking, shared technology and insecure application interface. Such threats\nneed remedies to make the consumer use its features in secure way. In this\nreview, we spot the light on the most security and privacy issues which can be\nattributed as gaps that sometimes the consumers or even the enterprises are not\naware of. We also define the parties that involve in scenario of cloud\ncomputing that also may attack the entire cloud systems. We also show the\nconsequences of these threats.\n",
        "title": "Security and Privacy Issues in Cloud Storage",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04077",
        "abstract_url": "http://arxiv.org/abs/2401.04077",
        "authors": [
            {
                "last_name": "Gallyas-Sanhueza",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Marti",
                "first_name": "Gian"
            },
            {
                "last_name": "Palhares",
                "first_name": "Victoria"
            },
            {
                "last_name": "Wiesmayr",
                "first_name": "Reinhard"
            },
            {
                "last_name": "Studer",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  We propose new low-fidelity (LoFi) user equipment (UE) scheduling algorithms\nfor multiuser multiple-input multiple-output (MIMO) wireless communication\nsystems. The proposed methods rely on an efficient guess-and-check procedure\nthat, given an objective function, performs paired comparisons between random\nsubsets of UEs that should be scheduled in certain time slots. The proposed\nLoFi scheduling methods are computationally efficient, highly parallelizable,\nand gradient-free, which enables the use of almost arbitrary,\nnon-differentiable objective functions. System simulations in a millimeter-wave\n(mmWave) multiuser MIMO scenario demonstrate that the proposed LoFi schedulers\noutperform a range of state-of-the-art user scheduling algorithms in terms of\nbit error-rate and/or computational complexity.\n",
        "title": "LoFi User Scheduling for Multiuser MIMO Wireless Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04079",
        "abstract_url": "http://arxiv.org/abs/2401.04079",
        "authors": [
            {
                "last_name": "Dippel",
                "first_name": "Jonas"
            },
            {
                "last_name": "Feulner",
                "first_name": "Barbara"
            },
            {
                "last_name": "Winterhoff",
                "first_name": "Tobias"
            },
            {
                "last_name": "Schallenberg",
                "first_name": "Simon"
            },
            {
                "last_name": "Dernbach",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Kunft",
                "first_name": "Andreas"
            },
            {
                "last_name": "Tietz",
                "first_name": "Stephan"
            },
            {
                "last_name": "Jurmeister",
                "first_name": "Philipp"
            },
            {
                "last_name": "Horst",
                "first_name": "David"
            },
            {
                "last_name": "Ruff",
                "first_name": "Lukas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Klaus-Robert"
            },
            {
                "last_name": "Klauschen",
                "first_name": "Frederick"
            },
            {
                "last_name": "Alber",
                "first_name": "Maximilian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabeled data into a foundation model before learning from, potentially\nlimited, labeled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 103k slides\ncorresponding to 750 million image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.\n",
        "title": "RudolfV: A Foundation Model by Pathologists for Pathologists",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04081",
        "abstract_url": "http://arxiv.org/abs/2401.04081",
        "authors": [
            {
                "last_name": "Pi\u00f3ro",
                "first_name": "Maciej"
            },
            {
                "last_name": "Ciebiera",
                "first_name": "Kamil"
            },
            {
                "last_name": "Kr\u00f3l",
                "first_name": "Krystian"
            },
            {
                "last_name": "Ludziejewski",
                "first_name": "Jan"
            },
            {
                "last_name": "Jaszczur",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CL"
        ],
        "abstract": "  State Space Models (SSMs) have become serious contenders in the field of\nsequential modeling, challenging the dominance of Transformers. At the same\ntime, Mixture of Experts (MoE) has significantly improved Transformer-based\nLLMs, including recent state-of-the-art open-source models. We propose that to\nunlock the potential of SSMs for scaling, they should be combined with MoE. We\nshowcase this on Mamba, a recent SSM-based model that achieves remarkable,\nTransformer-like performance. Our model, MoE-Mamba, outperforms both Mamba and\nTransformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba\nin 2.2x less training steps while preserving the inference performance gains of\nMamba against the Transformer.\n",
        "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of\n  Experts",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04082",
        "abstract_url": "http://arxiv.org/abs/2401.04082",
        "authors": [
            {
                "last_name": "Yim",
                "first_name": "Jason"
            },
            {
                "last_name": "Campbell",
                "first_name": "Andrew"
            },
            {
                "last_name": "Mathieu",
                "first_name": "Emile"
            },
            {
                "last_name": "Foong",
                "first_name": "Andrew Y. K."
            },
            {
                "last_name": "Gastegger",
                "first_name": "Michael"
            },
            {
                "last_name": "Jim\u00e9nez-Luna",
                "first_name": "Jos\u00e9"
            },
            {
                "last_name": "Lewis",
                "first_name": "Sarah"
            },
            {
                "last_name": "Satorras",
                "first_name": "Victor Garcia"
            },
            {
                "last_name": "Veeling",
                "first_name": "Bastiaan S."
            },
            {
                "last_name": "No\u00e9",
                "first_name": "Frank"
            },
            {
                "last_name": "Barzilay",
                "first_name": "Regina"
            },
            {
                "last_name": "Jaakkola",
                "first_name": "Tommi S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Protein design often begins with knowledge of a desired function from a motif\nwhich motif-scaffolding aims to construct a functional protein around.\nRecently, generative models have achieved breakthrough success in designing\nscaffolds for a diverse range of motifs. However, the generated scaffolds tend\nto lack structural diversity, which can hinder success in wet-lab validation.\nIn this work, we extend FrameFlow, an SE(3) flow matching model for protein\nbackbone generation, to perform motif-scaffolding with two complementary\napproaches. The first is motif amortization, in which FrameFlow is trained with\nthe motif as input using a data augmentation strategy. The second is motif\nguidance, which performs scaffolding using an estimate of the conditional score\nfrom FrameFlow, and requires no additional training. Both approaches achieve an\nequivalent or higher success rate than previous state-of-the-art methods, with\n2.5 times more structurally diverse scaffolds. Code: https://github.com/\nmicrosoft/frame-flow.\n",
        "title": "Improved motif-scaffolding with SE(3) flow matching",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04086",
        "abstract_url": "http://arxiv.org/abs/2401.04086",
        "authors": [
            {
                "last_name": "Balayla",
                "first_name": "Jacques"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  In this manuscript, we present various proposed methods estimate the\nprevalence of disease, a critical prerequisite for the adequate interpretation\nof screening tests. To address the limitations of these approaches, which\nrevolve primarily around their a posteriori nature, we introduce a novel method\nto estimate the pretest probability of disease, a priori, utilizing the Logit\nfunction from the logistic regression model. This approach is a modification of\nMcGee's heuristic, originally designed for estimating the posttest probability\nof disease. In a patient presenting with $n_\\theta$ signs or symptoms, the\nminimal bound of the pretest probability, $\\phi$, can be approximated by:\n  $\\phi \\approx\n\\frac{1}{5}{ln\\left[\\displaystyle\\prod_{\\theta=1}^{i}\\kappa_\\theta\\right]}$\n  where $ln$ is the natural logarithm, and $\\kappa_\\theta$ is the likelihood\nratio associated with the sign or symptom in question.\n",
        "title": "A Priori Determination of the Pretest Probability",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04088",
        "abstract_url": "http://arxiv.org/abs/2401.04088",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Albert Q."
            },
            {
                "last_name": "Sablayrolles",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Roux",
                "first_name": "Antoine"
            },
            {
                "last_name": "Mensch",
                "first_name": "Arthur"
            },
            {
                "last_name": "Savary",
                "first_name": "Blanche"
            },
            {
                "last_name": "Bamford",
                "first_name": "Chris"
            },
            {
                "last_name": "Chaplot",
                "first_name": "Devendra Singh"
            },
            {
                "last_name": "Casas",
                "first_name": "Diego de las"
            },
            {
                "last_name": "Hanna",
                "first_name": "Emma Bou"
            },
            {
                "last_name": "Bressand",
                "first_name": "Florian"
            },
            {
                "last_name": "Lengyel",
                "first_name": "Gianna"
            },
            {
                "last_name": "Bour",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Lample",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Lavaud",
                "first_name": "L\u00e9lio Renard"
            },
            {
                "last_name": "Saulnier",
                "first_name": "Lucile"
            },
            {
                "last_name": "Lachaux",
                "first_name": "Marie-Anne"
            },
            {
                "last_name": "Stock",
                "first_name": "Pierre"
            },
            {
                "last_name": "Subramanian",
                "first_name": "Sandeep"
            },
            {
                "last_name": "Yang",
                "first_name": "Sophia"
            },
            {
                "last_name": "Antoniak",
                "first_name": "Szymon"
            },
            {
                "last_name": "Scao",
                "first_name": "Teven Le"
            },
            {
                "last_name": "Gervet",
                "first_name": "Th\u00e9ophile"
            },
            {
                "last_name": "Lavril",
                "first_name": "Thibaut"
            },
            {
                "last_name": "Wang",
                "first_name": "Thomas"
            },
            {
                "last_name": "Lacroix",
                "first_name": "Timoth\u00e9e"
            },
            {
                "last_name": "Sayed",
                "first_name": "William El"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.\n",
        "title": "Mixtral of Experts",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04098",
        "abstract_url": "http://arxiv.org/abs/2401.04098",
        "authors": [
            {
                "last_name": "Cosandal",
                "first_name": "Ismail"
            },
            {
                "last_name": "Akar",
                "first_name": "Nail"
            },
            {
                "last_name": "Ulukus",
                "first_name": "Sennur"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI",
            "",
            ""
        ],
        "abstract": "  Age of incorrect information (AoII) has recently been proposed as an\nalternative to existing information freshness metrics for real-time sampling\nand estimation problems involving information sources that are tracked by\nremote monitors. Different from existing metrics, AoII penalizes the incorrect\ninformation by increasing linearly with time as long as the source and the\nmonitor are de-synchronized, and is reset when they are synchronized back.\nWhile AoII has generally been investigated for discrete time information\nsources, we develop a novel analytical model in this paper for push- and\npull-based sampling and transmission of a continuous time Markov chain (CTMC)\nprocess. In the pull-based model, the sensor starts transmitting information on\nthe observed CTMC only when a pull request from the monitor is received. On the\nother hand, in the push-based scenario, the sensor, being aware of the AoII\nprocess, samples and transmits when the AoII process exceeds a random\nthreshold. The proposed analytical model for both scenarios is based on the\nconstruction of a discrete time MC (DTMC) making state transitions at the\nembedded epochs of synchronization points, using the theory of absorbing CTMCs,\nand in particular phase-type distributions. For a given sampling policy,\nanalytical models to obtain the mean AoII and the average sampling rate are\ndeveloped. Numerical results are presented to validate the analytical model as\nwell as to provide insight on optimal sampling policies under sampling rate\nconstraints.\n",
        "title": "Modeling AoII in Push- and Pull-Based Sampling of Continuous Time Markov\n  Chains",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04099",
        "abstract_url": "http://arxiv.org/abs/2401.04099",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Dejia"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ye"
            },
            {
                "last_name": "Mardani",
                "first_name": "Morteza"
            },
            {
                "last_name": "Liu",
                "first_name": "Sifei"
            },
            {
                "last_name": "Song",
                "first_name": "Jiaming"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhangyang"
            },
            {
                "last_name": "Vahdat",
                "first_name": "Arash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Given the growing need for automatic 3D content creation pipelines, various\n3D representations have been studied to generate 3D objects from a single\nimage. Due to its superior rendering efficiency, 3D Gaussian splatting-based\nmodels have recently excelled in both 3D reconstruction and generation. 3D\nGaussian splatting approaches for image to 3D generation are often\noptimization-based, requiring many computationally expensive score-distillation\nsteps. To overcome these challenges, we introduce an Amortized Generative 3D\nGaussian framework (AGG) that instantly produces 3D Gaussians from a single\nimage, eliminating the need for per-instance optimization. Utilizing an\nintermediate hybrid representation, AGG decomposes the generation of 3D\nGaussian locations and other appearance attributes for joint optimization.\nMoreover, we propose a cascaded pipeline that first generates a coarse\nrepresentation of the 3D data and later upsamples it with a 3D Gaussian\nsuper-resolution module. Our method is evaluated against existing\noptimization-based 3D Gaussian frameworks and sampling-based pipelines\nutilizing other 3D representations, where AGG showcases competitive generation\nabilities both qualitatively and quantitatively while being several orders of\nmagnitude faster. Project page: https://ir1d.github.io/AGG/\n",
        "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04105",
        "abstract_url": "http://arxiv.org/abs/2401.04105",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Chen"
            },
            {
                "last_name": "Liu",
                "first_name": "Shuming"
            },
            {
                "last_name": "Mangalam",
                "first_name": "Karttikeya"
            },
            {
                "last_name": "Qian",
                "first_name": "Guocheng"
            },
            {
                "last_name": "Zohra",
                "first_name": "Fatimah"
            },
            {
                "last_name": "Alghannam",
                "first_name": "Abdulmohsen"
            },
            {
                "last_name": "Malik",
                "first_name": "Jitendra"
            },
            {
                "last_name": "Ghanem",
                "first_name": "Bernard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Large pretrained models are increasingly crucial in modern computer vision\ntasks. These models are typically used in downstream tasks by end-to-end\nfinetuning, which is highly memory-intensive for tasks with high-resolution\ndata, e.g., video understanding, small object detection, and point cloud\nanalysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks,\nor Dr$^2$Net, a novel family of network architectures that acts as a surrogate\nnetwork to finetune a pretrained model with substantially reduced memory\nconsumption. Dr$^2$Net contains two types of residual connections, one\nmaintaining the residual structure in the pretrained models, and the other\nmaking the network reversible. Due to its reversibility, intermediate\nactivations, which can be reconstructed from output, are cleared from memory\nduring training. We use two coefficients on either type of residual connections\nrespectively, and introduce a dynamic training strategy that seamlessly\ntransitions the pretrained model to a reversible network with much higher\nnumerical precision. We evaluate Dr$^2$Net on various pretrained models and\nvarious tasks, and show that it can reach comparable performance to\nconventional finetuning but with significantly less memory usage.\n",
        "title": "Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for\n  Memory-Efficient Finetuning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04108",
        "abstract_url": "http://arxiv.org/abs/2401.04108",
        "authors": [
            {
                "last_name": "F\u00f6rster",
                "first_name": "Frank"
            },
            {
                "last_name": "Romeo",
                "first_name": "Marta"
            },
            {
                "last_name": "Holthaus",
                "first_name": "Patrick"
            },
            {
                "last_name": "Trigo",
                "first_name": "Maria Jose Galvez"
            },
            {
                "last_name": "Fischer",
                "first_name": "Joel E."
            },
            {
                "last_name": "Nesset",
                "first_name": "Birthe"
            },
            {
                "last_name": "Dondrup",
                "first_name": "Christian"
            },
            {
                "last_name": "Murad",
                "first_name": "Christine"
            },
            {
                "last_name": "Munteanu",
                "first_name": "Cosmin"
            },
            {
                "last_name": "Cowan",
                "first_name": "Benjamin R."
            },
            {
                "last_name": "Clark",
                "first_name": "Leigh"
            },
            {
                "last_name": "Porcheron",
                "first_name": "Martin"
            },
            {
                "last_name": "Candello",
                "first_name": "Heloisa"
            },
            {
                "last_name": "Langevin",
                "first_name": "Raina"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL",
            "RO",
            "",
            "",
            ""
        ],
        "abstract": "  Workshop proceedings of two co-located workshops \"Working with Troubles and\nFailures in Conversation with Humans and Robots\" (WTF 2023) and \"Is CUI Design\nReady Yet?\", both of which were part of the ACM conference on conversational\nuser interfaces 2023.\n  WTF 23 aimed at bringing together researchers from human-robot interaction,\ndialogue systems, human-computer interaction, and conversation analysis.\nDespite all progress, robotic speech interfaces continue to be brittle in a\nnumber of ways and the experience of failure of such interfaces is commonplace\namongst roboticists. However, the technical literature is positively skewed\ntoward their good performance. The workshop aims to provide a platform for\ndiscussing communicative troubles and failures in human-robot interactions and\nrelated failures in non-robotic speech interfaces. Aims include a scrupulous\ninvestigation into communicative failures, to begin working on a taxonomy of\nsuch failures, and enable a preliminary discussion on possible mitigating\nstrategies. Workshop website: https://sites.google.com/view/wtf2023/overview\n  Is CUI Design Ready Yet? As CUIs become more prevalent in both academic\nresearch and the commercial market, it becomes more essential to design usable\nand adoptable CUIs. While research has been growing on the methods for\ndesigning CUIs for commercial use, there has been little discussion on the\noverall community practice of developing design resources to aid in practical\nCUI design. The aim of this workshop, therefore, is to bring the CUI community\ntogether to discuss the current practices for developing tools and resources\nfor practical CUI design, the adoption (or non-adoption) of these tools and\nresources, and how these resources are utilized in the training and education\nof new CUI designers entering the field. Workshop website:\nhttps://speech-interaction.org/cui2023_design_workshop/index.html\n",
        "title": "Working with Trouble and Failures in Conversation between Humans and\n  Robots (WTF 2023) & Is CUI Design Ready Yet?",
        "date": "2023-09-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04109",
        "abstract_url": "http://arxiv.org/abs/2401.04109",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Youngchan"
            },
            {
                "last_name": "Hwang",
                "first_name": "Eunseung"
            },
            {
                "last_name": "Kai",
                "first_name": "Chang"
            },
            {
                "last_name": "Xu",
                "first_name": "Kaichen"
            },
            {
                "last_name": "Pan",
                "first_name": "Heng"
            },
            {
                "last_name": "Hong",
                "first_name": "Sukjoon"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Recently, the growing interest in wearable technology for personal healthcare\nand smart VR/AR applications newly imposed a need for development of facile\nfabrication method. Regarding the issue, laser has long been proposing original\nanswers to such challenging technological demands with its remote, sterile,\nrapid, and site-selective processing characteristics for arbitrary materials.\nIn this review, recent developments in relevant laser processes are summarized\nin two separate categories. Firstly, transformative approaches represented by\nlaser-induced graphene (LIG) are introduced. Apart from design optimization and\nalteration of native substrate, latest advancements in the transformative\napproach now enable not only more complex material compositions but also\nmultilayer device configurations by simultaneous transformation of\nheterogeneous precursor or sequential addition of functional layers coupled\nwith other electronic elements. Besides, more conventional laser techniques\nsuch as ablation, sintering and synthesis are still accessible for enhancing\nthe functionality of the entire system through expansion of applicable\nmaterials and adoption of new mechanisms. Various wearable device components\ndeveloped through the corresponding laser processes are then organized with\nemphasis on chemical/physical sensors and energy devices. At the same time,\nspecial attention is given to the applications utilizing multiple laser sources\nor multiple laser processes, which pave the way towards all-laser fabrication\nof wearable devices.\n",
        "title": "Recent developments of selective laser processes for wearable devices",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04111",
        "abstract_url": "http://arxiv.org/abs/2401.04111",
        "authors": [
            {
                "last_name": "Orun",
                "first_name": "Ahmet"
            },
            {
                "last_name": "Orun",
                "first_name": "Emre"
            },
            {
                "last_name": "Kurugollu",
                "first_name": "Fatih"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Cyber-attacks keep threatening global networks and information\ninfrastructures. The threat is getting more and more destructive and hard to\ncounter day by day as the global networks continue to enlarge exponentially\nwith limited security counter-measures. As this fact requires more\nsophisticated methods and techniques in urgency, a multidisciplinary remote\ncognitive observation technique is proposed in this paper to meet today's\ncybersecurity needs. The proposed method introduces a non-traditional Cognitive\nPsychology and Artificial Intelligence (AI) based remote threat identification\nwhich can be considered during the cyber security system design. It also\nenables to access the cognitive behavioural parameters of an intruder/hacker\nremotely without any physical contact via online connection, disregarding the\ndistance of the thread. The ultimate goal of this work is to develop a\nsupplementary cognitive cyber security tool for next generations secure online\nbanking, finance or trade systems.\n",
        "title": "Recognition of Cyber-Intrusion patterns in user cognitive behavioural\n  characteristics for remote identification",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04112",
        "abstract_url": "http://arxiv.org/abs/2401.04112",
        "authors": [
            {
                "last_name": "Rosenberg",
                "first_name": "Louis"
            },
            {
                "last_name": "Willcox",
                "first_name": "Gregg"
            },
            {
                "last_name": "Schumann",
                "first_name": "Hans"
            },
            {
                "last_name": "Mani",
                "first_name": "Ganesh"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Conversational Swarm Intelligence (CSI) is a communication technology that\nenables large, networked groups (25 to 2500 people) to hold real-time\nconversational deliberations online. Modeled on the dynamics of biological\nswarms, CSI enables the reasoning benefits of small-groups with the collective\nintelligence benefits of large-groups. In this pilot study, groups of 25 to 30\nparticipants were asked to select players for a weekly Fantasy Football contest\nover an 11-week period. As a baseline, participants filled out a survey to\nrecord their player selections. As an experimental method, participants engaged\nin a real-time text-chat deliberation using a CSI platform called Thinkscape to\ncollaboratively select sets of players. The results show that the real-time\nconversational group using CSI outperformed 66% of survey participants,\ndemonstrating significant amplification of intelligence versus the median\nindividual (p=0.020). The CSI method also significantly outperformed the most\npopular choices from the survey (the Wisdom of Crowd, p<0.001). These results\nsuggest that CSI is an effective technology for amplifying the intelligence of\ngroups engaged in real-time large-scale conversational deliberation and may\noffer a path to collective superintelligence.\n",
        "title": "Conversational Swarm Intelligence amplifies the accuracy of networked\n  groupwise deliberations",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04114",
        "abstract_url": "http://arxiv.org/abs/2401.04114",
        "authors": [
            {
                "last_name": "Kaur",
                "first_name": "Harleen"
            },
            {
                "last_name": "Mendling",
                "first_name": "Jan"
            },
            {
                "last_name": "Rubensson",
                "first_name": "Christoffer"
            },
            {
                "last_name": "Kampik",
                "first_name": "Timotheus"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV",
            "LG"
        ],
        "abstract": "  A key concern of automatic process discovery is to provide insights into\nperformance aspects of business processes. Waiting times are of particular\nimportance in this context. For that reason, it is surprising that current\ntechniques for automatic process discovery generate directly-follows graphs and\ncomparable process models, but often miss the opportunity to explicitly\nrepresent the time axis. In this paper, we present an approach for\nautomatically constructing process models that explicitly align with a time\naxis. We exemplify our approach for directly-follows graphs. Our evaluation\nusing two BPIC datasets and a proprietary dataset highlight the benefits of\nthis representation in comparison to standard layout techniques.\n",
        "title": "Timeline-based Process Discovery",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04116",
        "abstract_url": "http://arxiv.org/abs/2401.04116",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Huaqiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Yangkai"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV"
        ],
        "abstract": "  Text-to-image generation is conducted through Generative Adversarial Networks\n(GANs) or transformer models. However, the current challenge lies in accurately\ngenerating images based on textual descriptions, especially in scenarios where\nthe content and theme of the target image are ambiguous. In this paper, we\npropose a method that utilizes artificial intelligence models for thematic\ncreativity, followed by a classification modeling of the actual painting\nprocess. The method involves converting all visual elements into quantifiable\ndata structures before creating images. We evaluate the effectiveness of this\napproach in terms of semantic accuracy, image reproducibility, and\ncomputational efficiency, in comparison with existing image generation\nalgorithms.\n",
        "title": "Semantic Draw Engineering for Text-to-Image Creation",
        "date": "2023-12-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04117",
        "abstract_url": "http://arxiv.org/abs/2401.04117",
        "authors": [
            {
                "last_name": "Choudhary",
                "first_name": "Safal"
            },
            {
                "last_name": "Randhawa",
                "first_name": "Princy"
            },
            {
                "last_name": "Jinka",
                "first_name": "Sampath Kumar P"
            },
            {
                "last_name": "C",
                "first_name": "Shiva Prasad H."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Juvenile Idiopathic Arthritis (JIA) is a widespread and chronic condition\nthat affects children and adolescents worldwide. The person suffering from JIA\nis characterized by chronic joint inflammation leading to pain, swelling,\nstiffness, and limited body movements. Individuals suffering from JIA require\nongoing treatment for their lifetime. Beyond inflammation, JIA patients have\nexpressed concerns about various factors and the lack of responsive services\naddressing their challenges. The implementation of smart garments offers a\npromising solution to assist individuals with Juvenile Idiopathic Arthritis in\nperforming their daily activities. These garments are designed to seamlessly\nintegrate technology and clothing, providing not only physical support but also\naddressing the psychological and emotional aspects of living with a chronic\ncondition. By incorporating sensors, these smart garments can monitor joint\nmovement, detect inflammation, and provide real-time feedback to both patients\nand healthcare providers. To tackle these comprehensive challenges, the\nresearch aims to offer a solution through the design of a smart garment,\ncreated with a holistic approach. This smart garment is intended to improve the\noverall well-being of JIA patients by enhancing their mobility, comfort, and\noverall quality of life. The integration of technology into clothing can\npotentially revolutionize the way JIA is managed, allowing patients to better\nmanage their condition and minimize its impact on their daily lives. The\nsynergy between healthcare and technology holds great potential in addressing\nthe multifaceted challenges posed by Juvenile Idiopathic Arthritis patients.\nThrough innovation and empathy, this research aims to pave the way for a\nbrighter future for individuals living with Juvenile Idiopathic Arthritis.\n",
        "title": "A Holistic Approach on Smart Garment for Patients with Juvenile\n  Idiopathic Arthritis",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04118",
        "abstract_url": "http://arxiv.org/abs/2401.04118",
        "authors": [
            {
                "last_name": "Bhattacharya",
                "first_name": "Aditya"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  With Artificial Intelligence (AI) becoming ubiquitous in every application\ndomain, the need for explanations is paramount to enhance transparency and\ntrust among non-technical users. Despite the potential shown by Explainable AI\n(XAI) for enhancing understanding of complex AI systems, most XAI methods are\ndesigned for technical AI experts rather than non-technical consumers.\nConsequently, such explanations are overwhelmingly complex and seldom guide\nusers in achieving their desired predicted outcomes. This paper presents\nongoing research for crafting XAI systems tailored to guide users in achieving\ndesired outcomes through improved human-AI interactions. This paper highlights\nthe research objectives and methods, key takeaways and implications learned\nfrom user studies. It outlines open questions and challenges for enhanced\nhuman-AI collaboration, which the author aims to address in future work.\n",
        "title": "Towards Directive Explanations: Crafting Explainable AI Systems for\n  Actionable Human-AI Interactions",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04119",
        "abstract_url": "http://arxiv.org/abs/2401.04119",
        "authors": [
            {
                "last_name": "Yada",
                "first_name": "Yuki"
            },
            {
                "last_name": "Matsumoto",
                "first_name": "Tsuneo"
            },
            {
                "last_name": "Kido",
                "first_name": "Fuyuko"
            },
            {
                "last_name": "Yamana",
                "first_name": "Hayato"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "LG"
        ],
        "abstract": "  Dark patterns are deceptive user interface designs for online services that\nmake users behave in unintended ways. Dark patterns, such as privacy invasion,\nfinancial loss, and emotional distress, can harm users. These issues have been\nthe subject of considerable debate in recent years. In this paper, we study\ninterpretable dark pattern auto-detection, that is, why a particular user\ninterface is detected as having dark patterns. First, we trained a model using\ntransformer-based pre-trained language models, BERT, on a text-based dataset\nfor the automatic detection of dark patterns in e-commerce. Then, we applied\npost-hoc explanation techniques, including local interpretable model agnostic\nexplanation (LIME) and Shapley additive explanations (SHAP), to the trained\nmodel, which revealed which terms influence each prediction as a dark pattern.\nIn addition, we extracted and analyzed terms that affected the dark patterns.\nOur findings may prevent users from being manipulated by dark patterns, and aid\nin the construction of more equitable internet services. Our code is available\nat https://github.com/yamanalab/why-darkpattern.\n",
        "title": "Why is the User Interface a Dark Pattern? : Explainable Auto-Detection\n  and its Analysis",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04120",
        "abstract_url": "http://arxiv.org/abs/2401.04120",
        "authors": [
            {
                "last_name": "Ramu",
                "first_name": "Dhruv"
            },
            {
                "last_name": "Jain",
                "first_name": "Rishab"
            },
            {
                "last_name": "Jain",
                "first_name": "Aditya"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL"
        ],
        "abstract": "  The growing popularity of generative artificial intelligence (AI) chatbots\nsuch as ChatGPT is having transformative effects on social media. As the\nprevalence of AI-generated content grows, concerns have been raised regarding\nprivacy and misinformation online. Among social media platforms, Discord\nenables AI integrations -- making their primarily \"Generation Z\" userbase\nparticularly exposed to AI-generated content. We surveyed Generation Z aged\nindividuals (n = 335) to evaluate their proficiency in discriminating between\nAI-generated and human-authored text on Discord. The investigation employed\none-shot prompting of ChatGPT, disguised as a text message received on the\nDiscord.com platform. We explore the influence of demographic factors on\nability, as well as participants' familiarity with Discord and artificial\nintelligence technologies. We find that Generation Z individuals are unable to\ndiscern between AI and human-authored text (p = 0.011), and that those with\nlower self-reported familiarity with Discord demonstrated an improved ability\nin identifying human-authored compared to those with self-reported experience\nwith AI (p << 0.0001). Our results suggest that there is a nuanced relationship\nbetween AI technology and popular modes of communication for Generation Z,\ncontributing valuable insights into human-computer interactions, digital\ncommunication, and artificial intelligence literacy.\n",
        "title": "Generation Z's Ability to Discriminate Between AI-generated and\n  Human-Authored Text on Discord",
        "date": "2023-12-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04122",
        "abstract_url": "http://arxiv.org/abs/2401.04122",
        "authors": [
            {
                "last_name": "Shah",
                "first_name": "Chirag"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  As LLMs make their way into many aspects of our lives, one place that\nwarrants increased scrutiny with LLM usage is scientific research. Using LLMs\nfor generating or analyzing data for research purposes is gaining popularity.\nBut when such application is marred with ad-hoc decisions and engineering\nsolutions, we need to be concerned about how it may affect that research, its\nfindings, or any future works based on that research. We need a more scientific\napproach to using LLMs in our research. While there are several active efforts\nto support more systematic construction of prompts, they are often focused more\non achieving desirable outcomes rather than producing replicable and\ngeneralizable knowledge with sufficient transparency, objectivity, or rigor.\nThis article presents a new methodology inspired by codebook construction\nthrough qualitative methods to address that. Using humans in the loop and a\nmulti-phase verification processes, this methodology lays a foundation for more\nsystematic, objective, and trustworthy way of applying LLMs for analyzing data.\nSpecifically, we show how a set of researchers can work through a rigorous\nprocess of labeling, deliberating, and documenting to remove subjectivity and\nbring transparency and replicability to prompt generation process. A set of\nexperiments are presented to show how this methodology can be put in practice.\n",
        "title": "From Prompt Engineering to Prompt Science With Human in the Loop",
        "date": "2023-12-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04124",
        "abstract_url": "http://arxiv.org/abs/2401.04124",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Tinghe"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Agents centered around Large Language Models (LLMs) are now capable of\nautomating mobile device operations for users. After fine-tuning to learn a\nuser's mobile operations, these agents can adhere to high-level user\ninstructions online. They execute tasks such as goal decomposition, sequencing\nof sub-goals, and interactive environmental exploration, until the final\nobjective is achieved. However, privacy concerns related to personalized user\ndata arise during mobile operations, requiring user confirmation. Moreover,\nusers' real-world operations are exploratory, with action data being complex\nand redundant, posing challenges for agent learning. To address these issues,\nin our practical application, we have designed interactive tasks between agents\nand humans to identify sensitive information and align with personalized user\nneeds. Additionally, we integrated Standard Operating Procedure (SOP)\ninformation within the model's in-context learning to enhance the agent's\ncomprehension of complex task execution. Our approach is evaluated on the new\ndevice control benchmark AitW, which encompasses 30K unique instructions across\nmulti-step tasks, including application operation, web searching, and web\nshopping. Experimental results show that the SOP-based agent achieves\nstate-of-the-art performance without incurring additional inference costs,\nboasting an overall action success rate of 66.92%.\n",
        "title": "MobileAgent: enhancing mobile control via human-machine interaction and\n  SOP integration",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04125",
        "abstract_url": "http://arxiv.org/abs/2401.04125",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Zili"
            },
            {
                "last_name": "Chen",
                "first_name": "Keyan"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Liang",
                "first_name": "Shunlin"
            },
            {
                "last_name": "Zou",
                "first_name": "Zhengxia"
            },
            {
                "last_name": "Shi",
                "first_name": "Zhenwei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Accurate weather forecasting holds significant importance to human\nactivities. Currently, there are two paradigms for weather forecasting:\nNumerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP).\nNWP utilizes atmospheric physics for weather modeling but suffers from poor\ndata utilization and high computational costs, while DLP can learn weather\npatterns from vast amounts of data directly but struggles to incorporate\nphysical laws. Both paradigms possess their respective strengths and\nweaknesses, and are incompatible, because physical laws adopted in NWP describe\nthe relationship between coordinates and meteorological variables, while DLP\ndirectly learns the relationships between meteorological variables without\nconsideration of coordinates. To address these problems, we introduce the\nDeepPhysiNet framework, incorporating physical laws into deep learning models\nfor accurate and continuous weather system modeling. First, we construct\nphysics networks based on multilayer perceptrons (MLPs) for individual\nmeteorological variable, such as temperature, pressure, and wind speed. Physics\nnetworks establish relationships between variables and coordinates by taking\ncoordinates as input and producing variable values as output. The physical laws\nin the form of Partial Differential Equations (PDEs) can be incorporated as a\npart of loss function. Next, we construct hyper-networks based on deep learning\nmethods to directly learn weather patterns from a large amount of\nmeteorological data. The output of hyper-networks constitutes a part of the\nweights for the physics networks. Experimental results demonstrate that, upon\nsuccessful integration of physical laws, DeepPhysiNet can accomplish multiple\ntasks simultaneously, not only enhancing forecast accuracy but also obtaining\ncontinuous spatiotemporal resolution results, which is unattainable by either\nthe NWP or DLP.\n",
        "title": "DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for\n  Accurate and Continuous Weather Modeling",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04126",
        "abstract_url": "http://arxiv.org/abs/2401.04126",
        "authors": [
            {
                "last_name": "Kremenchutskiy",
                "first_name": "Anatoliy"
            },
            {
                "last_name": "Gabdreshov",
                "first_name": "Galymzhan"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  The lack of an accessible and effective system for blind individuals to\ncreate handwritten signatures presents a significant barrier to their\nindependence and full participation in various aspects of life. This research\nintroduces the Tactile Signature System, a groundbreaking approach that\nempowers individuals with visual impairments to form their unique handwritten\nsignatures. Key features of the system include: Personalized customization:\nThrough tactile interaction and voice algorithmic guidance, individuals create\nsignatures reflecting their preferences and natural writing style. Real-time\nfeedback: AI-powered voice prompts and analysis ensure accuracy and consistency\nin signature formation. Accessibility: Installation in local service centers\nprovides a secure and supervised environment for signature creation. The\nsystem's impact reaches beyond the individual level: Promotes inclusivity and\nindependence: Blind individuals can engage in legal and financial transactions\nwithout relying on others. Empowers and fosters equal opportunities:\nParticipation in education, employment, and civic engagement becomes more\naccessible. Aligns with international conventions: Upholds the right of persons\nwith disabilities to participate fully in society. The Tactile Signature System\nrepresents a significant step towards an inclusive and accessible future for\nindividuals with visual impairments.\n",
        "title": "The Concept of the Tactile Signature System for Individuals with Visual\n  Impairments",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04127",
        "abstract_url": "http://arxiv.org/abs/2401.04127",
        "authors": [
            {
                "last_name": "Millot",
                "first_name": "Laurent"
            },
            {
                "last_name": "Pel\u00e9",
                "first_name": "G\u00e9rard"
            },
            {
                "last_name": "Elliq",
                "first_name": "Mohammed"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD",
            "",
            ""
        ],
        "abstract": "  Audio scene cartography for real or simulated stereo recordings is presented.\nThis audio scene analysis is performed doing successively: a perceptive\n10-subbands analysis, calculation of temporal laws for relative delays and\ngains between both channels of each subband using a short-time cons\\-tant scene\nassumption and channels inter-correlation which permit to follow a mobile\nsource in its moves, calculation of global and subbands histograms whose peaks\ngive the incidence information for fixed sources. Audio scenes composed of 2 to\n4 fixed sources or with a fixed source and a mobile one have been already\nsuccessfully tested. Further extensions and applications will be discussed.\nAudio illustrations of audio scenes, subband analysis and demonstration of\nreal-time stereo recording simulations will be given.Paper 6340 presented at\nthe 118th Convention of the Audio Engineering Society, Barcelona, 2005\n",
        "title": "Using perceptive subbands analysis to perform audio scenes cartography",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04131",
        "abstract_url": "http://arxiv.org/abs/2401.04131",
        "authors": [
            {
                "last_name": "Acay",
                "first_name": "Co\u015fku"
            },
            {
                "last_name": "Gancher",
                "first_name": "Joshua"
            },
            {
                "last_name": "Recto",
                "first_name": "Rolph"
            },
            {
                "last_name": "Myers",
                "first_name": "Andrew C."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "PL"
        ],
        "abstract": "  Developing secure distributed systems is difficult, and even harder when\nadvanced cryptography must be used to achieve security goals. Following prior\nwork, we advocate using secure program partitioning to synthesize cryptographic\napplications: instead of implementing a system of communicating processes, the\nprogrammer implements a centralized, sequential program, which is automatically\ncompiled into a secure distributed version that uses cryptography.\n  While this approach is promising, formal results for the security of such\ncompilers are limited in scope. In particular, no security proof yet\nsimultaneously addresses subtleties essential for robust, efficient\napplications: multiple cryptographic mechanisms, malicious corruption, and\nasynchronous communication.\n  In this work, we develop a compiler security proof that handles these\nsubtleties. Our proof relies on a novel unification of simulation-based\nsecurity, information-flow control, choreographic programming, and\nsequentialization techniques for concurrent programs. While our proof targets\nhybrid protocols, which abstract cryptographic mechanisms as idealized\nfunctionalities, our approach offers a clear path toward leveraging Universal\nComposability to obtain end-to-end, modular security results with fully\ninstantiated cryptographic mechanisms.\n  Finally, following prior observations about simulation-based security, we\nprove that our result guarantees robust hyperproperty preservation, an\nimportant criterion for compiler correctness that preserves all source-level\nsecurity properties in target programs.\n",
        "title": "Secure Synthesis of Distributed Cryptographic Applications (Technical\n  Report)",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04133",
        "abstract_url": "http://arxiv.org/abs/2401.04133",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Ming-Yi"
            },
            {
                "last_name": "Huang",
                "first_name": "Yi-Hsiang"
            },
            {
                "last_name": "Teng",
                "first_name": "You-Chen"
            },
            {
                "last_name": "Wang",
                "first_name": "Chih-Yu"
            },
            {
                "last_name": "Lin",
                "first_name": "Che"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "SI"
        ],
        "abstract": "  Graph Neural Networks (GNNs) excel in various domains, from detecting\ne-commerce spam to social network classification problems. However, the lack of\npublic graph datasets hampers research progress, particularly in heterogeneous\ninformation networks (HIN). The demand for datasets for fair HIN comparisons is\ngrowing due to advancements in GNN interpretation models. In response, we\npropose SynHIN, a unique method for generating synthetic heterogeneous\ninformation networks. SynHIN identifies motifs in real-world datasets,\nsummarizes graph statistics, and constructs a synthetic network. Our approach\nutilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN\nfrom primary motif clusters. After In/Our-Cluster mergers and a post-pruning\nprocess fitting the real dataset constraints, we ensure the synthetic graph\nstatistics align closely with the reference one. SynHIN generates a synthetic\nheterogeneous graph dataset for node classification tasks, using the primary\nmotif as the explanation ground truth. It can adapt and address the lack of\nheterogeneous graph datasets and motif ground truths, proving beneficial for\nassessing heterogeneous graph neural network explainers. We further present a\nbenchmark dataset for future heterogeneous graph explainer model research. Our\nwork marks a significant step towards explainable AI in HGNNs.\n",
        "title": "SynHIN: Generating Synthetic Heterogeneous Information Network for\n  Explainable AI",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04134",
        "abstract_url": "http://arxiv.org/abs/2401.04134",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Frank"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            ""
        ],
        "abstract": "  This paper introduces a new neural network model that aims to mimic the\nbiological brain more closely by structuring the network as a complete directed\ngraph that processes continuous data for each timestep. Current neural networks\nhave structures that vaguely mimic the brain structure, such as neurons,\nconvolutions, and recurrence. The model proposed in this paper adds additional\nstructural properties by introducing cycles into the neuron connections and\nremoving the sequential nature commonly seen in other network layers.\nFurthermore, the model has continuous input and output, inspired by spiking\nneural networks, which allows the network to learn a process of classification,\nrather than simply returning the final result.\n",
        "title": "Web Neural Network with Complete DiGraphs",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04135",
        "abstract_url": "http://arxiv.org/abs/2401.04135",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Haiyang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Chunjiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Detian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Traffic flow prediction plays a crucial role in alleviating traffic\ncongestion and enhancing transport efficiency. While combining graph\nconvolution networks with recurrent neural networks for spatial-temporal\nmodeling is a common strategy in this realm, the restricted structure of\nrecurrent neural networks limits their ability to capture global information.\nFor spatial modeling, many prior studies learn a graph structure that is\nassumed to be fixed and uniform at all time steps, which may not be true. This\npaper introduces a novel traffic prediction framework, Global-Aware Enhanced\nSpatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core\ncomponents: a spatial-temporal graph recurrent neural network and a global\nawareness layer. Within this framework, three innovative prediction models are\nformulated. A sequence-aware graph neural network is proposed and integrated\ninto the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time\nsteps and capture local temporal relationships. To enhance the model's global\nperception, three distinct global spatial-temporal transformer-like\narchitectures (GST^2) are devised for the global awareness layer. We conduct\nextensive experiments on four real traffic datasets and the results demonstrate\nthe superiority of our framework and the three concrete models.\n",
        "title": "Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New\n  Framework For Traffic Flow Prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04136",
        "abstract_url": "http://arxiv.org/abs/2401.04136",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haonan"
            },
            {
                "last_name": "Shen",
                "first_name": "Qianli"
            },
            {
                "last_name": "Tong",
                "first_name": "Yao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yang"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The commercialization of diffusion models, renowned for their ability to\ngenerate high-quality images that are often indistinguishable from real ones,\nbrings forth potential copyright concerns. Although attempts have been made to\nimpede unauthorized access to copyrighted material during training and to\nsubsequently prevent DMs from generating copyrighted images, the effectiveness\nof these solutions remains unverified. This study explores the vulnerabilities\nassociated with copyright protection in DMs by introducing a backdoor data\npoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.\nOur attack method operates without requiring access to or control over the\ndiffusion model's training or fine-tuning processes; it merely involves the\ninsertion of poisoning data into the clean training dataset. This data,\ncomprising poisoning images equipped with prompts, is generated by leveraging\nthe powerful capabilities of multimodal large language models and text-guided\nimage inpainting techniques. Our experimental results and analysis confirm the\nmethod's effectiveness. By integrating a minor portion of\nnon-copyright-infringing stealthy poisoning data into the clean\ndataset-rendering it free from suspicion-we can prompt the finetuned diffusion\nmodels to produce copyrighted content when activated by specific trigger\nprompts. These findings underline potential pitfalls in the prevailing\ncopyright protection strategies and underscore the necessity for increased\nscrutiny and preventative measures against the misuse of DMs.\n",
        "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data\n  Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04138",
        "abstract_url": "http://arxiv.org/abs/2401.04138",
        "authors": [
            {
                "last_name": "Torii",
                "first_name": "Maya Grace"
            },
            {
                "last_name": "Murakami",
                "first_name": "Takahito"
            },
            {
                "last_name": "Ochiai",
                "first_name": "Yoichi"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  How would research be like if we still needed to \"send\" papers typed with a\ntypewriter? Our life and research environment have continually evolved, often\naccompanied by controversial opinions about new methodologies. In this paper,\nwe embrace this change by introducing a new approach to qualitative analysis in\nHCI using Large Language Models (LLMs). We detail a method that uses LLMs for\nqualitative data analysis and present a quantitative framework using SBART\ncosine similarity for performance evaluation. Our findings indicate that LLMs\nnot only match the efficacy of traditional analysis methods but also offer\nunique insights. Through a novel dataset and benchmark, we explore LLMs'\ncharacteristics in HCI research, suggesting potential avenues for further\nexploration and application in the field.\n",
        "title": "Expanding Horizons in HCI Research Through LLM-Driven Qualitative\n  Analysis",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04139",
        "abstract_url": "http://arxiv.org/abs/2401.04139",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Hanbeot"
            },
            {
                "last_name": "Cho",
                "first_name": "Yunjeong"
            },
            {
                "last_name": "Kim",
                "first_name": "Hoon-Hee"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study introduces CCNETS (Causal Learning with Causal Cooperative Nets),\na novel generative model-based classifier designed to tackle the challenge of\ngenerating data for imbalanced datasets in pattern recognition. CCNETS is\nuniquely crafted to emulate brain-like information processing and comprises\nthree main components: Explainer, Producer, and Reasoner. Each component is\ndesigned to mimic specific brain functions, which aids in generating\nhigh-quality datasets and enhancing classification performance.\n  The model is particularly focused on addressing the common and significant\nchallenge of handling imbalanced datasets in machine learning. CCNETS's\neffectiveness is demonstrated through its application to a \"fraud dataset,\"\nwhere normal transactions significantly outnumber fraudulent ones (99.83% vs.\n0.17%). Traditional methods often struggle with such imbalances, leading to\nskewed performance metrics. However, CCNETS exhibits superior classification\nability, as evidenced by its performance metrics. Specifically, it achieved an\nF1-score of 0.7992, outperforming traditional models like Autoencoders and\nMulti-layer Perceptrons (MLP) in the same context. This performance indicates\nCCNETS's proficiency in more accurately distinguishing between normal and\nfraudulent patterns.\n  The innovative structure of CCNETS enhances the coherence between generative\nand classification models, helping to overcome the limitations of pattern\nrecognition that rely solely on generative models. This study emphasizes\nCCNETS's potential in diverse applications, especially where quality data\ngeneration and pattern recognition are key. It proves effective in machine\nlearning, particularly for imbalanced datasets. CCNETS overcomes current\nchallenges in these datasets and advances machine learning with brain-inspired\napproaches.\n",
        "title": "CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition\n  in Imbalanced Datasets",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04141",
        "abstract_url": "http://arxiv.org/abs/2401.04141",
        "authors": [
            {
                "last_name": "Zini",
                "first_name": "Julia El"
            },
            {
                "last_name": "Musharrafieh",
                "first_name": "Bassel"
            },
            {
                "last_name": "Awad",
                "first_name": "Mariette"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The fractal dimension provides a statistical index of object complexity by\nstudying how the pattern changes with the measuring scale. Although useful in\nseveral classification tasks, the fractal dimension is under-explored in deep\nlearning applications. In this work, we investigate the features that are\nlearned by deep models and we study whether these deep networks are able to\nencode features as complex and high-level as the fractal dimensions.\nSpecifically, we conduct a correlation analysis experiment to show that deep\nnetworks are not able to extract such a feature in none of their layers. We\ncombine our analytical study with a human evaluation to investigate the\ndifferences between deep learning networks and models that operate on the\nfractal feature solely. Moreover, we show the effectiveness of fractal features\nin applications where the object structure is crucial for the classification\ntask. We empirically show that training a shallow network on fractal features\nachieves performance comparable, even superior in specific cases, to that of\ndeep networks trained on raw data while requiring less computational resources.\nFractals improved the accuracy of the classification by 30% on average while\nrequiring up to 84% less time to train. We couple our empirical study with a\ncomplexity analysis of the computational cost of extracting the proposed\nfractal features, and we study its limitation.\n",
        "title": "On The Potential of The Fractal Geometry and The CNNs Ability to Encode\n  it",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04143",
        "abstract_url": "http://arxiv.org/abs/2401.04143",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Xianghui"
            },
            {
                "last_name": "Wang",
                "first_name": "Xi"
            },
            {
                "last_name": "Athanasiou",
                "first_name": "Nikos"
            },
            {
                "last_name": "Bhatnagar",
                "first_name": "Bharat Lal"
            },
            {
                "last_name": "Huang",
                "first_name": "Chun-Hao P."
            },
            {
                "last_name": "Mo",
                "first_name": "Kaichun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Jia",
                "first_name": "Xia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zerui"
            },
            {
                "last_name": "Cui",
                "first_name": "Liangxian"
            },
            {
                "last_name": "Lin",
                "first_name": "Xiao"
            },
            {
                "last_name": "Qian",
                "first_name": "Bingqiao"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jie"
            },
            {
                "last_name": "Yang",
                "first_name": "Wenfei"
            },
            {
                "last_name": "Nam",
                "first_name": "Hyeongjin"
            },
            {
                "last_name": "Jung",
                "first_name": "Daniel Sungho"
            },
            {
                "last_name": "Kim",
                "first_name": "Kihoon"
            },
            {
                "last_name": "Lee",
                "first_name": "Kyoung Mu"
            },
            {
                "last_name": "Hilliges",
                "first_name": "Otmar"
            },
            {
                "last_name": "Pons-Moll",
                "first_name": "Gerard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Modeling the interaction between humans and objects has been an emerging\nresearch direction in recent years. Capturing human-object interaction is\nhowever a very challenging task due to heavy occlusion and complex dynamics,\nwhich requires understanding not only 3D human pose, and object pose but also\nthe interaction between them. Reconstruction of 3D humans and objects has been\ntwo separate research fields in computer vision for a long time. We hence\nproposed the first RHOBIN challenge: reconstruction of human-object\ninteractions in conjunction with the RHOBIN workshop. It was aimed at bringing\nthe research communities of human and object reconstruction as well as\ninteraction modeling together to discuss techniques and exchange ideas. Our\nchallenge consists of three tracks of 3D reconstruction from monocular RGB\nimages with a focus on dealing with challenging interaction scenarios. Our\nchallenge attracted more than 100 participants with more than 300 submissions,\nindicating the broad interest in the research communities. This paper describes\nthe settings of our challenge and discusses the winning methods of each track\nin more detail. We observe that the human reconstruction task is becoming\nmature even under heavy occlusion settings while object pose estimation and\njoint reconstruction remain challenging tasks. With the growing interest in\ninteraction modeling, we hope this report can provide useful insights and\nfoster future research in this direction. Our workshop website can be found at\n\\href{https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}.\n",
        "title": "RHOBIN Challenge: Reconstruction of Human Object Interaction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04144",
        "abstract_url": "http://arxiv.org/abs/2401.04144",
        "authors": [
            {
                "last_name": "Gilda",
                "first_name": "Sankalp"
            },
            {
                "last_name": "Bhandari",
                "first_name": "Neel"
            },
            {
                "last_name": "Mak",
                "first_name": "Wendy"
            },
            {
                "last_name": "Panizza",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In this paper, we present results on improving out-of-domain weather\nprediction and uncertainty estimation as part of the \\texttt{Shifts Challenge\non Robustness and Uncertainty under Real-World Distributional Shift} challenge.\nWe find that by leveraging a mixture of experts in conjunction with an advanced\ndata augmentation technique borrowed from the computer vision domain, in\nconjunction with robust \\textit{post-hoc} calibration of predictive\nuncertainties, we can potentially achieve more accurate and better-calibrated\nresults with deep neural networks than with boosted tree models for tabular\ndata. We quantify our predictions using several metrics and propose several\nfuture lines of inquiry and experimentation to boost performance.\n",
        "title": "Robust Calibration For Improved Weather Prediction Under Distributional\n  Shift",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04145",
        "abstract_url": "http://arxiv.org/abs/2401.04145",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Guoming"
            },
            {
                "last_name": "Hou",
                "first_name": "Mingxin"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xiaofang"
            },
            {
                "last_name": "Huang",
                "first_name": "Shuqiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaonan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "RO"
        ],
        "abstract": "  Deep reinforcement learning (DRL) methods have recently shown promise in path\nplanning tasks. However, when dealing with global planning tasks, these methods\nface serious challenges such as poor convergence and generalization. To this\nend, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan\nArbitrarily) in this paper. Firstly, we analyze the reasons of these problems\nfrom the perspective of DRL's observation, revealing that the traditional\ndesign causes DRL to be interfered by irrelevant map information. Secondly, we\ndevelop the LOPA which utilizes a novel attention-enhanced mechanism to attain\nan improved attention capability towards the key information of the\nobservation. Such a mechanism is realized by two steps: (1) an attention model\nis built to transform the DRL's observation into two dynamic views: local and\nglobal, significantly guiding the LOPA to focus on the key information on the\ngiven maps; (2) a dual-channel network is constructed to process these two\nviews and integrate them to attain an improved reasoning capability. The LOPA\nis validated via multi-objective global path planning experiments. The result\nsuggests the LOPA has improved convergence and generalization performance as\nwell as great path planning efficiency.\n",
        "title": "Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep\n  Reinforcement Learning Method for Global Path Planning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04148",
        "abstract_url": "http://arxiv.org/abs/2401.04148",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Pengxin"
            },
            {
                "last_name": "Jin",
                "first_name": "Pengrong"
            },
            {
                "last_name": "Li",
                "first_name": "Ziyue"
            },
            {
                "last_name": "Bai",
                "first_name": "Lei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Accurate spatial-temporal traffic flow forecasting is crucial in aiding\ntraffic managers in implementing control measures and assisting drivers in\nselecting optimal travel routes. Traditional deep-learning based methods for\ntraffic flow forecasting typically rely on historical data to train their\nmodels, which are then used to make predictions on future data. However, the\nperformance of the trained model usually degrades due to the temporal drift\nbetween the historical and future data. To make the model trained on historical\ndata better adapt to future data in a fully online manner, this paper conducts\nthe first study of the online test-time adaptation techniques for\nspatial-temporal traffic flow forecasting problems. To this end, we propose an\nAdaptive Double Correction by Series Decomposition (ADCSD) method, which first\ndecomposes the output of the trained model into seasonal and trend-cyclical\nparts and then corrects them by two separate modules during the testing phase\nusing the latest observed data entry by entry. In the proposed ADCSD method,\ninstead of fine-tuning the whole trained model during the testing phase, a lite\nnetwork is attached after the trained model, and only the lite network is\nfine-tuned in the testing process each time a data entry is observed. Moreover,\nto satisfy that different time series variables may have different levels of\ntemporal drift, two adaptive vectors are adopted to provide different weights\nfor different time series variables. Extensive experiments on four real-world\ntraffic flow forecasting datasets demonstrate the effectiveness of the proposed\nADCSD method. The code is available at https://github.com/Pengxin-Guo/ADCSD.\n",
        "title": "Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04150",
        "abstract_url": "http://arxiv.org/abs/2401.04150",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Long"
            },
            {
                "last_name": "Li",
                "first_name": "Ziqiang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bingxin"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhongming"
            },
            {
                "last_name": "Li",
                "first_name": "Ao"
            },
            {
                "last_name": "Ge",
                "first_name": "Yongxin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Although few-shot action recognition based on metric learning paradigm has\nachieved significant success, it fails to address the following issues: (1)\ninadequate action relation modeling and underutilization of multi-modal\ninformation; (2) challenges in handling video matching problems with different\nlengths and speeds, and video matching problems with misalignment of video\nsub-actions. To address these issues, we propose a Two-Stream Joint Matching\nmethod based on contrastive learning (TSJM), which consists of two modules:\nMulti-modal Contrastive Learning Module (MCL) and Joint Matching Module (JMM).\nThe objective of the MCL is to extensively investigate the inter-modal mutual\ninformation relationships, thereby thoroughly extracting modal information to\nenhance the modeling of action relationships. The JMM aims to simultaneously\naddress the aforementioned video matching problems. The effectiveness of the\nproposed method is evaluated on two widely used few shot action recognition\ndatasets, namely, SSv2 and Kinetics. Comprehensive ablation experiments are\nalso conducted to substantiate the efficacy of our proposed approach.\n",
        "title": "Two-stream joint matching method based on contrastive learning for\n  few-shot action recognition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04151",
        "abstract_url": "http://arxiv.org/abs/2401.04151",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Qin",
                "first_name": "Chengwei"
            },
            {
                "last_name": "Hazan",
                "first_name": "Elad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Fine-tuning is the primary methodology for tailoring pre-trained large\nlanguage models to specific tasks. As the model's scale and the diversity of\ntasks expand, parameter-efficient fine-tuning methods are of paramount\nimportance. One of the most widely used family of methods is low-rank\nadaptation (LoRA) and its variants. LoRA encodes weight update as the product\nof two low-rank matrices. Despite its advantages, LoRA falls short of\nfull-parameter fine-tuning in terms of generalization error for certain tasks.\n  We introduce Chain of LoRA (COLA), an iterative optimization framework\ninspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full\nparameter fine-tuning, without incurring additional computational costs or\nmemory overheads. COLA employs a residual learning procedure where it merges\nlearned LoRA modules into the pre-trained language model parameters and\nre-initilize optimization for new born LoRA modules. We provide theoretical\nconvergence guarantees as well as empirical results to validate the\neffectiveness of our algorithm. Across various models (OPT and llama-2) and\nseven benchmarking tasks, we demonstrate that COLA can consistently outperform\nLoRA without additional computational or memory costs.\n",
        "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual\n  Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04152",
        "abstract_url": "http://arxiv.org/abs/2401.04152",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Meng",
                "first_name": "Lingwei"
            },
            {
                "last_name": "Cui",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Guo",
                "first_name": "Haohan"
            },
            {
                "last_name": "Wu",
                "first_name": "Xixin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xunying"
            },
            {
                "last_name": "Meng",
                "first_name": "Helen"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "CL",
            ""
        ],
        "abstract": "  End-to-end multi-talker speech recognition has garnered great interest as an\neffective approach to directly transcribe overlapped speech from multiple\nspeakers. Current methods typically adopt either 1) single-input\nmultiple-output (SIMO) models with a branched encoder, or 2) single-input\nsingle-output (SISO) models based on attention-based encoder-decoder\narchitecture with serialized output training (SOT). In this work, we propose a\nCross-Speaker Encoding (CSE) network to address the limitations of SIMO models\nby aggregating cross-speaker representations. Furthermore, the CSE model is\nintegrated with SOT to leverage both the advantages of SIMO and SISO while\nmitigating their drawbacks. To the best of our knowledge, this work represents\nan early effort to integrate SIMO and SISO for multi-talker speech recognition.\nExperiments on the two-speaker LibrispeechMix dataset show that the CES model\nreduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model\nreduces WER by 10% overall and by 16% on high-overlap speech compared to the\nSOT model.\n",
        "title": "Cross-Speaker Encoding Network for Multi-Talker Speech Recognition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04153",
        "abstract_url": "http://arxiv.org/abs/2401.04153",
        "authors": [
            {
                "last_name": "Zimmermann",
                "first_name": "Janos"
            },
            {
                "last_name": "Motejat",
                "first_name": "Michael"
            },
            {
                "last_name": "R\u00f6ssl",
                "first_name": "Christian"
            },
            {
                "last_name": "Theisel",
                "first_name": "Holger"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GR"
        ],
        "abstract": "  FTLE (Finite Time Lyapunov Exponent) computation is one of the standard\napproaches to Lagrangian flow analysis. The main features of interest in FTLE\nfields are ridges that represent hyperbolic Lagrangian Coherent Structures.\nFTLE ridges tend to become sharp and crisp with increasing integration time,\nwhere the sharpness of the ridges is an indicator of the strength of\nseparation. The additional consideration of uncertainty in flows leads to more\nblurred ridges in the FTLE fields. There are multiple causes for such blurred\nridges: either the locations of the ridges are uncertain, or the strength of\nthe ridges is uncertain, or there is low uncertainty but weak separation.\nExisting approaches for uncertain FTLE computation are unable to distinguish\nthese different sources of uncertainty in the ridges. We introduce a new\napproach to define and visualize FTLE fields for flow ensembles. Before\ncomputing and comparing FTLE fields for the ensemble members, we compute\noptimal displacements of the domains to mutually align the ridges of the\nensemble members as much as possible. We do so in a way that an explicit\ngeometry extraction and alignment of the ridges is not necessary. The\nadditional consideration of these displacements allows for a visual distinction\nbetween uncertainty in ridge location, ridge sharpness, and separation\nstrength. We apply the approach to several synthetic and real ensemble data\nsets.\n",
        "title": "FTLE for Flow Ensembles by Optimal Domain Displacement",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04154",
        "abstract_url": "http://arxiv.org/abs/2401.04154",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wentao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.\n",
        "title": "Efficient Selective Audio Masked Multimodal Bottleneck Transformer for\n  Audio-Video Classification",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04155",
        "abstract_url": "http://arxiv.org/abs/2401.04155",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiajia"
            },
            {
                "last_name": "Yang",
                "first_name": "Mengyuan"
            },
            {
                "last_name": "Yu",
                "first_name": "Yankai"
            },
            {
                "last_name": "Xu",
                "first_name": "Haixia"
            },
            {
                "last_name": "Li",
                "first_name": "Kang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Large language models (LLMs) are a class of artificial intelligence models\nbased on deep learning, which have great performance in various tasks,\nespecially in natural language processing (NLP). Large language models\ntypically consist of artificial neural networks with numerous parameters,\ntrained on large amounts of unlabeled input using self-supervised or\nsemi-supervised learning. However, their potential for solving bioinformatics\nproblems may even exceed their proficiency in modeling human language. In this\nreview, we will present a summary of the prominent large language models used\nin natural language processing, such as BERT and GPT, and focus on exploring\nthe applications of large language models at different omics levels in\nbioinformatics, mainly including applications of large language models in\ngenomics, transcriptomics, proteomics, drug discovery and single cell analysis.\nFinally, this review summarizes the potential and prospects of large language\nmodels in solving bioinformatic problems.\n",
        "title": "Large language models in bioinformatics: applications and perspectives",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04157",
        "abstract_url": "http://arxiv.org/abs/2401.04157",
        "authors": [
            {
                "last_name": "Skreta",
                "first_name": "Marta"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zihan"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jia Lin"
            },
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Aspuru-Guzik",
                "first_name": "Al\u00e1n"
            },
            {
                "last_name": "Garg",
                "first_name": "Animesh"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Advancements in large language models (LLMs) have demonstrated their\npotential in facilitating high-level reasoning, logical reasoning and robotics\nplanning. Recently, LLMs have also been able to generate reward functions for\nlow-level robot actions, effectively bridging the interface between high-level\nplanning and low-level robot control. However, the challenge remains that even\nwith syntactically correct plans, robots can still fail to achieve their\nintended goals. This failure can be attributed to imperfect plans proposed by\nLLMs or to unforeseeable environmental circumstances that hinder the execution\nof planned subtasks due to erroneous assumptions about the state of objects.\nOne way to prevent these challenges is to rely on human-provided step-by-step\ninstructions, limiting the autonomy of robotic systems. Vision Language Models\n(VLMs) have shown remarkable success in tasks such as visual question answering\nand image captioning. Leveraging the capabilities of VLMs, we present a novel\nframework called Robotic Replanning with Perception and Language Models\n(RePLan) that enables real-time replanning capabilities for long-horizon tasks.\nThis framework utilizes the physical grounding provided by a VLM's\nunderstanding of the world's state to adapt robot actions when the initial plan\nfails to achieve the desired goal. We test our approach within four\nenvironments containing seven long-horizion tasks. We find that RePLan enables\na robot to successfully adapt to unforeseen obstacles while accomplishing\nopen-ended, long-horizon goals, where baseline models cannot. Find more\ninformation at https://replan-lm.github.io/replan.github.io/\n",
        "title": "RePLan: Robotic Replanning with Perception and Language Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04168",
        "abstract_url": "http://arxiv.org/abs/2401.04168",
        "authors": [
            {
                "last_name": "Mart\u00ednez",
                "first_name": "Francisco Ard\u00e9vol"
            },
            {
                "last_name": "Min",
                "first_name": "Michiel"
            },
            {
                "last_name": "Huppenkothen",
                "first_name": "Daniela"
            },
            {
                "last_name": "Kamp",
                "first_name": "Inga"
            },
            {
                "last_name": "Palmer",
                "first_name": "Paul I."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Interpreting the observations of exoplanet atmospheres to constrain physical\nand chemical properties is typically done using Bayesian retrieval techniques.\nBecause these methods require many model computations, a compromise is made\nbetween model complexity and run time. Reaching this compromise leads to the\nsimplification of many physical and chemical processes (e.g. parameterised\ntemperature structure). Here we implement and test sequential neural posterior\nestimation (SNPE), a machine learning inference algorithm, for exoplanet\natmospheric retrievals. The goal is to speed up retrievals so they can be run\nwith more computationally expensive atmospheric models, such as those computing\nthe temperature structure using radiative transfer. We generate 100 synthetic\nobservations using ARCiS (ARtful Modeling Code for exoplanet Science, an\natmospheric modelling code with the flexibility to compute models in varying\ndegrees of complexity) and perform retrievals on them to test the faithfulness\nof the SNPE posteriors. The faithfulness quantifies whether the posteriors\ncontain the ground truth as often as we expect. We also generate a synthetic\nobservation of a cool brown dwarf using the self-consistent capabilities of\nARCiS and run a retrieval with self-consistent models to showcase the\npossibilities that SNPE opens. We find that SNPE provides faithful posteriors\nand is therefore a reliable tool for exoplanet atmospheric retrievals. We are\nable to run a self-consistent retrieval of a synthetic brown dwarf spectrum\nusing only 50,000 forward model evaluations. We find that SNPE can speed up\nretrievals between $\\sim2\\times$ and $\\geq10\\times$ depending on the\ncomputational load of the forward model, the dimensionality of the observation,\nand the signal-to-noise ratio of the observation. We make the code publicly\navailable for the community on Github.\n",
        "title": "FlopPITy: Enabling self-consistent exoplanet atmospheric retrievals with\n  machine learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04181",
        "abstract_url": "http://arxiv.org/abs/2401.04181",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Minjie"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yichen"
            },
            {
                "last_name": "Li",
                "first_name": "Jinming"
            },
            {
                "last_name": "Wen",
                "first_name": "Junjie"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Che",
                "first_name": "Zhengping"
            },
            {
                "last_name": "Shen",
                "first_name": "Chaomin"
            },
            {
                "last_name": "Peng",
                "first_name": "Yaxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Dong"
            },
            {
                "last_name": "Feng",
                "first_name": "Feifei"
            },
            {
                "last_name": "Tang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  The language-conditioned robotic manipulation aims to transfer natural\nlanguage instructions into executable actions, from simple pick-and-place to\ntasks requiring intent recognition and visual reasoning. Inspired by the dual\nprocess theory in cognitive science, which suggests two parallel systems of\nfast and slow thinking in human decision-making, we introduce Robotics with\nFast and Slow Thinking (RFST), a framework that mimics human cognitive\narchitecture to classify tasks and makes decisions on two systems based on\ninstruction types. Our RFST consists of two key components: 1) an instruction\ndiscriminator to determine which system should be activated based on the\ncurrent user instruction, and 2) a slow-thinking system that is comprised of a\nfine-tuned vision language model aligned with the policy networks, which allows\nthe robot to recognize user intention or perform reasoning tasks. To assess our\nmethodology, we built a dataset featuring real-world trajectories, capturing\nactions ranging from spontaneous impulses to tasks requiring deliberate\ncontemplation. Our results, both in simulation and real-world scenarios,\nconfirm that our approach adeptly manages intricate tasks that demand intent\nrecognition and reasoning. The project is available at\nhttps://jlm-z.github.io/RSFT/\n",
        "title": "Language-Conditioned Robotic Manipulation with Fast and Slow Thinking",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04190",
        "abstract_url": "http://arxiv.org/abs/2401.04190",
        "authors": [
            {
                "last_name": "D\u00edaz-Pach\u00f3n",
                "first_name": "Daniel Andr\u00e9s"
            },
            {
                "last_name": "H\u00f6ssjer",
                "first_name": "Ola"
            },
            {
                "last_name": "Mathew",
                "first_name": "Calvin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "",
            ""
        ],
        "abstract": "  Fine-tuning studies whether some physical parameters, or relevant ratios\nbetween them, are located within so-called life-permitting intervals of small\nprobability outside of which carbon-based life would not be possible. Recent\ndevelopments have found estimates of these probabilities that circumvent\nprevious concerns of measurability and selection bias. However, the question\nremains if fine-tuning can indeed be known. Using a mathematization of the\nepistemological concepts of learning and knowledge acquisition, we argue that\nmost examples that have been touted as fine-tuned cannot be formally assessed\nas such. Nevertheless, fine-tuning can be known when the physical parameter is\nseen as a random variable and it is supported in the nonnegative real line,\nprovided the size of the life-permitting interval is small in relation to the\nobserved value of the parameter.\n",
        "title": "Is it possible to know cosmological fine-tuning?",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04191",
        "abstract_url": "http://arxiv.org/abs/2401.04191",
        "authors": [
            {
                "last_name": "Th\u00e9riault",
                "first_name": "Robin"
            },
            {
                "last_name": "Tantari",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Dense Hopfield networks are known for their feature to prototype transition\nand adversarial robustness. However, previous theoretical studies have been\nmostly concerned with their storage capacity. We bridge this gap by studying\nthe phase diagram of p-body Hopfield networks in the teacher-student setting of\nan unsupervised learning problem, uncovering ferromagnetic phases reminiscent\nof the prototype and feature learning regimes. On the Nishimori line, we find\nthe critical size of the training set necessary for efficient pattern\nretrieval. Interestingly, we find that that the paramagnetic to ferromagnetic\ntransition of the teacher-student setting coincides with the paramagnetic to\nspin-glass transition of the direct model, i.e. with random patterns. Outside\nof the Nishimori line, we investigate the learning performance in relation to\nthe inference temperature and dataset noise. Moreover, we show that using a\nlarger p for the student than the teacher gives the student an extensive\ntolerance to noise. We then derive a closed-form expression measuring the\nadversarial robustness of such a student at zero temperature, corroborating the\npositive correlation between number of parameters and robustness observed in\nlarge neural networks. We also use our model to clarify why the prototype phase\nof modern Hopfield networks is adversarially robust.\n",
        "title": "Dense Hopfield Networks in the Teacher-Student Setting",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04192",
        "abstract_url": "http://arxiv.org/abs/2401.04192",
        "authors": [
            {
                "last_name": "Ram\u00edrez",
                "first_name": "Aurora"
            },
            {
                "last_name": "Romero",
                "first_name": "Jos\u00e9 Ra\u00fal"
            },
            {
                "last_name": "Ventura",
                "first_name": "Sebasti\u00e1n"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "NE",
            "",
            "",
            ""
        ],
        "abstract": "  While working on a software specification, designers usually need to evaluate\ndifferent architectural alternatives to be sure that quality criteria are met.\nEven when these quality aspects could be expressed in terms of multiple\nsoftware metrics, other qualitative factors cannot be numerically measured, but\nthey are extracted from the engineer's know-how and prior experiences. In fact,\ndetecting not only strong but also weak points in the different solutions seems\nto fit better with the way humans make their decisions. Putting the human in\nthe loop brings new challenges to the search-based software engineering field,\nespecially for those human-centered activities within the early analysis phase.\nThis paper explores how the interactive evolutionary computation can serve as a\nbasis for integrating the human's judgment into the search process. An\ninteractive approach is proposed to discover software architectures, in which\nboth quantitative and qualitative criteria are applied to guide a\nmulti-objective evolutionary algorithm. The obtained feedback is incorporated\ninto the fitness function using architectural preferences allowing the\nalgorithm to discern between promising and poor solutions. Experimentation with\nreal users has revealed that the proposed interaction mechanism can effectively\nguide the search towards those regions of the search space that are of real\ninterest to the expert.\n",
        "title": "Interactive Multi-Objective Evolutionary Optimization of Software\n  Architectures",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04196",
        "abstract_url": "http://arxiv.org/abs/2401.04196",
        "authors": [
            {
                "last_name": "Blanes",
                "first_name": "Sergio"
            },
            {
                "last_name": "Casas",
                "first_name": "Fernando"
            },
            {
                "last_name": "Gonz\u00e1lez",
                "first_name": "Ces\u00e1reo"
            },
            {
                "last_name": "Thalhammer",
                "first_name": "Mechthild"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  The present work provides a comprehensive study of symmetric-conjugate\noperator splitting methods in the context of linear parabolic problems and\ndemonstrates their additional benefits compared to symmetric splitting methods.\nRelevant applications include nonreversible systems and ground state\ncomputations for linear Schr\\\"odinger equations based on the imaginary time\npropagation. Numerical examples confirm the favourable error behaviour of\nhigher-order symmetric-conjugate splitting methods and illustrate the\nusefulness of a time stepsize control, where the local error estimation relies\non the computation of the imaginary parts and thus requires negligible costs.\n",
        "title": "Symmetric-conjugate splitting methods for evolution equations of\n  parabolic type",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04198",
        "abstract_url": "http://arxiv.org/abs/2401.04198",
        "authors": [
            {
                "last_name": "Dewan",
                "first_name": "Shaurya"
            },
            {
                "last_name": "Jain",
                "first_name": "Anisha"
            },
            {
                "last_name": "LaLena",
                "first_name": "Zoe"
            },
            {
                "last_name": "Yu",
                "first_name": "Lifan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The authors of 'Unsupervised Reinforcement Learning in Multiple environments'\npropose a method, alpha-MEPOL, to tackle unsupervised RL across multiple\nenvironments. They pre-train a task-agnostic exploration policy using\ninteractions from an entire environment class and then fine-tune this policy\nfor various tasks using supervision. We expanded upon this work, with the goal\nof improving performance. We primarily propose and experiment with five new\nmodifications to the original work: sampling trajectories using an\nentropy-based probability distribution, dynamic alpha, higher KL Divergence\nthreshold, curiosity-driven exploration, and alpha-percentile sampling on\ncuriosity. Dynamic alpha and higher KL-Divergence threshold both provided a\nsignificant improvement over the baseline from the earlier work. PDF-sampling\nfailed to provide any improvement due to it being approximately equivalent to\nthe baseline method when the sample space is small. In high-dimensional\nenvironments, the addition of curiosity-driven exploration enhances learning by\nencouraging the agent to seek diverse experiences and explore the unknown more.\nHowever, its benefits are limited in low-dimensional and simpler environments\nwhere exploration possibilities are constrained and there is little that is\ntruly unknown to the agent. Overall, some of our experiments did boost\nperformance over the baseline and there are a few directions that seem\npromising for further research.\n",
        "title": "Curiosity & Entropy Driven Unsupervised RL in Multiple Environments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04206",
        "abstract_url": "http://arxiv.org/abs/2401.04206",
        "authors": [
            {
                "last_name": "Kaufman",
                "first_name": "Robert"
            },
            {
                "last_name": "Costa",
                "first_name": "Jean"
            },
            {
                "last_name": "Kimani",
                "first_name": "Everlyne"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "RO"
        ],
        "abstract": "  In a pre-post experiment (n = 41), we test the impact of an AI Coach's\nexplanatory communications modeled after the instructions of human driving\nexperts. Participants were divided into four (4) groups to assess two (2)\ndimensions of the AI coach's explanations: information type ('what' and\n'why'-type explanations) and presentation modality (auditory and visual). We\ndirectly compare how AI Coaching sessions employing these techniques impact\ndriving performance, cognitive load, confidence, expertise, and trust in an\nobservation learning context. Through interviews, we delineate the learning\nprocess of our participants. Results show that an AI driving coach can be\nuseful for teaching performance driving skills to novices. Comparing between\ngroups, we find the type and modality of information influences performance\noutcomes. We attribute differences to how information directed attention,\nmitigated uncertainty, and influenced overload experienced by participants.\nThese, in turn, affected how successfully participants were able to learn.\nResults suggest efficient, modality-appropriate explanations should be opted\nfor when designing effective HMI communications that can instruct without\noverwhelming. Further, they support the need to align communications with human\nlearning and cognitive processes. Results are synthesized into eight design\nimplications for future autonomous vehicle HMI and AI coach design.\n",
        "title": "Learning Racing From an AI Coach: Effects of Multimodal Autonomous\n  Driving Explanations on Driving Performance, Cognitive Load, Expertise, and\n  Trust",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04210",
        "abstract_url": "http://arxiv.org/abs/2401.04210",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhi-Song"
            },
            {
                "last_name": "Courant",
                "first_name": "Robin"
            },
            {
                "last_name": "Kalogeiton",
                "first_name": "Vicky"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  Automatically understanding funny moments (i.e., the moments that make people\nlaugh) when watching comedy is challenging, as they relate to various features,\nsuch as body language, dialogues and culture. In this paper, we propose\nFunnyNet-W, a model that relies on cross- and self-attention for visual, audio\nand text data to predict funny moments in videos. Unlike most methods that rely\non ground truth data in the form of subtitles, in this work we exploit\nmodalities that come naturally with videos: (a) video frames as they contain\nvisual information indispensable for scene understanding, (b) audio as it\ncontains higher-level cues associated with funny moments, such as intonation,\npitch and pauses and (c) text automatically extracted with a speech-to-text\nmodel as it can provide rich information when processed by a Large Language\nModel. To acquire labels for training, we propose an unsupervised approach that\nspots and labels funny audio moments. We provide experiments on five datasets:\nthe sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive\nexperiments and analysis show that FunnyNet-W successfully exploits visual,\nauditory and textual cues to identify funny moments, while our findings reveal\nFunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the\nnew state of the art for funny moment detection with multimodal cues on all\ndatasets with and without using ground truth information.\n",
        "title": "FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04212",
        "abstract_url": "http://arxiv.org/abs/2401.04212",
        "authors": [
            {
                "last_name": "Rodriguez-Fernandez",
                "first_name": "Victor"
            },
            {
                "last_name": "Sarangerel",
                "first_name": "Sumiyajav"
            },
            {
                "last_name": "Siew",
                "first_name": "Peng Mun"
            },
            {
                "last_name": "Machuca",
                "first_name": "Pablo"
            },
            {
                "last_name": "Jang",
                "first_name": "Daniel"
            },
            {
                "last_name": "Linares",
                "first_name": "Richard"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  With the rapid increase in the number of Anthropogenic Space Objects (ASOs),\nLow Earth Orbit (LEO) is facing significant congestion, thereby posing\nchallenges to space operators and risking the viability of the space\nenvironment for varied uses. Current models for examining this evolution, while\ndetailed, are computationally demanding. To address these issues, we propose a\nnovel machine learning-based model, as an extension of the MIT Orbital Capacity\nTool (MOCAT). This advanced model is designed to accelerate the propagation of\nASO density distributions, and it is trained on hundreds of simulations\ngenerated by an established and accurate model of the space environment\nevolution. We study how different deep learning-based solutions can potentially\nbe good candidates for ASO propagation and manage the high-dimensionality of\nthe data. To assess the model's capabilities, we conduct experiments in long\nterm forecasting scenarios (around 100 years), analyze how and why the\nperformance degrades over time, and discuss potential solutions to make this\nsolution better.\n",
        "title": "Towards a Machine Learning-Based Approach to Predict Space Object\n  Density Distributions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04218",
        "abstract_url": "http://arxiv.org/abs/2401.04218",
        "authors": [
            {
                "last_name": "Fulman",
                "first_name": "Nir"
            },
            {
                "last_name": "Memduho\u011flu",
                "first_name": "Abdulkadir"
            },
            {
                "last_name": "Zipf",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We present a benchmark for assessing the capability of Large Language Models\n(LLMs) to discern intercardinal directions between geographic locations and\napply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark\nspecifically evaluates whether LLMs exhibit a hierarchical spatial bias similar\nto humans, where judgments about individual locations' spatial relationships\nare influenced by the perceived relationships of the larger groups that contain\nthem. To investigate this, we formulated 14 questions focusing on well-known\nAmerican cities. Seven questions were designed to challenge the LLMs with\nscenarios potentially influenced by the orientation of larger geographical\nunits, such as states or countries, while the remaining seven targeted\nlocations less susceptible to such hierarchical categorization. Among the\ntested models, GPT-4 exhibited superior performance with 55.3% accuracy,\nfollowed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed\nsignificantly reduced accuracy on tasks with suspected hierarchical bias. For\nexample, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on\nothers. Despite these inaccuracies, the models identified the nearest cardinal\ndirection in most cases, suggesting associative learning, embodying human-like\nmisconceptions. We discuss the potential of text-based data representing\ngeographic relationships directly to improve the spatial reasoning capabilities\nof LLMs.\n",
        "title": "Distortions in Judged Spatial Relations in Large Language Models: The\n  Dawn of Natural Language Geographic Data?",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04221",
        "abstract_url": "http://arxiv.org/abs/2401.04221",
        "authors": [
            {
                "last_name": "Malakar",
                "first_name": "Sanjay"
            },
            {
                "last_name": "Haider",
                "first_name": "Tameem Bin"
            },
            {
                "last_name": "Shahriar",
                "first_name": "Rifat"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "PL"
        ],
        "abstract": "  Fixing software bugs has always been an essential and time-consuming process\nin software development. Fixing concurrency bugs has become especially critical\nin the multicore era. However, fixing concurrency bugs is challenging due to\nnon-deterministic failures and tricky parallel reasoning. Beyond correctly\nfixing the original problem in the software, a good patch should also avoid\nintroducing new bugs, degrading performance unnecessarily, or damaging software\nreadability. Existing tools cannot automate the whole fixing process and\nprovide good-quality patches. We present RaceFixer, a tool that automates the\nprocess of fixing one common type of concurrency bug: single-variable atomicity\nviolations. RaceFixer starts from the bug reports of an existing bug-detection\ntool ThreadSanitizer. It augments these with static analysis to construct a\nsuitable patch for each bug report. It tries to combine the patches of multiple\nbugs for better performance and code readability. Finally, we test RaceFixer on\nbenchmarks from TheadSanitizer.\n",
        "title": "RaceFixer -- An Automated Data Race Fixer",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04226",
        "abstract_url": "http://arxiv.org/abs/2401.04226",
        "authors": [
            {
                "last_name": "Huin",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Martin",
                "first_name": "S\u00e9bastien"
            },
            {
                "last_name": "Leguay",
                "first_name": "J\u00e9r\u00e9mie"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Multi-topology routing (MTR) provides an attractive alternative to segment\nrouting for traffic engineering when network devices cannot be upgraded.\nHowever, due to a high overhead in terms of link state messages exchanged by\ntopologies and the need to frequently update link weights to follow evolving\nnetwork conditions, MTR is often limited to a small number of topologies and\nthe satisfaction of loose QoS constraints. To overcome these limitations we\npropose vMTR, an MTR extension where demands are routed over virtual topologies\nthat are silent, i.e., they do not exchange LSA messages, and that are\ncontinuously derived from a very limited set of real topologies, optimizing\neach a QoS parameter. In this context, we present a polynomial and exact\nalgorithm for vMTR and, as a benchmark, a local search algorithm for MTR. We\nshow that vMTR helps reducing drastically the number of real topologies and\nthat it is more robust to QoS changes.\n",
        "title": "Virtual Multi-Topology Routing for QoS Constraints",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04230",
        "abstract_url": "http://arxiv.org/abs/2401.04230",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Chengjie"
            },
            {
                "last_name": "Abdelzad",
                "first_name": "Vahdat"
            },
            {
                "last_name": "Sedwards",
                "first_name": "Sean"
            },
            {
                "last_name": "Czarnecki",
                "first_name": "Krzysztof"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We consider the problem of cross-sensor domain adaptation in the context of\nLiDAR-based 3D object detection and propose Stationary Object Aggregation\nPseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary\nobjects. In contrast to the current state-of-the-art in-domain practice of\naggregating just a few input scans, SOAP aggregates entire sequences of point\nclouds at the input level to reduce the sensor domain gap. Then, by means of\nwhat we call quasi-stationary training and spatial consistency post-processing,\nthe SOAP model generates accurate pseudo-labels for stationary objects, closing\na minimum of 30.3% domain gap compared to few-frame detectors. Our results also\nshow that state-of-the-art domain adaptation approaches can achieve even\ngreater performance in combination with SOAP, in both the unsupervised and\nsemi-supervised settings.\n",
        "title": "SOAP: Cross-sensor Domain Adaptation for 3D Object Detection Using\n  Stationary Object Aggregation Pseudo-labelling",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04232",
        "abstract_url": "http://arxiv.org/abs/2401.04232",
        "authors": [
            {
                "last_name": "Alves",
                "first_name": "Caio"
            },
            {
                "last_name": "Restrepo",
                "first_name": "Juan M."
            },
            {
                "last_name": "Ramirez",
                "first_name": "Jorge M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  In this paper we revisit the problem of decomposing a signal into a tendency\nand a residual. The tendency describes an executive summary of a signal that\nencapsulates its notable characteristics while disregarding seemingly random,\nless interesting aspects. Building upon the Intrinsic Time Decomposition (ITD)\nand information-theoretical analysis, we introduce two alternative procedures\nfor selecting the tendency from the ITD baselines. The first is based on the\nmaximum extrema prominence, namely the maximum difference between extrema\nwithin each baseline. Specifically this method selects the tendency as the\nbaseline from which an ITD step would produce the largest decline of the\nmaximum prominence. The second method uses the rotations from the ITD and\nselects the tendency as the last baseline for which the associated rotation is\nstatistically stationary. We delve into a comparative analysis of the\ninformation content and interpretability of the tendencies obtained by our\nproposed methods and those obtained through conventional low-pass filtering\nschemes, particularly the Hodrik-Prescott (HP) filter. Our findings underscore\na fundamental distinction in the nature and interpretability of these\ntendencies, highlighting their context-dependent utility with emphasis in\nmulti-scale signals. Through a series of real-world applications, we\ndemonstrate the computational robustness and practical utility of our proposed\ntendencies, emphasizing their adaptability and relevance in diverse time series\ncontexts.\n",
        "title": "Estimating an Executive Summary of a Time Series: The Tendency",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04235",
        "abstract_url": "http://arxiv.org/abs/2401.04235",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Christopher"
            },
            {
                "last_name": "Wang",
                "first_name": "Gary"
            },
            {
                "last_name": "Kastner",
                "first_name": "Kyle"
            },
            {
                "last_name": "Su",
                "first_name": "Heng"
            },
            {
                "last_name": "Chen",
                "first_name": "Allen"
            },
            {
                "last_name": "Rosenberg",
                "first_name": "Andrew"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhehuai"
            },
            {
                "last_name": "Wu",
                "first_name": "Zelin"
            },
            {
                "last_name": "Velikovich",
                "first_name": "Leonid"
            },
            {
                "last_name": "Rondon",
                "first_name": "Pat"
            },
            {
                "last_name": "Caseiro",
                "first_name": "Diamantino"
            },
            {
                "last_name": "Aleksic",
                "first_name": "Petar"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Automatic speech recognition (ASR) systems can suffer from poor recall for\nvarious reasons, such as noisy audio, lack of sufficient training data, etc.\n  Previous work has shown that recall can be improved by retrieving rewrite\ncandidates from a large database of likely, contextually-relevant alternatives\nto the hypothesis text using nearest-neighbors search over embeddings of the\nASR hypothesis text to correct and candidate corrections.\n  However, ASR-hypothesis-based retrieval can yield poor precision if the\ntextual hypotheses are too phonetically dissimilar to the transcript truth. In\nthis paper, we eliminate the hypothesis-audio mismatch problem by querying the\ncorrection database directly using embeddings derived from the utterance audio;\nthe embeddings of the utterance audio and candidate corrections are produced by\nmultimodal speech-text embedding networks trained to place the embedding of the\naudio of an utterance and the embedding of its corresponding textual transcript\nclose together.\n  After locating an appropriate correction candidate using nearest-neighbor\nsearch, we score the candidate with its speech-text embedding distance before\nadding the candidate to the original n-best list.\n  We show a relative word error rate (WER) reduction of 6% on utterances whose\ntranscripts appear in the candidate set, without increasing WER on general\nutterances.\n",
        "title": "High-precision Voice Search Query Correction via Retrievable Speech-text\n  Embedings",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04237",
        "abstract_url": "http://arxiv.org/abs/2401.04237",
        "authors": [
            {
                "last_name": "Iommazzo",
                "first_name": "Gabriele"
            },
            {
                "last_name": "D'Ambrosio",
                "first_name": "Claudia"
            },
            {
                "last_name": "Frangioni",
                "first_name": "Antonio"
            },
            {
                "last_name": "Liberti",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We propose a methodology, based on machine learning and optimization, for\nselecting a solver configuration for a given instance. First, we employ a set\nof solved instances and configurations in order to learn a performance function\nof the solver. Secondly, we formulate a mixed-integer nonlinear program where\nthe objective/constraints explicitly encode the learnt information, and which\nwe solve, upon the arrival of an unknown instance, to find the best solver\nconfiguration for that instance, based on the performance function. The main\nnovelty of our approach lies in the fact that the configuration set search\nproblem is formulated as a mathematical program, which allows us to a) enforce\nhard dependence and compatibility constraints on the configurations, and b)\nsolve it efficiently with off-the-shelf optimization tools.\n",
        "title": "A learning-based mathematical programming formulation for the automatic\n  configuration of optimization solvers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04241",
        "abstract_url": "http://arxiv.org/abs/2401.04241",
        "authors": [
            {
                "last_name": "Leyva",
                "first_name": "Roberto"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Victor"
            },
            {
                "last_name": "Epiphaniou",
                "first_name": "Gregory"
            },
            {
                "last_name": "Maple",
                "first_name": "Carsten"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face image synthesis detection is considerably gaining attention because of\nthe potential negative impact on society that this type of synthetic data\nbrings. In this paper, we propose a data-agnostic solution to detect the face\nimage synthesis process. Specifically, our solution is based on an anomaly\ndetection framework that requires only real data to learn the inference\nprocess. It is therefore data-agnostic in the sense that it requires no\nsynthetic face images. The solution uses the posterior probability with respect\nto the reference data to determine if new samples are synthetic or not. Our\nevaluation results using different synthesizers show that our solution is very\ncompetitive against the state-of-the-art, which requires synthetic data for\ntraining.\n",
        "title": "Data-Agnostic Face Image Synthesis Detection Using Bayesian CNNs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04242",
        "abstract_url": "http://arxiv.org/abs/2401.04242",
        "authors": [
            {
                "last_name": "Loregian",
                "first_name": "Fosco"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "FL"
        ],
        "abstract": "  We study generalized automata (in the sense of Ad\\'amek-Trnkov\\'a) in Joyal's\ncategory of (set-valued) combinatorial species, and as an important preliminary\nstep, we study coalgebras for its derivative endofunctor $\\partial$ and for the\n`Euler homogeneity operator' $L\\circ\\partial$ arising from the adjunction\n$L\\dashv\\partial\\dashv R$.\n",
        "title": "Automata and coalgebras in categories of species",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04244",
        "abstract_url": "http://arxiv.org/abs/2401.04244",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xingguang"
            },
            {
                "last_name": "Chimitt",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Chi",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Chan",
                "first_name": "Stanley H."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Recovering images distorted by atmospheric turbulence is a challenging\ninverse problem due to the stochastic nature of turbulence. Although numerous\nturbulence mitigation (TM) algorithms have been proposed, their efficiency and\ngeneralization to real-world dynamic scenarios remain severely limited.\nBuilding upon the intuitions of classical TM algorithms, we present the Deep\nAtmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major\nchallenges when transitioning from classical to deep learning approaches. By\ncarefully integrating the merits of classical multi-frame TM methods into a\ndeep network structure, we demonstrate that DATUM can efficiently perform\nlong-range temporal aggregation using a recurrent fashion, while deformable\nattention and temporal-channel attention seamlessly facilitate pixel\nregistration and lucky imaging. With additional supervision, tilt and blur\ndegradation can be jointly mitigated. These inductive biases empower DATUM to\nsignificantly outperform existing methods while delivering a tenfold increase\nin processing speed. A large-scale training dataset, ATSyn, is presented as a\nco-invention to enable generalization in real turbulence. Our code and datasets\nwill be available at\n\\href{https://xg416.github.io/DATUM}{\\textcolor{pink}{https://xg416.github.io/DATUM}}\n",
        "title": "Spatio-Temporal Turbulence Mitigation: A Translational Perspective",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04246",
        "abstract_url": "http://arxiv.org/abs/2401.04246",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Joseph C."
            },
            {
                "last_name": "Bloore",
                "first_name": "David"
            },
            {
                "last_name": "Kapoor",
                "first_name": "Karan"
            },
            {
                "last_name": "Feng",
                "first_name": "Jun"
            },
            {
                "last_name": "Hao",
                "first_name": "Ming-Hong"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengdi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The Boltzmann distribution of a protein provides a roadmap to all of its\nfunctional states. Normalizing flows are a promising tool for modeling this\ndistribution, but current methods are intractable for typical pharmacological\ntargets; they become computationally intractable due to the size of the system,\nheterogeneity of intra-molecular potential energy, and long-range interactions.\nTo remedy these issues, we present a novel flow architecture that utilizes\nsplit channels and gated attention to efficiently learn the conformational\ndistribution of proteins defined by internal coordinates. We show that by\nutilizing a 2-Wasserstein loss, one can smooth the transition from maximum\nlikelihood training to energy-based training, enabling the training of\nBoltzmann Generators for macromolecules. We evaluate our model and training\nstrategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein\nG, a 56-residue protein. We demonstrate that standard architectures and\ntraining strategies, such as maximum likelihood alone, fail while our novel\narchitecture and multi-stage training strategy are able to model the\nconformational distributions of protein G and HP35.\n",
        "title": "Scalable Normalizing Flows Enable Boltzmann Generators for\n  Macromolecules",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04247",
        "abstract_url": "http://arxiv.org/abs/2401.04247",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lijun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Martin",
                "first_name": "Antoni Viros"
            },
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            },
            {
                "last_name": "Brun",
                "first_name": "Yuriy"
            },
            {
                "last_name": "Guan",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Watermarking images is critical for tracking image provenance and claiming\nownership. With the advent of generative models, such as stable diffusion, able\nto create fake but realistic images, watermarking has become particularly\nimportant, e.g., to make generated images reliably identifiable. Unfortunately,\nthe very same stable diffusion technology can remove watermarks injected using\nexisting methods. To address this problem, we present a ZoDiac, which uses a\npre-trained stable diffusion model to inject a watermark into the trainable\nlatent space, resulting in watermarks that can be reliably detected in the\nlatent vector, even when attacked. We evaluate ZoDiac on three benchmarks,\nMS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against\nstate-of-the-art watermark attacks, with a watermark detection rate over 98%\nand a false positive rate below 6.4%, outperforming state-of-the-art\nwatermarking methods. Our research demonstrates that stable diffusion is a\npromising approach to robust watermarking, able to withstand even\nstable-diffusion-based attacks.\n",
        "title": "Robust Image Watermarking using Stable Diffusion",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04248",
        "abstract_url": "http://arxiv.org/abs/2401.04248",
        "authors": [
            {
                "last_name": "Dytso",
                "first_name": "Alex"
            },
            {
                "last_name": "Cardone",
                "first_name": "Martina"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper investigates the rate-distortion function, under a squared error\ndistortion $D$, for an $n$-dimensional random vector uniformly distributed on\nan $(n-1)$-sphere of radius $R$. First, an expression for the rate-distortion\nfunction is derived for any values of $n$, $D$, and $R$. Second, two types of\nasymptotics with respect to the rate-distortion function of a Gaussian source\nare characterized. More specifically, these asymptotics concern the\nlow-distortion regime (that is, $D \\to 0$) and the high-dimensional regime\n(that is, $n \\to \\infty$).\n",
        "title": "Uniform Distribution on $(n-1)$-Sphere: Rate-Distortion under Squared\n  Error Distortion",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04249",
        "abstract_url": "http://arxiv.org/abs/2401.04249",
        "authors": [
            {
                "last_name": "Ghahremani",
                "first_name": "Behzad"
            },
            {
                "last_name": "Babaee",
                "first_name": "Hessam"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We introduce a Tucker tensor cross approximation method that constructs a\nlow-rank representation of a $d$-dimensional tensor by sparsely sampling its\nfibers. These fibers are selected using the discrete empirical interpolation\nmethod (DEIM). Our proposed algorithm is referred to as DEIM fiber sampling\n(DEIM-FS). For a rank-$r$ approximation of an $\\mathcal{O}(N^d)$ tensor,\nDEIM-FS requires access to only $dNr^{d-1}$ tensor entries, a requirement that\nscales linearly with the tensor size along each mode. We demonstrate that\nDEIM-FS achieves an approximation accuracy close to the Tucker-tensor\napproximation obtained via higher-order singular value decomposition at a\nsignificantly reduced cost. We also present DEIM-FS (iterative) that does not\nrequire access to singular vectors of the target tensor unfolding and can be\nviewed as a black-box Tucker tensor algorithm. We employ DEIM-FS to reduce the\ncomputational cost associated with solving nonlinear tensor differential\nequations (TDEs) using dynamical low-rank approximation (DLRA). The\ncomputational cost of solving DLRA equations can become prohibitive when the\nexact rank of the right-hand side tensor is large. This issue arises in many\nTDEs, especially in cases involving non-polynomial nonlinearities, where the\nright-hand side tensor has full rank. This necessitates the storage and\ncomputation of tensors of size $\\mathcal{O}(N^d)$. We show that DEIM-FS results\nin significant computational savings for DLRA by constructing a low-rank Tucker\napproximation of the right-hand side tensor on the fly. Another advantage of\nusing DEIM-FS is to significantly simplify the implementation of DLRA\nequations, irrespective of the type of TDEs. We demonstrate the efficiency of\nthe algorithm through several examples including solving high-dimensional\npartial differential equations.\n",
        "title": "A DEIM Tucker Tensor Cross Algorithm and its Application to Dynamical\n  Low-Rank Approximation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04250",
        "abstract_url": "http://arxiv.org/abs/2401.04250",
        "authors": [
            {
                "last_name": "Taiwo",
                "first_name": "Funmilola Mary"
            },
            {
                "last_name": "Islambekov",
                "first_name": "Umar"
            },
            {
                "last_name": "Akcora",
                "first_name": "Cuneyt Gurcan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Topological Data Analysis (TDA) has been praised by researchers for its\nability to capture intricate shapes and structures within data. TDA is\nconsidered robust in handling noisy and high-dimensional datasets, and its\ninterpretability is believed to promote an intuitive understanding of model\nbehavior. However, claims regarding the power and usefulness of TDA have only\nbeen partially tested in application domains where TDA-based models are\ncompared to other graph machine learning approaches, such as graph neural\nnetworks. We meticulously test claims on TDA through a comprehensive set of\nexperiments and validate their merits. Our results affirm TDA's robustness\nagainst outliers and its interpretability, aligning with proponents' arguments.\nHowever, we find that TDA does not significantly enhance the predictive power\nof existing methods in our specific experiments, while incurring significant\ncomputational costs. We investigate phenomena related to graph characteristics,\nsuch as small diameters and high clustering coefficients, to mitigate the\ncomputational expenses of TDA computations. Our results offer valuable\nperspectives on integrating TDA into graph machine learning tasks.\n",
        "title": "Explaining the Power of Topological Data Analysis in Graph Machine\n  Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04257",
        "abstract_url": "http://arxiv.org/abs/2401.04257",
        "authors": [
            {
                "last_name": "Leyva",
                "first_name": "Roberto"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Victor"
            },
            {
                "last_name": "Epiphaniou",
                "first_name": "Gregory"
            },
            {
                "last_name": "Maple",
                "first_name": "Carsten"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face image synthesis is gaining more attention in computer security due to\nconcerns about its potential negative impacts, including those related to fake\nbiometrics. Hence, building models that can detect the synthesized face images\nis an important challenge to tackle. In this paper, we propose a fusion-based\nstrategy to detect face image synthesis while providing resiliency to several\nattacks. The proposed strategy uses a late fusion of the outputs computed by\nseveral undisclosed models by relying on random polynomial coefficients and\nexponents to conceal a new feature space. Unlike existing concealing solutions,\nour strategy requires no quantization, which helps to preserve the feature\nspace. Our experiments reveal that our strategy achieves state-of-the-art\nperformance while providing protection against poisoning, perturbation,\nbackdoor, and reverse model attacks.\n",
        "title": "Detecting Face Synthesis Using a Concealed Fusion Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04259",
        "abstract_url": "http://arxiv.org/abs/2401.04259",
        "authors": [
            {
                "last_name": "D'Arcy",
                "first_name": "Mike"
            },
            {
                "last_name": "Hope",
                "first_name": "Tom"
            },
            {
                "last_name": "Birnbaum",
                "first_name": "Larry"
            },
            {
                "last_name": "Downey",
                "first_name": "Doug"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We study the ability of LLMs to generate feedback for scientific papers and\ndevelop MARG, a feedback generation approach using multiple LLM instances that\nengage in internal discussion. By distributing paper text across agents, MARG\ncan consume the full text of papers beyond the input length limitations of the\nbase LLM, and by specializing agents and incorporating sub-tasks tailored to\ndifferent comment types (experiments, clarity, impact) it improves the\nhelpfulness and specificity of feedback. In a user study, baseline methods\nusing GPT-4 were rated as producing generic or very generic comments more than\nhalf the time, and only 1.7 comments per paper were rated as good overall in\nthe best baseline. Our system substantially improves the ability of GPT-4 to\ngenerate specific and helpful feedback, reducing the rate of generic comments\nfrom 60% to 29% and generating 3.7 good comments per paper (a 2.2x\nimprovement).\n",
        "title": "MARG: Multi-Agent Review Generation for Scientific Papers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04261",
        "abstract_url": "http://arxiv.org/abs/2401.04261",
        "authors": [
            {
                "last_name": "Langhammer",
                "first_name": "Martin"
            },
            {
                "last_name": "Constantinides",
                "first_name": "George A."
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Current soft processor architectures for FPGAs do not utilize the potential\nof the massive parallelism available. FPGAs now support many thousands of\nembedded floating point operators, and have similar computational densities to\nGPGPUs. Several soft GPGPU or SIMT processors have been published, but the\nreported large areas and modest Fmax makes their widespread use unlikely for\ncommercial designs. In this paper we take an alternative approach, building the\nsoft GPU microarchitecture around the FPGA resource mix available. We\ndemonstrate a statically scalable soft GPGPU processor (where both parameters\nand feature set can be determined at configuration time) that always closes\ntiming at the peak speed of the slowest embedded component in the FPGA (DSP or\nhard memory), with a completely unconstrained compile into a current Intel\nAgilex FPGA. We also show dynamic scalability, where a subset of the thread\nspace can be specified on an instruction-by-instruction basis.\n  For one example core type, we show a logic range -- depending on the\nconfiguration -- of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to\n250 M20K memories. All of these instances close timing at 771 MHz, a\nperformance level limited only by the DSP Blocks. We describe our methodology\nfor reliably achieving this clock rate by matching the processor pipeline\nstructure to the physical structure of the FPGA fabric. We also benchmark\nseveral algorithms across a range of data sizes, and compare to a commercial\nsoft RISC processor.\n",
        "title": "A Statically and Dynamically Scalable Soft GPGPU",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04264",
        "abstract_url": "http://arxiv.org/abs/2401.04264",
        "authors": [
            {
                "last_name": "Diamond",
                "first_name": "N'yoma"
            },
            {
                "last_name": "Murai",
                "first_name": "Fabricio"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "MA",
            "",
            ""
        ],
        "abstract": "  Many high-stakes decision-making problems, such as those found within\ncybersecurity and economics, can be modeled as competitive resource allocation\ngames. In these games, multiple players must allocate limited resources to\novercome their opponent(s), while minimizing any induced individual losses.\nHowever, existing means of assessing the performance of resource allocation\nalgorithms are highly disparate and problem-dependent. As a result, evaluating\nsuch algorithms is unreliable or impossible in many contexts and applications,\nespecially when considering differing levels of feedback. To resolve this\nproblem, we propose a generalized definition of payoff which uses an arbitrary\nuser-provided function. This unifies performance evaluation under all contexts\nand levels of feedback. Using this definition, we develop metrics for\nevaluating player performance, and estimators to approximate them under\nuncertainty (i.e., bandit or semi-bandit feedback). These metrics and their\nrespective estimators provide a problem-agnostic means to contextualize and\nevaluate algorithm performance. To validate the accuracy of our estimator, we\nexplore the Colonel Blotto ($\\mathcal{CB}$) game as an example. To this end, we\npropose a graph-pruning approach to efficiently identify feasible opponent\ndecisions, which are used in computing our estimation metrics. Using various\nresource allocation algorithms and game parameters, a suite of $\\mathcal{CB}$\ngames are simulated and used to compute and evaluate the quality of our\nestimates. These simulations empirically show our approach to be highly\naccurate at estimating the metrics associated with the unseen outcomes of an\nopponent's latent behavior.\n",
        "title": "General Performance Evaluation for Competitive Resource Allocation Games\n  via Unseen Payoff Estimation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04266",
        "abstract_url": "http://arxiv.org/abs/2401.04266",
        "authors": [
            {
                "last_name": "Rabbani",
                "first_name": "Shourav B."
            },
            {
                "last_name": "Medri",
                "first_name": "Ivan V."
            },
            {
                "last_name": "Samad",
                "first_name": "Manar D."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Despite groundbreaking success in image and text learning, deep learning has\nnot achieved significant improvements against traditional machine learning (ML)\nwhen it comes to tabular data. This performance gap underscores the need for\ndata-centric treatment and benchmarking of learning algorithms. Recently,\nattention and contrastive learning breakthroughs have shifted computer vision\nand natural language processing paradigms. However, the effectiveness of these\nadvanced deep models on tabular data is sparsely studied using a few data sets\nwith very large sample sizes, reporting mixed findings after benchmarking\nagainst a limited number of baselines. We argue that the heterogeneity of\ntabular data sets and selective baselines in the literature can bias the\nbenchmarking outcomes. This article extensively evaluates state-of-the-art\nattention and contrastive learning methods on a wide selection of 28 tabular\ndata sets (14 easy and 14 hard-to-classify) against traditional deep and\nmachine learning. Our data-centric benchmarking demonstrates when traditional\nML is preferred over deep learning and vice versa because no best learning\nmethod exists for all tabular data sets. Combining between-sample and\nbetween-feature attentions conquers the invincible traditional ML on tabular\ndata sets by a significant margin but fails on high dimensional data, where\ncontrastive learning takes a robust lead. While a hybrid attention-contrastive\nlearning strategy mostly wins on hard-to-classify data sets, traditional\nmethods are frequently superior on easy-to-classify data sets with presumably\nsimpler decision boundaries. To the best of our knowledge, this is the first\nbenchmarking paper with statistical analyses of attention and contrastive\nlearning performances on a diverse selection of tabular data sets against\ntraditional deep and machine learning baselines to facilitate further advances\nin this field.\n",
        "title": "Attention versus Contrastive Learning of Tabular Data -- A Data-centric\n  Benchmarking",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04268",
        "abstract_url": "http://arxiv.org/abs/2401.04268",
        "authors": [
            {
                "last_name": "Kutzke",
                "first_name": "Demetrious T."
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Gustavo E. Miranda"
            },
            {
                "last_name": "Herman",
                "first_name": "Robert J."
            },
            {
                "last_name": "Philippeaux",
                "first_name": "Harryel"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We introduce a launch device, called the remotely-enabled modular release\nmechanism, to augment rapid testing and prototyping of cooperative autonomy\nmaritime applications by facilitating autonomous deployment of an autonomous\nunderwater vehicle (AUV) from an autonomous surface vessel (ASV). While we\nfocus our development on a specific application of deploying an AUV from a\ncatamaran style ASV, the release mechanism can be adapted to different\ndeployable objects and towing vehicles, such as buoys and sensors for\noceanographic surveys or mono-hull ASVs. In this paper we explore a number of\nhardware and software design considerations to facilitate ease of integration\nwith existing maritime autonomy systems. We expound on bench tests and in-water\ntests used to explore the utility of the release system and diagnose system\nissues. Additionally, we make a first-principles argument, based on a\nhydrodynamics physics model, for assured deployment that is virtually\nindependent of sea state, making the release system a suitable alternative for\ndifferent maritime applications in varying environmental conditions.\n",
        "title": "Design and Development of a Remotely-enabled Modular Release Mechanism\n  for Autonomous Underwater Vehicles",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04280",
        "abstract_url": "http://arxiv.org/abs/2401.04280",
        "authors": [
            {
                "last_name": "Kandanaarachchi",
                "first_name": "Sevvandi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI",
            ""
        ],
        "abstract": "  Dynamic graph embeddings, inductive and incremental learning facilitate\npredictive tasks such as node classification and link prediction. However,\npredicting the structure of a graph at a future time step from a time series of\ngraphs, allowing for new nodes has not gained much attention. In this paper, we\npresent such an approach. We use time series methods to predict the node degree\nat future time points and combine it with flux balance analysis -- a linear\nprogramming method used in biochemistry -- to obtain the structure of future\ngraphs. Furthermore, we explore the predictive graph distribution for different\nparameter values. We evaluate this method using synthetic and real datasets and\ndemonstrate its utility and applicability.\n",
        "title": "Predicting the structure of dynamic graphs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04282",
        "abstract_url": "http://arxiv.org/abs/2401.04282",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Qinwu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study develops a graph search algorithm to find the optimal\ndiscrimination path for the binary classification problem. The objective\nfunction is defined as the difference of variations between the true positive\n(TP) and false positive (FP). It uses the depth first search (DFS) algorithm to\nfind the top-down paths for discrimination. It proposes a dynamic optimization\nprocedure to optimize TP at the upper levels and then reduce FP at the lower\nlevels. To accelerate computing speed with improving accuracy, it proposes a\nreduced histogram algorithm with variable bin size instead of looping over all\ndata points, to find the feature threshold of discrimination. The algorithm is\napplied on top of a Support Vector Machine (SVM) model for a binary\nclassification problem on whether a person is fit or unfit. It significantly\nimproves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a\nloss of only\\ 5% TP). The graph search auto-generates 39 ranked discrimination\npaths within 9 seconds on an input of total 328,464 objects, using a dual-core\nLaptop computer with a processor of 2.59 GHz.\n",
        "title": "A Fast Graph Search Algorithm with Dynamic Optimization and Reduced\n  Histogram for Discrimination of Binary Classification Problem",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04283",
        "abstract_url": "http://arxiv.org/abs/2401.04283",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Wan",
                "first_name": "Li"
            },
            {
                "last_name": "Li",
                "first_name": "Yun"
            },
            {
                "last_name": "Huang",
                "first_name": "Yiteng"
            },
            {
                "last_name": "Sun",
                "first_name": "Ming"
            },
            {
                "last_name": "Luan",
                "first_name": "James"
            },
            {
                "last_name": "Shi",
                "first_name": "Yangyang"
            },
            {
                "last_name": "Lei",
                "first_name": "Xin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Despite the potential of diffusion models in speech enhancement, their\ndeployment in Acoustic Echo Cancellation (AEC) has been restricted. In this\npaper, we propose DI-AEC, pioneering a diffusion-based stochastic regeneration\napproach dedicated to AEC. Further, we propose FADI-AEC, fast score-based\ndiffusion AEC framework to save computational demands, making it favorable for\nedge devices. It stands out by running the score model once per frame,\nachieving a significant surge in processing efficiency. Apart from that, we\nintroduce a novel noise generation technique where far-end signals are\nutilized, incorporating both far-end and near-end signals to refine the score\nmodel's accuracy. We test our proposed method on the ICASSP2023 Microsoft deep\necho cancellation challenge evaluation dataset, where our method outperforms\nsome of the end-to-end methods and other diffusion based echo cancellation\nmethods.\n",
        "title": "FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for\n  Acoustic Echo Cancellation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04285",
        "abstract_url": "http://arxiv.org/abs/2401.04285",
        "authors": [
            {
                "last_name": "Southworth",
                "first_name": "Ben S."
            },
            {
                "last_name": "Olivier",
                "first_name": "Samuel S."
            },
            {
                "last_name": "Park",
                "first_name": "HyeongKae"
            },
            {
                "last_name": "Buvoli",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Thermal radiation transport (TRT) is a time dependent, high dimensional\npartial integro-differential equation. In practical applications such as\ninertial confinement fusion, TRT is coupled to other physics such as\nhydrodynamics, plasmas, etc., and the timescales one is interested in capturing\nare often much slower than the radiation timescale. As a result, TRT is treated\nimplicitly, and due to its stiffness and high dimensionality, is often a\ndominant computational cost in multiphysics simulations. Here we develop a new\napproach for implicit-explicit (IMEX) integration of gray TRT in the\ndeterministic S$_N$ setting, which requires only one sweep per stage, with the\nsimplest first-order method requiring only one sweep per time step. The\npartitioning of equations is done via a moment-based high-order low-order\nformulation of TRT, where the streaming operator and first two moments are used\nto capture the asymptotic stiff regimes of the streaming limit and diffusion\nlimit. Absorption-reemission is treated explicitly, and although stiff, is\nsufficiently damped by the implicit solve that we achieve stable accurate time\nintegration without incorporating the coupling of the high order and low order\nequations implicitly. Due to nonlinear coupling of the high-order and low-order\nequations through temperature-dependent opacities, to facilitate IMEX\npartitioning and higher-order methods, we use a semi-implicit integration\napproach amenable to nonlinear partitions. Results are demonstrated on thick\nMarshak and crooked pipe benchmark problems, demonstrating orders of magnitude\nimprovement in accuracy and wallclock compared with the standard first-order\nimplicit integration typically used.\n",
        "title": "One-sweep moment-based semi-implicit-explicit integration for gray\n  thermal radiation transport",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04286",
        "abstract_url": "http://arxiv.org/abs/2401.04286",
        "authors": [
            {
                "last_name": "Ko",
                "first_name": "Hyunouk"
            },
            {
                "last_name": "Huo",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  In this paper, we first extend the result of FL93 and prove universal\nconsistency for a classification rule based on wide and deep ReLU neural\nnetworks trained on the logistic loss. Unlike the approach in FL93 that\ndecomposes the estimation and empirical error, we directly analyze the\nclassification risk based on the observation that a realization of a neural\nnetwork that is wide enough is capable of interpolating an arbitrary number of\npoints. Secondly, we give sufficient conditions for a class of probability\nmeasures under which classifiers based on neural networks achieve minimax\noptimal rates of convergence. Our result is motivated from the practitioner's\nobservation that neural networks are often trained to achieve 0 training error,\nwhich is the case for our proposed neural network classifiers. Our proofs hinge\non recent developments in empirical risk minimization and on approximation\nrates of deep ReLU neural networks for various function classes of interest.\nApplications to classical function spaces of smoothness illustrate the\nusefulness of our result.\n",
        "title": "Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax\n  Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04287",
        "abstract_url": "http://arxiv.org/abs/2401.04287",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Proksch",
                "first_name": "Sebastian"
            },
            {
                "last_name": "German",
                "first_name": "Daniel M."
            },
            {
                "last_name": "Godfrey",
                "first_name": "Michael W."
            },
            {
                "last_name": "Li",
                "first_name": "Li"
            },
            {
                "last_name": "McIntosh",
                "first_name": "Shane"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  \"App stores\" are online software stores where end users may browse, purchase,\ndownload, and install software applications. By far, the best known app stores\nare associated with mobile platforms, such as Google Play for Android and\nApple's App Store for iOS. The ubiquity of smartphones has led to mobile app\nstores becoming a touchstone experience of modern living. However, most of app\nstore research has concentrated on properties of the apps rather than the\nstores themselves. Today, there is a rich diversity of app stores and these\nstores have largely been overlooked by researchers: app stores exist on many\ndistinctive platforms, are aimed at different classes of users, and have\ndifferent end-goals beyond simply selling a standalone app to a smartphone\nuser.\n  We survey and characterize the broader dimensionality of app stores, and\nexplore how and why they influence software development practices, such as\nsystem design and release management. We begin by collecting a set of app store\nexamples from web search queries. By analyzing and curating the results, we\nderive a set of features common to app stores. We then build a dimensional\nmodel of app stores based on these features, and we fit each app store from our\nweb search result set into this model. Next, we performed unsupervised\nclustering to the app stores to find their natural groupings. Our results\nsuggest that app stores have become an essential stakeholder in modern software\ndevelopment. They control the distribution channel to end users and ensure that\nthe applications are of suitable quality; in turn, this leads to developers\nadhering to various store guidelines when creating their applications. However,\nwe found the app stores operational model could vary widely between stores, and\nthis variability could in turn affect the generalizability of existing\nunderstanding of app stores.\n",
        "title": "What Is an App Store? The Software Engineering Perspective",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04289",
        "abstract_url": "http://arxiv.org/abs/2401.04289",
        "authors": [
            {
                "last_name": "Wood",
                "first_name": "Kenan"
            },
            {
                "last_name": "Herlihy",
                "first_name": "Maurice"
            },
            {
                "last_name": "Mendes",
                "first_name": "Hammurabi"
            },
            {
                "last_name": "Pulaj",
                "first_name": "Jonad"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "DC",
            "",
            ""
        ],
        "abstract": "  An automated market maker (AMM) is a state machine that manages pools of\nassets, allowing parties to buy and sell those assets according to a fixed\nmathematical formula. AMMs are typically implemented as smart contracts on\nblockchains, and its prices are kept in line with the overall market price by\narbitrage: if the AMM undervalues an asset with respect to the market, an\n\"arbitrageur\" can make a risk-free profit by buying just enough of that asset\nto bring the AMM's price back in line with the market.\n  AMMs, however, are not designed for assets that expire: that is, assets that\ncannot be produced or resold after a specified date. As assets approach\nexpiration, arbitrage may not be able to reconcile supply and demand, and the\nliquidity providers that funded the AMM may have excessive exposure to risk due\nto rapid price variations.\n  This paper formally describes the design of a decentralized exchange (DEX)\nfor assets that expire, combining aspects of AMMs and limit-order books. We\nensure liveness and market clearance, providing mechanisms for liquidity\nproviders to control their exposure to risk and adjust prices dynamically in\nresponse to situations where arbitrage may fail.\n",
        "title": "Expiring Assets in Automated Market Makers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04290",
        "abstract_url": "http://arxiv.org/abs/2401.04290",
        "authors": [
            {
                "last_name": "Kulinski",
                "first_name": "Sean"
            },
            {
                "last_name": "Waytowich",
                "first_name": "Nicholas R."
            },
            {
                "last_name": "Hare",
                "first_name": "James Z."
            },
            {
                "last_name": "Inouye",
                "first_name": "David I."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "MA"
        ],
        "abstract": "  Spatial reasoning tasks in multi-agent environments such as event prediction,\nagent type identification, or missing data imputation are important for\nmultiple applications (e.g., autonomous surveillance over sensor networks and\nsubtasks for reinforcement learning (RL)). StarCraft II game replays encode\nintelligent (and adversarial) multi-agent behavior and could provide a testbed\nfor these tasks; however, extracting simple and standardized representations\nfor prototyping these tasks is laborious and hinders reproducibility. In\ncontrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled\nrapid prototyping and reproducibility of ML methods. Following the simplicity\nof these datasets, we construct a benchmark spatial reasoning dataset based on\nStarCraft II replays that exhibit complex multi-agent behaviors, while still\nbeing as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize\na window of 255 consecutive game states to create 3.6 million summary images\nfrom 60,000 replays, including all relevant metadata such as game outcome and\nplayer races. We develop three formats of decreasing complexity: Hyperspectral\nimages that include one channel for every unit type (similar to multispectral\ngeospatial images), RGB images that mimic CIFAR10, and grayscale images that\nmimic MNIST. We show how this dataset can be used for prototyping spatial\nreasoning methods. All datasets, code for extraction, and code for dataset\nloading can be found at https://starcraftdata.davidinouye.com\n",
        "title": "StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For\n  Multi-Agent Environments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04301",
        "abstract_url": "http://arxiv.org/abs/2401.04301",
        "authors": [
            {
                "last_name": "Dovonon",
                "first_name": "Gb\u00e8tondji J-S"
            },
            {
                "last_name": "Bronstein",
                "first_name": "Michael M."
            },
            {
                "last_name": "Kusner",
                "first_name": "Matt J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Transformer-based models have recently become wildly successful across a\ndiverse set of domains. At the same time, recent work has shown that\nTransformers are inherently low-pass filters that gradually oversmooth the\ninputs, reducing the expressivity of their representations. A natural question\nis: How can Transformers achieve these successes given this shortcoming? In\nthis work we show that in fact Transformers are not inherently low-pass\nfilters. Instead, whether Transformers oversmooth or not depends on the\neigenspectrum of their update equations. Our analysis extends prior work in\noversmoothing and in the closely-related phenomenon of rank collapse. We show\nthat many successful Transformer models have attention and weights which\nsatisfy conditions that avoid oversmoothing. Based on this analysis, we derive\na simple way to parameterize the weights of the Transformer update equations\nthat allows for control over its spectrum, ensuring that oversmoothing does not\noccur. Compared to a recent solution for oversmoothing, our approach improves\ngeneralization, even when training with more layers, fewer datapoints, and data\nthat is corrupted.\n",
        "title": "Setting the Record Straight on Transformer Oversmoothing",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04302",
        "abstract_url": "http://arxiv.org/abs/2401.04302",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Hang"
            },
            {
                "last_name": "Baloian",
                "first_name": "Artiom"
            },
            {
                "last_name": "Janak",
                "first_name": "Jan"
            },
            {
                "last_name": "Schulzrinne",
                "first_name": "Henning"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  eSIM(embedded SIM) is an advanced alternative to traditional physical SIM\ncards initially developed by the GSM Association(GSMA) in 2013 [1][2]. The eSIM\ntechnology has been deployed in many commercial products such as mobile\ndevices. However, the application of the eSIM technology in IoT devices has yet\nto start being primarily deployed. Understanding the eSIM architecture and the\nbasic ideas of the eSIM provisioning and operations is very important for\nengineers to promote eSIM technology deployment in more areas, both academics\nand industries.\n  The report focuses on the eSIM technology in the IoT architecture and two\nmajor operations of Remote SIM Provisioning(RSP) procedure: the Common Mutual\nAuthentication procedure, a process used to authenticate eSIM trusted\ncommunication parties over the public internet, and the Profile Downloading\nprocedure, the way to download the Profile from the operator SM-DP+ server and\neventually remotely provision the end-user devices.\n",
        "title": "eSIM Technology in IoT Architecture",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04305",
        "abstract_url": "http://arxiv.org/abs/2401.04305",
        "authors": [
            {
                "last_name": "Kirsch",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT"
        ],
        "abstract": "  At its core, this thesis aims to enhance the practicality of deep learning by\nimproving the label and training efficiency of deep learning models. To this\nend, we investigate data subset selection techniques, specifically active\nlearning and active sampling, grounded in information-theoretic principles.\nActive learning improves label efficiency, while active sampling enhances\ntraining efficiency. Supervised deep learning models often require extensive\ntraining with labeled data. Label acquisition can be expensive and\ntime-consuming, and training large models is resource-intensive, hindering the\nadoption outside academic research and ``big tech.'' Existing methods for data\nsubset selection in deep learning often rely on heuristics or lack a principled\ninformation-theoretic foundation. In contrast, this thesis examines several\nobjectives for data subset selection and their applications within deep\nlearning, striving for a more principled approach inspired by information\ntheory. We begin by disentangling epistemic and aleatoric uncertainty in single\nforward-pass deep neural networks, which provides helpful intuitions and\ninsights into different forms of uncertainty and their relevance for data\nsubset selection. We then propose and investigate various approaches for active\nlearning and data subset selection in (Bayesian) deep learning. Finally, we\nrelate various existing and proposed approaches to approximations of\ninformation quantities in weight or prediction space. Underpinning this work is\na principled and practical notation for information-theoretic quantities that\nincludes both random variables and observed outcomes. This thesis demonstrates\nthe benefits of working from a unified perspective and highlights the potential\nimpact of our contributions to the practical application of deep learning.\n",
        "title": "Advancing Deep Active Learning & Data Subset Selection: Unifying\n  Principles with Information-Theory Intuitions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04308",
        "abstract_url": "http://arxiv.org/abs/2401.04308",
        "authors": [
            {
                "last_name": "Nunes",
                "first_name": "Ivan De Oliveira"
            },
            {
                "last_name": "Jakkamsetti",
                "first_name": "Sashidhar"
            },
            {
                "last_name": "Rattanavipanon",
                "first_name": "Norrathep"
            },
            {
                "last_name": "Tsudik",
                "first_name": "Gene"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Lower-end IoT devices typically have strict cost constraints that rule out\nusual security mechanisms available in general-purpose computers or higher-end\ndevices. To secure low-end devices, various low-cost security architectures\nhave been proposed for remote verification of their software state via\nintegrity proofs. These proofs vary in terms of expressiveness, with simpler\nones confirming correct binary presence, while more expressive ones support\nverification of arbitrary code execution. This article provides a holistic and\nsystematic treatment of this family of architectures. It also compares\n(qualitatively and quantitatively) the types of software integrity proofs,\nrespective architectural support, and associated costs. Finally, we outline\nsome research directions and emerging challenges.\n",
        "title": "Towards Remotely Verifiable Software Integrity in Resource-Constrained\n  IoT Devices",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04311",
        "abstract_url": "http://arxiv.org/abs/2401.04311",
        "authors": [
            {
                "last_name": "Stemmer",
                "first_name": "Uri"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Private Everlasting Prediction (PEP), recently introduced by Naor et al.\n[2023], is a model for differentially private learning in which the learner\nnever publicly releases a hypothesis. Instead, it provides black-box access to\na \"prediction oracle\" that can predict the labels of an endless stream of\nunlabeled examples drawn from the underlying distribution. Importantly, PEP\nprovides privacy both for the initial training set and for the endless stream\nof classification queries. We present two conceptual modifications to the\ndefinition of PEP, as well as new constructions exhibiting significant\nimprovements over prior work. Specifically,\n  (1) Robustness: PEP only guarantees accuracy provided that all the\nclassification queries are drawn from the correct underlying distribution. A\nfew out-of-distribution queries might break the validity of the prediction\noracle for future queries, even for future queries which are sampled from the\ncorrect distribution. We incorporate robustness against such poisoning attacks\ninto the definition of PEP, and show how to obtain it.\n  (2) Dependence of the privacy parameter $\\delta$ in the time horizon: We\npresent a relaxed privacy definition, suitable for PEP, that allows us to\ndisconnect the privacy parameter $\\delta$ from the number of total time steps\n$T$. This allows us to obtain algorithms for PEP whose sample complexity is\nindependent from $T$, thereby making them \"truly everlasting\". This is in\ncontrast to prior work where the sample complexity grows with $polylog(T)$.\n  (3) New constructions: Prior constructions for PEP exhibit sample complexity\nthat is quadratic in the VC dimension of the target class. We present new\nconstructions of PEP for axis-aligned rectangles and for decision-stumps that\nexhibit sample complexity linear in the dimension (instead of quadratic). We\nshow that our constructions satisfy very strong robustness properties.\n",
        "title": "Private Truly-Everlasting Robust-Prediction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04312",
        "abstract_url": "http://arxiv.org/abs/2401.04312",
        "authors": [
            {
                "last_name": "Dong",
                "first_name": "Xue"
            },
            {
                "last_name": "Song",
                "first_name": "Xuemeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Tongliang"
            },
            {
                "last_name": "Guan",
                "first_name": "Weili"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Multi-interest learning method for sequential recommendation aims to predict\nthe next item according to user multi-faceted interests given the user\nhistorical interactions. Existing methods mainly consist of two modules: the\nmulti-interest extraction module that learns user multi-interest embeddings to\ncapture the user multi-interests, and the multi-interest weight prediction\nmodule that learns the weight of each interest for aggregating the learned\nmulti-interest embeddings to derive the user embedding, used for predicting the\nuser rating to an item. Despite their effectiveness, existing methods have two\nkey limitations: 1) they directly feed the user interactions into the two\nmodules, while ignoring their different learning objectives, and 2) they merely\nconsider the centrality of the user interactions to learn the user\nmulti-interests, while overlooking their dispersion. To tackle these\nlimitations, we propose a prompt-based multi-interest learning method (PoMRec),\nwhere specific prompts are inserted into user interactions to make them\nadaptive to different learning objectives of the two modules. Moreover, we\nutilize both the mean and variance embeddings of user interactions to derive\nthe user multi-interest embeddings for comprehensively model the user\nmulti-interests. We conduct extensive experiments on two public datasets, and\nthe results verify that our proposed PoMRec outperforms the state-of-the-art\nmulti-interest learning methods.\n",
        "title": "Prompt-based Multi-interest Learning Method for Sequential\n  Recommendation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04316",
        "abstract_url": "http://arxiv.org/abs/2401.04316",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Dai",
                "first_name": "Bo"
            },
            {
                "last_name": "Gu",
                "first_name": "Feng"
            },
            {
                "last_name": "Han",
                "first_name": "Jianda"
            },
            {
                "last_name": "Liu",
                "first_name": "Guangjun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Aerial manipulator, which is composed of an UAV (Unmanned Aerial Vehicle) and\na multi-link manipulator and can perform aerial manipulation, has shown great\npotential of applications. However, dynamic coupling between the UAV and the\nmanipulator makes it difficult to control the aerial manipulator with high\nperformance. In this paper, system modeling and control problem of the aerial\nmanipulator are studied. Firstly, an UAV dynamic model is proposed with\nconsideration of the dynamic coupling from an attached manipulator, which is\ntreated as disturbance for the UAV. In the dynamic model, the disturbance is\naffected by the variable inertia parameters of the aerial manipulator system.\nThen, based on the proposed dynamic model, a disturbance compensation robust\n$H_{\\infty}$ controller is designed to stabilize flight of the UAV while the\nmanipulator is in operation. Finally, experiments are conducted and the\nexperimental results demonstrate the feasibility and validity of the proposed\ncontrol scheme.\n",
        "title": "Robust Control of An Aerial Manipulator Based on A Variable Inertia\n  Parameters Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04317",
        "abstract_url": "http://arxiv.org/abs/2401.04317",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Jianyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bowen"
            },
            {
                "last_name": "Dubey",
                "first_name": "Amartansh"
            },
            {
                "last_name": "Murch",
                "first_name": "Ross"
            },
            {
                "last_name": "Jing",
                "first_name": "Liwen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Indoor imaging is a critical task for robotics and internet-of-things. WiFi\nas an omnipresent signal is a promising candidate for carrying out passive\nimaging and synchronizing the up-to-date information to all connected devices.\nThis is the first research work to consider WiFi indoor imaging as a\nmulti-modal image generation task that converts the measured WiFi power into a\nhigh-resolution indoor image. Our proposed WiFi-GEN network achieves a shape\nreconstruction accuracy that is 275% of that achieved by physical model-based\ninversion methods. Additionally, the Frechet Inception Distance score has been\nsignificantly reduced by 82%. To examine the effectiveness of models for this\ntask, the first large-scale dataset is released containing 80,000 pairs of WiFi\nsignal and imaging target. Our model absorbs challenges for the model-based\nmethods including the non-linearity, ill-posedness and non-certainty into\nmassive parameters of our generative AI network. The network is also designed\nto best fit measured WiFi signals and the desired imaging output. For\nreproducibility, we will release the data and code upon acceptance.\n",
        "title": "Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04318",
        "abstract_url": "http://arxiv.org/abs/2401.04318",
        "authors": [
            {
                "last_name": "Kawase",
                "first_name": "Yasushi"
            },
            {
                "last_name": "Roy",
                "first_name": "Bodhayan"
            },
            {
                "last_name": "Sanpui",
                "first_name": "Mohammad Azharuddin"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We study the problem of allocating indivisible items on a path among agents.\nThe objective is to find a fair and efficient allocation in which each agent's\nbundle forms a contiguous block on the line. We demonstrate that, even when the\nvaluations are binary additive, deciding whether every item can be allocated to\nan agent who wants it is NP-complete. Consequently, we provide two\nfixed-parameter tractable (FPT) algorithms for maximizing utilitarian social\nwelfare, with respect to the number of agents and the number of items.\nAdditionally, we present a 2-approximation algorithm for the special case when\nthe valuations are binary additive and the maximum utility is equal to the\nnumber of items. Furthermore, we establish that deciding whether the maximum\negalitarian social welfare is at least 2 or at most 1 is NP-complete, even when\nthe valuations are binary additive. We also explore the case where the order of\nthe blocks of items allocated to the agents is predetermined. In this case, we\nshow that both maximum utilitarian social welfare and egalitarian social\nwelfare can be computed in polynomial time. However, we determine that checking\nthe existence of an EF1 allocation is NP-complete, even when the valuations are\nbinary additive.\n",
        "title": "Contiguous Allocation of Indivisible Items on a Path",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04319",
        "abstract_url": "http://arxiv.org/abs/2401.04319",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Yang",
                "first_name": "Dan"
            },
            {
                "last_name": "Hu",
                "first_name": "Binbin"
            },
            {
                "last_name": "Shen",
                "first_name": "Yue"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziqi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wen"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinjie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhiqiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In this paper, we explore a new way for user targeting, where non-expert\nmarketers could select their target users solely given demands in natural\nlanguage form. The key to this issue is how to transform natural languages into\npractical structured logical languages, i.e., the structured understanding of\nmarketer demands. Considering the impressive natural language processing\nability of large language models (LLMs), we try to leverage LLMs to solve this\nissue. Past research indicates that the reasoning ability of LLMs can be\neffectively enhanced through chain-of-thought (CoT) prompting. But existing\nmethods still have some limitations: (1) Previous methods either use simple\n\"Let's think step by step\" spells or provide fixed examples in demonstrations\nwithout considering compatibility between prompts and questions, making LLMs\nineffective in some complex reasoning tasks such as structured language\ntransformation. (2) Previous methods are often implemented in closed-source\nmodels or excessively large models, which is not suitable in industrial\npractical scenarios. Based on these, we propose ARALLM (i.e., Analogical\nReasoning Augmented Large Language Models) consisting of two modules:\nAnalogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model\nDistillation.\n",
        "title": "Know Your Needs Better: Towards Structured Understanding of Marketer\n  Demands with Analogical Reasoning Augmented LLMs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04320",
        "abstract_url": "http://arxiv.org/abs/2401.04320",
        "authors": [
            {
                "last_name": "Kutzke",
                "first_name": "Demetrious T."
            },
            {
                "last_name": "Wariar",
                "first_name": "Ashwin"
            },
            {
                "last_name": "Sattar",
                "first_name": "Junaed"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The use of autonomous underwater vehicles (AUVs) to accomplish traditionally\nchallenging and dangerous tasks has proliferated thanks to advances in sensing,\nnavigation, manipulation, and on-board computing technologies. Utilizing AUVs\nin underwater human-robot interaction (UHRI) has witnessed comparatively\nsmaller levels of growth due to limitations in bi-directional communication and\nsignificant technical hurdles to bridge the gap between analogies with\nterrestrial interaction strategies and those that are possible in the\nunderwater domain. A necessary component to support UHRI is establishing a\nsystem for safe robotic-diver approach to establish face-to-face communication\nthat considers non-standard human body pose. In this work, we introduce a\nstereo vision system for enhancing UHRI that utilizes three-dimensional\nreconstruction from stereo image pairs and machine learning for localizing\nhuman joint estimates. We then establish a convention for a coordinate system\nthat encodes the direction the human is facing with respect to the camera\ncoordinate frame. This allows automatic setpoint computation that preserves\nhuman body scale and can be used as input to an image-based visual servo\ncontrol scheme. We show that our setpoint computations tend to agree both\nquantitatively and qualitatively with experimental setpoint baselines. The\nmethodology introduced shows promise for enhancing UHRI by improving robotic\nperception of human orientation underwater.\n",
        "title": "Autonomous robotic re-alignment for face-to-face underwater human-robot\n  interaction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04323",
        "abstract_url": "http://arxiv.org/abs/2401.04323",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Chenxing"
            },
            {
                "last_name": "Guo",
                "first_name": "Qingyue"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL",
            "IR",
            "",
            ""
        ],
        "abstract": "  This paper investigates differences in characteristics across publication\ntypes for aging-related genetic research. We utilized bibliometric data for\nfive model species retrieved from authoritative databases including PubMed.\nPublications are classified into types according to PubMed. Results indicate\nsubstantial divergence across publication types in attention paid to\naging-related research, scopes of studied genes, and topical preferences. For\ninstance, comparative studies and meta-analyses show a greater focus on aging\nthan validation studies. Reviews concentrate more on cell biology while\nclinical studies emphasize translational topics. Publication types also\nmanifest variations in highly studied genes, like APOE for reviews versus GH1\nfor clinical studies. Despite differences, top genes like insulin are\nuniversally emphasized. Publication types demonstrate similar levels of\nimbalance in research efforts to genes. Differences also exist in bibliometrics\nlike authorship numbers, citation counts, etc. Publication types show distinct\npreferences for journals of certain topical specialties and scope of\nreadership. Overall, findings showcase distinct characteristics of publication\ntypes in studying aging-related genetics, owing to their unique nature and\nobjectives. This study is the first endeavor to systematically depict the\ninherent structure of a biomedical research field from the perspective of\npublication types and provides insights into knowledge production and\nevaluation patterns across biomedical communities.\n",
        "title": "Divergent Characteristics of Biomedical Research across Publication\n  Types: A Quantitative Analysis on the Aging-related Research",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04325",
        "abstract_url": "http://arxiv.org/abs/2401.04325",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Han"
            },
            {
                "last_name": "Ma",
                "first_name": "Yukai"
            },
            {
                "last_name": "Gu",
                "first_name": "Yaqing"
            },
            {
                "last_name": "Hu",
                "first_name": "Kewei"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            },
            {
                "last_name": "Zuo",
                "first_name": "Xingxing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a novel approach for metric dense depth estimation based on the\nfusion of a single-view image and a sparse, noisy Radar point cloud. The direct\nfusion of heterogeneous Radar and image data, or their encodings, tends to\nyield dense depth maps with significant artifacts, blurred boundaries, and\nsuboptimal accuracy. To circumvent this issue, we learn to augment versatile\nand robust monocular depth prediction with the dense metric scale induced from\nsparse and noisy Radar data. We propose a Radar-Camera framework for highly\naccurate and fine-detailed dense depth estimation with four stages, including\nmonocular depth prediction, global scale alignment of monocular depth with\nsparse Radar points, quasi-dense scale estimation through learning the\nassociation between Radar points and image patches, and local scale refinement\nof dense depth using a scale map learner. Our proposed method significantly\noutperforms the state-of-the-art Radar-Camera depth estimation methods by\nreducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2%\non the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam\ndataset, respectively.\n",
        "title": "RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned\n  Metric Scale",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04328",
        "abstract_url": "http://arxiv.org/abs/2401.04328",
        "authors": [
            {
                "last_name": "Venn",
                "first_name": "Daniel R."
            },
            {
                "last_name": "Ruuth",
                "first_name": "Steven J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose and analyze a class of meshfree, super-algebraically convergent\nmethods for partial differential equations (PDEs) on surfaces using Fourier\nextensions minimizing a measure of non-smoothness (such as a Sobolev norm).\nCurrent spectral methods for surface PDEs are primarily limited to a small\nclass of surfaces, such as subdomains of spheres. Other high order methods for\nsurface PDEs typically use radial basis functions (RBFs). Many of these methods\nare not well-understood analytically for surface PDEs and are highly\nill-conditioned. Our methods work by extending a surface PDE into a box-shaped\ndomain so that differential operators of the extended function agree with the\nsurface differential operators, as in the Closest Point Method. The methods can\nbe proven to converge super-algebraically for certain well-posed linear PDEs,\nand spectral convergence to machine error has been observed numerically for a\nvariety of problems. Our approach works on arbitrary smooth surfaces (closed or\nnon-closed) defined by point clouds with minimal conditions.\n",
        "title": "Underdetermined Fourier Extensions for Partial Differential Equations on\n  Surfaces",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04330",
        "abstract_url": "http://arxiv.org/abs/2401.04330",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Yonghui"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaolong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yishu"
            },
            {
                "last_name": "Ai",
                "first_name": "Jinquan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The purpose of remote sensing image change detection (RSCD) is to detect\ndifferences between bi-temporal images taken at the same place. Deep learning\nhas been extensively used to RSCD tasks, yielding significant results in terms\nof result recognition. However, due to the shooting angle of the satellite, the\nimpacts of thin clouds, and certain lighting conditions, the problem of fuzzy\nedges in the change region in some remote sensing photographs cannot be\nproperly handled using current RSCD algorithms. To solve this issue, we\nproposed a Body Decouple Multi-Scale by fearure Aggregation change detection\n(BD-MSA), a novel model that collects both global and local feature map\ninformation in the channel and space dimensions of the feature map during the\ntraining and prediction phases. This approach allows us to successfully extract\nthe change region's boundary information while also divorcing the change\nregion's main body from its boundary. Numerous studies have shown that the\nassessment metrics and evaluation effects of the model described in this paper\non the publicly available datasets DSIFN-CD and S2Looking are the best when\ncompared to other models.\n",
        "title": "BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method\n  guided by multi-scale feature information aggregation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04331",
        "abstract_url": "http://arxiv.org/abs/2401.04331",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Qiyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Kai"
            },
            {
                "last_name": "Song",
                "first_name": "Yang"
            },
            {
                "last_name": "Xie",
                "first_name": "Yihang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yanan"
            },
            {
                "last_name": "Wang",
                "first_name": "Sijie"
            },
            {
                "last_name": "She",
                "first_name": "Rui"
            },
            {
                "last_name": "Tay",
                "first_name": "Wee Peng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In this work, we rigorously investigate the robustness of graph neural\nfractional-order differential equation (FDE) models. This framework extends\nbeyond traditional graph neural (integer-order) ordinary differential equation\n(ODE) models by implementing the time-fractional Caputo derivative. Utilizing\nfractional calculus allows our model to consider long-term memory during the\nfeature updating process, diverging from the memoryless Markovian updates seen\nin traditional graph neural ODE models. The superiority of graph neural FDE\nmodels over graph neural ODE models has been established in environments free\nfrom attacks or perturbations. While traditional graph neural ODE models have\nbeen verified to possess a degree of stability and resilience in the presence\nof adversarial attacks in existing literature, the robustness of graph neural\nFDE models, especially under adversarial conditions, remains largely\nunexplored. This paper undertakes a detailed assessment of the robustness of\ngraph neural FDE models. We establish a theoretical foundation outlining the\nrobustness characteristics of graph neural FDE models, highlighting that they\nmaintain more stringent output perturbation bounds in the face of input and\ngraph topology disturbances, compared to their integer-order counterparts. Our\nempirical evaluations further confirm the enhanced robustness of graph neural\nFDE models, highlighting their potential in adversarially robust applications.\n",
        "title": "Coupling Graph Neural Networks with Fractional Order Continuous\n  Dynamics: A Robustness Study",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04332",
        "abstract_url": "http://arxiv.org/abs/2401.04332",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Hou",
                "first_name": "Bingzhe"
            },
            {
                "last_name": "Wu",
                "first_name": "Tieru"
            },
            {
                "last_name": "Xin",
                "first_name": "Yue"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Two important problems in the field of Topological Data Analysis are defining\npractical multifiltrations on objects and showing ability of TDA to detect the\ngeometry. Motivated by the problems, we constuct three multifiltrations named\nmulti-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the\ninterleaving distance and multiparameter persistence landscape of multi-GENEO\nwith respect to the pseudometric of the subspace of bounded functions. We also\ngive the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we\nprovide experiment results on MNIST dataset to demonstrate our bifiltrations\nhave ability to detect geometric and topological differences of digital images.\n",
        "title": "Mix-GENEO: A flexible filtration for multiparameter persistent homology\n  detects digital images",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04334",
        "abstract_url": "http://arxiv.org/abs/2401.04334",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Wu",
                "first_name": "Zihao"
            },
            {
                "last_name": "Li",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Hanqi"
            },
            {
                "last_name": "Shu",
                "first_name": "Peng"
            },
            {
                "last_name": "Shi",
                "first_name": "Enze"
            },
            {
                "last_name": "Hu",
                "first_name": "Huawen"
            },
            {
                "last_name": "Ma",
                "first_name": "Chong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuhui"
            },
            {
                "last_name": "Yao",
                "first_name": "Yincheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Xuan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Huaqin"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengliang"
            },
            {
                "last_name": "Dai",
                "first_name": "Haixing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Lin"
            },
            {
                "last_name": "Ge",
                "first_name": "Bao"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shu"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Large language models (LLMs) have undergone significant expansion and have\nbeen increasingly integrated across various domains. Notably, in the realm of\nrobot task planning, LLMs harness their advanced reasoning and language\ncomprehension capabilities to formulate precise and efficient action plans\nbased on natural language instructions. However, for embodied tasks, where\nrobots interact with complex environments, text-only LLMs often face challenges\ndue to a lack of compatibility with robotic visual perception. This study\nprovides a comprehensive overview of the emerging integration of LLMs and\nmultimodal LLMs into various robotic tasks. Additionally, we propose a\nframework that utilizes multimodal GPT-4V to enhance embodied task planning\nthrough the combination of natural language instructions and robot visual\nperceptions. Our results, based on diverse datasets, indicate that GPT-4V\neffectively enhances robot performance in embodied tasks. This extensive survey\nand evaluation of LLMs and multimodal LLMs across a variety of robotic tasks\nenriches the understanding of LLM-centric embodied intelligence and provides\nforward-looking insights toward bridging the gap in Human-Robot-Environment\ninteraction.\n",
        "title": "Large Language Models for Robotics: Opportunities, Challenges, and\n  Perspectives",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04338",
        "abstract_url": "http://arxiv.org/abs/2401.04338",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Youshao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shangchun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhenglei"
            },
            {
                "last_name": "Huan",
                "first_name": "Zhaoxin"
            },
            {
                "last_name": "Ju",
                "first_name": "Lin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaolu"
            },
            {
                "last_name": "Wang",
                "first_name": "Lin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC",
            "IR"
        ],
        "abstract": "  Recently, a new paradigm, meta learning, has been widely applied to Deep\nLearning Recommendation Models (DLRM) and significantly improves statistical\nperformance, especially in cold-start scenarios. However, the existing systems\nare not tailored for meta learning based DLRM models and have critical problems\nregarding efficiency in distributed training in the GPU cluster. It is because\nthe conventional deep learning pipeline is not optimized for two task-specific\ndatasets and two update loops in meta learning. This paper provides a\nhigh-performance framework for large-scale training for Optimization-based Meta\nDLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta. Firstly,\nG-Meta utilizes both data parallelism and model parallelism with careful\norchestration regarding computation and communication efficiency, to enable\nhigh-speed distributed training. Secondly, it proposes a Meta-IO pipeline for\nefficient data ingestion to alleviate the I/O bottleneck. Various experimental\nresults show that G-Meta achieves notable training speed without loss of\nstatistical performance. Since early 2022, G-Meta has been deployed in Alipay's\ncore advertising and recommender system, shrinking the continuous delivery of\nmodels by four times. It also obtains 6.48\\% improvement in Conversion Rate\n(CVR) and 1.06\\% increase in CPM (Cost Per Mille) in Alipay's homepage display\nadvertising, with the benefit of larger training samples and tasks.\n",
        "title": "G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale\n  Recommender Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04339",
        "abstract_url": "http://arxiv.org/abs/2401.04339",
        "authors": [
            {
                "last_name": "Ryu",
                "first_name": "Hyogon"
            },
            {
                "last_name": "Lim",
                "first_name": "Seohyun"
            },
            {
                "last_name": "Shim",
                "first_name": "Hyunjung"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The rise of billion-parameter diffusion models like Stable Diffusion XL,\nImagen, and Dall-E3 markedly advances the field of generative AI. However,\ntheir large-scale nature poses challenges in fine-tuning and deployment due to\nhigh resource demands and slow inference speed. This paper ventures into the\nrelatively unexplored yet promising realm of fine-tuning quantized diffusion\nmodels. We establish a strong baseline by customizing three models: PEQA for\nfine-tuning quantization parameters, Q-Diffusion for post-training\nquantization, and DreamBooth for personalization. Our analysis reveals a\nnotable trade-off between subject and prompt fidelity within the baseline\nmodel. To address these issues, we introduce two strategies, inspired by the\ndistinct roles of different timesteps in diffusion models: S1 optimizing a\nsingle set of fine-tuning parameters exclusively at selected intervals, and S2\ncreating multiple fine-tuning parameter sets, each specialized for different\ntimestep intervals. Our approach not only enhances personalization but also\nupholds prompt fidelity and image quality, significantly outperforming the\nbaseline qualitatively and quantitatively. The code will be made publicly\navailable.\n",
        "title": "Memory-Efficient Personalization using Quantized Diffusion Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04340",
        "abstract_url": "http://arxiv.org/abs/2401.04340",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jianyi"
            },
            {
                "last_name": "Li",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Islam",
                "first_name": "Mohammad Jaminur"
            },
            {
                "last_name": "Ren",
                "first_name": "Shaolei"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "PF"
        ],
        "abstract": "  This paper studies online resource allocation with replenishable budgets,\nwhere budgets can be replenished on top of the initial budget and an agent\nsequentially chooses online allocation decisions without violating the\navailable budget constraint at each round. We propose a novel online algorithm,\ncalled OACP (Opportunistic Allocation with Conservative Pricing), that\nconservatively adjusts dual variables while opportunistically utilizing\navailable resources. OACP achieves a bounded asymptotic competitive ratio in\nadversarial settings as the number of decision rounds T gets large.\nImportantly, the asymptotic competitive ratio of OACP is optimal in the absence\nof additional assumptions on budget replenishment. To further improve the\ncompetitive ratio, we make a mild assumption that there is budget replenishment\nevery T^* >= 1 decision rounds and propose OACP+ to dynamically adjust the\ntotal budget assignment for online allocation. Next, we move beyond the\nworst-case and propose LA-OACP (Learning-Augmented OACP/OACP+), a novel\nlearning-augmented algorithm for online allocation with replenishable budgets.\nWe prove that LA-OACP can improve the average utility compared to OACP/OACP+\nwhen the ML predictor is properly trained, while still offering worst-case\nutility guarantees when the ML predictions are arbitrarily wrong. Finally, we\nrun simulation studies of sustainable AI inference powered by renewables,\nvalidating our analysis and demonstrating the empirical benefits of LA-OACP.\n",
        "title": "Online Allocation with Replenishable Budgets: Worst Case and Beyond",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04343",
        "abstract_url": "http://arxiv.org/abs/2401.04343",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Panda",
                "first_name": "Ashwinee"
            },
            {
                "last_name": "Nasr",
                "first_name": "Milad"
            },
            {
                "last_name": "Mahloujifar",
                "first_name": "Saeed"
            },
            {
                "last_name": "Mittal",
                "first_name": "Prateek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "CR"
        ],
        "abstract": "  Fine-tuning large pretrained models on private datasets may run the risk of\nviolating privacy. Differential privacy is a framework for mitigating privacy\nrisks by enforcing algorithmic stability. DP-SGD enables training models with\nprivate data in a privacy-preserving manner, but raises new obstacles in the\nform of performance loss and significant engineering challenges. We introduce\nDP-ZO, a new method for fine-tuning large language models that preserves the\nprivacy of training data by privatizing zeroth-order optimization. A key\ninsight into the design of our method is that the direction of the gradient in\nSPSA, the zeroth-order algorithm we use, is always random and the only\ninformation that depends on private data is the step size, i.e., a scalar.\nTherefore, we only need to privatize the scalar step size, which is\nmemory-efficient. DP-ZO, which can be instantiated with either Laplace or\nGaussian noise, provides a strong privacy-utility trade-off across different\ntasks, and model sizes, under conservative privacy budgets. One noteworthy\nresult is that DP-ZO exhibits just $1.86\\%$ performance degradation due to\nprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples\nfrom SQuAD.\n",
        "title": "Private Fine-tuning of Large Language Models with Zeroth-order\n  Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04345",
        "abstract_url": "http://arxiv.org/abs/2401.04345",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Hualie"
            },
            {
                "last_name": "Xu",
                "first_name": "Rui"
            },
            {
                "last_name": "Tan",
                "first_name": "Minglang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Wenjie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Omnidirectional stereo matching (OSM) is an essential and reliable means for\n$360^{\\circ}$ depth sensing. However, following earlier works on conventional\nstereo matching, prior state-of-the-art (SOTA) methods rely on a 3D\nencoder-decoder block to regularize the cost volume, causing the whole system\ncomplicated and sub-optimal results. Recently, the Recurrent All-pairs Field\nTransforms (RAFT) based approach employs the recurrent update in 2D and has\nefficiently improved image-matching tasks, \\ie, optical flow, and stereo\nmatching. To bridge the gap between OSM and RAFT, we mainly propose an opposite\nadaptive weighting scheme to seamlessly transform the outputs of spherical\nsweeping of OSM into the required inputs for the recurrent update, thus\ncreating a recurrent omnidirectional stereo matching (RomniStereo) algorithm.\nFurthermore, we introduce two techniques, \\ie, grid embedding and adaptive\ncontext feature generation, which also contribute to RomniStereo's performance.\nOur best model improves the average MAE metric by 40.7\\% over the previous SOTA\nbaseline across five datasets. When visualizing the results, our models\ndemonstrate clear advantages on both synthetic and realistic examples. The code\nis available at \\url{https://github.com/HalleyJiang/RomniStereo}.\n",
        "title": "RomniStereo: Recurrent Omnidirectional Stereo Matching",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04348",
        "abstract_url": "http://arxiv.org/abs/2401.04348",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Khoi M."
            },
            {
                "last_name": "Pham",
                "first_name": "Trinh"
            },
            {
                "last_name": "Quan",
                "first_name": "Tho"
            },
            {
                "last_name": "Luu",
                "first_name": "Anh Tuan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Paraphrases are texts that convey the same meaning while using different\nwords or sentence structures. It can be used as an automatic data augmentation\ntool for many Natural Language Processing tasks, especially when dealing with\nlow-resource languages, where data shortage is a significant problem. To\ngenerate a paraphrase in multilingual settings, previous studies have leveraged\nthe knowledge from the machine translation field, i.e., forming a paraphrase\nthrough zero-shot machine translation in the same language. Despite good\nperformance on human evaluation, those methods still require parallel\ntranslation datasets, thus making them inapplicable to languages that do not\nhave parallel corpora. To mitigate that problem, we proposed the first\nunsupervised multilingual paraphrasing model, LAMPAT ($\\textbf{L}$ow-rank\n$\\textbf{A}$daptation for $\\textbf{M}$ultilingual $\\textbf{P}$araphrasing using\n$\\textbf{A}$dversarial $\\textbf{T}$raining), by which monolingual dataset is\nsufficient enough to generate a human-like and diverse sentence. Throughout the\nexperiments, we found out that our method not only works well for English but\ncan generalize on unseen languages as well. Data and code are available at\nhttps://github.com/phkhanhtrinh23/LAMPAT.\n",
        "title": "LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using\n  Adversarial Training",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04349",
        "abstract_url": "http://arxiv.org/abs/2401.04349",
        "authors": [
            {
                "last_name": "Ferguson",
                "first_name": "Ethan"
            },
            {
                "last_name": "Wilson",
                "first_name": "Adam"
            },
            {
                "last_name": "Naghibijouybari",
                "first_name": "Hoda"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "AR"
        ],
        "abstract": "  Microarchitectural attacks on CPU structures have been studied in native\napplications, as well as in web browsers. These attacks continue to be a\nsubstantial threat to computing systems at all scales.\n  With the proliferation of heterogeneous systems and integration of hardware\naccelerators in every computing system, modern web browsers provide the support\nof GPU-based acceleration for the graphics and rendering processes. Emerging\nweb standards also support the GPU acceleration of general-purpose computation\nwithin web browsers.\n  In this paper, we present a new attack vector for microarchitectural attacks\nin web browsers. We use emerging GPU accelerating APIs in modern browsers\n(specifically WebGPU) to launch a GPU-based cache side channel attack on the\ncompute stack of the GPU that spies on victim activities on the graphics\n(rendering) stack of the GPU. Unlike prior works that rely on JavaScript APIs\nor software interfaces to build timing primitives, we build the timer using GPU\nhardware resources and develop a cache side channel attack on Intel's\nintegrated GPUs. We leverage the GPU's inherent parallelism at different levels\nto develop high-resolution parallel attacks. We demonstrate that GPU-based\ncache attacks can achieve a precision of 90 for website fingerprinting of 100\ntop websites. We also discuss potential countermeasures against the proposed\nattack to secure the systems at a critical time when these web standards are\nbeing developed and before they are widely deployed.\n",
        "title": "WebGPU-SPY: Finding Fingerprints in the Sandbox through GPU Cache\n  Attacks",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04350",
        "abstract_url": "http://arxiv.org/abs/2401.04350",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Sibo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Shan",
                "first_name": "Shiguang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Large-scale pre-trained vision-language models like CLIP have demonstrated\nimpressive performance across various tasks, and exhibit remarkable zero-shot\ngeneralization capability, while they are also vulnerable to imperceptible\nadversarial examples. Existing works typically employ adversarial training\n(fine-tuning) as a defense method against adversarial examples. However, direct\napplication to the CLIP model may result in overfitting, compromising the\nmodel's capacity for generalization. In this paper, we propose Pre-trained\nModel Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages\nsupervision from the original pre-trained model by carefully designing an\nauxiliary branch, to enhance the model's zero-shot adversarial robustness.\nSpecifically, PMG-AFT minimizes the distance between the features of\nadversarial examples in the target model and those in the pre-trained model,\naiming to preserve the generalization features already captured by the\npre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate\nthat PMG-AFT significantly outperforms the state-of-the-art method, improving\nthe top-1 robust accuracy by an average of 4.99%. Furthermore, our approach\nconsistently improves clean accuracy by an average of 8.72%.\n",
        "title": "Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial\n  Robustness",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04351",
        "abstract_url": "http://arxiv.org/abs/2401.04351",
        "authors": [
            {
                "last_name": "Arunan",
                "first_name": "Anushiya"
            },
            {
                "last_name": "Qin",
                "first_name": "Yan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaoli"
            },
            {
                "last_name": "Yuen",
                "first_name": "Chau"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  By informing the onset of the degradation process, health status evaluation\nserves as a significant preliminary step for reliable remaining useful life\n(RUL) estimation of complex equipment. This paper proposes a novel temporal\ndynamics learning-based model for detecting change points of individual\ndevices, even under variable operating conditions, and utilises the learnt\nchange points to improve the RUL estimation accuracy. During offline model\ndevelopment, the multivariate sensor data are decomposed to learn fused\ntemporal correlation features that are generalisable and representative of\nnormal operation dynamics across multiple operating conditions. Monitoring\nstatistics and control limit thresholds for normal behaviour are dynamically\nconstructed from these learnt temporal features for the unsupervised detection\nof device-level change points. The detected change points then inform the\ndegradation data labelling for training a long short-term memory (LSTM)-based\nRUL estimation model. During online monitoring, the temporal correlation\ndynamics of a query device is monitored for breach of the control limit derived\nin offline training. If a change point is detected, the device's RUL is\nestimated with the well-trained offline model for early preventive action.\nUsing C-MAPSS turbofan engines as the case study, the proposed method improved\nthe accuracy by 5.6\\% and 7.5\\% for two scenarios with six operating\nconditions, when compared to existing LSTM-based RUL estimation models that do\nnot consider heterogeneous change points.\n",
        "title": "A Change Point Detection Integrated Remaining Useful Life Estimation\n  Model under Variable Operating Conditions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04354",
        "abstract_url": "http://arxiv.org/abs/2401.04354",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Xuzheng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Chen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Gan",
                "first_name": "Tian"
            },
            {
                "last_name": "Chao",
                "first_name": "Linlin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jianan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yuan"
            },
            {
                "last_name": "Guo",
                "first_name": "Qingpei"
            },
            {
                "last_name": "Chu",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n",
        "title": "Knowledge-enhanced Multi-perspective Video Representation Learning for\n  Scene Recognition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04357",
        "abstract_url": "http://arxiv.org/abs/2401.04357",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Yifan"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyu"
            },
            {
                "last_name": "Li",
                "first_name": "Shiqi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jihua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  As a fundamental problem in computer vision, point cloud registration aims to\nseek the optimal transformation for aligning a pair of point clouds. In most\nexisting methods, the information flows are usually forward transferring, thus\nlacking the guidance from high-level information to low-level information.\nBesides, excessive high-level information may be overly redundant, and directly\nusing it may conflict with the original low-level information. In this paper,\nwe propose a novel Iterative Feedback Network (IFNet) for unsupervised point\ncloud registration, in which the representation of low-level features is\nefficiently enriched by rerouting subsequent high-level features. Specifically,\nour IFNet is built upon a series of Feedback Registration Block (FRB) modules,\nwith each module responsible for generating the feedforward rigid\ntransformation and feedback high-level features. These FRB modules are cascaded\nand recurrently unfolded over time. Further, the Feedback Transformer is\ndesigned to efficiently select relevant information from feedback high-level\nfeatures, which is utilized to refine the low-level features. What's more, we\nincorporate a geometry-awareness descriptor to empower the network for making\nfull use of most geometric information, which leads to more precise\nregistration results. Extensive experiments on various benchmark datasets\ndemonstrate the superior registration performance of our IFNet.\n",
        "title": "Iterative Feedback Network for Unsupervised Point Cloud Registration",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04358",
        "abstract_url": "http://arxiv.org/abs/2401.04358",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yun"
            },
            {
                "last_name": "Ji",
                "first_name": "Fei"
            },
            {
                "last_name": "Wen",
                "first_name": "Miaowen"
            },
            {
                "last_name": "Qing",
                "first_name": "Hua"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "",
            ""
        ],
        "abstract": "  As a new candidate waveform for the next generation wireless communications,\northogonal chirp division multiplexing (OCDM) has attracted growing attention\nfor its ability to achieve full diversity in uncoded transmission, and its\nrobustness to narrow-band interference or impulsive noise. Under high mobility\nchannels with multiple lags and multiple Doppler-shifts (MLMD), the signal\nsuffers doubly selective (DS) fadings in time and frequency domain, and data\nsymbols modulated on orthogonal chirps are interfered by each other. To address\nthe problem of symbol detection of OCDM over MLMD channel, under the assumption\nthat path attenuation factors, delays, and Doppler shifts of the channel are\navailable, we first derive the closed-form channel matrix in Fresnel domain,\nand then propose a low-complexity method to approximate it as a sparse matrix.\nBased on the approximated Fresnel-domain channel, we propose a message passing\n(MP) based detector to estimate the transmit symbols iteratively. Finally,\nunder two MLMD channels (an underspread channel for terrestrial vehicular\ncommunication, and an overspread channel for narrow-band underwater acoustic\ncommunications), Monte Carlo simulation results and analysis are provided to\nvalidate its advantages as a promising detector for OCDM.\n",
        "title": "Message-Passing Receiver for OCDM over Multi-Lag Multi-Doppler Channels",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04360",
        "abstract_url": "http://arxiv.org/abs/2401.04360",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shixin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we study a class of special linear codes involving their\nparameters, weight distributions, and self-orthogonal properties. On one hand,\nwe prove that such codes must be maximum distance separable (MDS) or near MDS\n(NMDS) codes and completely determine their weight distributions with the help\nof the solutions to some subset sum problems. Based on the well-known Schur\nmethod, we also show that such codes are non-equivalent to generalized\nReed-Solomon codes. On the other hand, a sufficient and necessary condition for\nsuch codes to be self-orthogonal is characterized. Based on this condition, we\nfurther deduce that there are no self-dual codes in this class of linear codes\nand explicitly construct two classes of almost self-dual codes.\n",
        "title": "New non-GRS type MDS codes and NMDS codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04361",
        "abstract_url": "http://arxiv.org/abs/2401.04361",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaan"
            },
            {
                "last_name": "Qu",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Kexin"
            },
            {
                "last_name": "Li",
                "first_name": "Zhixu"
            },
            {
                "last_name": "Hua",
                "first_name": "Wen"
            },
            {
                "last_name": "Li",
                "first_name": "Ximing"
            },
            {
                "last_name": "Liu",
                "first_name": "An"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Knowledge-grounded dialogue (KGD) learns to generate an informative response\nbased on a given dialogue context and external knowledge (\\emph{e.g.},\nknowledge graphs; KGs). Recently, the emergence of large language models (LLMs)\nand pre-training techniques has brought great success to knowledge-grounded\ndialogue. However, when building KGD systems in real applications, there are\nvarious real-world noises that are inevitable to face. For example, the\ndialogue context might involve perturbations such as misspellings and\nabbreviations. In addition, KGs typically suffer from incompletion and also\nmight contain erroneous and outdated facts. Such real-world noises pose a\nchallenge to the robustness of KGD systems and hinder their applications in the\nreal world. In this paper, we propose an entity-based contrastive learning\nframework for improving the robustness of KGD. Specifically, we make use of the\nentity information in a KGD sample to create both its positive and negative\nsamples which involve semantic-irrelevant and semantic-relevant perturbations,\nrespectively. The contrastive learning framework ensures the KGD model is aware\nof these two types of perturbations, thus generating informative responses with\nthe potentially noisy inputs in real applications. Experimental results on\nthree benchmark datasets show that our method achieves new state-of-the-art\nperformance in terms of automatic evaluation scores, verifying its\neffectiveness and potentiality. Furthermore, we show that our method can\ngenerate better responses than comparison models in both the noisy and the\nfew-shot settings.\n",
        "title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive\n  Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04362",
        "abstract_url": "http://arxiv.org/abs/2401.04362",
        "authors": [
            {
                "last_name": "Yun",
                "first_name": "Kwan"
            },
            {
                "last_name": "Kim",
                "first_name": "Youngseo"
            },
            {
                "last_name": "Seo",
                "first_name": "Kwanggyoon"
            },
            {
                "last_name": "Seo",
                "first_name": "Chang Wook"
            },
            {
                "last_name": "Noh",
                "first_name": "Junyong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "GR",
            "",
            ""
        ],
        "abstract": "  We introduce DiffSketch, a method for generating a variety of stylized\nsketches from images. Our approach focuses on selecting representative features\nfrom the rich semantics of deep features within a pretrained diffusion model.\nThis novel sketch generation method can be trained with one manual drawing.\nFurthermore, efficient sketch extraction is ensured by distilling a trained\ngenerator into a streamlined extractor. We select denoising diffusion features\nthrough analysis and integrate these selected features with VAE features to\nproduce sketches. Additionally, we propose a sampling scheme for training\nmodels using a conditional generative approach. Through a series of\ncomparisons, we verify that distilled DiffSketch not only outperforms existing\nstate-of-the-art sketch extraction methods but also surpasses diffusion-based\nstylization methods in the task of extracting sketches.\n",
        "title": "Representative Feature Extraction During Diffusion Process for Sketch\n  Extraction with One Example",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04364",
        "abstract_url": "http://arxiv.org/abs/2401.04364",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Binh M."
            },
            {
                "last_name": "Kim",
                "first_name": "Jiwon"
            },
            {
                "last_name": "Tariq",
                "first_name": "Shahroz"
            },
            {
                "last_name": "Moore",
                "first_name": "Kristen"
            },
            {
                "last_name": "Abuadbba",
                "first_name": "Alsharif"
            },
            {
                "last_name": "Woo",
                "first_name": "Simon S."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR",
            "LG"
        ],
        "abstract": "  Deepfakes have rapidly emerged as a profound and serious threat to society,\nprimarily due to their ease of creation and dissemination. This situation has\ntriggered an accelerated development of deepfake detection technologies.\nHowever, many existing detectors rely heavily on lab-generated datasets for\nvalidation, which may not effectively prepare them for novel, emerging, and\nreal-world deepfake techniques. In this paper, we conduct an extensive and\ncomprehensive review and analysis of the latest state-of-the-art deepfake\ndetectors, evaluating them against several critical criteria. These criteria\nfacilitate the categorization of these detectors into 4 high-level groups and\n13 fine-grained sub-groups, all aligned with a unified standard conceptual\nframework. This classification and framework offer deep and practical insights\ninto the factors that affect detector efficacy. We assess the generalizability\nof 16 leading detectors across various standard attack scenarios, including\nblack-box, white-box, and gray-box settings. Our systematized analysis and\nexperimentation lay the groundwork for a deeper understanding of deepfake\ndetectors and their generalizability, paving the way for future research\nfocused on creating detectors adept at countering various attack scenarios.\nAdditionally, this work offers insights for developing more proactive defenses\nagainst deepfakes.\n",
        "title": "SoK: Facial Deepfake Detectors",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04367",
        "abstract_url": "http://arxiv.org/abs/2401.04367",
        "authors": [
            {
                "last_name": "Murray",
                "first_name": "Curtis"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Lewis"
            },
            {
                "last_name": "Tuke",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Mackay",
                "first_name": "Mark"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This study introduces a novel methodology for modelling patient emotions from\nonline patient experience narratives. We employed metadata network topic\nmodelling to analyse patient-reported experiences from Care Opinion, revealing\nkey emotional themes linked to patient-caregiver interactions and clinical\noutcomes. We develop a probabilistic, context-specific emotion recommender\nsystem capable of predicting both multilabel emotions and binary sentiments\nusing a naive Bayes classifier using contextually meaningful topics as\npredictors. The superior performance of our predicted emotions under this model\ncompared to baseline models was assessed using the information retrieval\nmetrics nDCG and Q-measure, and our predicted sentiments achieved an F1 score\nof 0.921, significantly outperforming standard sentiment lexicons. This method\noffers a transparent, cost-effective way to understand patient feedback,\nenhancing traditional collection methods and informing individualised patient\ncare. Our findings are accessible via an R package and interactive dashboard,\nproviding valuable tools for healthcare researchers and practitioners.\n",
        "title": "Probabilistic emotion and sentiment modelling of patient-reported\n  experiences",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04368",
        "abstract_url": "http://arxiv.org/abs/2401.04368",
        "authors": [
            {
                "last_name": "Manalu",
                "first_name": "Gabriel D. M."
            },
            {
                "last_name": "Christian",
                "first_name": "Mulomba Mukendi"
            },
            {
                "last_name": "You",
                "first_name": "Songhee"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The relationship between acute kidney injury (AKI) prediction and nephrotoxic\ndrugs, or drugs that adversely affect kidney function, is one that has yet to\nbe explored in the critical care setting. One contributing factor to this gap\nin research is the limited investigation of drug modalities in the intensive\ncare unit (ICU) context, due to the challenges of processing prescription data\ninto the corresponding drug representations and a lack in the comprehensive\nunderstanding of these drug representations. This study addresses this gap by\nproposing a novel approach that leverages patient prescription data as a\nmodality to improve existing models for AKI prediction. We base our research on\nElectronic Health Record (EHR) data, extracting the relevant patient\nprescription information and converting it into the selected drug\nrepresentation for our research, the extended-connectivity fingerprint (ECFP).\nFurthermore, we adopt a unique multimodal approach, developing machine learning\nmodels and 1D Convolutional Neural Networks (CNN) applied to clinical drug\nrepresentations, establishing a procedure which has not been used by any\nprevious studies predicting AKI. The findings showcase a notable improvement in\nAKI prediction through the integration of drug embeddings and other patient\ncohort features. By using drug features represented as ECFP molecular\nfingerprints along with common cohort features such as demographics and lab\ntest values, we achieved a considerable improvement in model performance for\nthe AKI prediction task over the baseline model which does not include the drug\nrepresentations as features, indicating that our distinct approach enhances\nexisting baseline techniques and highlights the relevance of drug data in\npredicting AKI in the ICU setting\n",
        "title": "Enhancing Acute Kidney Injury Prediction through Integration of Drug\n  Features in Intensive Care Units",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04369",
        "abstract_url": "http://arxiv.org/abs/2401.04369",
        "authors": [
            {
                "last_name": "Christian",
                "first_name": "Mulomba Mukendi"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Air pollution stands as the fourth leading cause of death globally. While\nextensive research has been conducted in this domain, most approaches rely on\nlarge datasets when it comes to prediction. This limits their applicability in\nlow-resource settings though more vulnerable. This study addresses this gap by\nproposing a novel machine learning approach for accurate air quality prediction\nusing two months of air quality data. By leveraging the World Weather\nRepository, the meteorological, air pollutant, and Air Quality Index features\nfrom 197 capital cities were considered to predict air quality for the next\nday. The evaluation of several machine learning models demonstrates the\neffectiveness of the Random Forest algorithm in generating reliable\npredictions, particularly when applied to classification rather than\nregression, approach which enhances the model's generalizability by 42%,\nachieving a cross-validation score of 0.38 for regression and 0.89 for\nclassification. To instill confidence in the predictions, interpretable machine\nlearning was considered. Finally, a cost estimation comparing the\nimplementation of this solution in high-resource and low-resource settings is\npresented including a tentative of technology licensing business model. This\nresearch highlights the potential for resource-limited countries to\nindependently predict air quality while awaiting larger datasets to further\nrefine their predictions.\n",
        "title": "Air Quality Forecasting Using Machine Learning: A Global perspective\n  with Relevance to Low-Resource Settings",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04371",
        "abstract_url": "http://arxiv.org/abs/2401.04371",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Kazi Ashik"
            },
            {
                "last_name": "Chen",
                "first_name": "Da Qi"
            },
            {
                "last_name": "Marathe",
                "first_name": "Madhav"
            },
            {
                "last_name": "Mortveit",
                "first_name": "Henning"
            },
            {
                "last_name": "Swarup",
                "first_name": "Samarth"
            },
            {
                "last_name": "Vullikanti",
                "first_name": "Anil"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Evacuation planning is an essential part of disaster management where the\ngoal is to relocate people under imminent danger to safety. Although government\nauthorities may prescribe routes and a schedule, evacuees generally behave as\nself-interested agents and may choose their action according to their own\nselfish interests. It is crucial to understand the degree of inefficiency this\ncan cause to the evacuation process. However, existing research has mainly\nfocused on selfish routing, i.e., they consider route selection as the only\nstrategic action. In this paper, we present a strategic routing and scheduling\ngame, named the Evacuation Planning Game (EPG), where evacuees choose both\ntheir route and the time of departure. We focus on confluent evacuation plans,\nwhere, if two routes meet at a node then their remaining portion is identical.\nWe also use dynamic flows to model the time-varying traffic on roads during\nevacuation. We show that every instance of EPG has at least one pure strategy\nNash equilibrium. We then present a polynomial time algorithm, the Sequential\nAction Algorithm (SAA), for finding equilibria in a given instance.\nAdditionally, we provide bounds on how bad an equilibrium state can be compared\nto a socially optimal state. Finally, We use Harris County of Houston, Texas as\nour study area and construct a game instance for it. Our results show that, by\nutilizing SAA, we can efficiently find equilibria in this instance that have\nsocial objective close to the optimal value.\n",
        "title": "Strategic Routing and Scheduling for Evacuations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04372",
        "abstract_url": "http://arxiv.org/abs/2401.04372",
        "authors": [
            {
                "last_name": "Gottwald",
                "first_name": "Georg"
            },
            {
                "last_name": "Li",
                "first_name": "Fengyi"
            },
            {
                "last_name": "Marzouk",
                "first_name": "Youssef"
            },
            {
                "last_name": "Reich",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We consider the problem of sampling from an unknown distribution for which\nonly a sufficiently large number of training samples are available. Such\nsettings have recently drawn considerable interest in the context of generative\nmodelling. In this paper, we propose a generative model combining diffusion\nmaps and Langevin dynamics. Diffusion maps are used to approximate the drift\nterm from the available training samples, which is then implemented in a\ndiscrete-time Langevin sampler to generate new samples. By setting the kernel\nbandwidth to match the time step size used in the unadjusted Langevin\nalgorithm, our method effectively circumvents any stability issues typically\nassociated with time-stepping stiff stochastic differential equations. More\nprecisely, we introduce a novel split-step scheme, ensuring that the generated\nsamples remain within the convex hull of the training samples. Our framework\ncan be naturally extended to generate conditional samples. We demonstrate the\nperformance of our proposed scheme through experiments on synthetic datasets\nwith increasing dimensions and on a stochastic subgrid-scale parametrization\nconditional sampling problem.\n",
        "title": "Stable generative modeling using diffusion maps",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04374",
        "abstract_url": "http://arxiv.org/abs/2401.04374",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Haoyi"
            },
            {
                "last_name": "L",
                "first_name": "Xuhong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiamin"
            },
            {
                "last_name": "Sun",
                "first_name": "Xinhao"
            },
            {
                "last_name": "Li",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Sun",
                "first_name": "Zeyi"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Given the complexity and lack of transparency in deep neural networks (DNNs),\nextensive efforts have been made to make these systems more interpretable or\nexplain their behaviors in accessible terms. Unlike most reviews, which focus\non algorithmic and model-centric perspectives, this work takes a \"data-centric\"\nview, examining how data collection, processing, and analysis contribute to\nexplainable AI (XAI). We categorize existing work into three categories subject\nto their purposes: interpretations of deep models, referring to feature\nattributions and reasoning processes that correlate data points with model\noutputs; influences of training data, examining the impact of training data\nnuances, such as data valuation and sample anomalies, on decision-making\nprocesses; and insights of domain knowledge, discovering latent patterns and\nfostering new knowledge from data and models to advance social values and\nscientific discovery. Specifically, we distill XAI methodologies into data\nmining operations on training and testing data across modalities, such as\nimages, text, and tabular data, as well as on training logs, checkpoints,\nmodels and other DNN behavior descriptors. In this way, our study offers a\ncomprehensive, data-centric examination of XAI from a lens of data mining\nmethods and applications.\n",
        "title": "Towards Explainable Artificial Intelligence (XAI): A Data Mining\n  Perspective",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04377",
        "abstract_url": "http://arxiv.org/abs/2401.04377",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jingtao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaonan"
            },
            {
                "last_name": "Wang",
                "first_name": "Danwei"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Tracking the object 6-DoF pose is crucial for various downstream robot tasks\nand real-world applications. In this paper, we investigate the real-world robot\ntask of aerial vision guidance for aerial robotics manipulation, utilizing\ncategory-level 6-DoF pose tracking. Aerial conditions inevitably introduce\nspecial challenges, such as rapid viewpoint changes in pitch and roll. To\nsupport this task and challenge, we firstly introduce a robust category-level\n6-DoF pose tracker (Robust6DoF). This tracker leverages shape and temporal\nprior knowledge to explore optimal inter-frame keypoint pairs, generated under\na priori structural adaptive supervision in a coarse-to-fine manner. Notably,\nour Robust6DoF employs a Spatial-Temporal Augmentation module to deal with the\nproblems of the inter-frame differences and intra-class shape variations\nthrough both temporal dynamic filtering and shape-similarity filtering. We\nfurther present a Pose-Aware Discrete Servo strategy (PAD-Servo), serving as a\ndecoupling approach to implement the final aerial vision guidance task. It\ncontains two servo action policies to better accommodate the structural\nproperties of aerial robotics manipulation. Exhaustive experiments on four\nwell-known public benchmarks demonstrate the superiority of our Robust6DoF.\nReal-world tests directly verify that our Robust6DoF along with PAD-Servo can\nbe readily used in real-world aerial robotic applications.\n",
        "title": "Towards Real-World Aerial Vision Guidance with Categorical 6D Pose\n  Tracker",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04378",
        "abstract_url": "http://arxiv.org/abs/2401.04378",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Zan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lianzeng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  In this paper, we propose a new efficient method for calculating the\nGerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function\nusually satisfies a class of integro-differential equation. We introduce the\nphysics-informed neural networks (PINN) which embed a differential equation\ninto the loss of the neural network using automatic differentiation. In\naddition, PINN is more free to set boundary conditions and does not rely on the\ndetermination of the initial value. This gives us an idea to calculate more\ngeneral Gerber-Shiu functions. Numerical examples are provided to illustrate\nthe very good performance of our approximation.\n",
        "title": "Computing the Gerber-Shiu function with interest and a constant dividend\n  barrier by physics-informed neural networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04385",
        "abstract_url": "http://arxiv.org/abs/2401.04385",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Tang",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Li",
                "first_name": "Kenli"
            },
            {
                "last_name": "Datta",
                "first_name": "Anwitaman"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.\n",
        "title": "Machine unlearning through fine-grained model parameters perturbation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04389",
        "abstract_url": "http://arxiv.org/abs/2401.04389",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mingshuai"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuangqi"
            },
            {
                "last_name": "Yan",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Lv",
                "first_name": "Yuanjun"
            },
            {
                "last_name": "Xia",
                "first_name": "Xianjun"
            },
            {
                "last_name": "Huang",
                "first_name": "Chuanzeng"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yijian"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  This paper introduces our repairing and denoising network (RaD-Net) for the\nICASSP 2024 Speech Signal Improvement (SSI) Challenge. We extend our previous\nframework based on a two-stage network and propose an upgraded model.\nSpecifically, we replace the repairing network with COM-Net from TEA-PSE. In\naddition, multi-resolution discriminators and multi-band discriminators are\nadopted in the training stage. Finally, we use a three-step training strategy\nto optimize our model. We submit two models with different sets of parameters\nto meet the RTF requirement of the two tracks. According to the official\nresults, the proposed systems rank 2nd in track 1 and 3rd in track 2.\n",
        "title": "RaD-Net: A Repairing and Denoising Network for Speech Signal Improvement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04390",
        "abstract_url": "http://arxiv.org/abs/2401.04390",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Heewon"
            },
            {
                "last_name": "Chang",
                "first_name": "Hyun Sung"
            },
            {
                "last_name": "Cho",
                "first_name": "Kiho"
            },
            {
                "last_name": "Lee",
                "first_name": "Jaeyun"
            },
            {
                "last_name": "Han",
                "first_name": "Bohyung"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Labor-intensive labeling becomes a bottleneck in developing computer vision\nalgorithms based on deep learning. For this reason, dealing with imperfect\nlabels has increasingly gained attention and has become an active field of\nstudy. We address learning with noisy labels (LNL) problem, which is formalized\nas a task of finding a structured manifold in the midst of noisy data. In this\nframework, we provide a proper objective function and an optimization algorithm\nbased on two expectation-maximization (EM) cycles. The separate networks\nassociated with the two EM cycles collaborate to optimize the objective\nfunction, where one model is for distinguishing clean labels from corrupted\nones while the other is for refurbishing the corrupted labels. This approach\nresults in a non-collapsing LNL-flywheel model in the end. Experiments show\nthat our algorithm achieves state-of-the-art performance in multiple standard\nbenchmarks with substantial margins under various types of label noise.\n",
        "title": "Learning with Noisy Labels: Interconnection of Two\n  Expectation-Maximizations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04394",
        "abstract_url": "http://arxiv.org/abs/2401.04394",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhifeng"
            },
            {
                "last_name": "Yu",
                "first_name": "Shengye"
            },
            {
                "last_name": "Li",
                "first_name": "Mengtian"
            },
            {
                "last_name": "He",
                "first_name": "Qile"
            },
            {
                "last_name": "Chen",
                "first_name": "Chaofeng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yu-Gang"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "SD",
            ""
        ],
        "abstract": "  There has been a growing interest in the task of generating sound for silent\nvideos, primarily because of its practicality in streamlining video\npost-production. However, existing methods for video-sound generation attempt\nto directly create sound from visual representations, which can be challenging\ndue to the difficulty of aligning visual representations with audio\nrepresentations. In this paper, we present SonicVisionLM, a novel framework\naimed at generating a wide range of sound effects by leveraging vision language\nmodels. Instead of generating audio directly from video, we use the\ncapabilities of powerful vision language models (VLMs). When provided with a\nsilent video, our approach first identifies events within the video using a VLM\nto suggest possible sounds that match the video content. This shift in approach\ntransforms the challenging task of aligning image and audio into more\nwell-studied sub-problems of aligning image-to-text and text-to-audio through\nthe popular diffusion models. To improve the quality of audio recommendations\nwith LLMs, we have collected an extensive dataset that maps text descriptions\nto specific sound effects and developed temporally controlled audio adapters.\nOur approach surpasses current state-of-the-art methods for converting video to\naudio, resulting in enhanced synchronization with the visuals and improved\nalignment between audio and video components. Project page:\nhttps://yusiissy.github.io/SonicVisionLM.github.io/\n",
        "title": "SonicVisionLM: Playing Sound with Vision Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04397",
        "abstract_url": "http://arxiv.org/abs/2401.04397",
        "authors": [
            {
                "last_name": "Keurulainen",
                "first_name": "Oskar"
            },
            {
                "last_name": "Alcan",
                "first_name": "Gokhan"
            },
            {
                "last_name": "Kyrki",
                "first_name": "Ville"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  Building machines capable of efficiently collaborating with humans has been a\nlongstanding goal in artificial intelligence. Especially in the presence of\nuncertainties, optimal cooperation often requires that humans and artificial\nagents model each other's behavior and use these models to infer underlying\ngoals, beliefs or intentions, potentially involving multiple levels of\nrecursion. Empirical evidence for such higher-order cognition in human behavior\nis also provided by previous works in cognitive science, linguistics, and\nrobotics. We advocate for a new paradigm for active learning for human feedback\nthat utilises humans as active data sources while accounting for their higher\nlevels of agency. In particular, we discuss how increasing level of agency\nresults in qualitatively different forms of rational communication between an\nactive learning system and a teacher. Additionally, we provide a practical\nexample of active learning using a higher-order cognitive model. This is\naccompanied by a computational study that underscores the unique behaviors that\nthis model produces.\n",
        "title": "The Role of Higher-Order Cognitive Models in Active Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04398",
        "abstract_url": "http://arxiv.org/abs/2401.04398",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zilong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Chun-Liang"
            },
            {
                "last_name": "Eisenschlos",
                "first_name": "Julian Martin"
            },
            {
                "last_name": "Perot",
                "first_name": "Vincent"
            },
            {
                "last_name": "Wang",
                "first_name": "Zifeng"
            },
            {
                "last_name": "Miculicich",
                "first_name": "Lesly"
            },
            {
                "last_name": "Fujii",
                "first_name": "Yasuhisa"
            },
            {
                "last_name": "Shang",
                "first_name": "Jingbo"
            },
            {
                "last_name": "Lee",
                "first_name": "Chen-Yu"
            },
            {
                "last_name": "Pfister",
                "first_name": "Tomas"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.\n",
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table\n  Understanding",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04402",
        "abstract_url": "http://arxiv.org/abs/2401.04402",
        "authors": [
            {
                "last_name": "Ghosheh",
                "first_name": "Ghadeer O."
            },
            {
                "last_name": "Li",
                "first_name": "Jin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Tingting"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n",
        "title": "IGNITE: Individualized GeNeration of Imputations in Time-series\n  Electronic health records",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04403",
        "abstract_url": "http://arxiv.org/abs/2401.04403",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Long"
            },
            {
                "last_name": "Li",
                "first_name": "Shanghong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yongquan"
            },
            {
                "last_name": "Luo",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the field of Industrial Informatics, interactive segmentation has gained\nsignificant attention for its application in human-computer interaction and\ndata annotation. Existing algorithms, however, face challenges in balancing the\nsegmentation accuracy between large and small targets, often leading to an\nincreased number of user interactions. To tackle this, a novel multi-scale\ntoken adaptation algorithm, leveraging token similarity, has been devised to\nenhance segmentation across varying target sizes. This algorithm utilizes a\ndifferentiable top-k tokens selection mechanism, allowing for fewer tokens to\nbe used while maintaining efficient multi-scale token interaction. Furthermore,\na contrastive loss is introduced to better discriminate between target and\nbackground tokens, improving the correctness and robustness of the tokens\nsimilar to the target. Extensive benchmarking shows that the algorithm achieves\nstate-of-the-art (SOTA) performance compared to current methods. An interactive\ndemo and all reproducible codes will be released at\nhttps://github.com/hahamyt/mst.\n",
        "title": "MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04405",
        "abstract_url": "http://arxiv.org/abs/2401.04405",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jinhai"
            },
            {
                "last_name": "Guo",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shijie"
            },
            {
                "last_name": "Li",
                "first_name": "Junlin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Li"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "",
            "CV",
            ""
        ],
        "abstract": "  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n",
        "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title\n  Bitrate Ladder Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04406",
        "abstract_url": "http://arxiv.org/abs/2401.04406",
        "authors": [
            {
                "last_name": "Jyhne",
                "first_name": "Sander Riis\u00f8en"
            },
            {
                "last_name": "Goodwin",
                "first_name": "Morten"
            },
            {
                "last_name": "Andersen",
                "first_name": "Per Arne"
            },
            {
                "last_name": "Oveland",
                "first_name": "Ivar"
            },
            {
                "last_name": "Nossum",
                "first_name": "Alexander Salveson"
            },
            {
                "last_name": "Ormseth",
                "first_name": "Karianne"
            },
            {
                "last_name": "\u00d8rstavik",
                "first_name": "Mathilde"
            },
            {
                "last_name": "Flatman",
                "first_name": "Andrew C."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  MapAI: Precision in Building Segmentation is a competition arranged with the\nNorwegian Artificial Intelligence Research Consortium (NORA) in collaboration\nwith Centre for Artificial Intelligence Research at the University of Agder\n(CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency\nfor Data Supply and Infrastructure. The competition will be held in the fall of\n2022. It will be concluded at the Northern Lights Deep Learning conference\nfocusing on the segmentation of buildings using aerial images and laser data.\nWe propose two different tasks to segment buildings, where the first task can\nonly utilize aerial images, while the second must use laser data (LiDAR) with\nor without aerial images. Furthermore, we use IoU and Boundary IoU to properly\nevaluate the precision of the models, with the latter being an IoU measure that\nevaluates the results' boundaries. We provide the participants with a training\ndataset and keep a test dataset for evaluation.\n",
        "title": "MapAI: Precision in Building Segmentation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04408",
        "abstract_url": "http://arxiv.org/abs/2401.04408",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Qinyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Penghan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Lai",
                "first_name": "Fan"
            },
            {
                "last_name": "Mao",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Wei",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Song",
                "first_name": "Jun"
            },
            {
                "last_name": "Tsai",
                "first_name": "Wei-Yu"
            },
            {
                "last_name": "Yang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Hu",
                "first_name": "Yuxi"
            },
            {
                "last_name": "Qian",
                "first_name": "Xuehai"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG",
            "",
            ""
        ],
        "abstract": "  Huge embedding tables in modern Deep Learning Recommender Models (DLRM)\nrequire prohibitively large memory during training and inference. Aiming to\nreduce the memory footprint of training, this paper proposes FIne-grained\nIn-Training Embedding Dimension optimization (FIITED). Given the observation\nthat embedding vectors are not equally important, FIITED adjusts the dimension\nof each individual embedding vector continuously during training, assigning\nlonger dimensions to more important embeddings while adapting to dynamic\nchanges in data. A novel embedding storage system based on virtually-hashed\nphysically-indexed hash tables is designed to efficiently implement the\nembedding dimension adjustment and effectively enable memory saving.\nExperiments on two industry models show that FIITED is able to reduce the size\nof embeddings by more than 65% while maintaining the trained model's quality,\nsaving significantly more memory than a state-of-the-art in-training embedding\npruning method. On public click-through rate prediction datasets, FIITED is\nable to prune up to 93.75%-99.75% embeddings without significant accuracy loss.\n",
        "title": "Fine-Grained Embedding Dimension Optimization During Training for\n  Recommender Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04411",
        "abstract_url": "http://arxiv.org/abs/2401.04411",
        "authors": [
            {
                "last_name": "Ferdaus",
                "first_name": "Farah"
            },
            {
                "last_name": "Talukder",
                "first_name": "B. M. S. Bahar"
            },
            {
                "last_name": "Rahman",
                "first_name": "Md Tauhidur"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "CR"
        ],
        "abstract": "  This article introduces a novel, low-cost technique for hiding data in\ncommercially available resistive-RAM (ReRAM) chips. The data is kept hidden in\nReRAM cells by manipulating its analog physical properties through switching\n($\\textit{set/reset}$) operations. This hidden data, later, is retrieved by\nsensing the changes in cells' physical properties (i.e., $\\textit{set/reset}$\ntime of the memory cells). The proposed system-level hiding technique does not\naffect the normal memory operations and does not require any hardware\nmodifications. Furthermore, the proposed hiding approach is robust against\ntemperature variations and the aging of the devices through normal read/write\noperation. The silicon results show that our proposed data hiding technique is\nacceptably fast with ${\\sim}0.4bit/min$ of encoding and ${\\sim}15.625bits/s$ of\nretrieval rates, and the hidden message is unrecoverable without the knowledge\nof the secret key, which is used to enhance the security of hidden information.\n",
        "title": "Hiding Information for Secure and Covert Data Storage in Commercial\n  ReRAM Chips",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04422",
        "abstract_url": "http://arxiv.org/abs/2401.04422",
        "authors": [
            {
                "last_name": "der Br\u00fcck",
                "first_name": "Tim vor"
            },
            {
                "last_name": "Pouly",
                "first_name": "Marc"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings\nenjoy great success in the semantic representation of words, sentences, and\nwhole documents as well as for semantic similarity estimation. However, they\nhave the shortcoming that they are directly extracted from a surface\nrepresentation, which does not adequately represent human thought processes and\nalso performs poorly for highly ambiguous words. Therefore, we propose Semantic\nConcept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism,\nwhich addresses both shortcomings. The evaluation on a marketing target group\ndistribution task showed that the accuracy of predicted target groups can be\nincreased by combining traditional word embeddings with semantic CEs.\n",
        "title": "Estimating Text Similarity based on Semantic Concept Embeddings",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04423",
        "abstract_url": "http://arxiv.org/abs/2401.04423",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wei"
            },
            {
                "last_name": "Lin",
                "first_name": "Yujie"
            },
            {
                "last_name": "Ren",
                "first_name": "Pengjie"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhumin"
            },
            {
                "last_name": "Mine",
                "first_name": "Tsunenori"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jianli"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Moyan"
            },
            {
                "last_name": "Ben",
                "first_name": "Xianye"
            },
            {
                "last_name": "Li",
                "first_name": "Yujun"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Sequential recommendation has attracted a lot of attention from both academia\nand industry, however the privacy risks associated to gathering and\ntransferring users' personal interaction data are often underestimated or\nignored. Existing privacy-preserving studies are mainly applied to traditional\ncollaborative filtering or matrix factorization rather than sequential\nrecommendation. Moreover, these studies are mostly based on differential\nprivacy or federated learning, which often leads to significant performance\ndegradation, or has high requirements for communication. In this work, we\naddress privacy-preserving from a different perspective. Unlike existing\nresearch, we capture collaborative signals of neighbor interaction sequences\nand directly inject indistinguishable items into the target sequence before the\nrecommendation process begins, thereby increasing the perplexity of the target\nsequence. Even if the target interaction sequence is obtained by attackers, it\nis difficult to discern which ones are the actual user interaction records. To\nachieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,\nnamely CLOUD, which incorporates a collaborative confusion mechanism to edit\nthe raw interaction sequences before conducting recommendation. Specifically,\nCLOUD first calculates the similarity between the target interaction sequence\nand other neighbor sequences to find similar sequences. Then, CLOUD considers\nthe shared representation of the target sequence and similar sequences to\ndetermine the operation to be performed: keep, delete, or insert. We design a\ncopy mechanism to make items from similar sequences have a higher probability\nto be inserted into the target sequence. Finally, the modified sequence is used\nto train the recommender and predict the next item.\n",
        "title": "Privacy-Preserving Sequential Recommendation with Collaborative\n  Confusion",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04425",
        "abstract_url": "http://arxiv.org/abs/2401.04425",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Kosmas",
                "first_name": "Panagiotis"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Domain generalization is a popular machine learning technique that enables\nmodels to perform well on the unseen target domain, by learning from multiple\nsource domains. Domain generalization is useful in cases where data is limited,\ndifficult, or expensive to collect, such as in object recognition and\nbiomedicine. In this paper, we propose a novel domain generalization algorithm\ncalled \"meta-forests\", which builds upon the basic random forests model by\nincorporating the meta-learning strategy and maximum mean discrepancy measure.\nThe aim of meta-forests is to enhance the generalization ability of classifiers\nby reducing the correlation among trees and increasing their strength. More\nspecifically, meta-forests conducts meta-learning optimization during each\nmeta-task, while also utilizing the maximum mean discrepancy as a\nregularization term to penalize poor generalization performance in the\nmeta-test process. To evaluate the effectiveness of our algorithm, we test it\non two publicly object recognition datasets and a glucose monitoring dataset\nthat we have used in a previous study. Our results show that meta-forests\noutperforms state-of-the-art approaches in terms of generalization performance\non both object recognition and glucose monitoring datasets.\n",
        "title": "Meta-forests: Domain generalization on random forests with meta-learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04429",
        "abstract_url": "http://arxiv.org/abs/2401.04429",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Peiyan"
            },
            {
                "last_name": "Song",
                "first_name": "Qiyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Wanyuan"
            },
            {
                "last_name": "Wu",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wencan"
            },
            {
                "last_name": "Gao",
                "first_name": "Guanyu"
            },
            {
                "last_name": "Lyu",
                "first_name": "Yan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA"
        ],
        "abstract": "  Ride-hailing platforms have been facing the challenge of balancing demand and\nsupply. Existing vehicle reposition techniques often treat drivers as\nhomogeneous agents and relocate them deterministically, assuming compliance\nwith the reposition. In this paper, we consider a more realistic and\ndriver-centric scenario where drivers have unique cruising preferences and can\ndecide whether to take the recommendation or not on their own. We propose\ni-Rebalance, a personalized vehicle reposition technique with deep\nreinforcement learning (DRL). i-Rebalance estimates drivers' decisions on\naccepting reposition recommendations through an on-field user study involving\n99 real drivers. To optimize supply-demand balance and enhance preference\nsatisfaction simultaneously, i-Rebalance has a sequential reposition strategy\nwith dual DRL agents: Grid Agent to determine the reposition order of idle\nvehicles, and Vehicle Agent to provide personalized recommendations to each\nvehicle in the pre-defined order. This sequential learning strategy facilitates\nmore effective policy training within a smaller action space compared to\ntraditional joint-action methods. Evaluation of real-world trajectory data\nshows that i-Rebalance improves driver acceptance rate by 38.07% and total\ndriver income by 9.97%.\n",
        "title": "i-Rebalance: Personalized Vehicle Repositioning for Supply Demand\n  Balance",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04430",
        "abstract_url": "http://arxiv.org/abs/2401.04430",
        "authors": [
            {
                "last_name": "Dogukan",
                "first_name": "Ali Tugberk"
            },
            {
                "last_name": "Arslan",
                "first_name": "Emre"
            },
            {
                "last_name": "Basar",
                "first_name": "Ertugrul"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  Reconfigurable intelligent surfaces (RISs) bring great potential to the\nadvancement of 6G and beyond wireless communication technologies. RISs\nintroduce a great degree of flexibility, allowing some sort of virtual control\nover the wireless channel. Exploiting the flexibility introduced by RISs, we\npropose a novel RIS-enabled downlink (DL) non-orthogonal multiple access (NOMA)\nscheme where NOMA is enabled over-the-air rather than at the base station (BS)\nor the receiver (Rx). Here, the RIS is partitioned into distinctive groups\nwhere each part of the RIS serves a different user equipment (UE) to perform\nmultiple accessing. The BS transmits an unmodulated signal to the RIS, and each\npartition modulates the impinging signal over-the-air by introducing a phase\nshift according to the incoming information bits to serve the corresponding UE.\nFirst, the end-to-end system model for the proposed system is presented.\nFurthermore, outage probability calculations, theoretical error probability\nanalysis, and bit error rate (BER) derivations are discussed and reinforced\nwith comprehensive computer simulation results.\n",
        "title": "Reconfigurable Intelligent Surface-Enabled Downlink NOMA",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04431",
        "abstract_url": "http://arxiv.org/abs/2401.04431",
        "authors": [
            {
                "last_name": "Iafolla",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Fiorenza",
                "first_name": "Emiliano"
            },
            {
                "last_name": "Chiappini",
                "first_name": "Massimo"
            },
            {
                "last_name": "Carmisciano",
                "first_name": "Cosmo"
            },
            {
                "last_name": "Iafolla",
                "first_name": "Valerio Antonio"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Sea wave monitoring is key in many applications in oceanography such as the\nvalidation of weather and wave models. Conventional in situ solutions are based\non moored buoys whose measurements are often recognized as a standard. However,\nbeing exposed to a harsh environment, they are not reliable, need frequent\nmaintenance, and the datasets feature many gaps. To overcome the previous\nlimitations, we propose a system including a buoy, a micro-seismic measuring\nstation, and a machine learning algorithm. The working principle is based on\nmeasuring the micro-seismic signals generated by the sea waves. Thus, the\nmachine learning algorithm will be trained to reconstruct the missing buoy data\nfrom the micro-seismic data. As the micro-seismic station can be installed\nindoor, it assures high reliability while the machine learning algorithm\nprovides accurate reconstruction of the missing buoy data. In this work, we\npresent the methods to process the data, develop and train the machine learning\nalgorithm, and assess the reconstruction accuracy. As a case of study, we used\nexperimental data collected in 2014 from the Northern Tyrrhenian Sea\ndemonstrating that the data reconstruction can be done both for significant\nwave height and wave period. The proposed approach was inspired from Data\nScience, whose methods were the foundation for the new solutions presented in\nthis work. For example, estimating the period of the sea waves, often not\ndiscussed in previous works, was relatively simple with machine learning. In\nconclusion, the experimental results demonstrated that the new system can\novercome the reliability issues of the buoy keeping the same accuracy.\n",
        "title": "Sea wave data reconstruction using micro-seismic measurements and\n  machine learning methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04435",
        "abstract_url": "http://arxiv.org/abs/2401.04435",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Kuo"
            },
            {
                "last_name": "Li",
                "first_name": "Duo"
            },
            {
                "last_name": "Hu",
                "first_name": "Menghan"
            },
            {
                "last_name": "Zhai",
                "first_name": "Guangtao"
            },
            {
                "last_name": "Yang",
                "first_name": "Xiaokang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao-Ping"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  For semi-supervised learning with imbalance classes, the long-tailed\ndistribution of data will increase the model prediction bias toward dominant\nclasses, undermining performance on less frequent classes. Existing methods\nalso face challenges in ensuring the selection of sufficiently reliable\npseudo-labels for model training and there is a lack of mechanisms to adjust\nthe selection of more reliable pseudo-labels based on different training\nstages. To mitigate this issue, we introduce uncertainty into the modeling\nprocess for pseudo-label sampling, taking into account that the model\nperformance on the tailed classes varies over different training stages. For\nexample, at the early stage of model training, the limited predictive accuracy\nof model results in a higher rate of uncertain pseudo-labels. To counter this,\nwe propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach.\nThis approach allows the model to perceive the uncertainty of pseudo-labels at\ndifferent training stages, thereby adaptively adjusting the selection\nthresholds for different classes. Compared to other methods such as the\nbaseline method FixMatch, UDTS achieves an increase in accuracy of at least\napproximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image\ndatasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset\nTissueMNIST, respectively. The source code of UDTS is publicly available at:\nhttps://github.com/yangk/UDTS.\n",
        "title": "Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04436",
        "abstract_url": "http://arxiv.org/abs/2401.04436",
        "authors": [
            {
                "last_name": "van Dissel",
                "first_name": "Mauritz Cartier"
            },
            {
                "last_name": "Gora",
                "first_name": "Pawe\u0142"
            },
            {
                "last_name": "Manea",
                "first_name": "Drago\u015f"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Urban road transport is a major civilisational and economic challenge,\naffecting quality of life and economic activity. Addressing these challenges\nrequires a multidisciplinary approach and sustainable urban planning strategies\nto mitigate the negative effects of traffic in cities. In this paper, we will\nintroduce an extension of one of the most popular macroscopic traffic\nsimulation models, the Payne-Whitham model. We will investigate how this model,\noriginally designed to model highway traffic on straight road segments, can be\nadapted to more realistic conditions with arbitrary road network graphs and\nmultiple intersections with traffic signals. Furthermore, we will showcase the\npractical application of this extension in experiments aimed at optimising\ntraffic signal settings. For computational reasons, these experiments involve\nthe adoption of surrogate models for approximating our extended Payne-Whitham\nmodel, and subsequently, we utilise various optimisation algorithms, e.g.\nSLSQP, resulting in the identification of traffic signal settings that enhance\nthe average speed of cars, thereby facilitating smoother traffic flow.\n",
        "title": "A Payne-Whitham model of urban traffic networks in the presence of\n  traffic lights and its application to traffic optimisation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04437",
        "abstract_url": "http://arxiv.org/abs/2401.04437",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Dongeon"
            },
            {
                "last_name": "Park",
                "first_name": "YeongHyeon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recent studies try to use hyperspectral imaging (HSI) to detect foreign\nmatters in products because it enables to visualize the invisible wavelengths\nincluding ultraviolet and infrared. Considering the enormous image channels of\nthe HSI, several dimension reduction methods-e.g., PCA or UMAP-can be\nconsidered to reduce but those cannot ease the fundamental limitations, as\nfollows: (1) latency of HSI capturing. (2) less explanation ability of the\nimportant channels. In this paper, to circumvent the aforementioned methods,\none of the ways to channel reduction, on anomaly detection proposed HSI.\nDifferent from feature extraction methods (i.e., PCA or UMAP), feature\nselection can sort the feature by impact and show better explainability so we\nmight redesign the task-optimized and cost-effective spectroscopic camera. Via\nthe extensive experiment results with synthesized MVTec AD dataset, we confirm\nthat the feature selection method shows 6.90x faster at the inference phase\ncompared with feature extraction-based approaches while preserving anomaly\ndetection performance. Ultimately, we conclude the advantage of feature\nselection which is effective yet fast.\n",
        "title": "Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using\n  Dimension Reduction Methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04441",
        "abstract_url": "http://arxiv.org/abs/2401.04441",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Yishuang"
            },
            {
                "last_name": "Wang",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.\n",
        "title": "Image classification network enhancement methods based on knowledge\n  injection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04446",
        "abstract_url": "http://arxiv.org/abs/2401.04446",
        "authors": [
            {
                "last_name": "Schneider",
                "first_name": "Simon"
            },
            {
                "last_name": "Ferreyra",
                "first_name": "Nicol\u00e1s E. D\u00edaz"
            },
            {
                "last_name": "Qu\u00e9val",
                "first_name": "Pierre-Jean"
            },
            {
                "last_name": "Simhandl",
                "first_name": "Georg"
            },
            {
                "last_name": "Zdun",
                "first_name": "Uwe"
            },
            {
                "last_name": "Scandariato",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Models of software systems are used throughout the software development\nlifecycle. Dataflow diagrams (DFDs), in particular, are well-established\nresources for security analysis. Many techniques, such as threat modelling, are\nbased on DFDs of the analysed application. However, their impact on the\nperformance of analysts in a security analysis setting has not been explored\nbefore. In this paper, we present the findings of an empirical experiment\nconducted to investigate this effect. Following a within-groups design,\nparticipants were asked to solve security-relevant tasks for a given\nmicroservice application. In the control condition, the participants had to\nexamine the source code manually. In the model-supported condition, they were\nadditionally provided a DFD of the analysed application and traceability\ninformation linking model items to artefacts in source code. We found that the\nparticipants (n = 24) performed significantly better in answering the analysis\ntasks correctly in the model-supported condition (41% increase in analysis\ncorrectness). Further, participants who reported using the provided\ntraceability information performed better in giving evidence for their answers\n(315% increase in correctness of evidence). Finally, we identified three open\nchallenges of using DFDs for security analysis based on the insights gained in\nthe experiment.\n",
        "title": "How Dataflow Diagrams Impact Software Security Analysis: an Empirical\n  Experiment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04447",
        "abstract_url": "http://arxiv.org/abs/2401.04447",
        "authors": [
            {
                "last_name": "Mulimani",
                "first_name": "Manjunath"
            },
            {
                "last_name": "Mesaros",
                "first_name": "Annamaria"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  In this paper, we propose a method for class-incremental learning of\npotentially overlapping sounds for solving a sequence of multi-label audio\nclassification tasks. We design an incremental learner that learns new classes\nindependently of the old classes. To preserve knowledge about the old classes,\nwe propose a cosine similarity-based distillation loss that minimizes\ndiscrepancy in the feature representations of subsequent learners, and use it\nalong with a Kullback-Leibler divergence-based distillation loss that minimizes\ndiscrepancy in their respective outputs. Experiments are performed on a dataset\nwith 50 sound classes, with an initial classification task containing 30 base\nclasses and 4 incremental phases of 5 classes each. After each phase, the\nsystem is tested for multi-label classification with the entire set of classes\nlearned so far. The proposed method obtains an average F1-score of 40.9% over\nthe five phases, ranging from 45.2% in phase 0 on 30 classes, to 36.3% in phase\n4 on 50 classes. Average performance degradation over incremental phases is\nonly 0.7 percentage points from the initial F1-score of 45.2%.\n",
        "title": "Class-Incremental Learning for Multi-Label Audio Classification",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04448",
        "abstract_url": "http://arxiv.org/abs/2401.04448",
        "authors": [
            {
                "last_name": "Breci",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Guarnera",
                "first_name": "Luca"
            },
            {
                "last_name": "Battiato",
                "first_name": "Sebastiano"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Forensic handwriting examination is a branch of Forensic Science that aims to\nexamine handwritten documents in order to properly define or hypothesize the\nmanuscript's author. These analysis involves comparing two or more (digitized)\ndocuments through a comprehensive comparison of intrinsic local and global\nfeatures. If a correlation exists and specific best practices are satisfied,\nthen it will be possible to affirm that the documents under analysis were\nwritten by the same individual. The need to create sophisticated tools capable\nof extracting and comparing significant features has led to the development of\ncutting-edge software with almost entirely automated processes, improving the\nforensic examination of handwriting and achieving increasingly objective\nevaluations. This is made possible by algorithmic solutions based on purely\nmathematical concepts. Machine Learning and Deep Learning models trained with\nspecific datasets could turn out to be the key elements to best solve the task\nat hand. In this paper, we proposed a new and challenging dataset consisting of\ntwo subsets: the first consists of 21 documents written either by the classic\n``pen and paper\" approach (and later digitized) and directly acquired on common\ndevices such as tablets; the second consists of 362 handwritten manuscripts by\n124 different people, acquired following a specific pipeline. Our study\npioneered a comparison between traditionally handwritten documents and those\nproduced with digital tools (e.g., tablets). Preliminary results on the\nproposed datasets show that 90% classification accuracy can be achieved on the\nfirst subset (documents written on both paper and pen and later digitized and\non tablets) and 96% on the second portion of the data. The datasets are\navailable at\nhttps://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/.\n",
        "title": "A Novel Dataset for Non-Destructive Inspection of Handwritten Documents",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04452",
        "abstract_url": "http://arxiv.org/abs/2401.04452",
        "authors": [
            {
                "last_name": "Richard",
                "first_name": "Magali"
            },
            {
                "last_name": "Blum",
                "first_name": "Yuna"
            },
            {
                "last_name": "Guinney",
                "first_name": "Justin"
            },
            {
                "last_name": "Stolovitzky",
                "first_name": "Gustavo"
            },
            {
                "last_name": "Pav\u00e3o",
                "first_name": "Adrien"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This chapter provides a comprehensive overview of the pragmatic aspects\ninvolved in organizing AI competitions. We begin by discussing strategies to\nincentivize participation, touching upon effective communication techniques,\naligning with trending topics in the field, structuring awards, potential\nrecruitment opportunities, and more. We then shift to the essence of community\nengagement, and into organizational best practices and effective means of\ndisseminating challenge outputs. Lastly, the chapter addresses the logistics,\nexposing on costs, required manpower, and resource allocation for effectively\nmanaging and executing a challenge. By examining these practical problems,\nreaders will gain actionable insights to navigate the multifaceted landscape of\nAI competition organization, from inception to completion.\n",
        "title": "AI Competitions and Benchmarks, Practical issues: Proposals, grant\n  money, sponsors, prizes, dissemination, publicity",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04454",
        "abstract_url": "http://arxiv.org/abs/2401.04454",
        "authors": [
            {
                "last_name": "Bezuidenhout",
                "first_name": "Louise"
            },
            {
                "last_name": "Ratti",
                "first_name": "Emanuele"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  In this chapter, we propose a non-traditional RCR training in data science\nthat is grounded into a virtue theory framework. First, we delineate the\napproach in more theoretical detail, by discussing how the goal of RCR training\nis to foster the cultivation of certain moral abilities. We specify the nature\nof these abilities: while the ideal is the cultivation of virtues, the limited\nspace allowed by RCR modules can only facilitate the cultivation of superficial\nabilities or proto-virtues, which help students to familiarize with moral and\npolitical issues in the data science environment. Third, we operationalize our\napproach by stressing that (proto-)virtue acquisition (like skill acquisition)\noccurs through the technical and social tasks of daily data science activities,\nwhere these repetitive tasks provide the opportunities to develop\n(proto-)virtue capacity and to support the development of ethically robust data\nsystems. Finally, we discuss a concrete example of how this approach has been\nimplemented. In particular, we describe how this method is applied to teach\ndata ethics to students participating in the CODATA-RDA Data Science Summer\nSchools.\n",
        "title": "Character comes from practice: longitudinal practice-based ethics\n  training in data science",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04456",
        "abstract_url": "http://arxiv.org/abs/2401.04456",
        "authors": [
            {
                "last_name": "Di Pietro",
                "first_name": "Daniele A."
            },
            {
                "last_name": "Droniou",
                "first_name": "Jerome"
            },
            {
                "last_name": "Qian",
                "first_name": "Jia Jia"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this work we design and analyse a Discrete de Rham (DDR) method for the\nincompressible Navier-Stokes equations. Our focus is, more specifically, on the\nSDDR variant, where a reduction in the number of unknowns is obtained using\nserendipity techniques. The main features of the DDR approach are the support\nof general meshes and arbitrary approximation orders. The method we develop is\nbased on the curl-curl formulation of the momentum equation and, through\ncompatibility with the Helmholtz-Hodge decomposition, delivers pressure-robust\nerror estimates for the velocity. It also enables non-standard boundary\nconditions, such as imposing the value of the pressure on the boundary.\nIn-depth numerical validation on a complete panel of tests including general\npolyhedral meshes is provided. The paper also contains an appendix where bounds\non DDR potential reconstructions and differential operators are proved in the\nmore general framework of Polytopal Exterior Calculus.\n",
        "title": "A pressure-robust Discrete de Rham scheme for the Navier-Stokes\n  equations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04461",
        "abstract_url": "http://arxiv.org/abs/2401.04461",
        "authors": [
            {
                "last_name": "Klein",
                "first_name": "C."
            },
            {
                "last_name": "Stoilov",
                "first_name": "N."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose a method to numerically compute fractional derivatives (or the\nfractional Laplacian) on the whole real line via Riesz fractional integrals.\nThe compactified real line is divided into a number of intervals, thus\namounting to a multi-domain approach; after transformations in accordance with\nthe underlying $Z_{q}$ curve ensuring analyticity of the respective integrands,\nthe integrals over the different domains are computed with a Clenshaw-Curtis\nalgorithm. As an example, we consider solitary waves for fractional Korteweg-de\nVries equations and compare these to results obtained with a discrete Fourier\ntransform.\n",
        "title": "Multi-domain spectral approach to rational-order fractional derivatives",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04463",
        "abstract_url": "http://arxiv.org/abs/2401.04463",
        "authors": [
            {
                "last_name": "Tebbe",
                "first_name": "Justin"
            },
            {
                "last_name": "Tayyub",
                "first_name": "Jawad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diffusion models have found valuable applications in anomaly detection by\ncapturing the nominal data distribution and identifying anomalies via\nreconstruction. Despite their merits, they struggle to localize anomalies of\nvarying scales, especially larger anomalies like entire missing components.\nAddressing this, we present a novel framework that enhances the capability of\ndiffusion models, by extending the previous introduced implicit conditioning\napproach Meng et al. (2022) in three significant ways. First, we incorporate a\ndynamic step size computation that allows for variable noising steps in the\nforward process guided by an initial anomaly prediction. Second, we demonstrate\nthat denoising an only scaled input, without any added noise, outperforms\nconventional denoising process. Third, we project images in a latent space to\nabstract away from fine details that interfere with reconstruction of large\nmissing components. Additionally, we propose a fine-tuning mechanism that\nfacilitates the model to effectively grasp the nuances of the target domain.\nOur method undergoes rigorous evaluation on two prominent anomaly detection\ndatasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our\nframework effectively localizes anomalies regardless of their scale, marking a\npivotal advancement in diffusion-based anomaly detection.\n",
        "title": "D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly\n  Detection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04464",
        "abstract_url": "http://arxiv.org/abs/2401.04464",
        "authors": [
            {
                "last_name": "Fibaek",
                "first_name": "Casper"
            },
            {
                "last_name": "Camilleri",
                "first_name": "Luke"
            },
            {
                "last_name": "Luyts",
                "first_name": "Andreas"
            },
            {
                "last_name": "Dionelis",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "Saux",
                "first_name": "Bertrand Le"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Massive amounts of unlabelled data are captured by Earth Observation (EO)\nsatellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.\nThis makes Remote Sensing a data-rich domain well suited to Machine Learning\n(ML) solutions. However, a bottleneck in applying ML models to EO is the lack\nof annotated data as annotation is a labour-intensive and costly process. As a\nresult, research in this domain has focused on Self-Supervised Learning and\nFoundation Model approaches. This paper addresses the need to evaluate\ndifferent Foundation Models on a fair and uniform benchmark by introducing the\nPhilEO Bench, a novel evaluation framework for EO Foundation Models. The\nframework comprises of a testbed and a novel 400 GB Sentinel-2 dataset\ncontaining labels for three downstream tasks, building density estimation, road\nsegmentation, and land cover classification. We present experiments using our\nframework evaluating different Foundation Models, including Prithvi and SatMAE,\nat multiple n-shots and convergence rates.\n",
        "title": "PhilEO Bench: Evaluating Geo-Spatial Foundation Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04468",
        "abstract_url": "http://arxiv.org/abs/2401.04468",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weimin"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhijie"
            },
            {
                "last_name": "Yan",
                "first_name": "Jiangqiao"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuo"
            },
            {
                "last_name": "Low",
                "first_name": "Chetwin"
            },
            {
                "last_name": "Hoang",
                "first_name": "Tuyen"
            },
            {
                "last_name": "Wu",
                "first_name": "Jie"
            },
            {
                "last_name": "Liew",
                "first_name": "Jun Hao"
            },
            {
                "last_name": "Yan",
                "first_name": "Hanshu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Daquan"
            },
            {
                "last_name": "Feng",
                "first_name": "Jiashi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The growing demand for high-fidelity video generation from textual\ndescriptions has catalyzed significant research in this field. In this work, we\nintroduce MagicVideo-V2 that integrates the text-to-image model, video motion\ngenerator, reference image embedding module and frame interpolation module into\nan end-to-end video generation pipeline. Benefiting from these architecture\ndesigns, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution\nvideo with remarkable fidelity and smoothness. It demonstrates superior\nperformance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,\nMoon Valley and Stable Video Diffusion model via user evaluation at large\nscale.\n",
        "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04471",
        "abstract_url": "http://arxiv.org/abs/2401.04471",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xue"
            },
            {
                "last_name": "Shi",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Lou",
                "first_name": "Xinyue"
            },
            {
                "last_name": "Qi",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Yufeng"
            },
            {
                "last_name": "Xu",
                "first_name": "Jinan"
            },
            {
                "last_name": "Han",
                "first_name": "Wenjuan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave shown excellent general capabilities, even exhibiting adaptability in many\nprofessional domains such as law, economics, transportation, and medicine.\nCurrently, many domain-specific benchmarks have been proposed to verify the\nperformance of (M)LLMs in specific fields. Among various domains,\ntransportation plays a crucial role in modern society as it impacts the\neconomy, the environment, and the quality of life for billions of people.\nHowever, it is unclear how much traffic knowledge (M)LLMs possess and whether\nthey can reliably perform transportation-related tasks. To address this gap, we\npropose TransportationGames, a carefully designed and thorough evaluation\nbenchmark for assessing (M)LLMs in the transportation domain. By\ncomprehensively considering the applications in real-world scenarios and\nreferring to the first three levels in Bloom's Taxonomy, we test the\nperformance of various (M)LLMs in memorizing, understanding, and applying\ntransportation knowledge by the selected tasks. The experimental results show\nthat although some models perform well in some tasks, there is still much room\nfor improvement overall. We hope the release of TransportationGames can serve\nas a foundation for future research, thereby accelerating the implementation\nand application of (M)LLMs in the transportation domain.\n",
        "title": "TransportationGames: Benchmarking Transportation Knowledge of\n  (Multimodal) Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04472",
        "abstract_url": "http://arxiv.org/abs/2401.04472",
        "authors": [
            {
                "last_name": "Woisetschl\u00e4ger",
                "first_name": "Herbert"
            },
            {
                "last_name": "Isenko",
                "first_name": "Alexander"
            },
            {
                "last_name": "Wang",
                "first_name": "Shiqiang"
            },
            {
                "last_name": "Mayer",
                "first_name": "Ruben"
            },
            {
                "last_name": "Jacobsen",
                "first_name": "Hans-Arno"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DC",
            "",
            ""
        ],
        "abstract": "  Federated Learning (FL) has become an established technique to facilitate\nprivacy-preserving collaborative training. However, new approaches to FL often\ndiscuss their contributions involving small deep-learning models only. With the\ntremendous success of transformer models, the following question arises: What\nis necessary to operationalize foundation models in an FL application? Knowing\nthat computation and communication often take up similar amounts of time in FL,\nwe introduce a novel taxonomy focused on computational and communication\nefficiency methods in FL applications. This said, these methods aim to optimize\nthe training time and reduce communication between clients and the server. We\nalso look at the current state of widely used FL frameworks and discuss future\nresearch potentials based on existing approaches in FL research and beyond.\n",
        "title": "A Survey on Efficient Federated Learning Methods for Foundation Model\n  Training",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04474",
        "abstract_url": "http://arxiv.org/abs/2401.04474",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Ngoc Luyen"
            },
            {
                "last_name": "Abel",
                "first_name": "Marie-H\u00e9l\u00e8ne"
            },
            {
                "last_name": "Gouspillou",
                "first_name": "Philippe"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            ""
        ],
        "abstract": "  In today's data-rich environment, recommender systems play a crucial role in\ndecision support systems. They provide to users personalized recommendations\nand explanations about these recommendations. Embedding-based models, despite\ntheir widespread use, often suffer from a lack of interpretability, which can\nundermine trust and user engagement. This paper presents an approach that\ncombines embedding-based and semantic-based models to generate post-hoc\nexplanations in recommender systems, leveraging ontology-based knowledge graphs\nto improve interpretability and explainability. By organizing data within a\nstructured framework, ontologies enable the modeling of intricate relationships\nbetween entities, which is essential for generating explanations. By combining\nembedding-based and semantic based models for post-hoc explanations in\nrecommender systems, the framework we defined aims at producing meaningful and\neasy-to-understand explanations, enhancing user trust and satisfaction, and\npotentially promoting the adoption of recommender systems across the e-commerce\nsector.\n",
        "title": "Combining Embedding-Based and Semantic-Based Models for Post-hoc\n  Explanations in Recommender Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04478",
        "abstract_url": "http://arxiv.org/abs/2401.04478",
        "authors": [
            {
                "last_name": "Schuh",
                "first_name": "Maximilian G."
            },
            {
                "last_name": "Boldini",
                "first_name": "Davide"
            },
            {
                "last_name": "Sieber",
                "first_name": "Stephan A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  The success of drug discovery and development relies on the precise\nprediction of molecular activities and properties. While in silico molecular\nproperty prediction has shown remarkable potential, its use has been limited so\nfar to assays for which large amounts of data are available. In this study, we\nuse a fine-tuned large language model to integrate biological assays based on\ntheir textual information, coupled with Barlow Twins, a Siamese neural network\nusing a novel self-supervised learning approach. This architecture uses both\nassay information and molecular fingerprints to extract the true molecular\ninformation. TwinBooster enables the prediction of properties of unseen\nbioassays and molecules by providing state-of-the-art zero-shot learning tasks.\nRemarkably, our artificial intelligence pipeline shows excellent performance on\nthe FS-Mol benchmark. This breakthrough demonstrates the application of deep\nlearning to critical property prediction tasks where data is typically scarce.\nBy accelerating the early identification of active molecules in drug discovery\nand development, this method has the potential to help streamline the\nidentification of novel therapeutics.\n",
        "title": "TwinBooster: Synergising Large Language Models with Barlow Twins and\n  Gradient Boosting for Enhanced Molecular Property Prediction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04481",
        "abstract_url": "http://arxiv.org/abs/2401.04481",
        "authors": [
            {
                "last_name": "Satapara",
                "first_name": "Shrey"
            },
            {
                "last_name": "Mehta",
                "first_name": "Parth"
            },
            {
                "last_name": "Ganguly",
                "first_name": "Debasis"
            },
            {
                "last_name": "Modha",
                "first_name": "Sandip"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The recent success in language generation capabilities of large language\nmodels (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns\nabout their possible misuse in inducing mass agitation and communal hatred via\ngenerating fake news and spreading misinformation. Traditional means of\ndeveloping a misinformation ground-truth dataset does not scale well because of\nthe extensive manual effort required to annotate the data. In this paper, we\npropose an LLM-based approach of creating silver-standard ground-truth datasets\nfor identifying misinformation. Specifically speaking, given a trusted news\narticle, our proposed approach involves prompting LLMs to automatically\ngenerate a summarised version of the original article. The prompts in our\nproposed approach act as a controlling mechanism to generate specific types of\nfactual incorrectness in the generated summaries, e.g., incorrect quantities,\nfalse attributions etc. To investigate the usefulness of this dataset, we\nconduct a set of experiments where we train a range of supervised models for\nthe task of misinformation detection.\n",
        "title": "Fighting Fire with Fire: Adversarial Prompting to Generate a\n  Misinformation Detection Dataset",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04482",
        "abstract_url": "http://arxiv.org/abs/2401.04482",
        "authors": [
            {
                "last_name": "Huber",
                "first_name": "Christian"
            },
            {
                "last_name": "Waibel",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities and\ndomain-specific special words for which little or no data is available. To\naddress the problem of recognizing these words, we propose an self-supervised\ncontinual learning approach. Given the audio of a lecture talk with\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from previous work. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation dataset. Continual learning is then performed on this\nset by adapting low-rank matrix weights added to each weight matrix of the\nmodel. The whole procedure is iterated for many talks. We show that with this\napproach, we obtain increasing performance on the new words when they occur\nmore frequently (more than 80% recall) while preserving the general performance\nof the model.\n",
        "title": "Continuously Learning New Words in Automatic Speech Recognition",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04485",
        "abstract_url": "http://arxiv.org/abs/2401.04485",
        "authors": [
            {
                "last_name": "Alzaben",
                "first_name": "Linda"
            },
            {
                "last_name": "Boffi",
                "first_name": "Daniele"
            },
            {
                "last_name": "Dedner",
                "first_name": "Andreas"
            },
            {
                "last_name": "Gastaldi",
                "first_name": "Lucia"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  In this paper we introduce an abstract setting for the convergence analysis\nof the virtual element approximation of an acoustic vibration problem. We\ndiscuss the effect of the stabilization parameters and remark that in some\ncases it is possible to achieve optimal convergence without the need of any\nstabilization. This statement is rigorously proved for lowest order triangular\nelement and supported by several numerical experiments.\n",
        "title": "On the stabilization of a virtual element method for an acoustic\n  vibration problem",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04486",
        "abstract_url": "http://arxiv.org/abs/2401.04486",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Yufei"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuanpei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The Spiking Neural Network (SNN) is a biologically inspired neural network\ninfrastructure that has recently garnered significant attention. It utilizes\nbinary spike activations to transmit information, thereby replacing\nmultiplications with additions and resulting in high energy efficiency.\nHowever, training an SNN directly poses a challenge due to the undefined\ngradient of the firing spike process. Although prior works have employed\nvarious surrogate gradient training methods that use an alternative function to\nreplace the firing process during back-propagation, these approaches ignore an\nintrinsic problem: gradient vanishing. To address this issue, we propose a\nshortcut back-propagation method in our paper, which advocates for transmitting\nthe gradient directly from the loss to the shallow layers. This enables us to\npresent the gradient to the shallow layers directly, thereby significantly\nmitigating the gradient vanishing problem. Additionally, this method does not\nintroduce any burden during the inference phase. To strike a balance between\nfinal accuracy and ease of training, we also propose an evolutionary training\nframework and implement it by inducing a balance coefficient that dynamically\nchanges with the training epoch, which further improves the network's\nperformance. Extensive experiments conducted over static and dynamic datasets\nusing several popular network structures reveal that our method consistently\noutperforms state-of-the-art methods.\n",
        "title": "Take A Shortcut Back: Mitigating the Gradient Vanishing for Training\n  Spiking Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04487",
        "abstract_url": "http://arxiv.org/abs/2401.04487",
        "authors": [
            {
                "last_name": "Nonhoff",
                "first_name": "Marko"
            },
            {
                "last_name": "Dall'Anese",
                "first_name": "Emiliano"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Matthias A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This article investigates the problem of controlling linear time-invariant\nsystems subject to time-varying and a priori unknown cost functions, state and\ninput constraints, and exogenous disturbances. We combine the online convex\noptimization framework with tools from robust model predictive control to\npropose an algorithm that is able to guarantee robust constraint satisfaction.\nThe performance of the closed loop emerging from application of our framework\nis studied in terms of its dynamic regret, which is proven to be bounded\nlinearly by the variation of the cost functions and the magnitude of the\ndisturbances. We corroborate our theoretical findings and illustrate\nimplementational aspects of the proposed algorithm by a numerical case study of\na tracking control problem of an autonomous vehicle.\n",
        "title": "Online convex optimization for robust control of constrained dynamical\n  systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04489",
        "abstract_url": "http://arxiv.org/abs/2401.04489",
        "authors": [
            {
                "last_name": "Huisman",
                "first_name": "Tim"
            },
            {
                "last_name": "van der Linden",
                "first_name": "Jacobus G. M."
            },
            {
                "last_name": "Demirovi\u0107",
                "first_name": "Emir"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DS"
        ],
        "abstract": "  Survival analysis studies and predicts the time of death, or other singular\nunrepeated events, based on historical data, while the true time of death for\nsome instances is unknown. Survival trees enable the discovery of complex\nnonlinear relations in a compact human comprehensible model, by recursively\nsplitting the population and predicting a distinct survival distribution in\neach leaf node. We use dynamic programming to provide the first survival tree\nmethod with optimality guarantees, enabling the assessment of the optimality\ngap of heuristics. We improve the scalability of our method through a special\nalgorithm for computing trees up to depth two. The experiments show that our\nmethod's run time even outperforms some heuristics for realistic cases while\nobtaining similar out-of-sample performance with the state-of-the-art.\n",
        "title": "Optimal Survival Trees: A Dynamic Programming Approach",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04491",
        "abstract_url": "http://arxiv.org/abs/2401.04491",
        "authors": [
            {
                "last_name": "Gonzalez",
                "first_name": "Hector A."
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Kelber",
                "first_name": "Florian"
            },
            {
                "last_name": "Nazeer",
                "first_name": "Khaleelulla Khan"
            },
            {
                "last_name": "Langer",
                "first_name": "Tim"
            },
            {
                "last_name": "Liu",
                "first_name": "Chen"
            },
            {
                "last_name": "Lohrmann",
                "first_name": "Matthias"
            },
            {
                "last_name": "Rostami",
                "first_name": "Amirhossein"
            },
            {
                "last_name": "Sch\u00f6ne",
                "first_name": "Mark"
            },
            {
                "last_name": "Vogginger",
                "first_name": "Bernhard"
            },
            {
                "last_name": "Wunderlich",
                "first_name": "Timo C."
            },
            {
                "last_name": "Yan",
                "first_name": "Yexin"
            },
            {
                "last_name": "Akl",
                "first_name": "Mahmoud"
            },
            {
                "last_name": "Mayr",
                "first_name": "Christian"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG",
            "NE"
        ],
        "abstract": "  The joint progress of artificial neural networks (ANNs) and domain specific\nhardware accelerators such as GPUs and TPUs took over many domains of machine\nlearning research. This development is accompanied by a rapid growth of the\nrequired computational demands for larger models and more data. Concurrently,\nemerging properties of foundation models such as in-context learning drive new\nopportunities for machine learning applications. However, the computational\ncost of such applications is a limiting factor of the technology in data\ncenters, and more importantly in mobile devices and edge systems. To mediate\nthe energy footprint and non-trivial latency of contemporary systems,\nneuromorphic computing systems deeply integrate computational principles of\nneurobiological systems by leveraging low-power analog and digital\ntechnologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable\nmachine learning. The event-based and asynchronous design of SpiNNaker2 allows\nthe composition of large-scale systems involving thousands of chips. This work\nfeatures the operating principles of SpiNNaker2 systems, outlining the\nprototype of novel machine learning applications. These applications range from\nANNs over bio-inspired spiking neural networks to generalized event-based\nneural networks. With the successful development and deployment of SpiNNaker2,\nwe aim to facilitate the advancement of event-based and asynchronous algorithms\nfor future generations of machine learning systems.\n",
        "title": "SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and\n  Asynchronous Machine Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04492",
        "abstract_url": "http://arxiv.org/abs/2401.04492",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Junling"
            },
            {
                "last_name": "Pecorella",
                "first_name": "Matteo"
            },
            {
                "last_name": "Iovene",
                "first_name": "Elisa"
            },
            {
                "last_name": "Palumbo",
                "first_name": "Maria Chiara"
            },
            {
                "last_name": "Rota",
                "first_name": "Alberto"
            },
            {
                "last_name": "Redaelli",
                "first_name": "Alberto"
            },
            {
                "last_name": "Ferrigno",
                "first_name": "Giancarlo"
            },
            {
                "last_name": "De Momi",
                "first_name": "Elena"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  During Percutaneous Nephrolithotomy (PCNL) operations, the surgeon is\nrequired to define the incision point on the patient's back, align the needle\nto a pre-planned path, and perform puncture operations afterward. The procedure\nis currently performed manually using ultrasound or fluoroscopy imaging for\nneedle orientation, which, however, implies limited accuracy and low\nreproducibility. This work incorporates Augmented Reality (AR) visualization\nwith an optical see-through head-mounted display (OST-HMD) and Human-Robot\nCollaboration (HRC) framework to empower the surgeon's task completion\nperformance. In detail, Eye-to-Hand calibration, system registration, and\nhologram model registration are performed to realize visual guidance. A\nCartesian impedance controller is used to guide the operator during the needle\npuncture task execution. Experiments are conducted to verify the system\nperformance compared with conventional manual puncture procedures and a 2D\nmonitor-based visualisation interface. The results showed that the proposed\nframework achieves the lowest median and standard deviation error across all\nthe experimental groups, respectively. Furthermore, the NASA-TLX user\nevaluation results indicate that the proposed framework requires the lowest\nworkload score for task completion compared to other experimental setups. The\nproposed framework exhibits significant potential for clinical application in\nthe PCNL task, as it enhances the surgeon's perception capability, facilitates\ncollision-free needle insertion path planning, and minimises errors in task\ncompletion.\n",
        "title": "Augmented Reality and Human-Robot Collaboration Framework for\n  Percutaneous Nephrolithotomy",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04494",
        "abstract_url": "http://arxiv.org/abs/2401.04494",
        "authors": [
            {
                "last_name": "Fernandesa",
                "first_name": "Jo\u00e3o B."
            },
            {
                "last_name": "de Assis",
                "first_name": "\u00cdtalo A. S."
            },
            {
                "last_name": "Martins",
                "first_name": "Idalmis M. S."
            },
            {
                "last_name": "Barros",
                "first_name": "Tiago"
            },
            {
                "last_name": "Xavier-de-Souza",
                "first_name": "Samuel"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Supercomputers have revolutionized how industries and scientific fields\nprocess large amounts of data. These machines group hundreds or thousands of\ncomputing nodes working together to execute time-consuming programs that\nrequire a large amount of computational resources. Over the years,\nsupercomputers have expanded to include new and different technologies\ncharacterizing them as heterogeneous. However, executing a program in a\nheterogeneous environment requires attention to a specific aspect of\nperformance degradation: load imbalance. In this research, we address the\nchallenges associated with load imbalance when scheduling many homogeneous\ntasks in a heterogeneous environment. To address this issue, we introduce the\nconcept of adaptive asynchronous work-stealing. This approach collects\ninformation about the nodes and utilizes it to improve work-stealing aspects,\nsuch as victim selection and task offloading. Additionally, the proposed\napproach eliminates the need for extra threads to communicate information,\nthereby reducing overhead when implementing a fully asynchronous approach. Our\nexperimental results demonstrate a performance improvement of approximately\n10.1\\% compared to other conventional and state-of-the-art implementations.\n",
        "title": "Adaptive Asynchronous Work-Stealing for distributed load-balancing in\n  heterogeneous systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04495",
        "abstract_url": "http://arxiv.org/abs/2401.04495",
        "authors": [
            {
                "last_name": "Calderini",
                "first_name": "Marco"
            },
            {
                "last_name": "Civino",
                "first_name": "Roberto"
            },
            {
                "last_name": "Invernizzi",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "IT",
            "",
            ""
        ],
        "abstract": "  The use of alternative operations in differential cryptanalysis, or\nalternative notions of differentials, are lately receiving increasing\nattention. Recently, Civino et al. managed to design a block cipher which is\nsecure w.r.t. classical differential cryptanalysis performed using\nXOR-differentials, but weaker with respect to the attack based on an\nalternative difference operation acting on the first s-box of the block. We\nextend this result to parallel alternative operations, i.e. acting on each\ns-box of the block. First, we recall the mathematical framework needed to\ndefine and use such operations. After that, we perform some differential\nexperiments against a toy cipher and compare the effectiveness of the attack\nw.r.t. the one that uses XOR-differentials.\n",
        "title": "Differential experiments using parallel alternative operations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04507",
        "abstract_url": "http://arxiv.org/abs/2401.04507",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Chang",
                "first_name": "Yuying"
            },
            {
                "last_name": "Li",
                "first_name": "Zhong"
            },
            {
                "last_name": "An",
                "first_name": "Ning"
            },
            {
                "last_name": "Ma",
                "first_name": "Qi"
            },
            {
                "last_name": "Hei",
                "first_name": "Lei"
            },
            {
                "last_name": "Luo",
                "first_name": "Haibo"
            },
            {
                "last_name": "Lu",
                "first_name": "Yifei"
            },
            {
                "last_name": "Ren",
                "first_name": "Feiliang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models have exhibited robust performance across diverse\nnatural language processing tasks. This report introduces TechGPT-2.0, a\nproject designed to enhance the capabilities of large language models\nspecifically in knowledge graph construction tasks, including named entity\nrecognition (NER) and relationship triple extraction (RTE) tasks in NLP\napplications. Additionally, it serves as a LLM accessible for research within\nthe Chinese open-source model community. We offer two 7B large language model\nweights and a QLoRA weight specialized for processing lengthy texts.Notably,\nTechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all\nfunctionalities from TechGPT-1.0, it exhibits robust text processing\ncapabilities, particularly in the domains of medicine and law. Furthermore, we\nintroduce new capabilities to the model, enabling it to process texts in\nvarious domains such as geographical areas, transportation, organizations,\nliterary works, biology, natural sciences, astronomical objects, and\narchitecture. These enhancements also fortified the model's adeptness in\nhandling hallucinations, unanswerable queries, and lengthy texts. This report\nprovides a comprehensive and detailed introduction to the full fine-tuning\nprocess on Huawei's Ascend servers, encompassing experiences in Ascend server\ndebugging, instruction fine-tuning data processing, and model training. Our\ncode is available at https://github.com/neukg/TechGPT-2.0\n",
        "title": "TechGPT-2.0: A large language model project to solve the task of\n  knowledge graph construction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04508",
        "abstract_url": "http://arxiv.org/abs/2401.04508",
        "authors": [
            {
                "last_name": "Schulze",
                "first_name": "Jan C."
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We use Koopman theory for data-driven model reduction of nonlinear dynamical\nsystems with controls. We propose generic model structures combining\ndelay-coordinate encoding of measurements and full-state decoding to integrate\nreduced Koopman modeling and state estimation. We present a deep-learning\napproach to train the proposed models. A case study demonstrates that our\napproach provides accurate control models and enables real-time capable\nnonlinear model predictive control of a high-purity cryogenic distillation\ncolumn.\n",
        "title": "Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated\n  Control Form and NMPC Case Study",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04509",
        "abstract_url": "http://arxiv.org/abs/2401.04509",
        "authors": [
            {
                "last_name": "Inenaga",
                "first_name": "Shunsuke"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "FL"
        ],
        "abstract": "  The linear-size suffix tries (LSTries) [Crochemore et al., TCS 2016] are a\nversion of suffix trees in which the edge labels are single characters, yet are\nable to perform pattern matching queries in optimal time. Instead of explicitly\nstoring the input text, LSTries have some extra non-branching internal nodes\ncalled type-2 nodes. The extended techniques are then used in the linear-size\ncompact directed acyclic word graphs (LCDAWGs) [Takagi et al. SPIRE 2017],\nwhich can be stored with $O(el(T)+er(T))$ space (i.e. without the text), where\n$el(T)$ and $er(T)$ are the numbers of left- and right-extensions of the\nmaximal repeats in the input text string $T$, respectively. In this paper, we\npresent simpler alternatives to the aforementioned indexing structures, called\nthe simplified LSTries (simLSTries) and the simplified LCDAWGs (simLCDAWGs), in\nwhich most of the type-2 nodes are removed. In particular, our simLCDAWGs\nrequire only $O(er(T))$ space and work on a weaker model of computation (i.e.\nthe pointer machine). This contrasts the $O(er(T))$-space CDAWG representation\nof [Belazzougui \\& Cunial, SPIRE 2017], which works on the word RAM.\n",
        "title": "Linear-size Suffix Tries and Linear-size CDAWGs Simplified and Improved",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04511",
        "abstract_url": "http://arxiv.org/abs/2401.04511",
        "authors": [
            {
                "last_name": "Dutta",
                "first_name": "Soumya"
            },
            {
                "last_name": "Ganapathy",
                "first_name": "Sriram"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "SD"
        ],
        "abstract": "  The problem of audio-to-audio (A2A) style transfer involves replacing the\nstyle features of the source audio with those from the target audio while\npreserving the content related attributes of the source audio. In this paper,\nwe propose an efficient approach, termed as Zero-shot Emotion Style Transfer\n(ZEST), that allows the transfer of emotional content present in the given\nsource audio with the one embedded in the target audio while retaining the\nspeaker and speech content from the source. The proposed system builds upon\ndecomposing speech into semantic tokens, speaker representations and emotion\nembeddings. Using these factors, we propose a framework to reconstruct the\npitch contour of the given speech signal and train a decoder that reconstructs\nthe speech signal. The model is trained using a self-supervision based\nreconstruction loss. During conversion, the emotion embedding is alone derived\nfrom the target audio, while rest of the factors are derived from the source\naudio. In our experiments, we show that, even without using parallel training\ndata or labels from the source or target audio, we illustrate zero shot emotion\ntransfer capabilities of the proposed ZEST model using objective and subjective\nquality evaluations.\n",
        "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04514",
        "abstract_url": "http://arxiv.org/abs/2401.04514",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Haochen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xin"
            },
            {
                "last_name": "Shen",
                "first_name": "Zhiqi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "IR",
            "LG"
        ],
        "abstract": "  In code search, the Generation-Augmented Retrieval (GAR) framework, which\ngenerates exemplar code snippets to augment queries, has emerged as a promising\nstrategy to address the principal challenge of modality misalignment between\ncode snippets and natural language queries, particularly with the demonstrated\ncode generation capabilities of Large Language Models (LLMs). Nevertheless, our\npreliminary investigations indicate that the improvements conferred by such an\nLLM-augmented framework are somewhat constrained. This limitation could\npotentially be ascribed to the fact that the generated codes, albeit\nfunctionally accurate, frequently display a pronounced stylistic deviation from\nthe ground truth code in the codebase. In this paper, we extend the\nfoundational GAR framework and propose a simple yet effective method that\nadditionally Rewrites the Code (ReCo) within the codebase for style\nnormalization. Experimental results demonstrate that ReCo significantly boosts\nretrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),\nand fine-tuned dense (up to 23.6%) retrieval settings in diverse search\nscenarios. To further elucidate the advantages of ReCo and stimulate research\nin code style normalization, we introduce Code Style Similarity, the first\nmetric tailored to quantify stylistic similarities in code. Notably, our\nempirical findings reveal the inadequacy of existing metrics in capturing\nstylistic nuances.\n",
        "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented\n  Code Search",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04515",
        "abstract_url": "http://arxiv.org/abs/2401.04515",
        "authors": [
            {
                "last_name": "Tikhomirov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Loukachevitch",
                "first_name": "Natalia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This article investigates a zero-shot approach to hypernymy prediction using\nlarge language models (LLMs). The study employs a method based on text\nprobability calculation, applying it to various generated prompts. The\nexperiments demonstrate a strong correlation between the effectiveness of\nlanguage model prompts and classic patterns, indicating that preliminary prompt\nselection can be carried out using smaller models before moving to larger ones.\nWe also explore prompts for predicting co-hyponyms and improving hypernymy\npredictions by augmenting prompts with additional information through\nautomatically identified co-hyponyms. An iterative approach is developed for\npredicting higher-level concepts, which further improves the quality on the\nBLESS dataset (MAP = 0.8).\n",
        "title": "Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with\n  Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04518",
        "abstract_url": "http://arxiv.org/abs/2401.04518",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Shichao"
            },
            {
                "last_name": "Li",
                "first_name": "Junlong"
            },
            {
                "last_name": "Yuan",
                "first_name": "Weizhe"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ruifeng"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Liu",
                "first_name": "Pengfei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Critique, as a natural language description for assessing the quality of\nmodel-generated content, has been proven to play an essential role in the\ntraining, evaluation, and refinement of Large Language Models (LLMs). However,\nthere is a lack of principled understanding in evaluating the quality of the\ncritique itself. In this paper, we pioneer the critique of critique, termed\nMetaCritique, which is a framework to evaluate the critique from two aspects,\ni.e., factuality as precision score and comprehensiveness as recall score. We\ncalculate the harmonic mean of precision and recall as the overall rating\ncalled F1 score. To obtain a reliable evaluation outcome, we propose Atomic\nInformation Units (AIUs), which describe the critique in a more fine-grained\nmanner. MetaCritique takes each AIU into account and aggregates each AIU's\njudgment for the overall score. Moreover, given the evaluation process involves\nintricate reasoning, our MetaCritique provides a natural language rationale to\nsupport each judgment. We construct a meta-evaluation dataset containing 300\ncritiques (2653 AIUs) across four tasks (question answering, reasoning,\nentailment, and summarization), and we conduct a comparative study to\ndemonstrate the feasibility and effectiveness. Experiments also show superior\ncritique judged by MetaCritique leads to better refinement, indicating\ngenerative artificial intelligence indeed has the potential to be significantly\nadvanced with our MetaCritique. We will release relevant code and\nmeta-evaluation datasets at https://github.com/GAIR-NLP/MetaCritique.\n",
        "title": "The Critique of Critique",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04519",
        "abstract_url": "http://arxiv.org/abs/2401.04519",
        "authors": [
            {
                "last_name": "Gallistl",
                "first_name": "Dietmar"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  It is shown how mixed finite element methods for symmetric positive definite\neigenvalue problems related to partial differential operators can provide\nguaranteed lower eigenvalue bounds. The method is based on a classical\ncompatibility condition (inclusion of kernels) of the mixed scheme and on local\nconstants related to compact embeddings, which are often known explicitly.\nApplications include scalar second-order elliptic operators, linear elasticity,\nand the Steklov eigenvalue problem.\n",
        "title": "Mixed methods and lower eigenvalue bounds",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04522",
        "abstract_url": "http://arxiv.org/abs/2401.04522",
        "authors": [
            {
                "last_name": "Saidov",
                "first_name": "Marat"
            },
            {
                "last_name": "Bakalova",
                "first_name": "Aleksandra"
            },
            {
                "last_name": "Taktasheva",
                "first_name": "Ekaterina"
            },
            {
                "last_name": "Mikhailov",
                "first_name": "Vladislav"
            },
            {
                "last_name": "Artemova",
                "first_name": "Ekaterina"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The evaluation of Natural Language Generation (NLG) models has gained\nincreased attention, urging the development of metrics that evaluate various\naspects of generated text. LUNA addresses this challenge by introducing a\nunified interface for 20 NLG evaluation metrics. These metrics are categorized\nbased on their reference-dependence and the type of text representation they\nemploy, from string-based n-gram overlap to the utilization of static\nembeddings and pre-trained language models.\n  The straightforward design of LUNA allows for easy extension with novel\nmetrics, requiring just a few lines of code. LUNA offers a user-friendly tool\nfor evaluating generated texts.\n",
        "title": "LUNA: A Framework for Language Understanding and Naturalness Assessment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04524",
        "abstract_url": "http://arxiv.org/abs/2401.04524",
        "authors": [
            {
                "last_name": "Litvinov",
                "first_name": "Oleg"
            },
            {
                "last_name": "Sekuli\u0107",
                "first_name": "Ivan"
            },
            {
                "last_name": "Aliannejadi",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Crestani",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Clarifying user's information needs is an essential component of modern\nsearch systems. While most of the approaches for constructing clarifying\nprompts rely on query facets, the impact of the quality of the facets is\nrelatively unexplored. In this work, we concentrate on facet quality through\nthe notion of facet coherency and assess its importance for overall usefulness\nfor clarification in search. We find that existing evaluation procedures do not\naccount for facet coherency, as evident by the poor correlation of coherency\nwith automated metrics. Moreover, we propose a coherency classifier and assess\nthe prevalence of incoherent facets in a well-established dataset on\nclarification. Our findings can serve as motivation for future work on the\ntopic.\n",
        "title": "Analyzing Coherency in Facet-based Clarification Prompt Generation for\n  Search",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04531",
        "abstract_url": "http://arxiv.org/abs/2401.04531",
        "authors": [
            {
                "last_name": "Fenogenova",
                "first_name": "Alena"
            },
            {
                "last_name": "Chervyakov",
                "first_name": "Artem"
            },
            {
                "last_name": "Martynov",
                "first_name": "Nikita"
            },
            {
                "last_name": "Kozlova",
                "first_name": "Anastasia"
            },
            {
                "last_name": "Tikhonova",
                "first_name": "Maria"
            },
            {
                "last_name": "Akhmetgareeva",
                "first_name": "Albina"
            },
            {
                "last_name": "Emelyanov",
                "first_name": "Anton"
            },
            {
                "last_name": "Shevelev",
                "first_name": "Denis"
            },
            {
                "last_name": "Lebedev",
                "first_name": "Pavel"
            },
            {
                "last_name": "Sinev",
                "first_name": "Leonid"
            },
            {
                "last_name": "Isaeva",
                "first_name": "Ulyana"
            },
            {
                "last_name": "Kolomeytseva",
                "first_name": "Katerina"
            },
            {
                "last_name": "Moskovskiy",
                "first_name": "Daniil"
            },
            {
                "last_name": "Goncharova",
                "first_name": "Elizaveta"
            },
            {
                "last_name": "Savushkin",
                "first_name": "Nikita"
            },
            {
                "last_name": "Mikhailova",
                "first_name": "Polina"
            },
            {
                "last_name": "Dimitrov",
                "first_name": "Denis"
            },
            {
                "last_name": "Panchenko",
                "first_name": "Alexander"
            },
            {
                "last_name": "Markov",
                "first_name": "Sergei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.\n",
        "title": "MERA: A Comprehensive LLM Evaluation in Russian",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04534",
        "abstract_url": "http://arxiv.org/abs/2401.04534",
        "authors": [
            {
                "last_name": "Kaszuba",
                "first_name": "Sara"
            },
            {
                "last_name": "Sabbella",
                "first_name": "Sandeep Reddy"
            },
            {
                "last_name": "Leotta",
                "first_name": "Francesco"
            },
            {
                "last_name": "Serrarens",
                "first_name": "Pascal"
            },
            {
                "last_name": "Nardi",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  In recent years, an increasing number of Human-Robot Interaction (HRI)\napproaches have been implemented and evaluated in Virtual Reality (VR), as it\nallows to speed-up design iterations and makes it safer for the final user to\nevaluate and master the HRI primitives. However, identifying the most suitable\nVR experience is not straightforward. In this work, we evaluate how, in a smart\nagriculture scenario, immersive and non-immersive VR are perceived by users\nwith respect to a speech act understanding task. In particular, we collect\nopinions and suggestions from the 81 participants involved in both experiments\nto highlight the strengths and weaknesses of these different experiences.\n",
        "title": "Testing Human-Robot Interaction in Virtual Reality: Experience from a\n  Study on Speech Act Classification",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04535",
        "abstract_url": "http://arxiv.org/abs/2401.04535",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Zhao"
            },
            {
                "last_name": "Duan",
                "first_name": "Chenguang"
            },
            {
                "last_name": "Jiao",
                "first_name": "Yuling"
            },
            {
                "last_name": "Yang",
                "first_name": "Jerry Zhijian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  We propose SDORE, a semi-supervised deep Sobolev regressor, for the\nnonparametric estimation of the underlying regression function and its\ngradient. SDORE employs deep neural networks to minimize empirical risk with\ngradient norm regularization, allowing computation of the gradient norm on\nunlabeled data. We conduct a comprehensive analysis of the convergence rates of\nSDORE and establish a minimax optimal rate for the regression function.\nCrucially, we also derive a convergence rate for the associated plug-in\ngradient estimator, even in the presence of significant domain shift. These\ntheoretical findings offer valuable prior guidance for selecting regularization\nparameters and determining the size of the neural network, while showcasing the\nprovable advantage of leveraging unlabeled data in semi-supervised learning. To\nthe best of our knowledge, SDORE is the first provable neural network-based\napproach that simultaneously estimates the regression function and its\ngradient, with diverse applications including nonparametric variable selection\nand inverse problems. The effectiveness of SDORE is validated through an\nextensive range of numerical simulations and real data analysis.\n",
        "title": "Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection\n  and Beyond",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04536",
        "abstract_url": "http://arxiv.org/abs/2401.04536",
        "authors": [
            {
                "last_name": "Davidson",
                "first_name": "Tim R."
            },
            {
                "last_name": "Veselovsky",
                "first_name": "Veniamin"
            },
            {
                "last_name": "Josifoski",
                "first_name": "Martin"
            },
            {
                "last_name": "Peyrard",
                "first_name": "Maxime"
            },
            {
                "last_name": "Bosselut",
                "first_name": "Antoine"
            },
            {
                "last_name": "Kosinski",
                "first_name": "Michal"
            },
            {
                "last_name": "West",
                "first_name": "Robert"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Companies, organizations, and governments increasingly exploit Language\nModels' (LM) remarkable capability to display agent-like behavior. As LMs are\nadopted to perform tasks with growing autonomy, there exists an urgent need for\nreliable and scalable evaluation benchmarks. Current, predominantly static LM\nbenchmarks are ill-suited to evaluate such dynamic applications. Thus, we\npropose jointly evaluating LM performance and alignment through the lenses of\nnegotiation games. We argue that this common task better reflects real-world\ndeployment conditions while offering insights into LMs' decision-making\nprocesses. Crucially, negotiation games allow us to study multi-turn, and\ncross-model interactions, modulate complexity, and side-step accidental data\nleakage in evaluation. We report results for six publicly accessible LMs from\nseveral major providers on a variety of negotiation games, evaluating both\nself-play and cross-play performance. Noteworthy findings include: (i)\nopen-source models are currently unable to complete these tasks; (ii)\ncooperative bargaining games prove challenging; and (iii) the most powerful\nmodels do not always \"win\".\n",
        "title": "Evaluating Language Model Agency through Negotiations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04538",
        "abstract_url": "http://arxiv.org/abs/2401.04538",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shaohua"
            },
            {
                "last_name": "Su",
                "first_name": "Zhendong"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "PL",
            "SE"
        ],
        "abstract": "  In this paper, we propose a testing framework for validating sanitizer\nimplementations in compilers. Our core components are (1) a program generator\nspecifically designed for producing programs containing undefined behavior\n(UB), and (2) a novel test oracle for sanitizer testing. The program generator\nemploys Shadow Statement Insertion, a general and effective approach for\nintroducing UB into a valid seed program. The generated UB programs are\nsubsequently utilized for differential testing of multiple sanitizer\nimplementations. Nevertheless, discrepant sanitizer reports may stem from\neither compiler optimization or sanitizer bugs. To accurately determine if a\ndiscrepancy is caused by sanitizer bugs, we introduce a new test oracle called\ncrash-site mapping. We have incorporated our techniques into UBfuzz, a\npractical tool for testing sanitizers. Over a five-month testing period, UBfuzz\nsuccessfully found 31 bugs in both GCC and LLVM sanitizers. These bugs reveal\nthe serious false negative problems in sanitizers, where certain UBs in\nprograms went unreported. This research paves the way for further investigation\nin this crucial area of study.\n",
        "title": "UBfuzz: Finding Bugs in Sanitizer Implementations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04539",
        "abstract_url": "http://arxiv.org/abs/2401.04539",
        "authors": [
            {
                "last_name": "Mei",
                "first_name": "Haoran"
            },
            {
                "last_name": "Peng",
                "first_name": "Limei"
            },
            {
                "last_name": "Ho",
                "first_name": "Pin-Han"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  This article introduces a novel framework of multi-user detection (MUD) for\nK-repetition grant-free non-orthogonal multiple access (K-GF-NOMA), called\n$\\alpha$ iterative interference cancellation diversity slotted aloha\n($\\alpha$-IIC-DSA). The proposed framework targets at a simple yet effective\ndecoding process where the AP can intelligently exploit the correlation among\nsignals received at different resource blocks (RBs) so as to generate required\nmulti-access interference (MAI) for realizing the signal-interference\ncancellation (SIC) based MUD. By keeping all operation and hardware complexity\nat the access point (AP), the proposed framework is applicable to the scenarios\nwith random and uncoordinated access by numerous miniature mMTC devices\n(MTCDs). Numerical experiments are conducted to gain deep understanding on the\nperformance of launching the proposed framework for K-GF-NOMA.\n",
        "title": "A Novel Framework of K-repetition Grant-free Access via Diversity\n  Slotted Aloha (DSA)",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04543",
        "abstract_url": "http://arxiv.org/abs/2401.04543",
        "authors": [
            {
                "last_name": "Zhan",
                "first_name": "Xiao"
            },
            {
                "last_name": "Abdi",
                "first_name": "Noura"
            },
            {
                "last_name": "Seymour",
                "first_name": "William"
            },
            {
                "last_name": "Such",
                "first_name": "Jose"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  AI assistants such as Alexa, Google Assistant, and Siri, are making their way\ninto the healthcare sector, offering a convenient way for users to access\ndifferent healthcare services. Trust is a vital factor in the uptake of\nhealthcare services, but the factors affecting trust in voice assistants used\nfor healthcare are under-explored and this specialist domain introduces\nadditional requirements. This study explores the effects of different\nfunctional, personal, and risk factors on trust in and adoption of healthcare\nvoice AI assistants (HVAs), generating a partial least squares structural model\nfrom a survey of 300 voice assistant users. Our results indicate that trust in\nHVAs can be significantly explained by functional factors (usefulness, content\ncredibility, quality of service relative to a healthcare professional),\ntogether with security, and privacy risks and personal stance in technology. We\nalso discuss differences in terms of trust between HVAs and general-purpose\nvoice assistants as well as implications that are unique to HVAs.\n",
        "title": "Healthcare Voice AI Assistants: Factors Influencing Trust and Intention\n  to Use",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04545",
        "abstract_url": "http://arxiv.org/abs/2401.04545",
        "authors": [
            {
                "last_name": "Sabbella",
                "first_name": "Sandeep Reddy"
            },
            {
                "last_name": "Kaszuba",
                "first_name": "Sara"
            },
            {
                "last_name": "Leotta",
                "first_name": "Francesco"
            },
            {
                "last_name": "Serrarens",
                "first_name": "Pascal"
            },
            {
                "last_name": "Nardi",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  Human-Robot Interaction (HRI) has become increasingly important as robots are\nbeing integrated into various aspects of daily life. One key aspect of HRI is\ngesture recognition, which allows robots to interpret and respond to human\ngestures in real-time. Gesture recognition plays an important role in\nnon-verbal communication in HRI. To this aim, there is ongoing research on how\nsuch non-verbal communication can strengthen verbal communication and improve\nthe system's overall efficiency, thereby enhancing the user experience with the\nrobot. However, several challenges need to be addressed in gesture recognition\nsystems, which include data generation, transferability, scalability,\ngeneralizability, standardization, and lack of benchmarking of the gestural\nsystems. In this preliminary paper, we want to address the challenges of data\ngeneration using virtual reality simulations and standardization issues by\npresenting gestures to some commands that can be used as a standard in ground\nrobots.\n",
        "title": "Evaluating Gesture Recognition in Virtual Reality",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04550",
        "abstract_url": "http://arxiv.org/abs/2401.04550",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shengli"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Lin",
                "first_name": "Sen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Although deep convolutional neural networks have achieved remarkable success\nin removing synthetic fog, it is essential to be able to process images taken\nin complex foggy conditions, such as dense or non-homogeneous fog, in the real\nworld. However, the haze distribution in the real world is complex, and\ndownsampling can lead to color distortion or loss of detail in the output\nresults as the resolution of a feature map or image resolution decreases. In\naddition to the challenges of obtaining sufficient training data, overfitting\ncan also arise in deep learning techniques for foggy image processing, which\ncan limit the generalization abilities of the model, posing challenges for its\npractical applications in real-world scenarios. Considering these issues, this\npaper proposes a Transformer-based wavelet network (WaveletFormerNet) for\nreal-world foggy image recovery. We embed the discrete wavelet transform into\nthe Vision Transformer by proposing the WaveletFormer and IWaveletFormer\nblocks, aiming to alleviate texture detail loss and color distortion in the\nimage due to downsampling. We introduce parallel convolution in the Transformer\nblock, which allows for the capture of multi-frequency information in a\nlightweight mechanism. Additionally, we have implemented a feature aggregation\nmodule (FAM) to maintain image resolution and enhance the feature extraction\ncapacity of our model, further contributing to its impressive performance in\nreal-world foggy image recovery tasks. Extensive experiments demonstrate that\nour WaveletFormerNet performs better than state-of-the-art methods, as shown\nthrough quantitative and qualitative evaluations of minor model complexity.\nAdditionally, our satisfactory results on real-world dust removal and\napplication tests showcase the superior generalization ability and improved\nperformance of WaveletFormerNet in computer vision-related applications.\n",
        "title": "WaveletFormerNet: A Transformer-based Wavelet Network for Real-world\n  Non-homogeneous and Dense Fog Removal",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04552",
        "abstract_url": "http://arxiv.org/abs/2401.04552",
        "authors": [
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            },
            {
                "last_name": "Copik",
                "first_name": "Marcin"
            },
            {
                "last_name": "Beckman",
                "first_name": "Pete"
            },
            {
                "last_name": "Jones",
                "first_name": "Andrew"
            },
            {
                "last_name": "Foster",
                "first_name": "Ian"
            },
            {
                "last_name": "Parashar",
                "first_name": "Manish"
            },
            {
                "last_name": "Reed",
                "first_name": "Daniel"
            },
            {
                "last_name": "Troyer",
                "first_name": "Matthias"
            },
            {
                "last_name": "Schulthess",
                "first_name": "Thomas"
            },
            {
                "last_name": "Ernst",
                "first_name": "Dan"
            },
            {
                "last_name": "Dongarra",
                "first_name": "Jack"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  HPC and Cloud have evolved independently, specializing their innovations into\nperformance or productivity. Acceleration as a Service (XaaS) is a recipe to\nempower both fields with a shared execution platform that provides transparent\naccess to computing resources, regardless of the underlying cloud or HPC\nservice provider. Bridging HPC and cloud advancements, XaaS presents a unified\narchitecture built on performance-portable containers. Our converged model\nconcentrates on low-overhead, high-performance communication and computing,\ntargeting resource-intensive workloads from climate simulations to machine\nlearning. XaaS lifts the restricted allocation model of Function-as-a-Service\n(FaaS), allowing users to benefit from the flexibility and efficient resource\nutilization of serverless while supporting long-running and\nperformance-sensitive workloads from HPC.\n",
        "title": "XaaS: Acceleration as a Service to Enable Productive High-Performance\n  Cloud Computing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04553",
        "abstract_url": "http://arxiv.org/abs/2401.04553",
        "authors": [
            {
                "last_name": "Radhakrishnan",
                "first_name": "Adityanarayanan"
            },
            {
                "last_name": "Belkin",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Drusvyatskiy",
                "first_name": "Dmitriy"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  A fundamental problem in machine learning is to understand how neural\nnetworks make accurate predictions, while seemingly bypassing the curse of\ndimensionality. A possible explanation is that common training algorithms for\nneural networks implicitly perform dimensionality reduction - a process called\nfeature learning. Recent work posited that the effects of feature learning can\nbe elicited from a classical statistical estimator called the average gradient\nouter product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as\nan algorithm that explicitly performs feature learning by alternating between\n(1) reweighting the feature vectors by the AGOP and (2) learning the prediction\nfunction in the transformed space. In this work, we develop the first\ntheoretical guarantees for how RFM performs dimensionality reduction by\nfocusing on the class of overparametrized problems arising in sparse linear\nregression and low-rank matrix recovery. Specifically, we show that RFM\nrestricted to linear models (lin-RFM) generalizes the well-studied Iteratively\nReweighted Least Squares (IRLS) algorithm. Our results shed light on the\nconnection between feature learning in neural networks and classical sparse\nrecovery algorithms. In addition, we provide an implementation of lin-RFM that\nscales to matrices with millions of missing entries. Our implementation is\nfaster than the standard IRLS algorithm as it is SVD-free. It also outperforms\ndeep linear networks for sparse linear regression and low-rank matrix\ncompletion.\n",
        "title": "Linear Recursive Feature Machines provably recover low-rank matrices",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04554",
        "abstract_url": "http://arxiv.org/abs/2401.04554",
        "authors": [
            {
                "last_name": "Goedgebeur",
                "first_name": "Jan"
            },
            {
                "last_name": "Noguchi",
                "first_name": "Kenta"
            },
            {
                "last_name": "Renders",
                "first_name": "Jarne"
            },
            {
                "last_name": "Zamfirescu",
                "first_name": "Carol T."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM"
        ],
        "abstract": "  In a given graph, a HIST is a spanning tree without $2$-valent vertices.\nMotivated by developing a better understanding of HIST-free graphs, i.e. graphs\ncontaining no HIST, in this article's first part we study HIST-critical graphs,\ni.e. HIST-free graphs in which every vertex-deleted subgraph does contain a\nHIST (e.g. a triangle). We give an almost complete characterisation of the\norders for which these graphs exist and present an infinite family of planar\nexamples which are $3$-connected and in which nearly all vertices are\n$4$-valent. This leads naturally to the second part in which we investigate\nplanar $4$-regular graphs with and without HISTs, motivated by a conjecture of\nMalkevitch, which we computationally verify up to order $22$. First we\nenumerate HISTs in antiprisms, whereafter we present planar $4$-regular graphs\nwith and without HISTs, obtained via line graphs.\n",
        "title": "HIST-Critical Graphs and Malkevitch's Conjecture",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04556",
        "abstract_url": "http://arxiv.org/abs/2401.04556",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Qiulin"
            },
            {
                "last_name": "Ishii",
                "first_name": "Hideaki"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper studies novel epidemic spreading problems influenced by opinion\nevolution in social networks, where the opinions reflect the public health\nconcerns. A coupled bilayer network is proposed, where the epidemics spread\nover several communities through a physical network layer while the opinions\nevolve over the same communities through a social network layer. The epidemic\nspreading process is described by a susceptible-infected-vigilant (SIV) model,\nwhich introduces opinion-dependent epidemic vigilance state compared with the\nclassical epidemic models. The opinion process is modeled by a polar opinion\ndynamics model, which includes infection prevalence and human stubbornness into\nthe opinion evolution. By introducing an opinion-dependent reproduction number,\nwe analyze the stability of disease-free and endemic equilibria and derive\nsufficient conditions for their global asymptotic stability. We also discuss\nthe mutual effects between epidemic eradication and opinion consensus, and the\npossibility of suppressing epidemic by intervening in the opinions or\nimplementing public health strategies. Simulations are conducted to verify the\ntheoretical results and demonstrate the feasibility of epidemic suppression.\n",
        "title": "On a Discrete-Time Networked SIV Epidemic Model with Polar Opinion\n  Dynamics",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04558",
        "abstract_url": "http://arxiv.org/abs/2401.04558",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Akama",
                "first_name": "Taketo"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            ""
        ],
        "abstract": "  GANStrument, exploiting GANs with a pitch-invariant feature extractor and\ninstance conditioning technique, has shown remarkable capabilities in\nsynthesizing realistic instrument sounds. To further improve the reconstruction\nability and pitch accuracy to enhance the editability of user-provided sound,\nwe propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to\nmodulate the weights of a pre-trained GANStrument generator, given a one-shot\nsound as input. The hypernetwork modulation provides feedback for the generator\nin the reconstruction of the input sound. In addition, we take advantage of an\nadversarial fine-tuning scheme for the hypernetwork to improve the\nreconstruction fidelity and generation diversity of the generator. Experimental\nresults show that the proposed model not only enhances the generation\ncapability of GANStrument but also significantly improves the editability of\nsynthesized sounds. Audio examples are available at the online demo page.\n",
        "title": "HyperGANStrument: Instrument Sound Synthesis and Editing with\n  Pitch-Invariant Hypernetworks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04560",
        "abstract_url": "http://arxiv.org/abs/2401.04560",
        "authors": [
            {
                "last_name": "Hwang",
                "first_name": "Gyutae"
            },
            {
                "last_name": "Lee",
                "first_name": "Sang Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively.\n",
        "title": "Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04567",
        "abstract_url": "http://arxiv.org/abs/2401.04567",
        "authors": [
            {
                "last_name": "Mariot",
                "first_name": "Luca"
            },
            {
                "last_name": "Leporati",
                "first_name": "Alberto"
            },
            {
                "last_name": "Manzoni",
                "first_name": "Luca"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "CR"
        ],
        "abstract": "  A Particle Swarm Optimizer for the search of balanced Boolean functions with\ngood cryptographic properties is proposed in this paper. The algorithm is a\nmodified version of the permutation PSO by Hu, Eberhart and Shi which preserves\nthe Hamming weight of the particles positions, coupled with the Hill Climbing\nmethod devised by Millan, Clark and Dawson to improve the nonlinearity and\ndeviation from correlation immunity of Boolean functions. The parameters for\nthe PSO velocity equation are tuned by means of two meta-optimization\ntechniques, namely Local Unimodal Sampling (LUS) and Continuous Genetic\nAlgorithms (CGA), finding that CGA produces better results. Using the\nCGA-evolved parameters, the PSO algorithm is then run on the spaces of Boolean\nfunctions from $n=7$ to $n=12$ variables. The results of the experiments are\nreported, observing that this new PSO algorithm generates Boolean functions\nfeaturing similar or better combinations of nonlinearity, correlation immunity\nand propagation criterion with respect to the ones obtained by other\noptimization methods.\n",
        "title": "A Discrete Particle Swarm Optimizer for the Design of Cryptographic\n  Boolean Functions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04570",
        "abstract_url": "http://arxiv.org/abs/2401.04570",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Weijin"
            },
            {
                "last_name": "Sha",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Yang",
                "first_name": "Huihua"
            },
            {
                "last_name": "Jiang",
                "first_name": "Rongcai"
            },
            {
                "last_name": "Li",
                "first_name": "Zhanying"
            },
            {
                "last_name": "Liu",
                "first_name": "Wentao"
            },
            {
                "last_name": "Su",
                "first_name": "Ruisheng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Hemorrhagic Stroke (HS) has a rapid onset and is a serious condition that\nposes a great health threat. Promptly and accurately delineating the bleeding\nregion and estimating the volume of bleeding in Computer Tomography (CT) images\ncan assist clinicians in treatment planning, leading to improved treatment\noutcomes for patients. In this paper, a cascaded 3D model is constructed based\non UNet to perform a two-stage segmentation of the hemorrhage area in CT images\nfrom rough to fine, and the hemorrhage volume is automatically calculated from\nthe segmented area. On a dataset with 341 cases of hemorrhagic stroke CT scans,\nthe proposed model provides high-quality segmentation outcome with higher\naccuracy (DSC 85.66%) and better computation efficiency (6.2 second per sample)\nwhen compared to the traditional Tada formula with respect to hemorrhage volume\nestimation.\n",
        "title": "An Automatic Cascaded Model for Hemorrhagic Stroke Segmentation and\n  Hemorrhagic Volume Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04572",
        "abstract_url": "http://arxiv.org/abs/2401.04572",
        "authors": [
            {
                "last_name": "Amadori",
                "first_name": "Pierluigi Vito"
            },
            {
                "last_name": "Bradley",
                "first_name": "Timothy"
            },
            {
                "last_name": "Spick",
                "first_name": "Ryan"
            },
            {
                "last_name": "Moss",
                "first_name": "Guy"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Game development is a long process that involves many stages before a product\nis ready for the market. Human play testing is among the most time consuming,\nas testers are required to repeatedly perform tasks in the search for errors in\nthe code. Therefore, automated testing is seen as a key technology for the\ngaming industry, as it would dramatically improve development costs and\nefficiency. Toward this end, we propose EVOLUTE, a novel imitation\nlearning-based architecture that combines behavioural cloning (BC) with energy\nbased models (EBMs). EVOLUTE is a two-stream ensemble model that splits the\naction space of autonomous agents into continuous and discrete tasks. The EBM\nstream handles the continuous tasks, to have a more refined and adaptive\ncontrol, while the BC stream handles discrete actions, to ease training. We\nevaluate the performance of EVOLUTE in a shooting-and-driving game, where the\nagent is required to navigate and continuously identify targets to attack. The\nproposed model has higher generalisation capabilities than standard BC\napproaches, showing a wider range of behaviours and higher performances. Also,\nEVOLUTE is easier to train than a pure end-to-end EBM model, as discrete tasks\ncan be quite sparse in the dataset and cause model training to explore a much\nwider set of possible actions while training.\n",
        "title": "Robust Imitation Learning for Automated Game Testing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04575",
        "abstract_url": "http://arxiv.org/abs/2401.04575",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Yatong"
            },
            {
                "last_name": "Garg",
                "first_name": "Utsav"
            },
            {
                "last_name": "Shanker",
                "first_name": "Apaar"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haoming"
            },
            {
                "last_name": "Parajuli",
                "first_name": "Samyak"
            },
            {
                "last_name": "Bas",
                "first_name": "Erhan"
            },
            {
                "last_name": "Filipovic",
                "first_name": "Isidora"
            },
            {
                "last_name": "Chu",
                "first_name": "Amelia N."
            },
            {
                "last_name": "Fomitcheva",
                "first_name": "Eugenia D"
            },
            {
                "last_name": "Branson",
                "first_name": "Elliot"
            },
            {
                "last_name": "Kim",
                "first_name": "Aerin"
            },
            {
                "last_name": "Sojoudi",
                "first_name": "Somayeh"
            },
            {
                "last_name": "Cho",
                "first_name": "Kyunghyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Vision and vision-language applications of neural networks, such as image\nclassification and captioning, rely on large-scale annotated datasets that\nrequire non-trivial data-collecting processes. This time-consuming endeavor\nhinders the emergence of large-scale datasets, limiting researchers and\npractitioners to a small number of choices. Therefore, we seek more efficient\nways to collect and annotate images. Previous initiatives have gathered\ncaptions from HTML alt-texts and crawled social media postings, but these data\nsources suffer from noise, sparsity, or subjectivity. For this reason, we turn\nto commercial shopping websites whose data meet three criteria: cleanliness,\ninformativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset,\na large-scale public dataset with 15 million image-caption pairs from publicly\navailable e-commerce websites. When compared with existing general-domain\ndatasets, the LGS images focus on the foreground object and have less complex\nbackgrounds. Our experiments on LGS show that the classifiers trained on\nexisting benchmark datasets do not readily generalize to e-commerce data, while\nspecific self-supervised visual feature extractors can better generalize.\nFurthermore, LGS's high-quality e-commerce-focused images and bimodal nature\nmake it advantageous for vision-language bi-modal tasks: LGS enables\nimage-captioning models to generate richer captions and helps text-to-image\ngeneration models achieve e-commerce style transfer.\n",
        "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual\n  Concept Understanding",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04577",
        "abstract_url": "http://arxiv.org/abs/2401.04577",
        "authors": [
            {
                "last_name": "Ziv",
                "first_name": "Alon"
            },
            {
                "last_name": "Gat",
                "first_name": "Itai"
            },
            {
                "last_name": "Lan",
                "first_name": "Gael Le"
            },
            {
                "last_name": "Remez",
                "first_name": "Tal"
            },
            {
                "last_name": "Kreuk",
                "first_name": "Felix"
            },
            {
                "last_name": "D\u00e9fossez",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Copet",
                "first_name": "Jade"
            },
            {
                "last_name": "Synnaeve",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Adi",
                "first_name": "Yossi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "LG",
            ""
        ],
        "abstract": "  We introduce MAGNeT, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work,\nMAGNeT is comprised of a single-stage, non-autoregressive transformer. During\ntraining, we predict spans of masked tokens obtained from a masking scheduler,\nwhile during inference we gradually construct the output sequence using several\ndecoding steps. To further enhance the quality of the generated audio, we\nintroduce a novel rescoring method in which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNeT, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of\nMAGNeT, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel. We demonstrate the efficiency of\nMAGNeT for the task of text-to-music and text-to-audio generation and conduct\nan extensive empirical evaluation, considering both objective metrics and human\nstudies. The proposed approach is comparable to the evaluated baselines, while\nbeing significantly faster (x7 faster than the autoregressive baseline).\nThrough ablation studies and analysis, we shed light on the importance of each\nof the components comprising MAGNeT, together with pointing to the trade-offs\nbetween autoregressive and non-autoregressive modeling, considering latency,\nthroughput, and generation quality. Samples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.\n",
        "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04578",
        "abstract_url": "http://arxiv.org/abs/2401.04578",
        "authors": [
            {
                "last_name": "Abbas",
                "first_name": "Amro"
            },
            {
                "last_name": "Rusak",
                "first_name": "Evgenia"
            },
            {
                "last_name": "Tirumala",
                "first_name": "Kushal"
            },
            {
                "last_name": "Brendel",
                "first_name": "Wieland"
            },
            {
                "last_name": "Chaudhuri",
                "first_name": "Kamalika"
            },
            {
                "last_name": "Morcos",
                "first_name": "Ari S."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Utilizing massive web-scale datasets has led to unprecedented performance\ngains in machine learning models, but also imposes outlandish compute\nrequirements for their training. In order to improve training and data\nefficiency, we here push the limits of pruning large-scale multimodal datasets\nfor training CLIP-style models. Today's most effective pruning method on\nImageNet clusters data samples into separate concepts according to their\nembedding and prunes away the most prototypical samples. We scale this approach\nto LAION and improve it by noting that the pruning rate should be\nconcept-specific and adapted to the complexity of the concept. Using a simple\nand intuitive complexity measure, we are able to reduce the training cost to a\nquarter of regular training. By filtering from the LAION dataset, we find that\ntraining on a smaller set of high-quality data can lead to higher performance\nwith significantly lower training costs. More specifically, we are able to\noutperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot\naccuracy by 1.1p.p. while only using 27.7% of the data and training compute.\nDespite a strong reduction in training cost, we also see improvements on\nImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium\nbenchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a\ncompetitive average zero-shot accuracy on 38 evaluation tasks.\n",
        "title": "Effective pruning of web-scale datasets based on complexity of concept\n  clusters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04579",
        "abstract_url": "http://arxiv.org/abs/2401.04579",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuqian"
            },
            {
                "last_name": "Xue",
                "first_name": "Tengfei"
            },
            {
                "last_name": "Zekelman",
                "first_name": "Leo"
            },
            {
                "last_name": "Makris",
                "first_name": "Nikos"
            },
            {
                "last_name": "Rathi",
                "first_name": "Yogesh"
            },
            {
                "last_name": "Cai",
                "first_name": "Weidong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Donnell",
                "first_name": "Lauren J. O'"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Large datasets often contain multiple distinct feature sets, or views, that\noffer complementary information that can be exploited by multi-view learning\nmethods to improve results. We investigate anatomical multi-view data, where\neach brain anatomical structure is described with multiple feature sets. In\nparticular, we focus on sets of white matter microstructure and connectivity\nfeatures from diffusion MRI, as well as sets of gray matter area and thickness\nfeatures from structural MRI. We investigate machine learning methodology that\napplies multi-view approaches to improve the prediction of non-imaging\nphenotypes, including demographics (age), motor (strength), and cognition\n(picture vocabulary). We present an explainable multi-view network (EMV-Net)\nthat can use different anatomical views to improve prediction performance. In\nthis network, each individual anatomical view is processed by a view-specific\nfeature extractor and the extracted information from each view is fused using a\nlearnable weight. This is followed by a wavelet transform-based module to\nobtain complementary information across views which is then applied to\ncalibrate the view-specific information. Additionally, the calibrator produces\nan attention-based calibration score to indicate anatomical structures'\nimportance for interpretation.\n",
        "title": "A Deep Network for Explainable Prediction of Non-Imaging Phenotypes\n  using Anatomical Multi-View Data",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04585",
        "abstract_url": "http://arxiv.org/abs/2401.04585",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xuewen"
            },
            {
                "last_name": "Li",
                "first_name": "Zhikai"
            },
            {
                "last_name": "Xiao",
                "first_name": "Junrui"
            },
            {
                "last_name": "Gu",
                "first_name": "Qingyi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion models have achieved great success in image generation tasks\nthrough iterative noise estimation. However, the heavy denoising process and\ncomplex neural networks hinder their low-latency applications in real-world\nscenarios. Quantization can effectively reduce model complexity, and\npost-training quantization (PTQ), which does not require fine-tuning, is highly\npromising in accelerating the denoising process. Unfortunately, we find that\ndue to the highly dynamic distribution of activations in different denoising\nsteps, existing PTQ methods for diffusion models suffer from distribution\nmismatch issues at both calibration sample level and reconstruction output\nlevel, which makes the performance far from satisfactory, especially in low-bit\ncases. In this paper, we propose Enhanced Distribution Alignment for\nPost-Training Quantization of Diffusion Models (EDA-DM) to address the above\nissues. Specifically, at the calibration sample level, we select calibration\nsamples based on the density and diversity in the latent space, thus\nfacilitating the alignment of their distribution with the overall samples; and\nat the reconstruction output level, we propose Fine-grained Block\nReconstruction, which can align the outputs of the quantized model and the\nfull-precision model at different network granularity. Extensive experiments\ndemonstrate that EDA-DM outperforms the existing post-training quantization\nframeworks in both unconditional and conditional generation scenarios. At\nlow-bit precision, the quantized models with our method even outperform the\nfull-precision models on most datasets.\n",
        "title": "Enhanced Distribution Alignment for Post-Training Quantization of\n  Diffusion Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04592",
        "abstract_url": "http://arxiv.org/abs/2401.04592",
        "authors": [
            {
                "last_name": "Arcan",
                "first_name": "Mihael"
            },
            {
                "last_name": "Niland",
                "first_name": "Paul-David"
            },
            {
                "last_name": "Delahunty",
                "first_name": "Fionn"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Mental health challenges pose considerable global burdens on individuals and\ncommunities. Recent data indicates that more than 20% of adults may encounter\nat least one mental disorder in their lifetime. On the one hand, the\nadvancements in large language models have facilitated diverse applications,\nyet a significant research gap persists in understanding and enhancing the\npotential of large language models within the domain of mental health. On the\nother hand, across various applications, an outstanding question involves the\ncapacity of large language models to comprehend expressions of human mental\nhealth conditions in natural language. This study presents an initial\nevaluation of large language models in addressing this gap. Due to this, we\ncompare the performance of Llama-2 and ChatGPT with classical Machine as well\nas Deep learning models. Our results on the DAIC-WOZ dataset show that\ntransformer-based models, like BERT or XLNet, outperform the large language\nmodels.\n",
        "title": "An Assessment on Comprehending Mental Health through Large Language\n  Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04594",
        "abstract_url": "http://arxiv.org/abs/2401.04594",
        "authors": [
            {
                "last_name": "Zilberstein",
                "first_name": "Noam"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL"
        ],
        "abstract": "  Starting with Hoare Logic over 50 years ago, numerous sound and relatively\ncomplete program logics have been devised to reason about the diverse programs\nencountered in the real world. This includes reasoning about computational\neffects, particularly those effects that cause the program execution to branch\ninto multiple paths due to, e.g., nondeterministic or probabilistic choice.\n  The recently introduced Outcome Logic reimagines Hoare Logic with effects at\nits core, using an algebraic representation of choice to capture a variety of\neffects. In this paper, we give the first relatively complete proof system for\nOutcome Logic, handling general purpose looping for the first time. We also\nshow that this proof system applies to programs with various effects and that\nit facilitates the reuse of proof fragments across different kinds of\nspecifications.\n",
        "title": "A Relatively Complete Program Logic for Effectful Branching",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04595",
        "abstract_url": "http://arxiv.org/abs/2401.04595",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Sha",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Feitian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Underwater target localization uses real-time sensory measurements to\nestimate the position of underwater objects of interest, providing critical\nfeedback information for underwater robots. While acoustic sensing is the most\nacknowledged method in underwater robots and possibly the only effective\napproach for long-range underwater target localization, such a sensing modality\ngenerally suffers from low resolution, high cost and high energy consumption,\nthus leading to a mediocre performance when applied to close-range underwater\ntarget localization. On the other hand, optical sensing has attracted\nincreasing attention in the underwater robotics community for its advantages of\nhigh resolution and low cost, holding a great potential particularly in\nclose-range underwater target localization. However, most existing studies in\nunderwater optical sensing are restricted to specific types of targets due to\nthe limited training data available. In addition, these studies typically focus\non the design of estimation algorithms and ignore the influence of illumination\nconditions on the sensing performance, thus hindering wider applications in the\nreal world. To address the aforementioned issues, this paper proposes a novel\ntarget localization method that assimilates both optical and acoustic sensory\nmeasurements to estimate the 3D positions of close-range underwater targets. A\ntest platform with controllable illumination conditions is designed and\ndeveloped to experimentally investigate the proposed multi-modal sensing\napproach. A large vision model is applied to process the optical imaging\nmeasurements, eliminating the requirement for training data acquisition, thus\nsignificantly expanding the scope of potential applications. Extensive\nexperiments are conducted, the results of which validate the effectiveness of\nthe proposed underwater target localization method.\n",
        "title": "A Multi-Modal Approach Based on Large Vision Model for Close-Range\n  Underwater Target Localization",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04601",
        "abstract_url": "http://arxiv.org/abs/2401.04601",
        "authors": [
            {
                "last_name": "MacNeil",
                "first_name": "Stephen"
            },
            {
                "last_name": "Spurlock",
                "first_name": "Scott"
            },
            {
                "last_name": "Applebaum",
                "first_name": "Ian"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In the contemporary landscape of computing education, the ubiquity of\nGenerative Artificial Intelligence has significantly disrupted traditional\nassessment methods, rendering them obsolete and prompting educators to seek\ninnovative alternatives. This research paper explores the challenges posed by\nGenerative AI in the assessment domain and the persistent attempts to\ncircumvent its impact. Despite various efforts to devise workarounds, the\nacademic community is yet to find a comprehensive solution. Amidst this\nstruggle, ungrading emerges as a potential yet under-appreciated solution to\nthe assessment dilemma. Ungrading, a pedagogical approach that involves moving\naway from traditional grading systems, has faced resistance due to its\nperceived complexity and the reluctance of educators to depart from\nconventional assessment practices. However, as the inadequacies of current\nassessment methods become increasingly evident in the face of Generative AI,\nthe time is ripe to reconsider and embrace ungrading.\n",
        "title": "Imagining Computing Education Assessment after Generative AI",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04606",
        "abstract_url": "http://arxiv.org/abs/2401.04606",
        "authors": [
            {
                "last_name": "Grohe",
                "first_name": "Martin"
            },
            {
                "last_name": "Kimelfeld",
                "first_name": "Benny"
            },
            {
                "last_name": "Lindner",
                "first_name": "Peter"
            },
            {
                "last_name": "Standke",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  We propose and study a framework for quantifying the importance of the\nchoices of parameter values to the result of a query over a database. These\nparameters occur as constants in logical queries, such as conjunctive queries.\nIn our framework, the importance of a parameter is its SHAP score. This score\nis a popular instantiation of the game-theoretic Shapley value to measuring the\nimportance of feature values in machine learning models. We make the case for\nthe rationale of using this score by explaining the intuition behind SHAP, and\nby showing that we arrive at this score in two different, apparently opposing,\napproaches to quantifying the contribution of a parameter.\n  The application of the SHAP score requires two components in addition to the\nquery and the database: (a) a probability distribution over the combinations of\nparameter values, and (b) a utility function that measures the similarity\nbetween the result for the original parameters and the result for hypothetical\nparameters. The main question addressed in the paper is the complexity of\ncalculating the SHAP score for different distributions and similarity measures.\nWe first address the case of probabilistically independent parameters. The\nproblem is hard if we consider a fragment of queries that is hard to evaluate\n(as one would expect), and even for the fragment of acyclic conjunctive\nqueries. In some cases, though, one can efficiently list all relevant parameter\ncombinations, and then the SHAP score can be computed in polynomial time under\nreasonable general conditions. Also tractable is the case of full acyclic\nconjunctive queries for certain (natural) similarity functions. We extend our\nresults to conjunctive queries with inequalities between variables and\nparameters. Finally, we discuss a simple approximation technique for the case\nof correlated parameters.\n",
        "title": "The Importance of Parameters in Database Queries",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04608",
        "abstract_url": "http://arxiv.org/abs/2401.04608",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jingyuan"
            },
            {
                "last_name": "Feng",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Huang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent years have witnessed remarkable progress in image generation task,\nwhere users can create visually astonishing images with high-quality. However,\nexisting text-to-image diffusion models are proficient in generating concrete\nconcepts (dogs) but encounter challenges with more abstract ones (emotions).\nSeveral efforts have been made to modify image emotions with color and style\nadjustments, facing limitations in effectively conveying emotions with fixed\nimage contents. In this work, we introduce Emotional Image Content Generation\n(EICG), a new task to generate semantic-clear and emotion-faithful images given\nemotion categories. Specifically, we propose an emotion space and construct a\nmapping network to align it with the powerful Contrastive Language-Image\nPre-training (CLIP) space, providing a concrete interpretation of abstract\nemotions. Attribute loss and emotion confidence are further proposed to ensure\nthe semantic diversity and emotion fidelity of the generated images. Our method\noutperforms the state-of-the-art text-to-image approaches both quantitatively\nand qualitatively, where we derive three custom metrics, i.e., emotion\naccuracy, semantic clarity and semantic diversity. In addition to generation,\nour method can help emotion understanding and inspire emotional art design.\n",
        "title": "EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion\n  Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04609",
        "abstract_url": "http://arxiv.org/abs/2401.04609",
        "authors": [
            {
                "last_name": "Kraus",
                "first_name": "Johannes"
            },
            {
                "last_name": "Lymbery",
                "first_name": "Maria"
            },
            {
                "last_name": "Osthues",
                "first_name": "Kevin"
            },
            {
                "last_name": "Philo",
                "first_name": "Fadi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We consider the dynamic Biot model describing the interaction between fluid\nflow and solid deformation including wave propagation phenomena in both the\nliquid and solid phases of a saturated porous medium. The model couples a\nhyperbolic equation for momentum balance to a second-order in time dynamic\nDarcy law and a parabolic equation for the balance of mass and is here\nconsidered in three-field formulation with the displacement of the elastic\nmatrix, the fluid velocity, and the fluid pressure being the physical fields of\ninterest. A family of variational space-time finite element methods is proposed\nthat combines a continuous-in-time Galerkin ansatz of arbitrary polynomial\ndegree with inf-sup stable $H(\\rm{div})$-conforming approximations of\ndiscontinuous Galerkin (DG) type in case of the displacement and a mixed\napproximation of the flux, its time derivative and the pressure field. We prove\nerror estimates in a combined energy norm as well as $L^2$~error estimates in\nspace for the individual fields for both maximum and $L^2$ norm in time which\nare optimal for the displacement and pressure approximations.\n",
        "title": "Analysis of a family of time-continuous strongly conservative space-time\n  finite element methods for the dynamic Biot model",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04612",
        "abstract_url": "http://arxiv.org/abs/2401.04612",
        "authors": [
            {
                "last_name": "Dheur",
                "first_name": "Victor"
            },
            {
                "last_name": "Bosser",
                "first_name": "Tanguy"
            },
            {
                "last_name": "Izbicki",
                "first_name": "Rafael"
            },
            {
                "last_name": "Taieb",
                "first_name": "Souhaib Ben"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Sequences of labeled events observed at irregular intervals in continuous\ntime are ubiquitous across various fields. Temporal Point Processes (TPPs)\nprovide a mathematical framework for modeling these sequences, enabling\ninferences such as predicting the arrival time of future events and their\nassociated label, called mark. However, due to model misspecification or lack\nof training data, these probabilistic models may provide a poor approximation\nof the true, unknown underlying process, with prediction regions extracted from\nthem being unreliable estimates of the underlying uncertainty. This paper\ndevelops more reliable methods for uncertainty quantification in neural TPP\nmodels via the framework of conformal prediction. A primary objective is to\ngenerate a distribution-free joint prediction region for the arrival time and\nmark, with a finite-sample marginal coverage guarantee. A key challenge is to\nhandle both a strictly positive, continuous response and a categorical\nresponse, without distributional assumptions. We first consider a simple but\noverly conservative approach that combines individual prediction regions for\nthe event arrival time and mark. Then, we introduce a more effective method\nbased on bivariate highest density regions derived from the joint predictive\ndensity of event arrival time and mark. By leveraging the dependencies between\nthese two variables, this method exclude unlikely combinations of the two,\nresulting in sharper prediction regions while still attaining the pre-specified\ncoverage level. We also explore the generation of individual univariate\nprediction regions for arrival times and marks through conformal regression and\nclassification techniques. Moreover, we investigate the stronger notion of\nconditional coverage. Finally, through extensive experimentation on both\nsimulated and real-world datasets, we assess the validity and efficiency of\nthese methods.\n",
        "title": "Distribution-Free Conformal Joint Prediction Regions for Neural Marked\n  Temporal Point Processes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04614",
        "abstract_url": "http://arxiv.org/abs/2401.04614",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Ziyue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingming"
            },
            {
                "last_name": "Gong",
                "first_name": "Yuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Qingjie"
            },
            {
                "last_name": "Wang",
                "first_name": "Yunhong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning models are essential for scene classification, change\ndetection, land cover segmentation, and other remote sensing image\nunderstanding tasks. Most backbones of existing remote sensing deep learning\nmodels are typically initialized by pre-trained weights obtained from ImageNet\npre-training (IMP). However, domain gaps exist between remote sensing images\nand natural images (e.g., ImageNet), making deep learning models initialized by\npre-trained weights of IMP perform poorly for remote sensing image\nunderstanding. Although some pre-training methods are studied in the remote\nsensing community, current remote sensing pre-training methods face the problem\nof vague generalization by only using remote sensing images. In this paper, we\npropose a novel remote sensing pre-training framework, Generic Knowledge\nBoosted Remote Sensing Pre-training (GeRSP), to learn robust representations\nfrom remote sensing and natural images for remote sensing understanding tasks.\nGeRSP contains two pre-training branches: (1) A self-supervised pre-training\nbranch is adopted to learn domain-related representations from unlabeled remote\nsensing images. (2) A supervised pre-training branch is integrated into GeRSP\nfor general knowledge learning from labeled natural images. Moreover, GeRSP\ncombines two pre-training branches using a teacher-student architecture to\nsimultaneously learn representations with general and special knowledge, which\ngenerates a powerful pre-trained model for deep learning model initialization.\nFinally, we evaluate GeRSP and other remote sensing pre-training methods on\nthree downstream tasks, i.e., object detection, semantic segmentation, and\nscene classification. The extensive experimental results consistently\ndemonstrate that GeRSP can effectively learn robust representations in a\nunified manner, improving the performance of remote sensing downstream tasks.\n",
        "title": "Generic Knowledge Boosted Pre-training For Remote Sensing Images",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04619",
        "abstract_url": "http://arxiv.org/abs/2401.04619",
        "authors": [
            {
                "last_name": "S",
                "first_name": "Selva Kumar"
            },
            {
                "last_name": "Khan",
                "first_name": "Afifah Khan Mohammed Ajmal"
            },
            {
                "last_name": "Manjeshwar",
                "first_name": "Chirag"
            },
            {
                "last_name": "Banday",
                "first_name": "Imadh Ajaz"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            ""
        ],
        "abstract": "  In the contemporary digital era, the Internet functions as an unparalleled\ncatalyst, dismantling geographical and linguistic barriers particularly evident\nin texting. This evolution facilitates global communication, transcending\nphysical distances and fostering dynamic cultural exchange. A notable trend is\nthe widespread use of transliteration, where the English alphabet is employed\nto convey messages in native languages, posing a unique challenge for language\ntechnology in accurately detecting the source language. This paper addresses\nthis challenge through a dataset of phone text messages in Hindi and Russian\ntransliterated into English utilizing BERT for language classification and\nGoogle Translate API for transliteration conversion. The research pioneers\ninnovative approaches to identify and convert transliterated text, navigating\nchallenges in the diverse linguistic landscape of digital communication.\nEmphasizing the pivotal role of comprehensive datasets for training Large\nLanguage Models LLMs like BERT, our model showcases exceptional proficiency in\naccurately identifying and classifying languages from transliterated text. With\na validation accuracy of 99% our models robust performance underscores its\nreliability. The comprehensive exploration of transliteration dynamics\nsupported by innovative approaches and cutting edge technologies like BERT,\npositions our research at the forefront of addressing unique challenges in the\nlinguistic landscape of digital communication. Beyond contributing to language\nidentification and transliteration capabilities this work holds promise for\napplications in content moderation, analytics and fostering a globally\nconnected community engaged in meaningful dialogue.\n",
        "title": "Language Detection for Transliterated Content",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04621",
        "abstract_url": "http://arxiv.org/abs/2401.04621",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Runchu"
            },
            {
                "last_name": "Ye",
                "first_name": "Yining"
            },
            {
                "last_name": "Qin",
                "first_name": "Yujia"
            },
            {
                "last_name": "Cong",
                "first_name": "Xin"
            },
            {
                "last_name": "Lin",
                "first_name": "Yankai"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Sun",
                "first_name": "Maosong"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated exceptional coding capability.\nHowever, as another critical component of programming proficiency, the\ndebugging capability of LLMs remains relatively unexplored. Previous\nevaluations of LLMs' debugging ability are significantly limited by the risk of\ndata leakage, the scale of the dataset, and the variety of tested bugs. To\novercome these deficiencies, we introduce `DebugBench', an LLM debugging\nbenchmark consisting of 4,253 instances. It covers four major bug categories\nand 18 minor types in C++, Java, and Python. To construct DebugBench, we\ncollect code snippets from the LeetCode community, implant bugs into source\ndata with GPT-4, and assure rigorous quality checks. We evaluate two commercial\nand three open-source models in a zero-shot scenario. We find that (1) while\nclosed-source models like GPT-4 exhibit inferior debugging performance compared\nto humans, open-source models such as Code Llama fail to attain any pass rate\nscores; (2) the complexity of debugging notably fluctuates depending on the bug\ncategory; (3) incorporating runtime feedback has a clear impact on debugging\nperformance which is not always helpful. As an extension, we also compare LLM\ndebugging and code generation, revealing a strong correlation between them for\nclosed-source models. These findings will benefit the development of LLMs in\ndebugging.\n",
        "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04626",
        "abstract_url": "http://arxiv.org/abs/2401.04626",
        "authors": [
            {
                "last_name": "Feraudo",
                "first_name": "Angelo"
            },
            {
                "last_name": "Calvio",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Bellavista",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Vehicular cloud computing is gaining popularity thanks to the rapid\nadvancements in next generation wireless communication networks. Similarly,\nEdge Computing, along with its standard proposals such as European\nTelecommunications Standards Institute (ETSI) Multi-access Edge Computing\n(MEC), will play a vital role in these scenarios, by enabling the execution of\ncloud-based services at the edge of the network. Together, these solutions have\nthe potential to create real micro-datacenters at the network edge, favoring\nseveral benefits like minimal latency, real-time data processing, and data\nlocality. However, the research community has not yet the opportunity to use\nintegrated simulation frameworks for the easy testing of applications that\nexploit both the vehicular cloud paradigm and MEC-compliant 5G deployment\nenvironments. In this paper, we present our simulation tool as a platform for\nresearchers and engineers to design, test, and enhance applications utilizing\nthe concepts of vehicular and edge cloud. Our platform significantly extends\nOMNet++ and Simu5G, and implements our ETSI MEC-compliant architecture that\nleverages resources provided by far-edge nodes. In addition, the paper analyzes\nand reports performance results for our simulation platform, as well as\nprovides a use case where our simulator is used to support the design, test,\nand validation of an algorithm to distribute MEC application components on\nvehicular cloud resources.\n",
        "title": "A Novel OMNeT++-based Simulation Tool for Vehicular Cloud Computing in\n  ETSI MEC-compliant 5G Environments",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04628",
        "abstract_url": "http://arxiv.org/abs/2401.04628",
        "authors": [
            {
                "last_name": "Lynch",
                "first_name": "Nancy A."
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "DS"
        ],
        "abstract": "  We describe how hierarchical concepts can be represented in three types of\nlayered neural networks. The aim is to support recognition of the concepts when\npartial information about the concepts is presented, and also when some of the\nneurons in the network might fail. Our failure model involves initial random\nfailures. The three types of networks are: feed-forward networks with high\nconnectivity, feed-forward networks with low connectivity, and layered networks\nwith low connectivity and with both forward edges and \"lateral\" edges within\nlayers. In order to achieve fault-tolerance, the representations all use\nmultiple representative neurons for each concept. We show how recognition can\nwork in all three of these settings, and quantify how the probability of\ncorrect recognition depends on several parameters, including the number of\nrepresentatives and the neuron failure probability. We also discuss how these\nrepresentations might be learned, in all three types of networks. For the\nfeed-forward networks, the learning algorithms are similar to ones used in [4],\nwhereas for networks with lateral edges, the algorithms are generally inspired\nby work on the assembly calculus [3, 6, 7].\n",
        "title": "Multi-Neuron Representations of Hierarchical Concepts in Spiking Neural\n  Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04631",
        "abstract_url": "http://arxiv.org/abs/2401.04631",
        "authors": [
            {
                "last_name": "Luis",
                "first_name": "Samuel Yanes"
            },
            {
                "last_name": "Shutin",
                "first_name": "Dmitriy"
            },
            {
                "last_name": "G\u00f3mez",
                "first_name": "Juan Marchal"
            },
            {
                "last_name": "Reina",
                "first_name": "Daniel Guti\u00e9rrez"
            },
            {
                "last_name": "Mar\u00edn",
                "first_name": "Sergio Toral"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The conservation of hydrological resources involves continuously monitoring\ntheir contamination. A multi-agent system composed of autonomous surface\nvehicles is proposed in this paper to efficiently monitor the water quality. To\nachieve a safe control of the fleet, the fleet policy should be able to act\nbased on measurements and to the the fleet state. It is proposed to use Local\nGaussian Processes and Deep Reinforcement Learning to jointly obtain effective\nmonitoring policies. Local Gaussian processes, unlike classical global Gaussian\nprocesses, can accurately model the information in a dissimilar spatial\ncorrelation which captures more accurately the water quality information. A\nDeep convolutional policy is proposed, that bases the decisions on the\nobservation on the mean and variance of this model, by means of an information\ngain reward. Using a Double Deep Q-Learning algorithm, agents are trained to\nminimize the estimation error in a safe manner thanks to a Consensus-based\nheuristic. Simulation results indicate an improvement of up to 24% in terms of\nthe mean absolute error with the proposed models. Also, training results with\n1-3 agents indicate that our proposed approach returns 20% and 24% smaller\naverage estimation errors for, respectively, monitoring water quality variables\nand monitoring algae blooms, as compared to state-of-the-art approaches\n",
        "title": "Deep Reinforcement Multi-agent Learning framework for Information\n  Gathering with Local Gaussian Processes for Water Monitoring",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04632",
        "abstract_url": "http://arxiv.org/abs/2401.04632",
        "authors": [
            {
                "last_name": "Kycia",
                "first_name": "Rados\u0142aw"
            },
            {
                "last_name": "Niemczynowicz",
                "first_name": "Agnieszka"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  The three classes of architectures for time series prediction were tested.\nThey differ by input layers which contain either convolutional, LSTM, or dense\nhypercomplex layers for 4D algebras. The input was four related Stock Market\ntime series, and the prediction of one of them is expected. The optimization of\nhyperparameters related to the classes of architectures was performed in order\nto compare the best neural networks within the class. The results show that in\nmost cases, the architecture with a hypercomplex dense layer provides similar\nMAE accuracy to other architectures, however, with considerably less trainable\nparameters. Thanks to it, hypercomplex neural networks can be learned and\nprocess data faster than the other tested architectures. Moreover, the order of\nthe input time series has an impact on effectively.\n",
        "title": "Hypercomplex neural network in time series forecasting of stock data",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04636",
        "abstract_url": "http://arxiv.org/abs/2401.04636",
        "authors": [
            {
                "last_name": "Sabu",
                "first_name": "Nithin V."
            },
            {
                "last_name": "Gupta",
                "first_name": "Abhishek K."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A network of nanomachines (NMs) can be used to build a target detection\nsystem for a variety of promising applications. They have the potential to\ndetect toxic chemicals, infectious bacteria, and biomarkers of dangerous\ndiseases such as cancer within the human body. Many diseases and health\ndisorders can be detected early and efficiently treated in the future by\nutilizing these systems. To fully grasp the potential of these systems,\nmathematical analysis is required. This paper describes an analytical framework\nfor modeling and analyzing the performance of target detection systems composed\nof multiple mobile nanomachines of varying sizes with passive/absorbing\nboundaries. We consider both direct contact detection, in which NMs must\nphysically contact the target to detect it, and indirect sensing, in which NMs\nmust detect the marker molecules emitted by the target. The detection\nperformance of such systems is calculated for degradable and non-degradable\ntargets, as well as mobile and stationary targets. The derived expressions\nprovide various insights, such as the effect of NM density and target\ndegradation on detection probability.\n",
        "title": "On the Target Detection Performance of a Molecular Communication Network\n  with Multiple Mobile Nanomachines",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04637",
        "abstract_url": "http://arxiv.org/abs/2401.04637",
        "authors": [
            {
                "last_name": "Aracena",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Luster",
                "first_name": "Kyle"
            },
            {
                "last_name": "Santos",
                "first_name": "Fabio"
            },
            {
                "last_name": "Steinmacher",
                "first_name": "Igor"
            },
            {
                "last_name": "Gerosa",
                "first_name": "Marco A."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  Effective prioritization of issue reports is crucial in software engineering\nto optimize resource allocation and address critical problems promptly.\nHowever, the manual classification of issue reports for prioritization is\nlaborious and lacks scalability. Alternatively, many open source software (OSS)\nprojects employ automated processes for this task, albeit relying on\nsubstantial datasets for adequate training. This research seeks to devise an\nautomated approach that ensures reliability in issue prioritization, even when\ntrained on smaller datasets. Our proposed methodology harnesses the power of\nGenerative Pre-trained Transformers (GPT), recognizing their potential to\nefficiently handle this task. By leveraging the capabilities of such models, we\naim to develop a robust system for prioritizing issue reports accurately,\nmitigating the necessity for extensive training data while maintaining\nreliability. In our research, we have developed a reliable GPT-based approach\nto accurately label and prioritize issue reports with a reduced training\ndataset. By reducing reliance on massive data requirements and focusing on\nfew-shot fine-tuning, our methodology offers a more accessible and efficient\nsolution for issue prioritization in software engineering. Our model predicted\nissue types in individual projects up to 93.2% in precision, 95% in recall, and\n89.3% in F1-score.\n",
        "title": "Applying Large Language Models API to Issue Classification Problem",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04638",
        "abstract_url": "http://arxiv.org/abs/2401.04638",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Wenkai"
            },
            {
                "last_name": "Dinitz",
                "first_name": "Michael"
            },
            {
                "last_name": "Foerster",
                "first_name": "Klaus-Tycho"
            },
            {
                "last_name": "Luo",
                "first_name": "Long"
            },
            {
                "last_name": "Schmid",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF",
            "DM"
        ],
        "abstract": "  Emerging reconfigurable optical communication technologies allow to enhance\ndatacenter topologies with demand-aware links optimized towards traffic\npatterns. This paper studies the algorithmic problem of jointly optimizing\ntopology and routing in such demand-aware networks to minimize congestion,\nalong two dimensions: (1) splittable or unsplittable flows, and (2) whether\nrouting is segregated, i.e., whether routes can or cannot combine both\ndemand-aware and demand-oblivious (static) links.\n  For splittable and segregated routing, we show that the problem is generally\n$2$-approximable, but APX-hard even for uniform demands induced by a bipartite\ndemand graph. For unsplittable and segregated routing, we establish upper and\nlower bounds of $O\\left(\\log m/ \\log\\log m \\right)$ and $\\Omega\\left(\\log m/\n\\log\\log m \\right)$, respectively, for polynomial-time approximation\nalgorithms, where $m$ is the number of static links. We further reveal that\nunder un-/splittable and non-segregated routing, even for demands of a single\nsource (resp., destination), the problem cannot be approximated better than\n$\\Omega\\left(\\frac{c_{\\max}}{c_{\\min}} \\right)$ unless P=NP, where $c_{\\max}$\n(resp., $c_{\\min}$) denotes the maximum (resp., minimum) capacity. It remains\nNP-hard for uniform capacities, but is tractable for a single commodity and\nuniform capacities.\n  Our trace-driven simulations show a significant reduction in network\ncongestion compared to existing solutions.\n",
        "title": "Approximation Algorithms for Minimizing Congestion in Demand-Aware\n  Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04647",
        "abstract_url": "http://arxiv.org/abs/2401.04647",
        "authors": [
            {
                "last_name": "Garg",
                "first_name": "Tanmay"
            },
            {
                "last_name": "Vemuri",
                "first_name": "Deepika"
            },
            {
                "last_name": "Balasubramanian",
                "first_name": "Vineeth N"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  This paper presents a novel concept learning framework for enhancing model\ninterpretability and performance in visual classification tasks. Our approach\nappends an unsupervised explanation generator to the primary classifier network\nand makes use of adversarial training. During training, the explanation module\nis optimized to extract visual concepts from the classifier's latent\nrepresentations, while the GAN-based module aims to discriminate images\ngenerated from concepts, from true images. This joint training scheme enables\nthe model to implicitly align its internally learned concepts with\nhuman-interpretable visual properties. Comprehensive experiments demonstrate\nthe robustness of our approach, while producing coherent concept activations.\nWe analyse the learned concepts, showing their semantic concordance with object\nparts and visual attributes. We also study how perturbations in the adversarial\ntraining protocol impact both classification and concept acquisition. In\nsummary, this work presents a significant step towards building inherently\ninterpretable deep vision models with task-aligned concept representations - a\nkey enabler for developing trustworthy AI for real-world perception tasks.\n",
        "title": "Advancing Ante-Hoc Explainable Models through Generative Adversarial\n  Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04648",
        "abstract_url": "http://arxiv.org/abs/2401.04648",
        "authors": [
            {
                "last_name": "Kag",
                "first_name": "Vijay"
            },
            {
                "last_name": "Pal",
                "first_name": "Birupaksha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Modelling of systems where the full system information is unknown is an oft\nencountered problem for various engineering and industrial applications, as\nit's either impossible to consider all the complex physics involved or simpler\nmodels are considered to keep within the limits of the available resources.\nRecent advances in greybox modelling like the deep hidden physics models\naddress this space by combining data and physics. However, for most real-life\napplications, model generalizability is a key issue, as retraining a model for\nevery small change in system inputs and parameters or modification in domain\nconfiguration can render the model economically unviable. In this work we\npresent a novel enhancement to the idea of hidden physics models which can\ngeneralize for changes in system inputs, parameters and domains. We also show\nthat this approach holds promise in system discovery as well and helps learn\nthe hidden physics for the changed system inputs, parameters and domain\nconfiguration.\n",
        "title": "A novel framework for generalization of deep hidden physics models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04649",
        "abstract_url": "http://arxiv.org/abs/2401.04649",
        "authors": [
            {
                "last_name": "Nawratil",
                "first_name": "Georg"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            "RO"
        ],
        "abstract": "  We give a full classification of continuous flexible discrete axial\ncone-nets, which are called axial C-hedra. The obtained result can also be used\nto construct their semi-discrete analogs. Moreover, we identify a novel\nsubclass within the determined class of (semi-)discrete axial cone-nets, whose\nmembers are named axial P-nets as they fulfill the proportion (P) of the\nintercept theorem. Known special cases of these axial P-nets are the smooth and\ndiscrete conic crease patterns with reflecting rule lines. By using a\nparallelism operation one can even generalize axial P-nets. The resulting\ngeneral P-nets constitute a rich novel class of continuous flexible\n(semi-)discrete surfaces, which allow direct access to their spatial shapes by\nthree control polylines. This intuitive method makes them suitable for\ntransformable design tasks using interactive tools.\n",
        "title": "From axial C-hedra to general P-nets",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04650",
        "abstract_url": "http://arxiv.org/abs/2401.04650",
        "authors": [
            {
                "last_name": "Mensah",
                "first_name": "Immanuel Ampomah"
            },
            {
                "last_name": "Healey",
                "first_name": "Jessica"
            },
            {
                "last_name": "Wu",
                "first_name": "Celina"
            },
            {
                "last_name": "Lacunza",
                "first_name": "Andrea"
            },
            {
                "last_name": "Hanson",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Dorsey",
                "first_name": "Kristen L."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  An underdeveloped capability in soft robotics is proprioceptive feedback\ncontrol, where soft actuators can be sensed and controlled using only sensors\non the robot's body. Additionally, soft actuators are often unable to support\nhuman-scale loads due to the extremely compliant materials in use. Developing\nboth feedback control and the ability to actuate under large loads (e.g. 500 N)\nare key capacities required to move soft robotics into everyday applications.\nIn this work, we independently demonstrate these key factors towards\ncontrolling and actuating human-scale loads: proprioceptive (embodied) feedback\ncontrol of a soft, pneumatically-actuated origami robot; and actuation of these\norigami origami robots under a person's weight in an open-loop configuration.\nIn both demonstrations, the actuators are controlled by internal fluidic\npressure. Capacitive sensors patterned onto the robot provide position\nestimation and serve as input to a feedback controller. We demonstrate position\ncontrol of a single actuator during stepped setpoints and sinusoidal trajectory\nfollowing, with root mean square error (RMSE) below 4 mm. We also showcase the\nactuator's potential towards human-scale robotics as an \"origami balance board\"\nby joining three actuators into an open-loop controlled system with a platform\nthat varies its height, roll, and pitch. This work contributes to the field of\nsoft robotics by demonstrating closed-loop feedback position control without\nvisual tracking as an input and lightweight, soft actuators that can support a\nperson's weight. The project repository, including videos, CAD files, and ROS\ncode, is available at https://parses-lab.github.io/kresling_control.\n",
        "title": "Hold 'em and Fold 'em: Towards Human-scale, Feedback-Controlled Soft\n  Origami Robots",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04651",
        "abstract_url": "http://arxiv.org/abs/2401.04651",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Jiang",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingyi"
            },
            {
                "last_name": "Qiu",
                "first_name": "Han"
            },
            {
                "last_name": "Lu",
                "first_name": "Lewei"
            },
            {
                "last_name": "Lu",
                "first_name": "Shijian"
            },
            {
                "last_name": "Xing",
                "first_name": "Eric"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great\npotential in learning to segment anything. The core design of SAMs lies with\nPromptable Segmentation, which takes a handcrafted prompt as input and returns\nthe expected segmentation mask. SAMs work with two types of prompts including\nspatial prompts (e.g., points) and semantic prompts (e.g., texts), which work\ntogether to prompt SAMs to segment anything on downstream datasets. Despite the\nimportant role of prompts, how to acquire suitable prompts for SAMs is largely\nunder-explored. In this work, we examine the architecture of SAMs and identify\ntwo challenges for learning effective prompts for SAMs. To this end, we propose\nspatial-semantic prompt learning (SSPrompt) that learns effective semantic and\nspatial prompts for better SAMs. Specifically, SSPrompt introduces spatial\nprompt learning and semantic prompt learning, which optimize spatial prompts\nand semantic prompts directly over the embedding space and selectively leverage\nthe knowledge encoded in pre-trained prompt encoders. Extensive experiments\nshow that SSPrompt achieves superior image segmentation performance\nconsistently across multiple widely adopted datasets.\n",
        "title": "Learning to Prompt Segment Anything Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04653",
        "abstract_url": "http://arxiv.org/abs/2401.04653",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Liang"
            },
            {
                "last_name": "Ganko",
                "first_name": "Krystian"
            },
            {
                "last_name": "Braatz",
                "first_name": "Richard D."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Determining solving-time certificates of nonlinear model predictive control\n(NMPC) implementations is a pressing requirement when deploying NMPC in\nproduction environments. Such a certificate guarantees that the NMPC controller\nreturns a solution before the next sampling time. However, NMPC formulations\nproduce nonlinear programs (NLPs) for which it is very difficult to derive\ntheir solving-time certificates. Our previous work, Wu and Braatz (2023),\nchallenged this limitation with a proposed input-constrained MPC algorithm\nhaving exact iteration complexity but was restricted to linear MPC\nformulations. This work extends the algorithm to solve input-constrained NMPC\nproblems, by using the Koopman operator and a condensing MPC technique. We\nillustrate the algorithm performance on a high-dimensional, nonlinear partial\ndifferential equation (PDE) control case study, in which we theoretically and\nnumerically certify the solving time to be less than the sampling time.\n",
        "title": "Time-certified Input-constrained NMPC via Koopman Operator",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04655",
        "abstract_url": "http://arxiv.org/abs/2401.04655",
        "authors": [
            {
                "last_name": "Rahman",
                "first_name": "Abu Bakar Siddiqur"
            },
            {
                "last_name": "Ta",
                "first_name": "Hoang-Thang"
            },
            {
                "last_name": "Najjar",
                "first_name": "Lotfollah"
            },
            {
                "last_name": "Azadmanesh",
                "first_name": "Azad"
            },
            {
                "last_name": "G\u00f6n\u00fcl",
                "first_name": "Ali Saffet"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Emotions are integral to human social interactions, with diverse responses\nelicited by various situational contexts. Particularly, the prevalence of\nnegative emotional states has been correlated with negative outcomes for mental\nhealth, necessitating a comprehensive analysis of their occurrence and impact\non individuals. In this paper, we introduce a novel dataset named DepressionEmo\ndesigned to detect 8 emotions associated with depression by 6037 examples of\nlong Reddit user posts. This dataset was created through a majority vote over\ninputs by zero-shot classifications from pre-trained models and validating the\nquality by annotators and ChatGPT, exhibiting an acceptable level of interrater\nreliability between annotators. The correlation between emotions, their\ndistribution over time, and linguistic analysis are conducted on DepressionEmo.\nBesides, we provide several text classification methods classified into two\ngroups: machine learning methods such as SVM, XGBoost, and Light GBM; and deep\nlearning methods such as BERT, GAN-BERT, and BART. The pretrained BART model,\nbart-base allows us to obtain the highest F1- Macro of 0.76, showing its\noutperformance compared to other methods evaluated in our analysis. Across all\nemotions, the highest F1-Macro value is achieved by suicide intent, indicating\na certain value of our dataset in identifying emotions in individuals with\ndepression symptoms through text analysis. The curated dataset is publicly\navailable at: https://github.com/abuBakarSiddiqurRahman/DepressionEmo.\n",
        "title": "DepressionEmo: A novel dataset for multilabel classification of\n  depression emotions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04658",
        "abstract_url": "http://arxiv.org/abs/2401.04658",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Zhen"
            },
            {
                "last_name": "Sun",
                "first_name": "Weigao"
            },
            {
                "last_name": "Li",
                "first_name": "Dong"
            },
            {
                "last_name": "Shen",
                "first_name": "Xuyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Weixuan"
            },
            {
                "last_name": "Zhong",
                "first_name": "Yiran"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.\n",
        "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence\n  Lengths in Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04660",
        "abstract_url": "http://arxiv.org/abs/2401.04660",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Yuzhou"
            },
            {
                "last_name": "Disar\u00f2",
                "first_name": "Giorgia"
            },
            {
                "last_name": "Liu",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Sun",
                "first_name": "Jian"
            },
            {
                "last_name": "Valcher",
                "first_name": "Maria Elena"
            },
            {
                "last_name": "Wang",
                "first_name": "Gang"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Unknown inputs related to, e.g., sensor aging, modeling errors, or device\nbias, represent a major concern in wireless sensor networks, as they degrade\nthe state estimation performance. To improve the performance, unknown-input\nobservers (UIOs) have been proposed. Most of the results available to design\nUIOs are based on explicit system models, which can be difficult or impossible\nto obtain in real-world applications. Data-driven techniques, on the other\nhand, have become a viable alternative for the design and analysis of unknown\nsystems using only data. In this context, a novel data-driven distributed\nunknown-input observer (D-DUIO) for an unknown linear system is developed,\nwhich leverages solely some data collected offline, without any prior knowledge\nof the system matrices. In the paper, first, the design of a DUIO is\ninvestigated by resorting to a traditional model-based approach. By resorting\nto a Lyapunov equation, it is proved that under some conditions, the state\nestimates at all nodes of the DUIO achieve consensus and collectively converge\nto the state of the system. Moving to a data-driven approach, it is shown that\nthe input/output/state trajectories of the system are compatible with the\nequations of a D-DUIO, and this allows, under suitable assumptions, to express\nthe matrices of a possible DUIO in terms of the matrices of pre-collected data.\nThen, necessary and sufficient conditions for the existence of the proposed\nD-DUIO are given. Finally, the efficacy of the D-DUIO is illustrated by means\nof numerical examples.\n",
        "title": "Distributed Data-driven Unknown-input Observers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04662",
        "abstract_url": "http://arxiv.org/abs/2401.04662",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhou"
            },
            {
                "last_name": "Wang",
                "first_name": "Kailong"
            },
            {
                "last_name": "Ma",
                "first_name": "Kai"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuo"
            },
            {
                "last_name": "Luo",
                "first_name": "Xiapu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yajin"
            },
            {
                "last_name": "Wu",
                "first_name": "Lei"
            },
            {
                "last_name": "Bai",
                "first_name": "Guangdong"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoyu"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The dark web has emerged as the state-of-the-art solution for enhanced\nanonymity. Just like a double-edged sword, it also inadvertently becomes the\nsafety net and breeding ground for illicit activities. Among them,\ncryptocurrencies have been prevalently abused to receive illicit income while\nevading regulations. Despite the continuing efforts to combat illicit\nactivities, there is still a lack of an in-depth understanding regarding the\ncharacteristics and dynamics of cryptocurrency abuses on the dark web. In this\nwork, we conduct a multi-dimensional and systematic study to track\ncryptocurrency-related illicit activities and campaigns on the dark web. We\nfirst harvest a dataset of 4,923 cryptocurrency-related onion sites with over\n130K pages. Then, we detect and extract the illicit blockchain transactions to\ncharacterize the cryptocurrency abuses, targeting features from\nsingle/clustered addresses and illicit campaigns. Throughout our study, we have\nidentified 2,564 illicit sites with 1,189 illicit blockchain addresses, which\naccount for 90.8 BTC in revenue. Based on their inner connections, we further\nidentify 66 campaigns behind them. Our exploration suggests that illicit\nactivities on the dark web have strong correlations, which can guide us to\nidentify new illicit blockchain addresses and onions, and raise alarms at the\nearly stage of their deployment.\n",
        "title": "The Devil Behind the Mirror: Tracking the Campaigns of Cryptocurrency\n  Abuses on the Dark Web",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04666",
        "abstract_url": "http://arxiv.org/abs/2401.04666",
        "authors": [
            {
                "last_name": "Himel",
                "first_name": "Galib Muhammad Shahriar"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Masudul"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  As the most basic application and implementation of deep learning, image\nclassification has grown in popularity. Various datasets are provided by\nrenowned data science communities for benchmarking machine learning algorithms\nand pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is\nbeing used in this research for its overall acceptance and benchmark standards.\nA comparison of various pre-trained models is demonstrated by using different\ntypes of optimizers and loss functions. Hyper-parameters are changed to gain\nthe best result from a model. By applying this approach, we have got higher\naccuracy without major changes in the training model. To run the experiment, we\nused three different computer architectures: a laptop equipped with NVIDIA\nGeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a\ndesktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate\nsupremacy in terms of accuracy over the previously done experiments on this\ndataset. From this experiment, the highest accuracy which is 99.65% is gained\nusing the NASNet Large.\n",
        "title": "Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA\n  Cats and Dogs Dataset",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04669",
        "abstract_url": "http://arxiv.org/abs/2401.04669",
        "authors": [
            {
                "last_name": "Randall",
                "first_name": "Thomas"
            },
            {
                "last_name": "Koo",
                "first_name": "Jaehoon"
            },
            {
                "last_name": "Videau",
                "first_name": "Brice"
            },
            {
                "last_name": "Kruse",
                "first_name": "Michael"
            },
            {
                "last_name": "Wu",
                "first_name": "Xingfu"
            },
            {
                "last_name": "Hovland",
                "first_name": "Paul"
            },
            {
                "last_name": "Hall",
                "first_name": "Mary"
            },
            {
                "last_name": "Ge",
                "first_name": "Rong"
            },
            {
                "last_name": "Balaprakash",
                "first_name": "Prasanna"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            ""
        ],
        "abstract": "  As diverse high-performance computing (HPC) systems are built, many\nopportunities arise for applications to solve larger problems than ever before.\nGiven the significantly increased complexity of these HPC systems and\napplication tuning, empirical performance tuning, such as autotuning, has\nemerged as a promising approach in recent years. Despite its effectiveness,\nautotuning is often a computationally expensive approach. Transfer learning\n(TL)-based autotuning seeks to address this issue by leveraging the data from\nprior tuning. Current TL methods for autotuning spend significant time modeling\nthe relationship between parameter configurations and performance, which is\nineffective for few-shot (that is, few empirical evaluations) tuning on new\ntasks. We introduce the first generative TL-based autotuning approach based on\nthe Gaussian copula (GC) to model the high-performing regions of the search\nspace from prior data and then generate high-performing configurations for new\ntasks. This allows a sampling-based approach that maximizes few-shot\nperformance and provides the first probabilistic estimation of the few-shot\nbudget for effective TL-based autotuning. We compare our generative TL approach\nwith state-of-the-art autotuning techniques on several benchmarks. We find that\nthe GC is capable of achieving 64.37% of peak few-shot performance in its first\nevaluation. Furthermore, the GC model can determine a few-shot transfer budget\nthat yields up to 33.39$\\times$ speedup, a dramatic improvement over the\n20.58$\\times$ speedup using prior techniques.\n",
        "title": "Transfer-Learning-Based Autotuning Using Gaussian Copula",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04670",
        "abstract_url": "http://arxiv.org/abs/2401.04670",
        "authors": [
            {
                "last_name": "Karim",
                "first_name": "Ramin Goudarzi"
            },
            {
                "last_name": "Dulal",
                "first_name": "Dipak"
            },
            {
                "last_name": "Navasca",
                "first_name": "Carmeliza"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper explores a new version of the Levenberg-Marquardt algorithm used\nfor Tensor Canonical Polyadic (CP) decomposition with an emphasis on image\ncompression and reconstruction. Tensor computation, especially CP\ndecomposition, holds significant applications in data compression and analysis.\nIn this study, we formulate CP as a nonlinear least squares optimization\nproblem. Then, we present an iterative Levenberg-Marquardt (LM) based algorithm\nfor computing the CP decomposition. Ultimately, we test the algorithm on\nvarious datasets, including randomly generated tensors and RGB images. The\nproposed method proves to be both efficient and effective, offering a reduced\ncomputational burden when compared to the traditional Levenberg-Marquardt\ntechnique.\n",
        "title": "Modified Levenberg-Marquardt Algorithm For Tensor CP Decomposition in\n  Image Compression",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04674",
        "abstract_url": "http://arxiv.org/abs/2401.04674",
        "authors": [
            {
                "last_name": "Epstein",
                "first_name": "Charles L."
            },
            {
                "last_name": "Mazzeo",
                "first_name": "Rafe"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  This paper continues the analysis of the scattering problem for a network of\nopen wave-guides started in [arXiv:2302.04353, arXiv:2310.05816]. In this part\nwe present explicit, physically motivated radiation conditions that ensure\nuniqueness of the solution to the scattering problem. These conditions stem\nfrom a 2000 paper of A. Vasy on 3-body Schrodinger operators; we discuss\nclosely related conditions from a 1994 paper of H. Isozaki. Vasy's paper also\nproves the existence of the limiting absorption resolvents, and that the\nlimiting solutions satisfy the radiation conditions. The statements of these\nresults require a calculus of pseudodifferential operators, called the 3-body\nscattering calculus, which is briefly introduced here. We show that the\nsolutions to the model problem obtained in arXiv:2302.04353 satisfy these\nradiation conditions, which makes it possible to prove uniqueness, and\ntherefore existence, for the system of Fredholm integral equations introduced\nin that paper.\n",
        "title": "Solving the Scattering Problem for Open Wave-Guides, III: Radiation\n  Conditions and Uniqueness",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04675",
        "abstract_url": "http://arxiv.org/abs/2401.04675",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Schwartz",
                "first_name": "Moshe"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Motivated by applications in DNA storage, we study a setting in which strings\nare affected by tandem-duplication errors. In particular, we look at two\nsettings: disjoint tandem-duplication errors, and equal-length\ntandem-duplication errors. We construct codes, with positive asymptotic rate,\nfor the two settings, as well as for their combination. Our constructions are\nduplication-free codes, comprising codewords that do not contain tandem\nduplications of specific lengths. Additionally, our codes generalize previous\nconstructions, containing them as special cases.\n",
        "title": "On Duplication-Free Codes for Disjoint or Equal-Length Errors",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04680",
        "abstract_url": "http://arxiv.org/abs/2401.04680",
        "authors": [
            {
                "last_name": "Howard",
                "first_name": "Sunny"
            },
            {
                "last_name": "Norreys",
                "first_name": "Peter"
            },
            {
                "last_name": "D\u00f6pp",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Optical imaging systems are inherently limited in their resolution due to the\npoint spread function (PSF), which applies a static, yet spatially-varying,\nconvolution to the image. This degradation can be addressed via Convolutional\nNeural Networks (CNNs), particularly through deblurring techniques. However,\ncurrent solutions face certain limitations in efficiently computing\nspatially-varying convolutions. In this paper we propose CoordGate, a novel\nlightweight module that uses a multiplicative gate and a coordinate encoding\nnetwork to enable efficient computation of spatially-varying convolutions in\nCNNs. CoordGate allows for selective amplification or attenuation of filters\nbased on their spatial position, effectively acting like a locally connected\nneural network. The effectiveness of the CoordGate solution is demonstrated\nwithin the context of U-Nets and applied to the challenging problem of image\ndeblurring. The experimental results show that CoordGate outperforms\nconventional approaches, offering a more robust and spatially aware solution\nfor CNNs in various computer vision applications.\n",
        "title": "CoordGate: Efficiently Computing Spatially-Varying Convolutions in\n  Convolutional Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04682",
        "abstract_url": "http://arxiv.org/abs/2401.04682",
        "authors": [
            {
                "last_name": "De Santiago",
                "first_name": "Kylliann"
            },
            {
                "last_name": "Szafranski",
                "first_name": "Marie"
            },
            {
                "last_name": "Ambroise",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  In this work, we propose an original method for aggregating multiple\nclustering coming from different sources of information. Each partition is\nencoded by a co-membership matrix between observations. Our approach uses a\nmixture of multilayer Stochastic Block Models (SBM) to group co-membership\nmatrices with similar information into components and to partition observations\ninto different clusters, taking into account their specificities within the\ncomponents. The identifiability of the model parameters is established and a\nvariational Bayesian EM algorithm is proposed for the estimation of these\nparameters. The Bayesian framework allows for selecting an optimal number of\nclusters and components. The proposed approach is compared using synthetic data\nwith consensus clustering and tensor-based algorithms for community detection\nin large-scale complex networks. Finally, the method is utilized to analyze\nglobal food trading networks, leading to structures of interest.\n",
        "title": "Mixture of multilayer stochastic block models for multiview clustering",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04691",
        "abstract_url": "http://arxiv.org/abs/2401.04691",
        "authors": [
            {
                "last_name": "Estopinan",
                "first_name": "Joaquim"
            },
            {
                "last_name": "Servajean",
                "first_name": "Maximilien"
            },
            {
                "last_name": "Bonnet",
                "first_name": "Pierre"
            },
            {
                "last_name": "Joly",
                "first_name": "Alexis"
            },
            {
                "last_name": "Munoz",
                "first_name": "Fran\u00e7ois"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Although increasing threats on biodiversity are now widely recognised, there\nare no accurate global maps showing whether and where species assemblages are\nat risk. We hereby assess and map at kilometre resolution the conservation\nstatus of the iconic orchid family, and discuss the insights conveyed at\nmultiple scales. We introduce a new Deep Species Distribution Model trained on\n1M occurrences of 14K orchid species to predict their assemblages at global\nscale and at kilometre resolution. We propose two main indicators of the\nconservation status of the assemblages: (i) the proportion of threatened\nspecies, and (ii) the status of the most threatened species in the assemblage.\nWe show and analyze the variation of these indicators at World scale and in\nrelation to currently protected areas in Sumatra island. Global and interactive\nmaps available online show the indicators of conservation status of orchid\nassemblages, with sharp spatial variations at all scales. The highest level of\nthreat is found at Madagascar and the neighbouring islands. In Sumatra, we\nfound good correspondence of protected areas with our indicators, but\nsupplementing current IUCN assessments with status predictions results in\nalarming levels of species threat across the island. Recent advances in deep\nlearning enable reliable mapping of the conservation status of species\nassemblages on a global scale. As an umbrella taxon, orchid family provides a\nreference for identifying vulnerable ecosystems worldwide, and prioritising\nconservation actions both at international and local levels.\n",
        "title": "AI-based Mapping of the Conservation Status of Orchid Assemblages at\n  Global Scale",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04692",
        "abstract_url": "http://arxiv.org/abs/2401.04692",
        "authors": [
            {
                "last_name": "Rodrigues",
                "first_name": "Nils"
            },
            {
                "last_name": "Dennig",
                "first_name": "Frederik L."
            },
            {
                "last_name": "Brandt",
                "first_name": "Vincent"
            },
            {
                "last_name": "Keim",
                "first_name": "Daniel A."
            },
            {
                "last_name": "Weiskopf",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Scatter plots are popular for displaying 2D data, but in practice, many data\nsets have more than two dimensions. For the analysis of such multivariate data,\nit is often necessary to switch between scatter plots of different dimension\npairs, e.g., in a scatter plot matrix (SPLOM). Alternative approaches include a\n\"grand tour\" for an overview of the entire data set or creating artificial axes\nfrom dimensionality reduction (DR). A cross-cutting concern in all techniques\nis the ability of viewers to find correspondence between data points in\ndifferent views. Previous work proposed animations to preserve the mental map\nbetween view changes and to trace points as well as clusters between scatter\nplots of the same underlying data set. In this paper, we evaluate a variety of\nspline- and rotation-based view transitions in a crowdsourced user study\nfocusing on ecological validity. Using the study results, we assess each\nanimation's suitability for tracing points and clusters across view changes. We\nevaluate whether the order of horizontal and vertical rotation is relevant for\ntask accuracy. The results show that rotations with an orthographic camera or\nstaged expansion of a depth axis significantly outperform all other animation\ntechniques for the traceability of individual points. Further, we provide a\nranking of the animated transition techniques for traceability of individual\npoints. However, we could not find any significant differences for the\ntraceability of clusters. Furthermore, we identified differences by animation\ndirection that could guide further studies to determine potential confounds for\nthese differences. We publish the study data for reuse and provide the\nanimation framework as a D3.js plug-in.\n",
        "title": "Comparative Evaluation of Animated Scatter Plot Transitions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04694",
        "abstract_url": "http://arxiv.org/abs/2401.04694",
        "authors": [
            {
                "last_name": "Pashazad",
                "first_name": "Hossein"
            },
            {
                "last_name": "Song",
                "first_name": "Xiaoyu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Dynamic crack branching in unsaturated porous media holds significant\nrelevance in various fields, including geotechnical engineering, geosciences,\nand petroleum engineering. This article presents a numerical investigation into\ndynamic crack branching in unsaturated porous media using a recently developed\ncoupled micro-periporomechanics paradigm. This paradigm extends the\nperiporomechanics model by incorporating the micro-rotation of the solid\nskeleton. Within this framework, each material point is equipped with three\ndegrees of freedom: displacement, micro-rotation, and fluid pressure.\nConsistent with the Cosserat continuum theory, a length scale associated with\nthe micro-rotation of material points is inherently integrated into the model.\nThis study encompasses several key aspects: (1) Validation of the coupled\nmicro-periporomechanics paradigm for effectively modeling crack branching in\ndeformable porous media, (2) Examination of the transition from a single branch\nto multiple branches in porous media under drained conditions, (3) Simulation\nof single crack branching in unsaturated porous media under dynamic loading\nconditions, and (4) Investigation of multiple crack branching in unsaturated\nporous media under dynamic loading conditions. The numerical results obtained\nin this study are systematically analyzed to elucidate the factors that\ninfluence dynamic crack branching in porous media subjected to dynamic loading.\nFurthermore, the comprehensive numerical findings underscore the efficacy and\nrobustness of the coupled micro-periporomechanics paradigm in accurately\nmodeling dynamic crack branching in variably saturated porous media.\n",
        "title": "Modeling dynamic crack branching in unsaturated porous media through\n  multi-phase micro-periporomechanics",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04695",
        "abstract_url": "http://arxiv.org/abs/2401.04695",
        "authors": [
            {
                "last_name": "Yona",
                "first_name": "Gal"
            },
            {
                "last_name": "Aharoni",
                "first_name": "Roee"
            },
            {
                "last_name": "Geva",
                "first_name": "Mor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Factual questions typically can be answered correctly at different levels of\ngranularity. For example, both ``August 4, 1961'' and ``1961'' are correct\nanswers to the question ``When was Barack Obama born?''. Standard question\nanswering (QA) evaluation protocols, however, do not explicitly take this into\naccount and compare a predicted answer against answers of a single granularity\nlevel. In this work, we propose GRANOLA QA, a novel evaluation setting where a\npredicted answer is evaluated in terms of accuracy and informativeness against\na set of multi-granularity answers. We present a simple methodology for\nenriching existing datasets with multi-granularity answers, and create\nGRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We\nevaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,\ncalled Decoding with Response Aggregation (DRAG), that is geared towards\naligning the response granularity with the model's uncertainty. Our experiments\nshow that large language models with standard decoding tend to generate\nspecific answers, which are often incorrect. In contrast, when evaluated on\nmulti-granularity answers, DRAG yields a nearly 20 point increase in accuracy\non average, which further increases for rare entities. Overall, this reveals\nthat standard evaluation and decoding schemes may significantly underestimate\nthe knowledge encapsulated in LMs.\n",
        "title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering\n  with Multi-Granularity Answers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04700",
        "abstract_url": "http://arxiv.org/abs/2401.04700",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Jia-Chen"
            },
            {
                "last_name": "Xu",
                "first_name": "Hao-Xiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Jun-Yu"
            },
            {
                "last_name": "Lu",
                "first_name": "Pan"
            },
            {
                "last_name": "Ling",
                "first_name": "Zhen-Hua"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            },
            {
                "last_name": "Peng",
                "first_name": "Nanyun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advances in large language models (LLMs) have opened up new paradigms\nfor accessing the knowledge stored in their parameters. One critical challenge\nthat has emerged is the presence of hallucinations in LLM outputs due to false\nor outdated knowledge. Since retraining LLMs with updated information is\nresource-intensive, there has been a growing interest in model editing.\nHowever, many model editing methods, while effective in various scenarios, tend\nto overemphasize aspects such as efficacy, generalization, and locality in\nediting performance, often overlooking potential side effects on the general\nabilities of LLMs. In this paper, we raise concerns that the improvement of\nmodel factuality may come at the cost of a significant degradation of these\ngeneral abilities, which is not conducive to the sustainable development of\nLLMs. Systematically, we analyze side effects by evaluating four popular\nediting methods on two LLMs across eight representative task categories.\nExtensive empirical research reveals that model editing does improve model\nfactuality but at the expense of substantially impairing general abilities.\nTherefore, we advocate for more research efforts to minimize the loss of\ngeneral abilities acquired during LLM pre-training and to ultimately preserve\nthem during model editing.\n",
        "title": "Model Editing Can Hurt General Abilities of Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04701",
        "abstract_url": "http://arxiv.org/abs/2401.04701",
        "authors": [
            {
                "last_name": "Jacobson",
                "first_name": "John"
            },
            {
                "last_name": "Burtscher",
                "first_name": "Martin"
            },
            {
                "last_name": "Gopalakrishnan",
                "first_name": "Ganesh"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Data races are egregious parallel programming bugs on CPUs. They are even\nworse on GPUs due to the hierarchical thread and memory structure, which makes\nit possible to write code that is correctly synchronized within a thread group\nwhile not being correct across groups. Thus far, all major data-race checkers\nfor GPUs suffer from at least one of the following problems: they do not check\nraces in global memory, do not work on recent GPUs, scale poorly, have not been\nextensively tested, miss simple data races, or are not dependable without\ndetailed knowledge of the compiler.\n  Our new data-race detection tool, HiRace, overcomes these limitations. Its\nkey novelty is an innovative parallel finite-state machine that condenses an\narbitrarily long access history into a constant-length state, thus allowing it\nto handle large and long-running programs. HiRace is a dynamic tool that checks\nfor thread-group shared memory and global device memory races. It utilizes\nsource-code instrumentation, thus avoiding driver, compiler, and hardware\ndependencies. We evaluate it on a modern calibrated data-race benchmark suite.\nOn the 580 tested CUDA kernels, 346 of which contain data races, HiRace finds\nraces missed by other tools without false alarms and is more than 10 times\nfaster on average than the current state of the art, while incurring only half\nthe memory overhead.\n",
        "title": "HiRace: Accurate and Fast Source-Level Race Checking of GPU Programs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04705",
        "abstract_url": "http://arxiv.org/abs/2401.04705",
        "authors": [
            {
                "last_name": "Balogun",
                "first_name": "Emmanuel"
            },
            {
                "last_name": "Buechler",
                "first_name": "Elizabeth"
            },
            {
                "last_name": "Bhela",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Onori",
                "first_name": "Simona"
            },
            {
                "last_name": "Rajagopal",
                "first_name": "Ram"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA",
            "SE",
            ""
        ],
        "abstract": "  To enable the electrification of transportation systems, it is important to\nunderstand how technologies such as grid storage, solar photovoltaic systems,\nand control strategies can aid the deployment of electric vehicle charging at\nscale. In this work, we present EV-EcoSim, a co-simulation platform that\ncouples electric vehicle charging, battery systems, solar photovoltaic systems,\ngrid transformers, control strategies, and power distribution systems, to\nperform cost quantification and analyze the impacts of electric vehicle\ncharging on the grid. This python-based platform can run a receding horizon\ncontrol scheme for real-time operation and a one-shot control scheme for\nplanning problems, with multi-timescale dynamics for different systems to\nsimulate realistic scenarios. We demonstrate the utility of EV-EcoSim through a\ncase study focused on economic evaluation of battery size to reduce electricity\ncosts while considering impacts of fast charging on the power distribution\ngrid. We present qualitative and quantitative evaluations on the battery size\nin tabulated results. The tabulated results delineate the trade-offs between\ncandidate battery sizing solutions, providing comprehensive insights for\ndecision-making under uncertainty. Additionally, we demonstrate the\nimplications of the battery controller model fidelity on the system costs and\nshow that the fidelity of the battery controller can completely change\ndecisions made when planning an electric vehicle charging site.\n",
        "title": "EV-EcoSim: A grid-aware co-simulation platform for the design and\n  optimization of electric vehicle charging infrastructure",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04707",
        "abstract_url": "http://arxiv.org/abs/2401.04707",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Muhammad Shahbaz"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Jawad"
            },
            {
                "last_name": "Al-Dubai",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Ghaleb",
                "first_name": "Baraq"
            },
            {
                "last_name": "Pitropakis",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "Buchanan",
                "first_name": "William J."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Given the security concerns of Internet of Things (IoT) networks and limited\ncomputational resources of IoT devices, this paper presents RNA-TransCrypt, a\nnovel image encryption scheme that is not only highly secure but also efficient\nand lightweight. RNA-TransCrypt integrates the biocryptographic properties of\nRNA encoding with the non-linearity and unpredictability of chaos theory. This\nscheme introduces three novel contributions: 1) the two-base RNA encoding\nmethod, which transforms the image into RNA strands-like sequence, ensuring\nefficient scrambling; 2) the transformative substitution technique, which\ntransforms the s-box values before replacing the pixel values, and is\nresponsible for making the scheme lightweight; and 3) three mathematical\ncryptographic operations designed especially for image encryption that ensure\nthe effective transformation of the s-box values, resulting in a new outcome\neven for the same input values. These modules are key-dependent, utilizing\nchaotic keys generated by the De Jong Fractal Map and the Van der Pol\nOscillator. Extensive security analysis, including histogram analysis,\ncorrelation analysis, and the results of the statistical security parameters\nobtained from the Gray-Level Co-occurrence Matrix (GLCM) validate the efficacy\nof the proposed scheme in encrypting input images with close-to-ideal results\nof 7.997 entropy and 0.0006 correlation.\n",
        "title": "RNA-TransCrypt: Image Encryption Using Chaotic RNA Encoding, Novel\n  Transformative Substitution, and Tailored Cryptographic Operations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04714",
        "abstract_url": "http://arxiv.org/abs/2401.04714",
        "authors": [
            {
                "last_name": "Hebbar",
                "first_name": "Anish"
            },
            {
                "last_name": "Khan",
                "first_name": "Arindam"
            },
            {
                "last_name": "Sreenivas",
                "first_name": "K. V. N."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Best-Fit is one of the most prominent and practically used algorithms for the\nbin packing problem, where a set of items with associated sizes needs to be\npacked in the minimum number of unit-capacity bins. Kenyon [SODA '96] studied\nonline bin packing under random-order arrival, where the adversary chooses the\nlist of items, but the items arrive one by one according to an arrival order\ndrawn uniformly randomly from the set of all permutations of the items.\nKenyon's seminal result established an upper bound of $1.5$ and a lower bound\nof $1.08$ on the random-order ratio of Best-Fit, and it was conjectured that\nthe true ratio is $\\approx 1.15$. The conjecture, if true, will also imply that\nBest-Fit (on randomly permuted input) has the best performance guarantee among\nall the widely-used simple algorithms for (offline) bin packing. This\nconjecture has remained one of the major open problems in the area, as\nhighlighted in the recent survey on random-order models by Gupta and Singla\n[Beyond the Worst-Case Analysis of Algorithms '20]. Recently, Albers et al.\n[Algorithmica '21] improved the upper bound to $1.25$ for the special case when\nall the item sizes are greater than $1/3$, and they improve the lower bound to\n$1.1$. Ayyadevara et al. [ICALP '22] obtained an improved result for the\nspecial case when all the item sizes lie in $(1/4, 1/2]$, which corresponds to\nthe $3$-partition problem. The upper bound of $3/2$ for the general case,\nhowever, has remained unimproved.\n  In this paper, we make the first progress towards the conjecture, by showing\nthat Best-Fit achieves a random-order ratio of at most $1.5 - \\varepsilon$, for\na small constant $\\varepsilon>0$. Furthermore, we establish an improved lower\nbound of $1.144$ on the random-order ratio of Best-Fit, nearly reaching the\nconjectured ratio.\n",
        "title": "Bin Packing under Random-Order: Breaking the Barrier of 3/2",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04718",
        "abstract_url": "http://arxiv.org/abs/2401.04718",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xiaojuan"
            },
            {
                "last_name": "Park",
                "first_name": "Taesung"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yang"
            },
            {
                "last_name": "Shechtman",
                "first_name": "Eli"
            },
            {
                "last_name": "Zhang",
                "first_name": "Richard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A jump cut offers an abrupt, sometimes unwanted change in the viewing\nexperience. We present a novel framework for smoothing these jump cuts, in the\ncontext of talking head videos. We leverage the appearance of the subject from\nthe other source frames in the video, fusing it with a mid-level representation\ndriven by DensePose keypoints and face landmarks. To achieve motion, we\ninterpolate the keypoints and landmarks between the end frames around the cut.\nWe then use an image translation network from the keypoints and source frames,\nto synthesize pixels. Because keypoints can contain errors, we propose a\ncross-modal attention scheme to select and pick the most appropriate source\namongst multiple options for each key point. By leveraging this mid-level\nrepresentation, our method can achieve stronger results than a strong video\ninterpolation baseline. We demonstrate our method on various jump cuts in the\ntalking head videos, such as cutting filler words, pauses, and even random\ncuts. Our experiments show that we can achieve seamless transitions, even in\nthe challenging cases where the talking head rotates or moves drastically in\nthe jump cut.\n",
        "title": "Jump Cut Smoothing for Talking Heads",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04720",
        "abstract_url": "http://arxiv.org/abs/2401.04720",
        "authors": [
            {
                "last_name": "Roth",
                "first_name": "Benedikt"
            },
            {
                "last_name": "Koch",
                "first_name": "Valentin"
            },
            {
                "last_name": "Wagner",
                "first_name": "Sophia J."
            },
            {
                "last_name": "Schnabel",
                "first_name": "Julia A."
            },
            {
                "last_name": "Marr",
                "first_name": "Carsten"
            },
            {
                "last_name": "Peng",
                "first_name": "Tingying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To handle the large scale of whole slide images in computational pathology,\nmost approaches first tessellate the images into smaller patches, extract\nfeatures from these patches, and finally aggregate the feature vectors with\nweakly-supervised learning. The performance of this workflow strongly depends\non the quality of the extracted features. Recently, foundation models in\ncomputer vision showed that leveraging huge amounts of data through supervised\nor self-supervised learning improves feature quality and generalizability for a\nvariety of tasks. In this study, we benchmark the most popular vision\nfoundation models as feature extractors for histopathology data. We evaluate\nthe models in two settings: slide-level classification and patch-level\nclassification. We show that foundation models are a strong baseline. Our\nexperiments demonstrate that by finetuning a foundation model on a single GPU\nfor only two hours or three days depending on the dataset, we can match or\noutperform state-of-the-art feature extractors for computational pathology.\nThese findings imply that even with little resources one can finetune a feature\nextractor tailored towards a specific downstream task and dataset. This is a\nconsiderable shift from the current state, where only few institutions with\nlarge amounts of resources and datasets are able to train a feature extractor.\nWe publish all code used for training and evaluation as well as the finetuned\nmodels.\n",
        "title": "Low-resource finetuning of foundation models beats state-of-the-art in\n  histopathology",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04722",
        "abstract_url": "http://arxiv.org/abs/2401.04722",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Jun"
            },
            {
                "last_name": "Li",
                "first_name": "Feifei"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Convolutional Neural Networks (CNNs) and Transformers have been the most\npopular architectures for biomedical image segmentation, but both of them have\nlimited ability to handle long-range dependencies because of inherent locality\nor computational complexity. To address this challenge, we introduce U-Mamba, a\ngeneral-purpose network for biomedical image segmentation. Inspired by the\nState Space Sequence Models (SSMs), a new family of deep sequence models known\nfor their strong capability in handling long sequences, we design a hybrid\nCNN-SSM block that integrates the local feature extraction power of\nconvolutional layers with the abilities of SSMs for capturing the long-range\ndependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it\nto automatically adapt to various datasets without manual intervention. We\nconduct extensive experiments on four diverse tasks, including the 3D abdominal\norgan segmentation in CT and MR images, instrument segmentation in endoscopy\nimages, and cell segmentation in microscopy images. The results reveal that\nU-Mamba outperforms state-of-the-art CNN-based and Transformer-based\nsegmentation networks across all tasks. This opens new avenues for efficient\nlong-range dependency modeling in biomedical image analysis. The code, models,\nand data are publicly available at https://wanglab.ai/u-mamba.html.\n",
        "title": "U-Mamba: Enhancing Long-range Dependency for Biomedical Image\n  Segmentation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04726",
        "abstract_url": "http://arxiv.org/abs/2401.04726",
        "authors": [
            {
                "last_name": "Batagelj",
                "first_name": "Vladimir"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DS",
            "",
            ""
        ],
        "abstract": "  Large bibliographic networks are sparse -- the average node degree is small.\nThis is not necessarily true for their product -- in some cases, it can\n``explode'' (it is not sparse, increases in time and space complexity). An\napproach in such cases is to reduce the complexity of the problem by limiting\nour attention to a selected subset of important nodes and computing with\ncorresponding truncated networks. The nodes can be selected by different\ncriteria. An option is to consider the most important nodes in the derived\nnetwork -- nodes with the largest weighted degree. It turns out that the\nweighted degrees in the derived network can be computed efficiently without\ncomputing the derived network itself.\n",
        "title": "Weighted degrees and truncated derived bibliographic networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04727",
        "abstract_url": "http://arxiv.org/abs/2401.04727",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Li",
                "first_name": "Xianhang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongru"
            },
            {
                "last_name": "Xie",
                "first_name": "Cihang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The machine learning community has witnessed a drastic change in the training\npipeline, pivoted by those ''foundation models'' with unprecedented scales.\nHowever, the field of adversarial training is lagging behind, predominantly\ncentered around small model sizes like ResNet-50, and tiny and low-resolution\ndatasets like CIFAR-10. To bridge this transformation gap, this paper provides\na modern re-examination with adversarial training, investigating its potential\nbenefits when applied at scale. Additionally, we introduce an efficient and\neffective training strategy to enable adversarial training with giant models\nand web-scale data at an affordable computing cost. We denote this newly\nintroduced framework as AdvXL.\n  Empirical results demonstrate that AdvXL establishes new state-of-the-art\nrobust accuracy records under AutoAttack on ImageNet-1K. For example, by\ntraining on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to\nsubstantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and\n$l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.\nThis achievement posits AdvXL as a pioneering approach, charting a new\ntrajectory for the efficient training of robust visual representations at\nsignificantly larger scales. Our code is available at\nhttps://github.com/UCSC-VLAA/AdvXL.\n",
        "title": "Revisiting Adversarial Training at Scale",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04728",
        "abstract_url": "http://arxiv.org/abs/2401.04728",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xiyi"
            },
            {
                "last_name": "Mihajlovic",
                "first_name": "Marko"
            },
            {
                "last_name": "Wang",
                "first_name": "Shaofei"
            },
            {
                "last_name": "Prokudin",
                "first_name": "Sergey"
            },
            {
                "last_name": "Tang",
                "first_name": "Siyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multiview-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks.\n",
        "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar\n  Creation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04729",
        "abstract_url": "http://arxiv.org/abs/2401.04729",
        "authors": [
            {
                "last_name": "Spitzer",
                "first_name": "Philipp"
            },
            {
                "last_name": "Holstein",
                "first_name": "Joshua"
            },
            {
                "last_name": "Hemmer",
                "first_name": "Patrick"
            },
            {
                "last_name": "V\u00f6ssing",
                "first_name": "Michael"
            },
            {
                "last_name": "K\u00fchl",
                "first_name": "Niklas"
            },
            {
                "last_name": "Martin",
                "first_name": "Dominik"
            },
            {
                "last_name": "Satzger",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  The constantly increasing capabilities of artificial intelligence (AI) open\nnew possibilities for human-AI collaboration. One promising approach to\nleverage existing complementary capabilities is allowing humans to delegate\nindividual instances to the AI. However, enabling humans to delegate instances\neffectively requires them to assess both their own and the AI's capabilities in\nthe context of the given task. In this work, we explore the effects of\nproviding contextual information on human decisions to delegate instances to an\nAI. We find that providing participants with contextual information\nsignificantly improves the human-AI team performance. Additionally, we show\nthat the delegation behavior changes significantly when participants receive\nvarying types of contextual information. Overall, this research advances the\nunderstanding of human-AI interaction in human delegation and provides\nactionable insights for designing more effective collaborative systems.\n",
        "title": "On the Effect of Contextual Information on Human Delegation Behavior in\n  Human-AI collaboration",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04730",
        "abstract_url": "http://arxiv.org/abs/2401.04730",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Ronglai"
            },
            {
                "last_name": "Wei",
                "first_name": "Fangyun"
            },
            {
                "last_name": "Chen",
                "first_name": "Zenggui"
            },
            {
                "last_name": "Mak",
                "first_name": "Brian"
            },
            {
                "last_name": "Yang",
                "first_name": "Jiaolong"
            },
            {
                "last_name": "Tong",
                "first_name": "Xin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The objective of this paper is to develop a functional system for translating\nspoken languages into sign languages, referred to as Spoken2Sign translation.\nThe Spoken2Sign task is orthogonal and complementary to traditional sign\nlanguage to spoken language (Sign2Spoken) translation. To enable Spoken2Sign\ntranslation, we present a simple baseline consisting of three steps: 1)\ncreating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2)\nestimating a 3D sign for each sign video in the dictionary; 3) training a\nSpoken2Sign model, which is composed of a Text2Gloss translator, a sign\nconnector, and a rendering module, with the aid of the yielded gloss-3D sign\ndictionary. The translation results are then displayed through a sign avatar.\nAs far as we know, we are the first to present the Spoken2Sign task in an\noutput format of 3D signs. In addition to its capability of Spoken2Sign\ntranslation, we also demonstrate that two by-products of our approach-3D\nkeypoint augmentation and multi-view understanding-can assist in keypoint-based\nsign language understanding. Code and models will be available at\nhttps://github.com/FangyunWei/SLRT\n",
        "title": "A Simple Baseline for Spoken Language to Sign Language Translation with\n  3D Avatars",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04732",
        "abstract_url": "http://arxiv.org/abs/2401.04732",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Manpreet"
            },
            {
                "last_name": "Pasricha",
                "first_name": "Ravdeep"
            },
            {
                "last_name": "Singh",
                "first_name": "Nitish"
            },
            {
                "last_name": "Kondapalli",
                "first_name": "Ravi Prasad"
            },
            {
                "last_name": "R",
                "first_name": "Manoj"
            },
            {
                "last_name": "R",
                "first_name": "Kiran"
            },
            {
                "last_name": "Bou\u00e9",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "LG"
        ],
        "abstract": "  In this paper, we design a real-time question-answering system specifically\ntargeted for helping sellers get relevant material/documentation they can share\nlive with their customers or refer to during a call. Taking the Seismic content\nrepository as a relatively large scale example of a diverse dataset of sales\nmaterial, we demonstrate how LLM embeddings of sellers' queries can be matched\nwith the relevant content. We achieve this by engineering prompts in an\nelaborate fashion that makes use of the rich set of meta-features available for\ndocuments and sellers. Using a bi-encoder with cross-encoder re-ranker\narchitecture, we show how the solution returns the most relevant content\nrecommendations in just a few seconds even for large datasets. Our recommender\nsystem is deployed as an AML endpoint for real-time inferencing and has been\nintegrated into a Copilot interface that is now deployed in the production\nversion of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.\n",
        "title": "A case study of Generative AI in MSX Sales Copilot: Improving seller\n  productivity with a real-time question-answering system for content\n  recommendation",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04734",
        "abstract_url": "http://arxiv.org/abs/2401.04734",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Xiaofan"
            },
            {
                "last_name": "Khan",
                "first_name": "Muhammad Aadil"
            },
            {
                "last_name": "Onori",
                "first_name": "Simona"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  A key challenge that is currently hindering the widespread deployment and use\nof retired electric vehicle (EV) batteries for second-life (SL) applications is\nthe ability to accurately estimate and monitor their state of health (SOH).\nSecond-life battery systems can be sourced from different battery packs with a\nlack of knowledge of their historical usage.\n  To facilitate the on-the-field use of SL batteries, this paper introduces an\nonline adaptive health estimation strategy with guaranteed stability. This\nmethod relies exclusively on operational data that can be accessed in real-time\nfrom SL batteries. The adaptation algorithm is designed to ensure\nbounded-input-bounded-output (BIBO) stability. The effectiveness of the\nproposed approach is shown on a laboratory-aged experimental data set of\nretired EV batteries. The estimator gains are dynamically adapted to\naccommodate the distinct characteristics of each individual cell, making it a\npromising candidate for future SL battery management systems (BMS2).\n",
        "title": "Online Adaptive Data-driven State-of-health Estimation for Second-life\n  Batteries with BIBO Stability Guarantees",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04736",
        "abstract_url": "http://arxiv.org/abs/2401.04736",
        "authors": [
            {
                "last_name": "Choudhury",
                "first_name": "Tashfique Hasnine"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  The extensive use of distributed vehicle platoon controllers has resulted in\nseveral benefits for transportation systems, such as increased traffic flow,\nfuel efficiency, and decreased pollution. The rising reliance on interconnected\nsystems and communication networks, on the other hand, exposes these\ncontrollers to potential cyber-attacks, which may compromise their safety and\nfunctionality. This thesis aims to improve the security of distributed vehicle\nplatoon controllers by investigating attack scenarios and assessing their\ninfluence on system performance. Various attack techniques, including\nman-in-the-middle (MITM) and false data injection (FDI), are simulated using\nModel Predictive Control (MPC) controller to identify vulnerabilities and\nweaknesses of the platoon controller. Countermeasures are offered and tested,\nthat includes attack analysis and reinforced communication protocols using\nMachine Learning techniques for detection. The findings emphasize the\nsignificance of integrating security issues into their design and\nimplementation, which helps to construct safe and resilient distributed platoon\ncontrollers.\n",
        "title": "Exploring Attack Resilience in Distributed Platoon Controllers with\n  Model Predictive Control",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04737",
        "abstract_url": "http://arxiv.org/abs/2401.04737",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Yigang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  In recent years, various well-designed algorithms have empowered music\nplatforms to provide content based on one's preferences. Music genres are\ndefined through various aspects, including acoustic features and cultural\nconsiderations. Music genre classification works well with content-based\nfiltering, which recommends content based on music similarity to users. Given a\nconsiderable dataset, one premise is automatic annotation using machine\nlearning or deep learning methods that can effectively classify audio files.\nThe effectiveness of systems largely depends on feature and model selection, as\ndifferent architectures and features can facilitate each other and yield\ndifferent results. In this study, we conduct a comparative study investigating\nthe performances of three models: a proposed convolutional neural network\n(CNN), the VGG16 with fully connected layers (FC), and an eXtreme Gradient\nBoosting (XGBoost) approach on different features: 30-second Mel spectrogram\nand 3-second Mel-frequency cepstral coefficients (MFCCs). The results show that\nthe MFCC XGBoost model outperformed the others. Furthermore, applying data\nsegmentation in the data preprocessing phase can significantly enhance the\nperformance of the CNNs.\n",
        "title": "Music Genre Classification: A Comparative Analysis of CNN and XGBoost\n  Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04739",
        "abstract_url": "http://arxiv.org/abs/2401.04739",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            },
            {
                "last_name": "Gao",
                "first_name": "Eryang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In recent years, the recognition of free-hand sketches has remained a popular\ntask. However, in some special fields such as the military field, free-hand\nsketches are difficult to sample on a large scale. Common data augmentation and\nimage generation techniques are difficult to produce images with various\nfree-hand sketching styles. Therefore, the recognition and segmentation tasks\nin related fields are limited. In this paper, we propose a novel adversarial\ngenerative network that can accurately generate realistic free-hand sketches\nwith various styles. We explore the performance of the model, including using\nstyles randomly sampled from a prior normal distribution to generate images\nwith various free-hand sketching styles, disentangling the painters' styles\nfrom known free-hand sketches to generate images with specific styles, and\ngenerating images of unknown classes that are not in the training set. We\nfurther demonstrate with qualitative and quantitative evaluations our\nadvantages in visual quality, content accuracy, and style imitation on\nSketchIME.\n",
        "title": "Content-Conditioned Generation of Stylized Free hand Sketches",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04740",
        "abstract_url": "http://arxiv.org/abs/2401.04740",
        "authors": [
            {
                "last_name": "Chenna",
                "first_name": "Dwith"
            },
            {
                "last_name": "Bhogawar",
                "first_name": "Suyash"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Brain extraction and removal of skull artifacts from magnetic resonance\nimages (MRI) is an important preprocessing step in neuroimaging analysis. There\nare many tools developed to handle human fMRI images, which could involve\nmanual steps for verifying results from brain segmentation that makes it time\nconsuming and inefficient. In this study, we will use the segment anything\nmodel (SAM), a freely available neural network released by Meta[4], which has\nshown promising results in many generic segmentation applications. We will\nanalyze the efficiency of SAM for neuroimaging brain segmentation by removing\nskull artifacts. The results of the experiments showed promising results that\nexplore using automated segmentation algorithms for neuroimaging without the\nneed to train on custom medical imaging dataset.\n",
        "title": "Segment anything model (SAM) for brain extraction in fMRI studies",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04741",
        "abstract_url": "http://arxiv.org/abs/2401.04741",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yuanchi"
            },
            {
                "last_name": "He",
                "first_name": "Hui"
            },
            {
                "last_name": "Lei",
                "first_name": "Zhongxiang"
            },
            {
                "last_name": "Niu",
                "first_name": "Zhendong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph clustering algorithms with autoencoder structures have recently gained\npopularity due to their efficient performance and low training cost. However,\nfor existing graph autoencoder clustering algorithms based on GCN or GAT, not\nonly do they lack good generalization ability, but also the number of clusters\nclustered by such autoencoder models is difficult to determine automatically.\nTo solve this problem, we propose a new framework called Graph Clustering with\nMasked Autoencoders (GCMA). It employs our designed fusion autoencoder based on\nthe graph masking method for the fusion coding of graph. It introduces our\nimproved density-based clustering algorithm as a second decoder while decoding\nwith multi-target reconstruction. By decoding the mask embedding, our model can\ncapture more generalized and comprehensive knowledge. The number of clusters\nand clustering results can be output end-to-end while improving the\ngeneralization ability. As a nonparametric class method, extensive experiments\ndemonstrate the superiority of \\textit{GCMA} over state-of-the-art baselines.\n",
        "title": "Masked AutoEncoder for Graph Clustering without Pre-defined Cluster\n  Number k",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04744",
        "abstract_url": "http://arxiv.org/abs/2401.04744",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            },
            {
                "last_name": "Hefenbrock",
                "first_name": "Michael"
            },
            {
                "last_name": "Prenat",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Anghel",
                "first_name": "Lorena"
            },
            {
                "last_name": "Tahoori",
                "first_name": "Mehdi B."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "AR",
            "LG"
        ],
        "abstract": "  Bayesian Neural Networks (BayNNs) can inherently estimate predictive\nuncertainty, facilitating informed decision-making. Dropout-based BayNNs are\nincreasingly implemented in spintronics-based computation-in-memory\narchitectures for resource-constrained yet high-performance safety-critical\napplications. Although uncertainty estimation is important, the reliability of\nDropout generation and BayNN computation is equally important for target\napplications but is overlooked in existing works. However, testing BayNNs is\nsignificantly more challenging compared to conventional NNs, due to their\nstochastic nature. In this paper, we present for the first time the model of\nthe non-idealities of the spintronics-based Dropout module and analyze their\nimpact on uncertainty estimates and accuracy. Furthermore, we propose a testing\nframework based on repeatability ranking for Dropout-based BayNN with up to\n$100\\%$ fault coverage while using only $0.2\\%$ of training data as test\nvectors.\n",
        "title": "Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian\n  Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04746",
        "abstract_url": "http://arxiv.org/abs/2401.04746",
        "authors": [
            {
                "last_name": "Himel",
                "first_name": "Galib Muhammad Shahriar"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Masudul"
            },
            {
                "last_name": "Al-Aff",
                "first_name": "Kh Abdullah"
            },
            {
                "last_name": "Karim",
                "first_name": "Shams Ibne"
            },
            {
                "last_name": "Sikder",
                "first_name": "Md. Kabir Uddin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Skin cancer is a global health concern, necessitating early and accurate\ndiagnosis for improved patient outcomes. This study introduces a groundbreaking\napproach to skin cancer classification, employing the Vision Transformer, a\nstate-of-the-art deep learning architecture renowned for its success in diverse\nimage analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously\nannotated skin lesion images, the model undergoes preprocessing for enhanced\nrobustness. The Vision Transformer, adapted to the skin cancer classification\ntask, leverages the self-attention mechanism to capture intricate spatial\ndependencies, achieving superior performance over traditional deep learning\narchitectures. Segment Anything Model aids in precise segmentation of cancerous\nareas, attaining high IOU and Dice Coefficient. Extensive experiments highlight\nthe model's supremacy, particularly the Google-based ViT patch-32 variant,\nwhich achieves 96.15% accuracy and showcases potential as an effective tool for\ndermatologists in skin cancer diagnosis, contributing to advancements in\ndermatological practices.\n",
        "title": "Skin Cancer Segmentation and Classification Using Vision Transformer for\n  Automatic Analysis in Dermatoscopy-based Non-invasive Digital System",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04747",
        "abstract_url": "http://arxiv.org/abs/2401.04747",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junming"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Ailing"
            },
            {
                "last_name": "Li",
                "first_name": "Yu"
            },
            {
                "last_name": "Chen",
                "first_name": "Qifeng"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "CV",
            "GR",
            ""
        ],
        "abstract": "  We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D\nExpression and Gesture generation with arbitrary length. While previous works\nfocused on co-speech gesture or expression generation individually, the joint\ngeneration of synchronized expressions and gestures remains barely explored. To\naddress this, our diffusion-based co-speech motion generation transformer\nenables uni-directional information flow from expression to gesture,\nfacilitating improved matching of joint expression-gesture distributions.\nFurthermore, we introduce an outpainting-based sampling strategy for arbitrary\nlong sequence generation in diffusion models, offering flexibility and\ncomputational efficiency. Our method provides a practical solution that\nproduces high-quality synchronized expression and gesture generation driven by\nspeech. Evaluated on two public datasets, our approach achieves\nstate-of-the-art performance both quantitatively and qualitatively.\nAdditionally, a user study confirms the superiority of DiffSHEG over prior\napproaches. By enabling the real-time generation of expressive and synchronized\nmotions, DiffSHEG showcases its potential for various applications in the\ndevelopment of digital humans and embodied agents.\n",
        "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven\n  Holistic 3D Expression and Gesture Generation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04748",
        "abstract_url": "http://arxiv.org/abs/2401.04748",
        "authors": [
            {
                "last_name": "Olisah",
                "first_name": "Chollette C."
            },
            {
                "last_name": "Trewhella",
                "first_name": "Ben"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Smith",
                "first_name": "Melvyn L."
            },
            {
                "last_name": "Winstone",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Whitfield",
                "first_name": "E. Charles"
            },
            {
                "last_name": "Fern\u00e1ndez",
                "first_name": "Felicidad Fern\u00e1ndez"
            },
            {
                "last_name": "Duncalfe",
                "first_name": "Harriet"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Fruit ripeness estimation models have for decades depended on spectral index\nfeatures or colour-based features, such as mean, standard deviation, skewness,\ncolour moments, and/or histograms for learning traits of fruit ripeness.\nRecently, few studies have explored the use of deep learning techniques to\nextract features from images of fruits with visible ripeness cues. However, the\nblackberry (Rubus fruticosus) fruit does not show obvious and reliable visible\ntraits of ripeness when mature and therefore poses great difficulty to fruit\npickers. The mature blackberry, to the human eye, is black before, during, and\npost-ripening. To address this engineering application challenge, this paper\nproposes a novel multi-input convolutional neural network (CNN) ensemble\nclassifier for detecting subtle traits of ripeness in blackberry fruits. The\nmulti-input CNN was created from a pre-trained visual geometry group 16-layer\ndeep convolutional network (VGG16) model trained on the ImageNet dataset. The\nfully connected layers were optimized for learning traits of ripeness of mature\nblackberry fruits. The resulting model served as the base for building\nhomogeneous ensemble learners that were ensemble using the stack generalization\nensemble (SGE) framework. The input to the network is images acquired with a\nstereo sensor using visible and near-infrared (VIS-NIR) spectral filters at\nwavelengths of 700 nm and 770 nm. Through experiments, the proposed model\nachieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-field\nconditions. Further experiments reveal that machine sensory is highly and\npositively correlated to human sensory over blackberry fruit skin texture.\n",
        "title": "Convolutional Neural Network Ensemble Learning for Hyperspectral\n  Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm\n  Environment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04749",
        "abstract_url": "http://arxiv.org/abs/2401.04749",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieqiao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bo"
            },
            {
                "last_name": "peng",
                "first_name": "Junran"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "SE"
        ],
        "abstract": "  Log anomaly detection is a key component in the field of artificial\nintelligence for IT operations (AIOps). Considering log data of variant\ndomains, retraining the whole network for unknown domains is inefficient in\nreal industrial scenarios. However, previous deep models merely focused on\nextracting the semantics of log sequences in the same domain, leading to poor\ngeneralization on multi-domain logs. To alleviate this issue, we propose a\nunified Transformer-based framework for Log anomaly detection (LogFormer) to\nimprove the generalization ability across different domains, where we establish\na two-stage process including the pre-training and adapter-based tuning stage.\nSpecifically, our model is first pre-trained on the source domain to obtain\nshared semantic knowledge of log data. Then, we transfer such knowledge to the\ntarget domain via shared parameters. Besides, the Log-Attention module is\nproposed to supplement the information ignored by the log-paring. The proposed\nmethod is evaluated on three public and one real-world datasets. Experimental\nresults on multiple benchmarks demonstrate the effectiveness of our LogFormer\nwith fewer trainable parameters and lower training costs.\n",
        "title": "LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04750",
        "abstract_url": "http://arxiv.org/abs/2401.04750",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shengli"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Lin",
                "first_name": "Sen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While dust significantly affects the environmental perception of automated\nagricultural machines, the existing deep learning-based methods for dust\nremoval require further research and improvement in this area to improve the\nperformance and reliability of automated agricultural machines in agriculture.\nWe propose an end-to-end trainable learning network (DedustNet) to solve the\nreal-world agricultural dust removal task. To our knowledge, DedustNet is the\nfirst time Swin Transformer-based units have been used in wavelet networks for\nagricultural image dusting. Specifically, we present the frequency-dominated\nblock (DWTFormer block and IDWTFormer block) by adding a spatial features\naggregation scheme (SFAS) to the Swin Transformer and combining it with the\nwavelet transform, the DWTFormer block and IDWTFormer block, alleviating the\nlimitation of the global receptive field of Swin Transformer when dealing with\ncomplex dusty backgrounds. Furthermore, We propose a cross-level information\nfusion module to fuse different levels of features and effectively capture\nglobal and long-range feature relationships. In addition, we present a dilated\nconvolution module to capture contextual information guided by wavelet\ntransform at multiple scales, which combines the advantages of wavelet\ntransform and dilated convolution. Our algorithm leverages deep learning\ntechniques to effectively remove dust from images while preserving the original\nstructural and textural features. Compared to existing state-of-the-art\nmethods, DedustNet achieves superior performance and more reliable results in\nagricultural image dedusting, providing strong support for the application of\nagricultural machinery in dusty environments. Additionally, the impressive\nperformance on real-world hazy datasets and application tests highlights\nDedustNet superior generalization ability and computer vision-related\napplication performance.\n",
        "title": "DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network\n  for Agricultural Dust Removal",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04751",
        "abstract_url": "http://arxiv.org/abs/2401.04751",
        "authors": [
            {
                "last_name": "Howard",
                "first_name": "Daniel Anthony"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "PF",
            ""
        ],
        "abstract": "  Improving energy efficiency in industrial production processes is crucial for\ncompetitiveness, and compliance with climate policies. This paper introduces a\ndata-driven approach to identify optimal melting patterns in induction\nfurnaces. Through time-series K-means clustering the melting patterns could be\nclassified into distinct clusters based on temperature profiles. Using the\nelbow method, 12 clusters were identified, representing the range of melting\npatterns. Performance parameters such as melting time, energy-specific\nperformance, and carbon cost were established for each cluster, indicating\nfurnace efficiency and environmental impact. Multiple criteria decision-making\nmethods including Simple Additive Weighting, Multiplicative Exponential\nWeighting, Technique for Order of Preference by Similarity to Ideal Solution,\nmodified TOPSIS, and VlseKriterijumska Optimizacija I Kompromisno Resenje were\nutilized to determine the best-practice cluster. The study successfully\nidentified the cluster with the best performance. Implementing the best\npractice operation resulted in an 8.6 % reduction in electricity costs,\nhighlighting the potential energy savings in the foundry.\n",
        "title": "Identifying Best Practice Melting Patterns in Induction Furnaces: A\n  Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria\n  Decision Making",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04757",
        "abstract_url": "http://arxiv.org/abs/2401.04757",
        "authors": [
            {
                "last_name": "Owen",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  We investigate large language model performance across five orders of\nmagnitude of compute scaling in eleven recent model architectures. We show that\naverage benchmark performance, aggregating over many individual tasks and\nevaluations as in the commonly-used BIG-Bench dataset, is decently predictable\nas a function of training compute scale. Specifically, when extrapolating\nBIG-Bench Hard performance across one order of magnitude in compute, we observe\naverage absolute errors of 6 percentage points (pp). By contrast, extrapolation\nfor individual BIG-Bench tasks across an order of magnitude in compute yields\nhigher average errors of 18pp. Nonetheless, individual task performance remains\nsignificantly more predictable than chance. Overall, our work suggests compute\nscaling provides a promising basis to forecast AI capabilities in diverse\nbenchmarks, though predicting performance in specific tasks poses challenges.\n",
        "title": "How predictable is language model benchmark performance?",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04758",
        "abstract_url": "http://arxiv.org/abs/2401.04758",
        "authors": [
            {
                "last_name": "Gatterbauer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Dunne",
                "first_name": "Cody"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "HC",
            "LO"
        ],
        "abstract": "  Comparing relational languages by their logical expressiveness is well\nunderstood. Less well understood is how to compare relational languages by\ntheir ability to represent relational query patterns. Indeed, what are query\npatterns other than \"a certain way of writing a query\"? And how can query\npatterns be defined across procedural and declarative languages, irrespective\nof their syntax? To the best of our knowledge, we provide the first semantic\ndefinition of relational query patterns by using a variant of\nstructure-preserving mappings between the relational tables of queries. This\nformalism allows us to analyze the relative pattern expressiveness of\nrelational language fragments and create a hierarchy of languages with equal\nlogical expressiveness yet different pattern expressiveness. Notably, for the\nnon-disjunctive language fragment, we show that relational calculus can express\na larger class of patterns than the basic operators of relational algebra.\n  Our language-independent definition of query patterns opens novel paths for\nassisting database users. For example, these patterns could be leveraged to\ncreate visual query representations that faithfully represent query patterns,\nspeed up interpretation, and provide visual feedback during query editing. As a\nconcrete example, we propose Relational Diagrams, a complete and sound\ndiagrammatic representation of safe relational calculus that is provably (i)\nunambiguous, (ii) relationally complete, and (iii) able to represent all query\npatterns for unions of non-disjunctive queries. Among all diagrammatic\nrepresentations for relational queries that we are aware of, ours is the only\none with these three properties. Furthermore, our anonymously preregistered\nuser study shows that Relational Diagrams allow users to recognize patterns\nmeaningfully faster and more accurately than SQL.\n",
        "title": "On The Reasonable Effectiveness of Relational Diagrams: Explaining\n  Relational Query Patterns and the Pattern Expressiveness of Relational\n  Languages",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04771",
        "abstract_url": "http://arxiv.org/abs/2401.04771",
        "authors": [
            {
                "last_name": "Smiley",
                "first_name": "Octavious"
            },
            {
                "last_name": "Hoffmann",
                "first_name": "Till"
            },
            {
                "last_name": "Onnela",
                "first_name": "Jukka-Pekka"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            ""
        ],
        "abstract": "  Network science explores intricate connections among objects, employed in\ndiverse domains like social interactions, fraud detection, and disease spread.\nVisualization of networks facilitates conceptualizing research questions and\nforming scientific hypotheses. Networks, as mathematical high-dimensional\nobjects, require dimensionality reduction for (planar) visualization.\nVisualizing empirical networks present additional challenges. They often\ncontain false positive (spurious) and false negative (missing) edges.\nTraditional visualization methods don't account for errors in observation,\npotentially biasing interpretations. Moreover, contemporary network data\nincludes rich nodal attributes. However, traditional methods neglect these\nattributes when computing node locations. Our visualization approach aims to\nleverage nodal attribute richness to compensate for network data limitations.\nWe employ a statistical model estimating the probability of edge connections\nbetween nodes based on their covariates. We enhance the Fruchterman-Reingold\nalgorithm to incorporate estimated dyad connection probabilities, allowing\npractitioners to balance reliance on observed versus estimated edges. We\nexplore optimal smoothing levels, offering a natural way to include relevant\nnodal information in layouts. Results demonstrate the effectiveness of our\nmethod in achieving robust network visualization, providing insights for\nimproved analysis.\n",
        "title": "Network Layout Algorithm with Covariate Smoothing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04778",
        "abstract_url": "http://arxiv.org/abs/2401.04778",
        "authors": [
            {
                "last_name": "Br\u00fcck",
                "first_name": "Florian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  In this work, we provide a simulation algorithm to simulate from a\n(multivariate) characteristic function, which is only accessible in a black-box\nformat. We construct a generative neural network, whose loss function exploits\na specific representation of the Maximum-Mean-Discrepancy metric to directly\nincorporate the targeted characteristic function. The construction is universal\nin the sense that it is independent of the dimension and that it does not\nrequire any assumptions on the given characteristic function. Furthermore,\nfinite sample guarantees on the approximation quality in terms of the\nMaximum-Mean Discrepancy metric are derived. The method is illustrated in a\nshort simulation study.\n",
        "title": "Generative neural networks for characteristic functions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04781",
        "abstract_url": "http://arxiv.org/abs/2401.04781",
        "authors": [
            {
                "last_name": "Mazaev",
                "first_name": "A. V."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In the development of materials for structural purposes, the main focus is on\nthe advantageous combination of mechanical and volume-mass properties. Due to\nthe development of production, solid plates are increasingly being replaced by\nmodern composite materials with improved properties, one of the varieties of\nwhich is layered composites with a honeycomb core. The most widespread are\nthree-layer plates with solid face layers and a hexagonal honeycomb core.\nHowever, with the development of new technologies, including 3D printing,\nhoneycombs with new geometries are gaining popularity, the mechanical\nproperties of which make it possible to obtain layered composites with improved\nfeatures. In this paper, three-layer plates with solid face layers and a\ntetrachiral honeycomb core are investigated. The influence of discretization\n(number of unit cells), relative density and thickness of the honeycomb core on\nthe stress state of three-layer composites subjected to static bending under\nvarious boundary conditions is studied. Mathematical modeling is carried out\nwithin the framework of the theory of elasticity by the finite element method\nvia three-dimensional modeling in the Comsol Multiphysics system, as well as\nusing algorithms developed by the author for analyzing the stress state of\nmultilayer plates with tetrachiral honeycombs by solving a plane problem of the\ntheory of elasticity. As a result, good agreement is shown between the\nnumerical results obtained using algorithms for solving a plane problem and via\nthree-dimensional finite element modeling in the Comsol Multiphysics system,\nwhile the numerical results are qualitatively consistent with laboratory test\ndata.\n",
        "title": "Mathematical modeling of mechanical behavior of three-layer plates with\n  tetrachiral honeycomb core",
        "date": "2023-11-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04783",
        "abstract_url": "http://arxiv.org/abs/2401.04783",
        "authors": [
            {
                "last_name": "Christlieb",
                "first_name": "Andrew J."
            },
            {
                "last_name": "Ding",
                "first_name": "Mingchang"
            },
            {
                "last_name": "Huang",
                "first_name": "Juntao"
            },
            {
                "last_name": "Krupansky",
                "first_name": "Nicholas A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We introduce a hyperbolic closure for the Grad moment expansion of the\nBhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained\non BGK's moment data. This closure is motivated by the exact closure for the\nfree streaming limit that we derived in our paper on closures in transport\n\\cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest\nmoment to the gradient of four lower moments. As with our past work, the model\npresented here learns the gradient of the highest moment in terms of the\ncoefficients of gradients for all lower ones. By necessity, this means that the\nresulting hyperbolic system is not conservative in the highest moment. For\nstability, the output layers of the NN are designed to enforce hyperbolicity\nand Galilean invariance. This ensures the model can be run outside of the\ntraining window of the NN. Unlike our previous work on radiation transport that\ndealt with linear models, the BGK model's nonlinearity demanded advanced\ntraining tools. These comprised an optimal learning rate discovery, one cycle\ntraining, batch normalization in each neural layer, and the use of the\n\\texttt{AdamW} optimizer. To address the non-conservative structure of the\nhyperbolic model, we adopt the FORCE numerical method to achieve robust\nsolutions. This results in a comprehensive computing model combining learned\nclosures with methods for solving hyperbolic models. The proposed model can\ncapture accurate moment solutions across a broad spectrum of Knudsen numbers.\nOur paper details the multi-scale model construction and is run on a range of\ntest problems.\n",
        "title": "Hyperbolic Machine Learning Moment Closures for the BGK Equations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04787",
        "abstract_url": "http://arxiv.org/abs/2401.04787",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Shih-Chi"
            },
            {
                "last_name": "Heide",
                "first_name": "A. Leonid"
            },
            {
                "last_name": "Hemati",
                "first_name": "Maziar S."
            },
            {
                "last_name": "Seiler",
                "first_name": "Peter J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Quadratic systems with lossless quadratic terms arise in many applications,\nincluding models of atmosphere and incompressible fluid flows. Such systems\nhave a trapping region if all trajectories eventually converge to and stay\nwithin a bounded set. Conditions for the existence and characterization of\ntrapping regions have been established in prior works for boundedness analysis.\nHowever, prior solutions have used non-convex optimization methods, resulting\nin conservative estimates. In this paper, we build on this prior work and\nprovide a convex semidefinite programming condition for the existence of a\ntrapping region. The condition allows precise verification or falsification of\nthe existence of a trapping region. If a trapping region exists, then we\nprovide a second semidefinite program to compute the least conservative\ntrapping region in the form of a ball. Two low-dimensional systems are provided\nas examples to illustrate the results. A third high-dimensional example is also\nincluded to demonstrate that the computation required for the analysis can be\nscaled to systems of up to $\\sim O(100)$ states. The proposed method provides a\nprecise and computationally efficient numerical approach for computing trapping\nregions. We anticipate this work will benefit future studies on modeling and\ncontrol of lossless quadratic dynamical systems.\n",
        "title": "A Convex Optimization Approach to Compute Trapping Regions for Lossless\n  Quadratic Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04791",
        "abstract_url": "http://arxiv.org/abs/2401.04791",
        "authors": [
            {
                "last_name": "Kinnari",
                "first_name": "Jouko"
            },
            {
                "last_name": "Thomas",
                "first_name": "Annika"
            },
            {
                "last_name": "Lusk",
                "first_name": "Parker"
            },
            {
                "last_name": "Kondo",
                "first_name": "Kota"
            },
            {
                "last_name": "How",
                "first_name": "Jonathan P."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  We present a novel framework for open-set Simultaneous Localization and\nMapping (SLAM) in unstructured environments that uses segmentation to create a\nmap of objects and geometric relationships between objects for localization.\nOur system consists of 1) a front-end mapping pipeline using a zero-shot\nsegmentation model to extract object masks from images and track them across\nframes to generate an object-based map and 2) a frame alignment pipeline that\nuses the geometric consistency of objects to efficiently localize within maps\ntaken in a variety of conditions. This approach is shown to be more robust to\nchanges in lighting and appearance than traditional feature-based SLAM systems\nor global descriptor methods. This is established by evaluating SOS-SLAM on the\nBatvik seasonal dataset which includes drone flights collected over a coastal\nplot of southern Finland during different seasons and lighting conditions.\nAcross flights during varying environmental conditions, our approach achieves\nhigher recall than benchmark methods with precision of 1.0. SOS-SLAM localizes\nwithin a reference map up to 14x faster than other feature based approaches and\nhas a map size less than 0.4% the size of the most compact other maps. When\nconsidering localization performance from varying viewpoints, our approach\noutperforms all benchmarks from the same viewpoint and most benchmarks from\ndifferent viewpoints. SOS-SLAM is a promising new approach for SLAM in\nunstructured environments that is robust to changes in lighting and appearance\nand is more computationally efficient than other approaches. We release our\ncode and datasets: https://acl.mit.edu/SOS-SLAM/.\n",
        "title": "SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04792",
        "abstract_url": "http://arxiv.org/abs/2401.04792",
        "authors": [
            {
                "last_name": "Hamad",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Finkenzeller",
                "first_name": "Andreas"
            },
            {
                "last_name": "K\u00fchr",
                "first_name": "Michael"
            },
            {
                "last_name": "Roberts",
                "first_name": "Andrew"
            },
            {
                "last_name": "Maennel",
                "first_name": "Olaf"
            },
            {
                "last_name": "Prevelakis",
                "first_name": "Vassilis"
            },
            {
                "last_name": "Steinhorst",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Autonomous and connected vehicles are rapidly evolving, integrating numerous\ntechnologies and software. This progress, however, has made them appealing\ntargets for cybersecurity attacks. As the risk of cyber threats escalates with\nthis advancement, the focus is shifting from solely preventing these attacks to\nalso mitigating their impact. Current solutions rely on vehicle security\noperation centers, where attack information is analyzed before deciding on a\nresponse strategy. However, this process can be time-consuming and faces\nscalability challenges, along with other issues stemming from vehicle\nconnectivity. This paper proposes a dynamic intrusion response system\nintegrated within the vehicle. This system enables the vehicle to respond to a\nvariety of incidents almost instantly, thereby reducing the need for\ninteraction with the vehicle security operation center. The system offers a\ncomprehensive list of potential responses, a methodology for response\nevaluation, and various response selection methods. The proposed solution was\nimplemented on an embedded platform. Two distinct cyberattack use cases served\nas the basis for evaluating the system. The evaluation highlights the system's\nadaptability, its ability to respond swiftly, its minimal memory footprint, and\nits capacity for dynamic system parameter adjustments. The proposed solution\nunderscores the necessity and feasibility of incorporating dynamic response\nmechanisms in smart vehicles. This is a crucial factor in ensuring the safety\nand resilience of future smart mobility.\n",
        "title": "REACT: Autonomous Intrusion Response System for Intelligent Vehicles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04795",
        "abstract_url": "http://arxiv.org/abs/2401.04795",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Gauri"
            },
            {
                "last_name": "Kapila",
                "first_name": "Ritvik"
            },
            {
                "last_name": "Chopra",
                "first_name": "Ayush"
            },
            {
                "last_name": "Raskar",
                "first_name": "Ramesh"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "LG",
            "SI",
            ""
        ],
        "abstract": "  Pandemics, notably the recent COVID-19 outbreak, have impacted both public\nhealth and the global economy. A profound understanding of disease progression\nand efficient response strategies is thus needed to prepare for potential\nfuture outbreaks. In this paper, we emphasize the potential of Agent-Based\nModels (ABM) in capturing complex infection dynamics and understanding the\nimpact of interventions. We simulate realistic pharmaceutical, behavioral, and\ndigital interventions that mirror challenges in real-world policy adoption and\nsuggest a holistic combination of these interventions for pandemic response.\nUsing these simulations, we study the trends of emergent behavior on a\nlarge-scale population based on real-world socio-demographic and geo-census\ndata from Kings County in Washington. Our analysis reveals the pivotal role of\nthe initial 100 days in dictating a pandemic's course, emphasizing the\nimportance of quick decision-making and efficient policy development. Further,\nwe highlight that investing in behavioral and digital interventions can reduce\nthe burden on pharmaceutical interventions by reducing the total number of\ninfections and hospitalizations, and by delaying the pandemic's peak. We also\ninfer that allocating the same amount of dollars towards extensive testing with\ncontact tracing and self-quarantine offers greater cost efficiency compared to\nspending the entire budget on vaccinations.\n",
        "title": "First 100 days of pandemic; an interplay of pharmaceutical, behavioral\n  and digital interventions -- A study using agent based modeling",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04799",
        "abstract_url": "http://arxiv.org/abs/2401.04799",
        "authors": [
            {
                "last_name": "Owonikoko",
                "first_name": "Waheed"
            },
            {
                "last_name": "Elsaadany",
                "first_name": "Mazen"
            },
            {
                "last_name": "Pandey",
                "first_name": "Amritanshu"
            },
            {
                "last_name": "Almassalkhi",
                "first_name": "Mads R."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  High penetration of renewable resources results in a power system with lower\ninertia and higher frequency sensitivity to power imbalances. Such systems are\nbecoming increasingly susceptible to frequency collapse during extreme\ndisturbances. Under-Frequency Load Shedding (UFLS) is a last-resort protection\nscheme and acts as an emergency brake by shedding load to arrest frequency\ndecline. Current and emerging efforts to optimize UFLS settings and frequency\nthresholds are mostly network agnostic, ignoring network spatial information.\nWith the prevalence of Distributed Energy Resources (DERs) in the\nhigh-renewable paradigm, the power grid is becoming more bidirectional, making\nsome locations in the network less effective for UFLS action than others. This\nwork proposes a Mixed Integer Linear Program that optimizes the UFLS setpoints\n(prioritizing one location over another) to minimize frequency deviation and\nload-shed for a given disturbance. The formulation considers system information\nand DER generation mix at different network locations, increasing model\nfidelity. The formulation also captures the discrete nature and practical time\ndelays and deadbands associated with UFLS using a minimal set of binary\nvariables, reducing problem complexity. We empirically validate the\noptimization approach on the dynamic IEEE 39-bus system for performance\nmetrics, including frequency nadir, steady-state frequency and total load shed.\n",
        "title": "Optimization-based Framework for Selecting Under-frequency Load Shedding\n  Parameters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04801",
        "abstract_url": "http://arxiv.org/abs/2401.04801",
        "authors": [
            {
                "last_name": "Vance",
                "first_name": "Nathan"
            },
            {
                "last_name": "Flynn",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Model architecture refinement is a challenging task in deep learning research\nfields such as remote photoplethysmography (rPPG). One architectural\nconsideration, the depth of the model, can have significant consequences on the\nresulting performance. In rPPG models that are overprovisioned with more layers\nthan necessary, redundancies exist, the removal of which can result in faster\ntraining and reduced computational load at inference time. With too few layers\nthe models may exhibit sub-optimal error rates. We apply Centered Kernel\nAlignment (CKA) to an array of rPPG architectures of differing depths,\ndemonstrating that shallower models do not learn the same representations as\ndeeper models, and that after a certain depth, redundant layers are added\nwithout significantly increased functionality. An empirical study confirms\nthese findings and shows how this method could be used to refine rPPG\narchitectures.\n",
        "title": "Refining Remote Photoplethysmography Architectures using CKA and\n  Empirical Methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04805",
        "abstract_url": "http://arxiv.org/abs/2401.04805",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Clifton Paul"
            },
            {
                "last_name": "Uvaydov",
                "first_name": "Daniel"
            },
            {
                "last_name": "D'Oro",
                "first_name": "Salvatore"
            },
            {
                "last_name": "Melodia",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Spectrum sensing is an essential component of modern wireless networks as it\noffers a tool to characterize spectrum usage and better utilize it. Deep\nLearning (DL) has become one of the most used techniques to perform spectrum\nsensing as they are capable of delivering high accuracy and reliability.\nHowever, current techniques suffer from ad-hoc implementations and high\ncomplexity, which makes them unsuited for practical deployment on wireless\nsystems where flexibility and fast inference time are necessary to support\nreal-time spectrum sensing. In this paper, we introduce DeepSweep, a novel\nDL-based transceiver design that allows scalable, accurate, and fast spectrum\nsensing while maintaining a high level of customizability to adapt its design\nto a broad range of application scenarios and use cases. DeepSweep is designed\nto be seamlessly integrated with well-established transceiver designs and\nleverages shallow convolutional neural network (CNN) to \"sweep\" the spectrum\nand process captured IQ samples fast and reliably without interrupting ongoing\ndemodulation and decoding operations. DeepSweep reduces training and inference\ntimes by more than 2 times and 10 times respectively, achieves up to 98 percent\naccuracy in locating spectrum activity, and produces outputs in less than 1 ms,\nthus showing that DeepSweep can be used for a broad range of spectrum sensing\napplications and scenarios.\n",
        "title": "DeepSweep: Parallel and Scalable Spectrum Sensing via Convolutional\n  Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04810",
        "abstract_url": "http://arxiv.org/abs/2401.04810",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Eugene"
            },
            {
                "last_name": "Lawrie",
                "first_name": "Dawn"
            },
            {
                "last_name": "Mayfield",
                "first_name": "James"
            },
            {
                "last_name": "Oard",
                "first_name": "Douglas W."
            },
            {
                "last_name": "Miller",
                "first_name": "Scott"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Prior work on English monolingual retrieval has shown that a cross-encoder\ntrained using a large number of relevance judgments for query-document pairs\ncan be used as a teacher to train more efficient, but similarly effective,\ndual-encoder student models. Applying a similar knowledge distillation approach\nto training an efficient dual-encoder model for Cross-Language Information\nRetrieval (CLIR), where queries and documents are in different languages, is\nchallenging due to the lack of a sufficiently large training collection when\nthe query and document languages differ. The state of the art for CLIR thus\nrelies on translating queries, documents, or both from the large English MS\nMARCO training set, an approach called Translate-Train. This paper proposes an\nalternative, Translate-Distill, in which knowledge distillation from either a\nmonolingual cross-encoder or a CLIR cross-encoder is used to train a\ndual-encoder CLIR student model. This richer design space enables the teacher\nmodel to perform inference in an optimized setting, while training the student\nmodel directly for CLIR. Trained models and artifacts are publicly available on\nHuggingface.\n",
        "title": "Translate-Distill: Learning Cross-Language Dense Retrieval by\n  Translation and Distillation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04812",
        "abstract_url": "http://arxiv.org/abs/2401.04812",
        "authors": [
            {
                "last_name": "Zhai",
                "first_name": "Yaoguang"
            },
            {
                "last_name": "Qin",
                "first_name": "Zhizhen"
            },
            {
                "last_name": "Gao",
                "first_name": "Sicun"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Standard approaches for global optimization of non-convex functions, such as\nbranch-and-bound, maintain partition trees to systematically prune the domain.\nThe tree size grows exponentially in the number of dimensions. We propose new\nsampling-based methods for non-convex optimization that adapts Monte Carlo Tree\nSearch (MCTS) to improve efficiency. Instead of the standard use of visitation\ncount in Upper Confidence Bounds, we utilize numerical overapproximations of\nthe objective as an uncertainty metric, and also take into account of sampled\nestimates of first-order and second-order information. The Monte Carlo tree in\nour approach avoids the usual fixed combinatorial patterns in growing the tree,\nand aggressively zooms into the promising regions, while still balancing\nexploration and exploitation. We evaluate the proposed algorithms on\nhigh-dimensional non-convex optimization benchmarks against competitive\nbaselines and analyze the effects of the hyper parameters.\n",
        "title": "Sample-and-Bound for Non-Convex Optimization",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04820",
        "abstract_url": "http://arxiv.org/abs/2401.04820",
        "authors": [
            {
                "last_name": "\u00c7olhak",
                "first_name": "Furkan"
            },
            {
                "last_name": "Ecevit",
                "first_name": "Mert \u0130lhan"
            },
            {
                "last_name": "U\u00e7ar",
                "first_name": "Bilal Emir"
            },
            {
                "last_name": "Creutzburg",
                "first_name": "Reiner"
            },
            {
                "last_name": "Da\u011f",
                "first_name": "Hasan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The way we communicate and work has changed significantly with the rise of\nthe Internet. While it has opened up new opportunities, it has also brought\nabout an increase in cyber threats. One common and serious threat is phishing,\nwhere cybercriminals employ deceptive methods to steal sensitive\ninformation.This study addresses the pressing issue of phishing by introducing\nan advanced detection model that meticulously focuses on HTML content. Our\nproposed approach integrates a specialized Multi-Layer Perceptron (MLP) model\nfor structured tabular data and two pretrained Natural Language Processing\n(NLP) models for analyzing textual features such as page titles and content.\nThe embeddings from these models are harmoniously combined through a novel\nfusion process. The resulting fused embeddings are then input into a linear\nclassifier. Recognizing the scarcity of recent datasets for comprehensive\nphishing research, our contribution extends to the creation of an up-to-date\ndataset, which we openly share with the community. The dataset is meticulously\ncurated to reflect real-life phishing conditions, ensuring relevance and\napplicability. The research findings highlight the effectiveness of the\nproposed approach, with the CANINE demonstrating superior performance in\nanalyzing page titles and the RoBERTa excelling in evaluating page content. The\nfusion of two NLP and one MLP model,termed MultiText-LP, achieves impressive\nresults, yielding a 96.80 F1 score and a 97.18 accuracy score on our research\ndataset. Furthermore, our approach outperforms existing methods on the\nCatchPhish HTML dataset, showcasing its efficacies.\n",
        "title": "Phishing Website Detection through Multi-Model Analysis of HTML Content",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04821",
        "abstract_url": "http://arxiv.org/abs/2401.04821",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Haotian"
            },
            {
                "last_name": "Liu",
                "first_name": "Yihong"
            },
            {
                "last_name": "Ma",
                "first_name": "Chunlan"
            },
            {
                "last_name": "Sch\u00fctze",
                "first_name": "Hinrich"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Transformer-based pre-trained language models (PLMs) have achieved remarkable\nperformance in various natural language processing (NLP) tasks. However,\npre-training such models can take considerable resources that are almost only\navailable to high-resource languages. On the contrary, static word embeddings\nare easier to train in terms of computing resources and the amount of data\nrequired. In this paper, we introduce MoSECroT Model Stitching with Static Word\nEmbeddings for Crosslingual Zero-shot Transfer), a novel and challenging task\nthat is especially relevant to low-resource languages for which static word\nembeddings are available. To tackle the task, we present the first framework\nthat leverages relative representations to construct a common space for the\nembeddings of a source language PLM and the static word embeddings of a target\nlanguage. In this way, we can train the PLM on source-language training data\nand perform zero-shot transfer to the target language by simply swapping the\nembedding layer. However, through extensive experiments on two classification\ndatasets, we show that although our proposed framework is competitive with weak\nbaselines when addressing MoSECroT, it fails to achieve competitive results\ncompared with some strong baselines. In this paper, we attempt to explain this\nnegative result and provide several thoughts on possible improvement.\n",
        "title": "MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual\n  Zero-shot Transfer",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04823",
        "abstract_url": "http://arxiv.org/abs/2401.04823",
        "authors": [
            {
                "last_name": "\u0160petl\u00edk",
                "first_name": "Martin"
            },
            {
                "last_name": "B\u0159ezina",
                "first_name": "Jan"
            },
            {
                "last_name": "Laloy",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Simulating 2D flow in fractured crystalline rock requires 2D stochastic\ndiscrete-fracture matrix (DFM) models. To obtain the simulation statistics of\ninterest at an affordable computational cost, we aim to use the multilevel\nMonte Carlo method. To use this multiscale approach, one needs to upscale the\nhydraulic conductivity of the fractures by numerical homogenization. In this\nwork, we substitute numerical homogenization with a surrogate model to speed up\nthe computations. In particular, we resort to a deep convolutional neural\nnetwork (CNN) connected to a deep feed-forward neural network. The equivalent\nhydraulic conductivity tensor $K_{eq}$ is predicted based on an input spatial\nrandom field (SRF) of hydraulic conductivity tensors, cross-section, and\nhydraulic conductivity of fractures. Three independent surrogates with the same\narchitecture are trained using data from DFM models with three different ratios\nof hydraulic conductivities of fracture and bulk $K_f/K_b$. As the $K_f/K_b$\nratio increases, the multivariate $K_{eq}$ distribution becomes more complex,\nand thus, the prediction accuracy of the trained surrogates deteriorates.\nRegardless of $K_f/K_b$, however, an improvement in the prediction accuracy of\nthe trained surrogates is noted as the considered fracture density of the\nmodeling setup decreases. We also investigate prediction accuracy on input SRFs\nof different correlation lengths. Upscaling by numerical homogenization and by\nsurrogate modeling is compared on two practical problems: upscaling of the\nhydraulic conductivity tensor and groundwater flow through a given surface. We\nobtained equally accurate results for the equivalent hydraulic tensor\ncalculation of upscaled DFM models regardless of the upscaling method. For the\ngroundwater flow problem, the accuracy of quantity of interest imitates the\naccuracy of $K_{eq}$ predictions.\n",
        "title": "Deep learning surrogate for predicting hydraulic conductivity tensors\n  from stochastic discrete fracture-matrix models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04827",
        "abstract_url": "http://arxiv.org/abs/2401.04827",
        "authors": [
            {
                "last_name": "Barrett",
                "first_name": "Christopher"
            },
            {
                "last_name": "Bura",
                "first_name": "Andrei"
            },
            {
                "last_name": "Huang",
                "first_name": "Fenix"
            },
            {
                "last_name": "Reidys",
                "first_name": "Christian"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A new perspective is introduced regarding the analysis of Multiple Sequence\nAlignments (MSA), representing aligned data defined over a finite alphabet of\nsymbols. The framework is designed to produce a block decomposition of an MSA,\nwhere each block is comprised of sequences exhibiting a certain site-coherence.\nThe key component of this framework is an information theoretical potential\ndefined on pairs of sites (links) within the MSA. This potential quantifies the\nexpected drop in variation of information between the two constituent sites,\nwhere the expectation is taken with respect to all possible sub-alignments,\nobtained by removing a finite, fixed collection of rows. It is proved that the\npotential is zero for linked sites representing columns, whose symbols are in\nbijective correspondence and it is strictly positive, otherwise. It is\nfurthermore shown that the potential assumes its unique minimum for links at\nwhich each symbol pair appears with the same multiplicity. Finally, an\napplication is presented regarding anomaly detection in an MSA, composed of\ninverse fold solutions of a fixed tRNA secondary structure, where the anomalies\nare represented by inverse fold solutions of a different RNA structure.\n",
        "title": "The site linkage spectrum of data arrays",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04829",
        "abstract_url": "http://arxiv.org/abs/2401.04829",
        "authors": [
            {
                "last_name": "Akkas",
                "first_name": "Selahattin"
            },
            {
                "last_name": "Azad",
                "first_name": "Ariful"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Graph neural networks (GNNs) are popular machine learning models for graphs\nwith many applications across scientific domains. However, GNNs are considered\nblack box models, and it is challenging to understand how the model makes\npredictions. Game theory-based Shapley value approaches are popular explanation\nmethods in other domains but are not well-studied for graphs. Some studies have\nproposed Shapley value-based GNN explanations, yet they have several\nlimitations: they consider limited samples to approximate Shapley values; some\nmainly focus on small and large coalition sizes, and they are an order of\nmagnitude slower than other explanation methods, making them inapplicable to\neven moderate-size graphs. In this work, we propose GNNShap, which provides\nexplanations for edges since they provide more natural explanations for graphs\nand more fine-grained explanations. We overcome the limitations by sampling\nfrom all coalition sizes, parallelizing the sampling on GPUs, and speeding up\nmodel predictions by batching. GNNShap gives better fidelity scores and faster\nexplanations than baselines on real-world datasets.\n",
        "title": "GNNShap: Fast and Accurate GNN Explanations using Shapley Values",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04831",
        "abstract_url": "http://arxiv.org/abs/2401.04831",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Jaeyoung"
            },
            {
                "last_name": "Achermann",
                "first_name": "Florian"
            },
            {
                "last_name": "Girod",
                "first_name": "Rik"
            },
            {
                "last_name": "Lawrance",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Siegwart",
                "first_name": "Roland"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Fixed-wing aerial vehicles provide an efficient way to navigate long\ndistances or cover large areas for environmental monitoring applications. By\ndesign, they also require large open spaces due to limited maneuverability.\nHowever, strict regulatory and safety altitude limits constrain the available\nspace. Especially in complex, confined, or steep terrain, ensuring the vehicle\ndoes not enter an inevitable collision state(ICS) can be challenging. In this\nwork, we propose a strategy to find safe paths that do not enter an ICS while\nnavigating within tight altitude constraints. The method uses periodic paths to\nefficiently classify ICSs. A sampling-based planner creates collision-free and\nkinematically feasible paths that begin and end in safe periodic (circular)\npaths. We show that, in realistic terrain, using circular periodic paths can\nsimplify the goal selection process by making it yaw agnostic and constraining\nyaw. We demonstrate our approach by dynamically planning safe paths in\nreal-time while navigating steep terrain on a flight test in complex alpine\nterrain.\n",
        "title": "Safe Low-Altitude Navigation in Steep Terrain with Fixed-Wing Aerial\n  Vehicles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04836",
        "abstract_url": "http://arxiv.org/abs/2401.04836",
        "authors": [
            {
                "last_name": "Raje",
                "first_name": "Saurabh"
            },
            {
                "last_name": "Xu",
                "first_name": "Yufan"
            },
            {
                "last_name": "Rountev",
                "first_name": "Atanas"
            },
            {
                "last_name": "Valeev",
                "first_name": "Edward F."
            },
            {
                "last_name": "Sadayappan",
                "first_name": "Saday"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "DC",
            "PF"
        ],
        "abstract": "  Sparse tensor networks are commonly used to represent contractions over\nsparse tensors. Tensor contractions are higher-order analogs of matrix\nmultiplication. Tensor networks arise commonly in many domains of scientific\ncomputing and data science. After a transformation into a tree of binary\ncontractions, the network is implemented as a sequence of individual\ncontractions. Several critical aspects must be considered in the generation of\nefficient code for a contraction tree, including sparse tensor layout mode\norder, loop fusion to reduce intermediate tensors, and the interdependence of\nloop order, mode order, and contraction order. We propose CoNST, a novel\napproach that considers these factors in an integrated manner using a single\nformulation. Our approach creates a constraint system that encodes these\ndecisions and their interdependence, while aiming to produce reduced-order\nintermediate tensors via fusion. The constraint system is solved by the Z3 SMT\nsolver and the result is used to create the desired fused loop structure and\ntensor mode layouts for the entire contraction tree. This structure is lowered\nto the IR of the TACO compiler, which is then used to generate executable code.\nOur experimental evaluation demonstrates very significant (sometimes orders of\nmagnitude) performance improvements over current state-of-the-art sparse tensor\ncompiler/library alternatives.\n",
        "title": "CoNST: Code Generator for Sparse Tensor Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04837",
        "abstract_url": "http://arxiv.org/abs/2401.04837",
        "authors": [
            {
                "last_name": "Belgiovine",
                "first_name": "Mauro"
            },
            {
                "last_name": "Groen",
                "first_name": "Joshua"
            },
            {
                "last_name": "Sirera",
                "first_name": "Miquel"
            },
            {
                "last_name": "Tassie",
                "first_name": "Chinenye"
            },
            {
                "last_name": "Yildiz",
                "first_name": "Ayberk Yarkin"
            },
            {
                "last_name": "Trudeau",
                "first_name": "Sage"
            },
            {
                "last_name": "Ioannidis",
                "first_name": "Stratis"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Kaushik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI",
            ""
        ],
        "abstract": "  Spectrum sharing allows different protocols of the same standard (e.g.,\n802.11 family) or different standards (e.g., LTE and DVB) to coexist in\noverlapping frequency bands. As this paradigm continues to spread, wireless\nsystems must also evolve to identify active transmitters and unauthorized\nwaveforms in real time under intentional distortion of preambles, extremely low\nsignal-to-noise ratios and challenging channel conditions. We overcome\nlimitations of correlation-based preamble matching methods in such conditions\nthrough the design of T-PRIME: a Transformer-based machine learning approach.\nT-PRIME learns the structural design of transmitted frames through its\nattention mechanism, looking at sequence patterns that go beyond the preamble\nalone. The paper makes three contributions: First, it compares Transformer\nmodels and demonstrates their superiority over traditional methods and\nstate-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's\nreal-time feasibility on DeepWave's AIR-T platform. Third, it utilizes an\nextensive 66 GB dataset of over-the-air (OTA) WiFi transmissions for training,\nwhich is released along with the code for community use. Results reveal nearly\nperfect (i.e. $>98\\%$) classification accuracy under simulated scenarios,\nshowing $100\\%$ detection improvement over legacy methods in low SNR ranges,\n$97\\%$ classification accuracy for OTA single-protocol transmissions and up to\n$75\\%$ double-protocol classification accuracy in interference scenarios.\n",
        "title": "T-PRIME: Transformer-based Protocol Identification for Machine-learning\n  at the Edge",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04842",
        "abstract_url": "http://arxiv.org/abs/2401.04842",
        "authors": [
            {
                "last_name": "Arabzadeh",
                "first_name": "Negar"
            },
            {
                "last_name": "Bigdeli",
                "first_name": "Amin"
            },
            {
                "last_name": "Clarke",
                "first_name": "Charles L. A."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large language models can now directly generate answers to many factual\nquestions without referencing external sources. Unfortunately, relatively\nlittle attention has been paid to methods for evaluating the quality and\ncorrectness of these answers, for comparing the performance of one model to\nanother, or for comparing one prompt to another. In addition, the quality of\ngenerated answers are rarely directly compared to the quality of retrieved\nanswers. As models evolve and prompts are modified, we have no systematic way\nto measure improvements without resorting to expensive human judgments. To\naddress this problem we adapt standard retrieval benchmarks to evaluate answers\ngenerated by large language models. Inspired by the BERTScore metric for\nsummarization, we explore two approaches. In the first, we base our evaluation\non the benchmark relevance judgments. We empirically run experiments on how\ninformation retrieval relevance judgments can be utilized as an anchor to\nevaluating the generated answers. In the second, we compare generated answers\nto the top results retrieved by a diverse set of retrieval models, ranging from\ntraditional approaches to advanced methods, allowing us to measure improvements\nwithout human judgments. In both cases, we measure the similarity between an\nembedded representation of the generated answer and an embedded representation\nof a known, or assumed, relevant passage from the retrieval benchmark.\n",
        "title": "Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04846",
        "abstract_url": "http://arxiv.org/abs/2401.04846",
        "authors": [
            {
                "last_name": "Glinsky",
                "first_name": "Michael E."
            },
            {
                "last_name": "Sievert",
                "first_name": "Sharon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper will examine what makes a being intelligent, whether that be a\nbiological being or an artificial silicon being on a computer. Special\nattention will be paid to the being having the ability to characterize and\ncontrol a collective system of many identical conservative sub-systems\nconservatively interacting. The essence of intelligence will be found to be the\ngolden rule -- \"the collective acts as one\" or \"knowing the global consequences\nof local actions\". The flow of the collective is a small set of twinkling\ntextures, that are governed by a puppeteer who is pulling a small number of\nstrings according to a geodesic motion of least action, determined by the\nsymmetries. Controlling collective conservative systems is difficult and has\nhistorically been done by adding significant viscosity to the system to\nstabilize the desirable meta stable equilibriums of maximum performance, but it\ndegrades or destroys them in the process. There is an alternative. Once the\noptimum twinkling textures of the meta stable equilibriums are identified by\nthe intelligent being (that is the collective system is characterized), the\ncollective system can be moved by the intelligent being to the optimum\ntwinkling textures, then quickly vibrated by the intelligent being according to\nthe textures so that the collective system remains at the meta stable\nequilibrium. Well educated intelligence knows the global consequences of its\nlocal actions so that it will not take short term actions that will lead to\npoor long term outcomes. In contrast, trained intelligence or trained stupidity\nwill optimize its short term actions, leading to poor long term outcomes. Well\neducated intelligence is inherently good, but trained stupidity is inherently\nevil and should be feared. Particular attention is paid to the control and\noptimization of economic and social collectives.\n",
        "title": "The inherent goodness of well educated intelligence",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04847",
        "abstract_url": "http://arxiv.org/abs/2401.04847",
        "authors": [
            {
                "last_name": "Won",
                "first_name": "Joong-Ho"
            },
            {
                "last_name": "Jung",
                "first_name": "Jinan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  This paper presents an in-depth analysis of the generalized isotonic\nrecursive partitioning (GIRP) algorithm for fitting isotonic models under\nseparable convex losses, proposed by Luss and Rosset [J. Comput. Graph.\nStatist., 23 (2014), pp. 192--201] for differentiable losses and extended by\nPainsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp.\n308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive\nfeature that in each step of the algorithm, the intermediate solution satisfies\nthe isotonicity constraint. The paper begins with an example showing that the\nGIRP algorithm as described in the literature may fail to produce an isotonic\nmodel, suggesting that the existence and uniqueness of the solution to the\nisotonic regression problem must be carefully addressed. It proceeds with\nshowing that, among possibly many solutions, there indeed exists a solution\nthat can be found by recursive binary partitioning of the set of observed data.\nA small modification of the GIRP algorithm suffices to obtain a correct\nsolution and preserve the desired property that all the intermediate solutions\nare isotonic. This proposed modification includes a proper choice of\nintermediate solutions and a simplification of the partitioning step from\nternary to binary.\n",
        "title": "On the Correctness of the Generalized Isotonic Recursive Partitioning\n  Algorithm",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04848",
        "abstract_url": "http://arxiv.org/abs/2401.04848",
        "authors": [
            {
                "last_name": "Skiredj",
                "first_name": "Abderrahman"
            },
            {
                "last_name": "Berrada",
                "first_name": "Ismail"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Automatic diacritization of Arabic text involves adding diacritical marks\n(diacritics) to the text. This task poses a significant challenge with\nnoteworthy implications for computational processing and comprehension. In this\npaper, we introduce PTCAD (Pre-FineTuned Token Classification for Arabic\nDiacritization, a novel two-phase approach for the Arabic Text Diacritization\ntask. PTCAD comprises a pre-finetuning phase and a finetuning phase, treating\nArabic Text Diacritization as a token classification task for pre-trained\nmodels. The effectiveness of PTCAD is demonstrated through evaluations on two\nbenchmark datasets derived from the Tashkeela dataset, where it achieves\nstate-of-the-art results, including a 20\\% reduction in Word Error Rate (WER)\ncompared to existing benchmarks and superior performance over GPT-4 in ATD\ntasks.\n",
        "title": "Arabic Text Diacritization In The Age Of Transfer Learning: Token\n  Classification Is All You Need",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04849",
        "abstract_url": "http://arxiv.org/abs/2401.04849",
        "authors": [
            {
                "last_name": "Hao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Existing Spatial Interaction Models (SIMs) are limited in capturing the\ncomplex and context-aware interactions between business clusters and trade\nareas. To address the limitation, we propose a SIM-GAT model to predict\nspatiotemporal visitation flows between community business clusters and their\ntrade areas. The model innovatively represents the integrated system of\nbusiness clusters, trade areas, and transportation infrastructure within an\nurban region using a connected graph. Then, a graph-based deep learning model,\ni.e., Graph AttenTion network (GAT), is used to capture the complexity and\ninterdependencies of business clusters. We developed this model with data\ncollected from the Miami metropolitan area in Florida. We then demonstrated its\neffectiveness in capturing varying attractiveness of business clusters to\ndifferent residential neighborhoods and across scenarios with an eXplainable AI\napproach. We contribute a novel method supplementing conventional SIMs to\npredict and analyze the dynamics of inter-connected community business\nclusters. The analysis results can inform data-evidenced and place-specific\nplanning strategies helping community business clusters better accommodate\ntheir customers across scenarios, and hence improve the resilience of community\nbusinesses.\n",
        "title": "A Deep Learning Representation of Spatial Interaction Model for\n  Resilient Spatial Planning of Community Business Clusters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04850",
        "abstract_url": "http://arxiv.org/abs/2401.04850",
        "authors": [
            {
                "last_name": "Abdelmoniem",
                "first_name": "Ahmed M."
            },
            {
                "last_name": "Bensaou",
                "first_name": "Brahim"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The peculiar congestion patterns in data centers are caused by the bursty and\ncomposite nature of traffic, the small bandwidth-delay product, and the tiny\nswitch buffers. It is not practical to modify TCP to adapt to data centers,\nespecially in public clouds where multiple congestion control protocols\ncoexist. In this work, we design a switch-based method to address such\ncongestion issues; our approach does not require any modification to TCP, which\nenables easy and seamless deployment in public data centers via switch software\nupdate. We first present a simple analysis to demonstrate the stability and\neffectiveness of the scheme, and then we discuss a hardware NetFPGA\nswitch-based prototype. The experimental results from real deployments in a\nsmall testbed cluster show the effectiveness of our approach.\n",
        "title": "FairQ: Fair and Fast Rate Allocation in Data Centers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04851",
        "abstract_url": "http://arxiv.org/abs/2401.04851",
        "authors": [
            {
                "last_name": "Paul",
                "first_name": "Steve"
            },
            {
                "last_name": "Witter",
                "first_name": "Jhoel"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Souma"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "LG"
        ],
        "abstract": "  This paper develops a graph reinforcement learning approach to online\nplanning of the schedule and destinations of electric aircraft that comprise an\nurban air mobility (UAM) fleet operating across multiple vertiports. This fleet\nscheduling problem is formulated to consider time-varying demand, constraints\nrelated to vertiport capacity, aircraft capacity and airspace safety\nguidelines, uncertainties related to take-off delay, weather-induced route\nclosures, and unanticipated aircraft downtime. Collectively, such a formulation\npresents greater complexity, and potentially increased realism, than in\nexisting UAM fleet planning implementations. To address these complexities, a\nnew policy architecture is constructed, primary components of which include:\ngraph capsule conv-nets for encoding vertiport and aircraft-fleet states both\nabstracted as graphs; transformer layers encoding time series information on\ndemand and passenger fare; and a Multi-head Attention-based decoder that uses\nthe encoded information to compute the probability of selecting each available\ndestination for an aircraft. Trained with Proximal Policy Optimization, this\npolicy architecture shows significantly better performance in terms of daily\naveraged profits on unseen test scenarios involving 8 vertiports and 40\naircraft, when compared to a random baseline and genetic algorithm-derived\noptimal solutions, while being nearly 1000 times faster in execution than the\nlatter.\n",
        "title": "Graph Learning-based Fleet Scheduling for Urban Air Mobility under\n  Operational Constraints, Varying Demand & Uncertainties",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04852",
        "abstract_url": "http://arxiv.org/abs/2401.04852",
        "authors": [
            {
                "last_name": "Askari",
                "first_name": "Arian"
            },
            {
                "last_name": "Yang",
                "first_name": "Zihui"
            },
            {
                "last_name": "Ren",
                "first_name": "Zhaochun"
            },
            {
                "last_name": "Verberne",
                "first_name": "Suzan"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The task of answer retrieval in the legal domain aims to help users to seek\nrelevant legal advice from massive amounts of professional responses. Two main\nchallenges hinder applying existing answer retrieval approaches in other\ndomains to the legal domain: (1) a huge knowledge gap between lawyers and\nnon-professionals; and (2) a mix of informal and formal content on legal QA\nwebsites. To tackle these challenges, we propose CE_FS, a novel cross-encoder\n(CE) re-ranker based on the fine-grained structured inputs. CE_FS uses\nadditional structured information in the CQA data to improve the effectiveness\nof cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world\nbenchmark dataset for evaluating answer retrieval in the legal domain.\nExperiments conducted on LegalQA show that our proposed method significantly\noutperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel\nfinding is that adding the question tags of each question besides the question\ndescription and title into the input of cross-encoder re-rankers structurally\nboosts the rankers' effectiveness. While we study our proposed method in the\nlegal domain, we believe that our method can be applied in similar applications\nin other domains.\n",
        "title": "Answer Retrieval in Legal Community Question Answering",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04853",
        "abstract_url": "http://arxiv.org/abs/2401.04853",
        "authors": [
            {
                "last_name": "Babaian",
                "first_name": "Tamara"
            },
            {
                "last_name": "Xu",
                "first_name": "Jennifer"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Extraction of concepts and entities of interest from non-formal texts such as\nsocial media posts and informal communication is an important capability for\ndecision support systems in many domains, including healthcare, customer\nrelationship management, and others. Despite the recent advances in training\nlarge language models for a variety of natural language processing tasks, the\ndeveloped models and techniques have mainly focused on formal texts and do not\nperform as well on colloquial data, which is characterized by a number of\ndistinct challenges. In our research, we focus on the healthcare domain and\ninvestigate the problem of symptom recognition from colloquial texts by\ndesigning and evaluating several training strategies for BERT-based model\nfine-tuning. These strategies are distinguished by the choice of the base\nmodel, the training corpora, and application of term perturbations in the\ntraining data. The best-performing models trained using these strategies\noutperform the state-of-the-art specialized symptom recognizer by a large\nmargin. Through a series of experiments, we have found specific patterns of\nmodel behavior associated with the training strategies we designed. We present\ndesign principles for training strategies for effective entity recognition in\ncolloquial texts based on our findings.\n",
        "title": "Entity Recognition from Colloquial Text",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04854",
        "abstract_url": "http://arxiv.org/abs/2401.04854",
        "authors": [
            {
                "last_name": "Lederman",
                "first_name": "Harvey"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Are LLMs cultural technologies like photocopiers or printing presses, which\ntransmit information but cannot create new content? A challenge for this idea,\nwhich we call bibliotechnism, is that LLMs often do generate entirely novel\ntext. We begin by defending bibliotechnism against this challenge, showing how\nnovel text may be meaningful only in a derivative sense, so that the content of\nthis generated text depends in an important sense on the content of original\nhuman text. We go on to present a different, novel challenge for\nbibliotechnism, stemming from examples in which LLMs generate \"novel\nreference\", using novel names to refer to novel entities. Such examples could\nbe smoothly explained if LLMs were not cultural technologies but possessed a\nlimited form of agency (beliefs, desires, and intentions). According to\ninterpretationism in the philosophy of mind, a system has beliefs, desires and\nintentions if and only if its behavior is well-explained by the hypothesis that\nit has such states. In line with this view, we argue that cases of novel\nreference provide evidence that LLMs do in fact have beliefs, desires, and\nintentions, and thus have a limited form of agency.\n",
        "title": "Are Language Models More Like Libraries or Like Librarians?\n  Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04855",
        "abstract_url": "http://arxiv.org/abs/2401.04855",
        "authors": [
            {
                "last_name": "Agarwal",
                "first_name": "Saurav"
            },
            {
                "last_name": "Muthukrishnan",
                "first_name": "Ramya"
            },
            {
                "last_name": "Gosrich",
                "first_name": "Walker"
            },
            {
                "last_name": "Ribeiro",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vijay"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Coverage control is the problem of navigating a robot swarm to\ncollaboratively monitor features or a phenomenon of interest not known a\npriori. The problem is challenging in decentralized settings with robots that\nhave limited communication and sensing capabilities. This paper proposes a\nlearnable Perception-Action-Communication (LPAC) architecture for the coverage\ncontrol problem. In the proposed solution, a convolution neural network (CNN)\nprocesses localized perception of the environment; a graph neural network (GNN)\nenables communication of relevant information between neighboring robots;\nfinally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN\nin the communication module enables collaboration in the robot swarm by\ncomputing what information to communicate with neighbors and how to use\nreceived information to take appropriate actions. We train models using\nimitation learning with a centralized clairvoyant algorithm that is aware of\nthe entire environment. Evaluations show that the LPAC models outperform\nstandard decentralized and centralized coverage control algorithms. The learned\npolicy generalizes to environments different from the training dataset,\ntransfers to larger environments with an increased number of robots, and is\nrobust to noisy position estimates. The results indicate that LPAC\narchitectures are well-suited for decentralized navigation in robot swarms to\nachieve collaborative behavior.\n",
        "title": "LPAC: Learnable Perception-Action-Communication Loops with Applications\n  to Coverage Control",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04856",
        "abstract_url": "http://arxiv.org/abs/2401.04856",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Sixu"
            },
            {
                "last_name": "Chen",
                "first_name": "Shi"
            },
            {
                "last_name": "Li",
                "first_name": "Qin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Score-based Generative Models (SGMs) is one leading method in generative\nmodeling, renowned for their ability to generate high-quality samples from\ncomplex, high-dimensional data distributions. The method enjoys empirical\nsuccess and is supported by rigorous theoretical convergence properties. In\nparticular, it has been shown that SGMs can generate samples from a\ndistribution that is close to the ground-truth if the underlying score function\nis learned well, suggesting the success of SGM as a generative model. We\nprovide a counter-example in this paper. Through the sample complexity\nargument, we provide one specific setting where the score function is learned\nwell. Yet, SGMs in this setting can only output samples that are Gaussian\nblurring of training data points, mimicking the effects of kernel density\nestimation. The finding resonates a series of recent finding that reveal that\nSGMs can demonstrate strong memorization effect and fail to generate.\n",
        "title": "A Good Score Does not Lead to A Good Generative Model",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04857",
        "abstract_url": "http://arxiv.org/abs/2401.04857",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Haotian"
            },
            {
                "last_name": "Jacobs",
                "first_name": "Tim"
            },
            {
                "last_name": "Kaminsky",
                "first_name": "Philip"
            },
            {
                "last_name": "Guo",
                "first_name": "Xin"
            },
            {
                "last_name": "Li",
                "first_name": "Xinyu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Currently, Amazon relies on third parties for transportation marketplace rate\nforecasts, despite the poor quality and lack of interpretability of these\nforecasts. While transportation marketplace rates are typically very\nchallenging to forecast accurately, we have developed a novel signature-based\nstatistical technique to address these challenges and built a predictive and\nadaptive model to forecast marketplace rates. This novel technique is based on\ntwo key properties of the signature transform. The first is its universal\nnonlinearity which linearizes the feature space and hence translates the\nforecasting problem into a linear regression analysis; the second is the\nsignature kernel which allows for comparing computationally efficiently\nsimilarities between time series data. Combined, these properties allow for\nefficient feature generation and more precise identification of seasonality and\nregime switching in the forecasting process. Preliminary result by the model\nshows that this new technique leads to far superior forecast accuracy versus\ncommercially available industry models with better interpretability, even\nduring the period of Covid-19 and with the sudden onset of the Ukraine war.\n",
        "title": "Transportation Market Rate Forecast Using Signature Transform",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04858",
        "abstract_url": "http://arxiv.org/abs/2401.04858",
        "authors": [
            {
                "last_name": "Doddapaneni",
                "first_name": "Sumanth"
            },
            {
                "last_name": "Sayana",
                "first_name": "Krishna"
            },
            {
                "last_name": "Jash",
                "first_name": "Ambarish"
            },
            {
                "last_name": "Sodhi",
                "first_name": "Sukhdeep"
            },
            {
                "last_name": "Kuzmin",
                "first_name": "Dima"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "IR",
            "LG"
        ],
        "abstract": "  Modeling long histories plays a pivotal role in enhancing recommendation\nsystems, allowing to capture user's evolving preferences, resulting in more\nprecise and personalized recommendations. In this study we tackle the\nchallenges of modeling long user histories for preference understanding in\nnatural language. Specifically, we introduce a new User Embedding Module (UEM)\nthat efficiently processes user history in free-form text by compressing and\nrepresenting them as embeddings, to use them as soft prompts to a LM. Our\nexperiments demonstrate the superior capability of this approach in handling\nsignificantly longer histories compared to conventional text based prompting\nmethods, yielding substantial improvements in predictive performance. The main\ncontribution of this research is to demonstrate the ability to bias language\nmodels with user signals represented as embeddings.\n",
        "title": "User Embedding Model for Personalized Language Prompting",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04859",
        "abstract_url": "http://arxiv.org/abs/2401.04859",
        "authors": [
            {
                "last_name": "Buvoli",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Southworth",
                "first_name": "Ben S."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This work introduces a new class of Runge-Kutta methods for solving\nnonlinearly partitioned initial value problems. These new methods, named\nnonlinearly partitioned Runge-Kutta (NPRK), generalize existing additive and\ncomponent-partitioned Runge-Kutta methods, and allow one to distribute\ndifferent types of implicitness within nonlinear terms. The paper introduces\nthe NPRK framework and discusses order conditions, linear stability, and the\nderivation of implicit-explicit and implicit-implicit NPRK integrators. The\npaper concludes with numerical experiments that demonstrate the utility of NPRK\nmethods for solving viscous Burger's and the gray thermal radiation transport\nequations.\n",
        "title": "A New Class of Runge-Kutta Methods for Nonlinearly Partitioned Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04860",
        "abstract_url": "http://arxiv.org/abs/2401.04860",
        "authors": [
            {
                "last_name": "Lyou",
                "first_name": "Eunyi"
            },
            {
                "last_name": "Lee",
                "first_name": "Doyeon"
            },
            {
                "last_name": "Kim",
                "first_name": "Jooeun"
            },
            {
                "last_name": "Lee",
                "first_name": "Joonseok"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Zero-shot learning offers an efficient solution for a machine learning model\nto treat unseen categories, avoiding exhaustive data collection. Zero-shot\nSketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it\nis hard and costly to collect paired sketch-photo samples. We propose a novel\nframework that indirectly aligns sketches and photos by contrasting them\nthrough texts, removing the necessity of access to sketch-photo pairs. With an\nexplicit modality encoding learned from data, our approach disentangles\nmodality-agnostic semantics from modality-specific information, bridging the\nmodality gap and enabling effective cross-modal content retrieval within a\njoint latent space. From comprehensive experiments, we verify the efficacy of\nthe proposed model on ZS-SBIR, and it can be also applied to generalized and\nfine-grained settings.\n",
        "title": "Modality-Aware Representation Learning for Zero-shot Sketch-based Image\n  Retrieval",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04861",
        "abstract_url": "http://arxiv.org/abs/2401.04861",
        "authors": [
            {
                "last_name": "Miao",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Bai",
                "first_name": "Yang"
            },
            {
                "last_name": "Duan",
                "first_name": "Haoran"
            },
            {
                "last_name": "Huang",
                "first_name": "Yawen"
            },
            {
                "last_name": "Wan",
                "first_name": "Fan"
            },
            {
                "last_name": "Long",
                "first_name": "Yang"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yefeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The goal of our work is to generate high-quality novel views from monocular\nvideos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have\nshown impressive performance by leveraging time-varying dynamic radiation\nfields. However, these methods have limitations when it comes to accurately\nmodeling the motion of complex objects, which can lead to inaccurate and blurry\nrenderings of details. To address this limitation, we propose a novel approach\nthat builds upon a recent generalization NeRF, which aggregates nearby views\nonto new viewpoints. However, such methods are typically only effective for\nstatic scenes. To overcome this challenge, we introduce a module that operates\nin both the time and frequency domains to aggregate the features of object\nmotion. This allows us to learn the relationship between frames and generate\nhigher-quality images. Our experiments demonstrate significant improvements\nover state-of-the-art methods on dynamic scene datasets. Specifically, our\napproach outperforms existing methods in terms of both the accuracy and visual\nquality of the synthesized views.\n",
        "title": "CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from\n  Monocular Video",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04864",
        "abstract_url": "http://arxiv.org/abs/2401.04864",
        "authors": [
            {
                "last_name": "Charleston",
                "first_name": "M. A."
            },
            {
                "last_name": "Chowdhury",
                "first_name": "S. M."
            },
            {
                "last_name": "Marashdeh",
                "first_name": "Q. M."
            },
            {
                "last_name": "Straiton",
                "first_name": "B. J."
            },
            {
                "last_name": "Teixeira",
                "first_name": "F. L."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The use of capacitance sensors for fuel mass gauging has been in\nconsideration since the early days of manned space flight. However, certain\ndifficulties arise when considering tanks in microgravity environments. Surface\ntension effects lead to fluid wetting of the interior surface of the tank,\nleaving large interior voids, while thrust/settling effects can lead to\ndispersed two-phase mixtures. With the exception of Electrical Capacitance\nVolume Tomography (ECVT), few sensing technologies are well suited for\nmeasuring annular, stratified, and dispersed fluid configurations as well as\nhandling the additional complications of mechanical installation inside a\nspherical tank. To optimize the design of future ECVT based spherical tank mass\ngauging sensors, different electrode plate layouts are considered, and their\neffect on the performance of the sensor as a fuel mass gauge is analyzed\nthrough the use of imaging and averaging techniques.\n",
        "title": "Microgravity Mass Gauging with Capacitance Sensing: Sensor Design and\n  Experiment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04867",
        "abstract_url": "http://arxiv.org/abs/2401.04867",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Koji"
            },
            {
                "last_name": "Lala",
                "first_name": "Divesh"
            },
            {
                "last_name": "Ochi",
                "first_name": "Keiko"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Skantze",
                "first_name": "Gabriel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC"
        ],
        "abstract": "  Establishing evaluation schemes for spoken dialogue systems is important, but\nit can also be challenging. While subjective evaluations are commonly used in\nuser experiments, objective evaluations are necessary for research comparison\nand reproducibility. To address this issue, we propose a framework for\nindirectly but objectively evaluating systems based on users' behaviours. In\nthis paper, to this end, we investigate the relationship between user\nbehaviours and subjective evaluation scores in social dialogue tasks: attentive\nlistening, job interview, and first-meeting conversation. The results reveal\nthat in dialogue tasks where user utterances are primary, such as attentive\nlistening and job interview, indicators like the number of utterances and words\nplay a significant role in evaluation. Observing disfluency also can indicate\nthe effectiveness of formal tasks, such as job interview. On the other hand, in\ndialogue tasks with high interactivity, such as first-meeting conversation,\nbehaviours related to turn-taking, like average switch pause length, become\nmore important. These findings suggest that selecting appropriate user\nbehaviours can provide valuable insights for objective evaluation in each\nsocial dialogue task.\n",
        "title": "An Analysis of User Behaviours for Objectively Evaluating Spoken\n  Dialogue Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04868",
        "abstract_url": "http://arxiv.org/abs/2401.04868",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Koji"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bing'er"
            },
            {
                "last_name": "Ekstedt",
                "first_name": "Erik"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Skantze",
                "first_name": "Gabriel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC",
            "SD",
            ""
        ],
        "abstract": "  A demonstration of a real-time and continuous turn-taking prediction system\nis presented. The system is based on a voice activity projection (VAP) model,\nwhich directly maps dialogue stereo audio to future voice activities. The VAP\nmodel includes contrastive predictive coding (CPC) and self-attention\ntransformers, followed by a cross-attention transformer. We examine the effect\nof the input context audio length and demonstrate that the proposed system can\noperate in real-time with CPU settings, with minimal performance degradation.\n",
        "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity\n  Projection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04872",
        "abstract_url": "http://arxiv.org/abs/2401.04872",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuexin"
            },
            {
                "last_name": "Li",
                "first_name": "Kunming"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yongliang"
            },
            {
                "last_name": "Worrall",
                "first_name": "Stewart"
            },
            {
                "last_name": "Li",
                "first_name": "You-Fu"
            },
            {
                "last_name": "Kong",
                "first_name": "He"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "RO"
        ],
        "abstract": "  Predicting pedestrian motion trajectories is crucial for path planning and\nmotion control of autonomous vehicles. Accurately forecasting crowd\ntrajectories is challenging due to the uncertain nature of human motions in\ndifferent environments. For training, recent deep learning-based prediction\napproaches mainly utilize information like trajectory history and interactions\nbetween pedestrians, among others. This can limit the prediction performance\nacross various scenarios since the discrepancies between training datasets have\nnot been properly incorporated. To overcome this limitation, this paper\nproposes a graph transformer structure to improve prediction performance,\ncapturing the differences between the various sites and scenarios contained in\nthe datasets. In particular, a self-attention mechanism and a domain adaption\nmodule have been designed to improve the generalization ability of the model.\nMoreover, an additional metric considering cross-dataset sequences is\nintroduced for training and performance evaluation purposes. The proposed\nframework is validated and compared against existing methods using popular\npublic datasets, i.e., ETH and UCY. Experimental results demonstrate the\nimproved performance of our proposed scheme.\n",
        "title": "Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04874",
        "abstract_url": "http://arxiv.org/abs/2401.04874",
        "authors": [
            {
                "last_name": "Mu",
                "first_name": "Xinying"
            },
            {
                "last_name": "Kon",
                "first_name": "Mark"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  A machine learning (ML) feature network is a graph that connects ML features\nin learning tasks based on their similarity. This network representation allows\nus to view feature vectors as functions on the network. By leveraging function\noperations from Fourier analysis and from functional analysis, one can easily\ngenerate new and novel features, making use of the graph structure imposed on\nthe feature vectors. Such network structures have previously been studied\nimplicitly in image processing and computational biology. We thus describe\nfeature networks as graph structures imposed on feature vectors, and provide\napplications in machine learning. One application involves graph-based\ngeneralizations of convolutional neural networks, involving structured deep\nlearning with hierarchical representations of features that have varying depth\nor complexity. This extends also to learning algorithms that are able to\ngenerate useful new multilevel features. Additionally, we discuss the use of\nfeature networks to engineer new features, which can enhance the expressiveness\nof the model. We give a specific example of a deep tree-structured feature\nnetwork, where hierarchical connections are formed through feature clustering\nand feed-forward learning. This results in low learning complexity and\ncomputational efficiency. Unlike \"standard\" neural features which are limited\nto modulated (thresholded) linear combinations of adjacent ones, feature\nnetworks offer more general feedforward dependencies among features. For\nexample, radial basis functions or graph structure-based dependencies between\nfeatures can be utilized.\n",
        "title": "Feature Network Methods in Machine Learning and Applications",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04875",
        "abstract_url": "http://arxiv.org/abs/2401.04875",
        "authors": [
            {
                "last_name": "Kobayashi",
                "first_name": "Tsutomu"
            },
            {
                "last_name": "Bondu",
                "first_name": "Martin"
            },
            {
                "last_name": "Ishikawa",
                "first_name": "Fuyuki"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring the safety of autonomous vehicles (AVs) is the key requisite for\ntheir acceptance in society. This complexity is the core challenge in formally\nproving their safety conditions with AI-based black-box controllers and\nsurrounding objects under various traffic scenarios. This paper describes our\nstrategy and experience in modelling, deriving, and proving the safety\nconditions of AVs with the Event-B refinement mechanism to reduce complexity.\nOur case study targets the state-of-the-art model of goal-aware\nresponsibility-sensitive safety to argue over interactions with surrounding\nvehicles. We also employ the Simplex architecture to involve advanced black-box\nAI controllers. Our experience has demonstrated that the refinement mechanism\ncan be effectively used to gradually develop the complex system over scenario\nvariations.\n",
        "title": "Formal Modelling of Safety Architecture for Responsibility-Aware\n  Autonomous Vehicle via Event-B Refinement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04881",
        "abstract_url": "http://arxiv.org/abs/2401.04881",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Zi"
            },
            {
                "last_name": "Hua",
                "first_name": "Nan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As LLMs have become capable of processing more complex types of inputs,\nresearchers have recently studied how to efficiently and affordably process\npossibly arbitrarily long sequences. One effective approach is to use a FIFO\nmemory to store keys and values of an attention sublayer from past chunks to\nallow subsequent queries to attend. However, this approach requires a large\nmemory and/or takes into the consideration the specific LM architecture.\nMoreover, due to the causal nature between the key-values in prior context and\nthe queries at present, this approach cannot be extended to bidirectional\nattention such as in an encoder-decoder or PrefixLM decoder-only architecture.\nIn this paper, we propose to use eviction policies, such as LRA and LFA, to\nreduce the memory size and adapt to various architectures, and we also propose\nthe Attendre layer, a wait-to-attend mechanism by retrieving the key-value\nmemory (K/V memory) with evicted queries in the query memory (Q memory). As a\nfirst step, we evaluate this method in the context length extension setup using\nthe TriviaQA reading comprehension task, and show the effectiveness of the\napproach.\n",
        "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in\n  Memory-Based Transformers for Long Context Processing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04883",
        "abstract_url": "http://arxiv.org/abs/2401.04883",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Manqing"
            },
            {
                "last_name": "Ting",
                "first_name": "Paishun"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yijian"
            },
            {
                "last_name": "Xu",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Julia"
            },
            {
                "last_name": "Lin",
                "first_name": "Jianzhe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancements in large language models (LLMs) have provided a new\navenue for chatbot development, while most existing research has primarily\ncentered on single-user chatbots that focus on deciding \"What\" to answer after\nuser inputs. In this paper, we identified that multi-user chatbots have more\ncomplex 3W design dimensions -- \"What\" to say, \"When\" to respond, and \"Who\" to\nanswer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an\nLLM-based framework for chatbots specifically designed for group discussions.\nMUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and\nUtterance Strategies Arbitrator. These modules jointly determine suitable\nresponse contents, timings, and the appropriate recipients. To make the\noptimizing process for MUCA easier, we further propose an LLM-based Multi-User\nSimulator (MUS) that can mimic real user behavior. This enables faster\nsimulation of a conversation between the chatbot and simulated users, making\nthe early development of the chatbot framework much more efficient. MUCA\ndemonstrates effectiveness, including appropriate chime-in timing, relevant\ncontent, and positive user engagement, in goal-oriented conversations with a\nsmall to medium number of participants, as evidenced by case studies and\nexperimental results from user studies.\n",
        "title": "Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate\n  Group Conversations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04884",
        "abstract_url": "http://arxiv.org/abs/2401.04884",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junren"
            },
            {
                "last_name": "Scarlett",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "DM",
            "",
            ""
        ],
        "abstract": "  In recent years, the mathematical limits and algorithmic bounds for\nprobabilistic group testing having become increasingly well-understood, with\nexact asymptotic thresholds now being known in general scaling regimes for the\nnoiseless setting. In the noisy setting where each test outcome is flipped with\nconstant probability, there have been similar developments, but the overall\nunderstanding has lagged significantly behind the noiseless setting. In this\npaper, we substantially narrow this gap by deriving exact asymptotic thresholds\nfor the noisy setting under two widely-studied random test designs: i.i.d.\nBernoulli and near-constant tests-per-item. These thresholds are established by\ncombining components of an existing information-theoretic threshold decoder\nwith a novel analysis of maximum-likelihood decoding (upper bounds), and\nderiving a novel set of impossibility results by analyzing certain failure\nevents for optimal maximum-likelihood decoding (lower bounds). Our results show\nthat existing algorithmic upper bounds for the noisy setting are strictly\nsuboptimal, and leave open the interesting question of whether our thresholds\ncan be attained using computationally efficient algorithms.\n",
        "title": "Exact Thresholds for Noisy Non-Adaptive Group Testing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04885",
        "abstract_url": "http://arxiv.org/abs/2401.04885",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Xie",
                "first_name": "Conghui"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Ding",
                "first_name": "Cunsheng"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Sum-rank codes have known applications in the multishot network coding, the\ndistributed storage and the construction of space-time codes. U.\nMart\\'{\\i}nez-Pe\\~{n}as introduced the cyclic-skew-cyclic sum-rank codes and\nproposed the BCH bound on the cyclic-skew-cyclic sum-rank codes in his paper\npublished in IEEE Trans. Inf. Theory, vol. 67, no. 8, 2021. Afterwards, many\nsum-rank BCH codes with lower bounds on their dimensions and minimum sum-rank\ndistances were constructed. Sum-rank Hartmann-Tzeng bound and sum-rank Roos\nbound on cyclic-skew-cyclic codes were proposed and proved by G. N. Alfarano,\nF. J. Lobillo, A. Neri, and A. Wachter-Zeh in 2022. In this paper, cyclic,\nnegacyclic and constacyclic sum-rank codes are introduced and a direct\nconstruction of cyclic, negacyclic and constacyclic sum-rank codes of the\nmatrix size $m \\times m$ from cyclic, negacyclic and constacyclic codes over\n${\\bf F}_{q^m}$ in the Hamming metric is proposed. The cyclic-skew-cylic\nsum-rank codes are special cyclic sum-rank codes. In addition, BCH and\nHartmann-Tzeng bounds for a type of cyclic sum-rank codes are developed.\nSpecific constructions of cyclic, negacyclic and constacyclic sum-rank codes\nwith known dimensions and controllable minimum sum-rank distances are proposed.\nMoreover, many distance-optimal binary sum-rank codes and an infinite family of\ndistance-optimal binary cyclic sum-rank codes with minimum sum-rank distance\nfour are constructed. This is the first infinite family of distance-optimal\nsum-rank codes with minimum sum-rank distance four in the literature.\n",
        "title": "Cyclic and Negacyclic Sum-Rank Codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04887",
        "abstract_url": "http://arxiv.org/abs/2401.04887",
        "authors": [
            {
                "last_name": "Escamilla",
                "first_name": "Emily"
            },
            {
                "last_name": "Klein",
                "first_name": "Martin"
            },
            {
                "last_name": "Cooper",
                "first_name": "Talya"
            },
            {
                "last_name": "Rampin",
                "first_name": "Vicky"
            },
            {
                "last_name": "Weigle",
                "first_name": "Michele C."
            },
            {
                "last_name": "Nelson",
                "first_name": "Michael L."
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  One in five arXiv articles published in 2021 contained a URI to a Git Hosting\nPlatform (GHP), which demonstrates the growing prevalence of GHP URIs in\nscholarly publications. However, GHP URIs are vulnerable to the same reference\nrot that plagues the Web at large. The disappearance of software hosting\nplatforms, like Gitorious and Google Code, and the source code they contain\nthreatens research reproducibility. Archiving the source code and development\nhistory available in GHPs enables the long-term reproducibility of research.\nSoftware Heritage and Web archives contain archives of GHP URI resources.\nHowever, are the GHP URIs referenced by scholarly publications contained within\nthe Software Heritage and Web archive collections? We analyzed a dataset of GHP\nURIs extracted from scholarly publications to determine (1) is the URI still\npublicly available on the live Web?, (2) has the URI been archived by Software\nHeritage?, and (3) has the URI been archived by Web archives? Of all GHP URIs,\nwe found that 93.98% were still publicly available on the live Web, 68.39% had\nbeen archived by Software Heritage, and 81.43% had been archived by Web\narchives.\n",
        "title": "Cited But Not Archived: Analyzing the Status of Code References in\n  Scholarly Articles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04890",
        "abstract_url": "http://arxiv.org/abs/2401.04890",
        "authors": [
            {
                "last_name": "Lachapelle",
                "first_name": "S\u00e9bastien"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Pau Rodr\u00edguez"
            },
            {
                "last_name": "Sharma",
                "first_name": "Yash"
            },
            {
                "last_name": "Everett",
                "first_name": "Katie"
            },
            {
                "last_name": "Priol",
                "first_name": "R\u00e9mi Le"
            },
            {
                "last_name": "Lacoste",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Lacoste-Julien",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  This work introduces a novel principle for disentanglement we call mechanism\nsparsity regularization, which applies when the latent factors of interest\ndepend sparsely on observed auxiliary variables and/or past latent factors. We\npropose a representation learning method that induces disentanglement by\nsimultaneously learning the latent factors and the sparse causal graphical\nmodel that explains them. We develop a nonparametric identifiability theory\nthat formalizes this principle and shows that the latent factors can be\nrecovered by regularizing the learned causal graph to be sparse. More\nprecisely, we show identifiablity up to a novel equivalence relation we call\n\"consistency\", which allows some latent factors to remain entangled (hence the\nterm partial disentanglement). To describe the structure of this entanglement,\nwe introduce the notions of entanglement graphs and graph preserving functions.\nWe further provide a graphical criterion which guarantees complete\ndisentanglement, that is identifiability up to permutations and element-wise\ntransformations. We demonstrate the scope of the mechanism sparsity principle\nas well as the assumptions it relies on with several worked out examples. For\ninstance, the framework shows how one can leverage multi-node interventions\nwith unknown targets on the latent factors to disentangle them. We further draw\nconnections between our nonparametric results and the now popular exponential\nfamily assumption. Lastly, we propose an estimation procedure based on\nvariational autoencoders and a sparsity constraint and demonstrate it on\nvarious synthetic datasets. This work is meant to be a significantly extended\nversion of Lachapelle et al. (2022).\n",
        "title": "Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse\n  Actions, Interventions and Sparse Temporal Dependencies",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04898",
        "abstract_url": "http://arxiv.org/abs/2401.04898",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Bingchao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Recently, various Large Language Models (LLMs) evaluation datasets have\nemerged, but most of them have issues with distorted rankings and difficulty in\nmodel capabilities analysis. Addressing these concerns, this paper introduces\nANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes\n\\textit{Keypoint} categorization standard for the first time, each question in\nANGO can correspond to multiple keypoints, effectively enhancing\ninterpretability of evaluation results. Base on performance of real humans, we\nbuild a quantifiable question difficulty standard and divide ANGO questions\ninto 9 difficulty levels, which provide more precise guidance for model\ntraining. To minimize data leakage impact and fully leverage ANGO's innovative\nfeatures, we have engineered exclusive sampling strategies and a new evaluation\nframework that support swift testset iteration. Our experiments demonstrate\nthat ANGO poses a stronger challenge to models and reveals more details in\nevaluation result compared to existing benchmarks.\n",
        "title": "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language\n  Models In Chinese Domain",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04900",
        "abstract_url": "http://arxiv.org/abs/2401.04900",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Mengmeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Fan"
            },
            {
                "last_name": "Bu",
                "first_name": "Yude"
            },
            {
                "last_name": "Li",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Yi",
                "first_name": "Zhenping"
            },
            {
                "last_name": "Liu",
                "first_name": "Meng"
            },
            {
                "last_name": "Kong",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  The age and mass of red giants are essential for understanding the structure\nand evolution of the Milky Way. Traditional isochrone methods for these\nestimations are inherently limited due to overlapping isochrones in the\nHertzsprung-Russell diagram, while asteroseismology, though more precise,\nrequires high-precision, long-term observations. In response to these\nchallenges, we developed a novel framework, Spectral Transformer (SPT), to\npredict the age and mass of red giants aligned with asteroseismology from their\nspectra. A key component of SPT, the Multi-head Hadamard Self-Attention\nmechanism, designed specifically for spectra, can capture complex relationships\nacross different wavelength. Further, we introduced a Mahalanobis\ndistance-based loss function to address scale imbalance and interaction mode\nloss, and incorporated Monte Carlo dropout for quantitative analysis of\nprediction uncertainty.Trained and tested on 3,880 red giant spectra from\nLAMOST, the SPT achieved remarkable age and mass estimations with average\npercentage errors of 17.64% and 6.61%, respectively, and provided uncertainties\nfor each corresponding prediction. The results significantly outperform those\nof traditional machine learning algorithms and demonstrate a high level of\nconsistency with asteroseismology methods and isochrone fitting techniques. In\nthe future, our work will leverage datasets from the Chinese Space Station\nTelescope and the Large Synoptic Survey Telescope to enhance the precision of\nthe model and broaden its applicability in the field of astronomy and\nastrophysics.\n",
        "title": "SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04903",
        "abstract_url": "http://arxiv.org/abs/2401.04903",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jianqiao"
            },
            {
                "last_name": "Su",
                "first_name": "Yudi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Zeng",
                "first_name": "Zequn"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhengjue"
            },
            {
                "last_name": "Chen",
                "first_name": "Bo"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Captioning (VC) is a challenging multi-modal task since it requires\ndescribing the scene in language by understanding various and complex videos.\nFor machines, the traditional VC follows the\n\"imaging-compression-decoding-and-then-captioning\" pipeline, where compression\nis pivot for storage and transmission. However, in such a pipeline, some\npotential shortcomings are inevitable, i.e., information redundancy resulting\nin low efficiency and information loss during the sampling process for\ncaptioning. To address these problems, in this paper, we propose a novel VC\npipeline to generate captions directly from the compressed measurement, which\ncan be captured by a snapshot compressive sensing camera and we dub our model\nSnapCap. To be more specific, benefiting from the signal simulation, we have\naccess to obtain abundant measurement-video-annotation data pairs for our\nmodel. Besides, to better extract language-related visual representations from\nthe compressed measurement, we propose to distill the knowledge from videos via\na pre-trained CLIP with plentiful language-vision associations to guide the\nlearning of our SnapCap. To demonstrate the effectiveness of SnapCap, we\nconduct experiments on two widely-used VC datasets. Both the qualitative and\nquantitative results verify the superiority of our pipeline over conventional\nVC pipelines. In particular, compared to the \"caption-after-reconstruction\"\nmethods, our SnapCap can run at least 3$\\times$ faster, and achieve better\ncaption results.\n",
        "title": "SnapCap: Efficient Snapshot Compressive Video Captioning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04904",
        "abstract_url": "http://arxiv.org/abs/2401.04904",
        "authors": [
            {
                "last_name": "Akar",
                "first_name": "Nail"
            },
            {
                "last_name": "Liyanaarachchi",
                "first_name": "Sahan"
            },
            {
                "last_name": "Ulukus",
                "first_name": "Sennur"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI",
            "PF"
        ],
        "abstract": "  We study cyclic scheduling for generate-at-will (GAW) multi-source status\nupdate systems with heterogeneous service times and packet drop probabilities,\nwith the aim of minimizing the weighted sum age of information (AoI), called\nsystem AoI, or the weighted sum peak AoI (PAoI), called system PAoI. In\nparticular, we obtain well-performing cyclic schedulers which can easily scale\nto thousands of information sources and which also have low online\nimplementation complexity. The proposed schedulers are comparatively studied\nagainst existing scheduling algorithms in terms of computational load and\nsystem AoI/PAoI performance, to validate their effectiveness.\n",
        "title": "Scalable Cyclic Schedulers for Age of Information Optimization in\n  Large-Scale Status Update Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04906",
        "abstract_url": "http://arxiv.org/abs/2401.04906",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xinxin"
            },
            {
                "last_name": "Gao",
                "first_name": "Lei"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Device-to-device (D2D) technology is one of the key research areas in 5G/6G\nnetworks, and full-duplex (FD) D2D will further enhance its spectral efficiency\n(SE). In recent years, deep learning approaches have shown remarkable\nperformance in D2D resource allocation tasks. However, most schemes only model\nthe channel state information (CSI) as an independent feature, neglecting the\nspatial relationships among multiple channels and users within the scenario. In\nthis paper, we first design an objective function for FD D2D communication\nresource allocation, which aims to maximize the SE of D2D users while ensuring\nthe minimal required SE of cellular users. Then, considering the complex CSI\nconstituted by all the users in different channels as a three-dimensional\nvector, a centralized resource allocation model based on multi-dimensional\nspatial convolutional networks and attention mechanisms (SP-Conv-Att) is\nproposed. To alleviate the burden of base station, we develop two distributed\nmodels, Dist-Att and Dist-Att-Conv, to facilitate users to perform channel and\npower allocation locally, based on attention and multi-user convolutional\nnetworks respectively. Numerical results demonstrate that our models outperform\ntraditional schemes and recent deep neural network models, significantly\napproximating the optimal solution computed by exhaustive algorithm with\nextremely low latency.\n",
        "title": "Deep Learning Based Resource Allocation for Full-duplex Device-to-Device\n  Communication",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04908",
        "abstract_url": "http://arxiv.org/abs/2401.04908",
        "authors": [
            {
                "last_name": "Mei",
                "first_name": "Haoran"
            },
            {
                "last_name": "Peng",
                "first_name": "Limei"
            },
            {
                "last_name": "Ho",
                "first_name": "Pin-Han"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  Grant-free access (GFA) has been envisioned to play an active role in massive\nMachine Type Communication (mMTC) under 5G and Beyond mobile systems, which\ntargets at achieving significant reduction of signaling overhead and access\nlatency in the presence of sporadic traffic and small-size data. The paper\nfocuses on a novel K-repetition GFA (K-GFA) scheme by incorporating\nReed-Solomon (RS) code with the contention resolution diversity slotted ALOHA\n(CRDSA), aiming to achieve high-reliability and low-latency access in the\npresence of massive uncoordinated MTC devices (MTCDs). We firstly defines a MAC\nlayer transmission structure at each MTCD for supporting message-level RS\ncoding on a data message of $Q$ packets, where a RS code of $KQ$ packets is\ngenerated and sent in a super time frame (STF) that is composed of $Q$ time\nframes. The access point (AP) can recover the original $Q$ packets of the data\nmessage if at least $Q$ out of the $KQ$ packets of the RS code are successfully\nreceived. The AP buffers the received MTCD signals of each resource block (RB)\nwithin an STF and exercises the CRDSA based multi-user detection (MUD) by\nexploring signal-level inter-RB correlation via iterative interference\ncancellation (IIC). With the proposed CRDSA based K-GFA scheme, we provide the\ncomplexity analysis, and derive a closed-form analytical model on the access\nprobability for each MTCD as well as its simplified approximate form. Extensive\nnumerical experiments are conducted to validate its effectiveness on the\nproposed CRDSA based K-GFA scheme and gain deep understanding on its\nperformance regarding various key operational parameters.\n",
        "title": "On Achieving High-Fidelity Grant-free Non-Orthogonal Multiple Access",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04912",
        "abstract_url": "http://arxiv.org/abs/2401.04912",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhongyan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhifang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Node repair is a crucial problem in erasure-code-based distributed storage\nsystems. An important metric for repair efficiency is the I/O cost which equals\nthe total amount of data accessed at helper nodes to repair a failed node. In\nthis work, a general formula for computing the I/O cost of linear repair\nschemes is derived from a new perspective, i.e., by investigating the Hamming\nweight of a related linear space. Applying the formula to Reed-Solomon (RS)\ncodes, we obtain lower bounds on the I/O cost for full-length RS codes with two\nand three parities. Furthermore, we build linear repair schemes for the RS\ncodes with improved I/O cost. For full-length RS codes with two parities, our\nscheme meets the lower bound on the I/O cost.\n",
        "title": "A Formula for the I/O Cost of Linear Repair Schemes and Application to\n  Reed-Solomon Codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04914",
        "abstract_url": "http://arxiv.org/abs/2401.04914",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Guohui"
            },
            {
                "last_name": "Li",
                "first_name": "Jianjun"
            },
            {
                "last_name": "Wang",
                "first_name": "Chaoyang"
            },
            {
                "last_name": "Shi",
                "first_name": "Si"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Learning precise representations of users and items to fit observed\ninteraction data is the fundamental task of collaborative filtering. Existing\nstudies usually infer entangled representations to fit such interaction data,\nneglecting to model the diverse matching relationships between users and items\nbehind their interactions, leading to limited performance and weak\ninterpretability. To address this problem, we propose a Dual Disentangled\nVariational AutoEncoder (DualVAE) for collaborative recommendation, which\ncombines disentangled representation learning with variational inference to\nfacilitate the generation of implicit interaction data. Specifically, we first\nimplement the disentangling concept by unifying an attention-aware dual\ndisentanglement and disentangled variational autoencoder to infer the\ndisentangled latent representations of users and items. Further, to encourage\nthe correspondence and independence of disentangled representations of users\nand items, we design a neighborhood-enhanced representation constraint with a\ncustomized contrastive mechanism to improve the representation quality.\nExtensive experiments on three real-world benchmarks show that our proposed\nmodel significantly outperforms several recent state-of-the-art baselines.\nFurther empirical experimental results also illustrate the interpretability of\nthe disentangled representations learned by DualVAE.\n",
        "title": "DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04915",
        "abstract_url": "http://arxiv.org/abs/2401.04915",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Erica"
            },
            {
                "last_name": "Simek",
                "first_name": "Olga"
            },
            {
                "last_name": "Miller",
                "first_name": "Benjamin A."
            },
            {
                "last_name": "Sullivan-Pao",
                "first_name": "Danielle"
            },
            {
                "last_name": "Young",
                "first_name": "Evan"
            },
            {
                "last_name": "Smith",
                "first_name": "Christopher L."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  We propose a pipeline for identifying important entities from intelligence\nreports that constructs a knowledge graph, where nodes correspond to entities\nof fine-grained types (e.g. traffickers) extracted from the text and edges\ncorrespond to extracted relations between entities (e.g. cartel membership).\nThe important entities in intelligence reports then map to central nodes in the\nknowledge graph. We introduce a novel method that extracts fine-grained\nentities in a few-shot setting (few labeled examples), given limited resources\navailable to label the frequently changing entity types that intelligence\nanalysts are interested in. It outperforms other state-of-the-art methods.\nNext, we identify challenges facing previous evaluations of zero-shot (no\nlabeled examples) methods for extracting relations, affecting the step of\npopulating edges. Finally, we explore the utility of the pipeline: given the\ngoal of identifying important entities, we evaluate the impact of relation\nextraction errors on the identification of central nodes in several real and\nsynthetic networks. The impact of these errors varies significantly by graph\ntopology, suggesting that confidence in measurements based on automatically\nextracted relations should depend on observed network features.\n",
        "title": "From low resource information extraction to identifying influential\n  nodes in knowledge graphs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04916",
        "abstract_url": "http://arxiv.org/abs/2401.04916",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Kan"
            },
            {
                "last_name": "Mei",
                "first_name": "Jie"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojun"
            },
            {
                "last_name": "Hou",
                "first_name": "Lu"
            },
            {
                "last_name": "Ma",
                "first_name": "Siwei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Vehicles are no longer isolated entities in traffic environments, thanks to\nthe development of IoV powered by 5G networks and their evolution into 6G.\nHowever, it is not enough for vehicles in a highly dynamic and complex traffic\nenvironment to make reliable and efficient decisions. As a result, this paper\nproposes a cloud-edge-end computing system with multi-streams for IoV, referred\nto as Vehicular Digital Retina (VDR). Local computing and edge computing are\neffectively integrated in the VDR system through the aid of\nvehicle-to-everything (V2X) networks, resulting in a heterogeneous computing\nenvironment that improves vehicles' perception and decision-making abilities\nwith collaborative strategies. Once the system framework is presented, various\nimportant functions in the VDR system are explained in detail, including\nV2X-aided collaborative perception, V2X-aided stream sharing for collaborative\nlearning, and V2X-aided secured collaboration. All of them enable the\ndevelopment of efficient mechanisms of data sharing and information interaction\nwith high security for collaborative intelligent driving. We also present a\ncase study with simulation results to demonstrate the effectiveness of the\nproposed VDR system.\n",
        "title": "Digital Retina for IoV Towards 6G: Architecture, Opportunities, and\n  Challenges",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04918",
        "abstract_url": "http://arxiv.org/abs/2401.04918",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Kaitao"
            },
            {
                "last_name": "Masouros",
                "first_name": "Christos"
            },
            {
                "last_name": "Chen",
                "first_name": "Guangji"
            },
            {
                "last_name": "Liu",
                "first_name": "Fan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this study, we explore integrated sensing and communication (ISAC)\nnetworks to strike a more effective balance between sensing and communication\n(S&C) performance at the network scale. We leverage stochastic geometry to\nanalyze the S&C performance, shedding light on critical cooperative\ndependencies of ISAC networks. According to the derived expressions of network\nperformance, we optimize the user/target loads and the cooperative base station\ncluster sizes for S&C to achieve a flexible trade-off between network-scale S&C\nperformance. It is observed that the optimal strategy emphasizes the full\nutilization of spatial resources to enhance multiplexing and diversity gain\nwhen maximizing communication ASE. In contrast, for sensing objectives, parts\nof spatial resources are allocated to cancel inter-cell sensing interference to\nmaximize sensing ASE. Simulation results validate that the proposed ISAC scheme\nrealizes a remarkable enhancement in overall S&C network performance.\n",
        "title": "BS Coordination Optimization in Integrated Sensing and Communication: A\n  Stochastic Geometric View",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04921",
        "abstract_url": "http://arxiv.org/abs/2401.04921",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Wang",
                "first_name": "Yong"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengyuan"
            },
            {
                "last_name": "Wu",
                "first_name": "Doudou"
            },
            {
                "last_name": "Liu",
                "first_name": "Peng"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xinlin"
            },
            {
                "last_name": "Yang",
                "first_name": "Wenming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed to\nenhance pose accuracy by generating multiple hypotheses. However, most of the\nhypotheses generated deviate substantially from the true pose. Compared to\ndeterministic models, the excessive uncertainty in probabilistic models leads\nto weaker performance in single-hypothesis prediction. To address these two\nchallenges, we propose a diffusion-based refinement framework called DRPose,\nwhich refines the output of deterministic models by reverse diffusion and\nachieves more suitable multi-hypothesis prediction for the current pose\nbenchmark by multi-step refinement with multiple noises. To this end, we\npropose a Scalable Graph Convolution Transformer (SGCT) and a Pose Refinement\nModule (PRM) for denoising and refining. Extensive experiments on Human3.6M and\nMPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-art\nperformance on both single and multi-hypothesis 3DHPE. Code is available at\nhttps://github.com/KHB1698/DRPose.\n",
        "title": "Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D\n  Human Pose Estimaiton",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04923",
        "abstract_url": "http://arxiv.org/abs/2401.04923",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Ruiyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Ouyang"
            },
            {
                "last_name": "Guo",
                "first_name": "Yunhui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Active learning is a commonly used approach that reduces the labeling effort\nrequired to train deep neural networks. However, the effectiveness of current\nactive learning methods is limited by their closed-world assumptions, which\nassume that all data in the unlabeled pool comes from a set of predefined known\nclasses. This assumption is often not valid in practical situations, as there\nmay be unknown classes in the unlabeled data, leading to the active open-set\nannotation problem. The presence of unknown classes in the data can\nsignificantly impact the performance of existing active learning methods due to\nthe uncertainty they introduce. To address this issue, we propose a novel\ndata-centric active learning method called NEAT that actively annotates\nopen-set data. NEAT is designed to label known classes data from a pool of both\nknown and unknown classes unlabeled data. It utilizes the clusterability of\nlabels to identify the known classes from the unlabeled pool and selects\ninformative samples from those classes based on a consistency criterion that\nmeasures inconsistencies between model predictions and local feature\ndistribution. Unlike the recently proposed learning-centric method for the same\nproblem, NEAT is much more computationally efficient and is a data-centric\nactive open-set annotation method. Our experiments demonstrate that NEAT\nachieves significantly better performance than state-of-the-art active learning\nmethods for active open-set annotation.\n",
        "title": "Inconsistency-Based Data-Centric Active Open-Set Annotation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04925",
        "abstract_url": "http://arxiv.org/abs/2401.04925",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Yu",
                "first_name": "Qinkai"
            },
            {
                "last_name": "shu",
                "first_name": "Dong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Hua",
                "first_name": "Wenyue"
            },
            {
                "last_name": "Meng",
                "first_name": "Yanda"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongfeng"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations,\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences.\n",
        "title": "The Impact of Reasoning Step Length on Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04928",
        "abstract_url": "http://arxiv.org/abs/2401.04928",
        "authors": [
            {
                "last_name": "Seo",
                "first_name": "Seonguk"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinkyu"
            },
            {
                "last_name": "Kim",
                "first_name": "Geeho"
            },
            {
                "last_name": "Han",
                "first_name": "Bohyung"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a novel contrastive learning framework to effectively address the\nchallenges of data heterogeneity in federated learning. We first analyze the\ninconsistency of gradient updates across clients during local training and\nestablish its dependence on the distribution of feature representations,\nleading to the derivation of the supervised contrastive learning (SCL)\nobjective to mitigate local deviations. In addition, we show that a na\\\"ive\nadoption of SCL in federated learning leads to representation collapse,\nresulting in slow convergence and limited performance gains. To address this\nissue, we introduce a relaxed contrastive learning loss that imposes a\ndivergence penalty on excessively similar sample pairs within each class. This\nstrategy prevents collapsed representations and enhances feature\ntransferability, facilitating collaborative training and leading to significant\nperformance improvements. Our framework outperforms all existing federated\nlearning approaches by huge margins on the standard benchmarks through\nextensive experimental results.\n",
        "title": "Relaxed Contrastive Learning for Federated Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04929",
        "abstract_url": "http://arxiv.org/abs/2401.04929",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Haonan"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Tu"
            },
            {
                "last_name": "Wang",
                "first_name": "An"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG"
        ],
        "abstract": "  Machine learning models, in particular deep neural networks, are currently an\nintegral part of various applications, from healthcare to finance. However,\nusing sensitive data to train these models raises concerns about privacy and\nsecurity. One method that has emerged to verify if the trained models are\nprivacy-preserving is Membership Inference Attacks (MIA), which allows\nadversaries to determine whether a specific data point was part of a model's\ntraining dataset. While a series of MIAs have been proposed in the literature,\nonly a few can achieve high True Positive Rates (TPR) in the low False Positive\nRate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA\nto be practically useful in real-world settings. In this paper, we present a\nnovel approach to MIA that is aimed at significantly improving TPR at low FPRs.\nOur method, named learning-based difficulty calibration for MIA(LDC-MIA),\ncharacterizes data records by their hardness levels using a neural network\nclassifier to determine membership. The experiment results show that LDC-MIA\ncan improve TPR at low FPR by up to 4x compared to the other difficulty\ncalibration based MIAs. It also has the highest Area Under ROC curve (AUC)\nacross all datasets. Our method's cost is comparable with most of the existing\nMIAs, but is orders of magnitude more efficient than one of the\nstate-of-the-art methods, LiRA, while achieving similar performance.\n",
        "title": "Learning-Based Difficulty Calibration for Enhanced Membership Inference\n  Attacks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04933",
        "abstract_url": "http://arxiv.org/abs/2401.04933",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Sicong"
            },
            {
                "last_name": "He",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Lui",
                "first_name": "Kry Yik Chau"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While likelihood is attractive in theory, its estimates by deep generative\nmodels (DGMs) are often broken in practice, and perform poorly for out of\ndistribution (OOD) Detection. Various recent works started to consider\nalternative scores and achieved better performances. However, such recipes do\nnot come with provable guarantees, nor is it clear that their choices extract\nsufficient information.\n  We attempt to change this by conducting a case study on variational\nautoencoders (VAEs). First, we introduce the likelihood path (LPath) principle,\ngeneralizing the likelihood principle. This narrows the search for informative\nsummary statistics down to the minimal sufficient statistics of VAEs'\nconditional likelihoods. Second, introducing new theoretic tools such as nearly\nessential support, essential distance and co-Lipschitzness, we obtain\nnon-asymptotic provable OOD detection guarantees for certain distillation of\nthe minimal sufficient statistics. The corresponding LPath algorithm\ndemonstrates SOTA performances, even using simple and small VAEs with poor\nlikelihood estimates. To our best knowledge, this is the first provable\nunsupervised OOD method that delivers excellent empirical results, better than\nany other VAEs based techniques. We use the same model as\n\\cite{xiao2020likelihood}, open sourced from:\nhttps://github.com/XavierXiao/Likelihood-Regret\n",
        "title": "Rethinking Test-time Likelihood: The Likelihood Path Principle and Its\n  Application to OOD Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04934",
        "abstract_url": "http://arxiv.org/abs/2401.04934",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Jiechuan"
            },
            {
                "last_name": "Su",
                "first_name": "Kefan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zongqing"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "LG"
        ],
        "abstract": "  Cooperative multi-agent reinforcement learning is a powerful tool to solve\nmany real-world cooperative tasks, but restrictions of real-world applications\nmay require training the agents in a fully decentralized manner. Due to the\nlack of information about other agents, it is challenging to derive algorithms\nthat can converge to the optimal joint policy in a fully decentralized setting.\nThus, this research area has not been thoroughly studied. In this paper, we\nseek to systematically review the fully decentralized methods in two settings:\nmaximizing a shared reward of all agents and maximizing the sum of individual\nrewards of all agents, and discuss open questions and future research\ndirections.\n",
        "title": "Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A\n  Survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04935",
        "abstract_url": "http://arxiv.org/abs/2401.04935",
        "authors": [
            {
                "last_name": "Vosoughi",
                "first_name": "Ali"
            },
            {
                "last_name": "Bondi",
                "first_name": "Luca"
            },
            {
                "last_name": "Wu",
                "first_name": "Ho-Hsiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenliang"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Conventional audio classification relied on predefined classes, lacking the\nability to learn from free-form text. Recent methods unlock learning joint\naudio-text embeddings from raw audio-text pairs describing audio in natural\nlanguage. Despite recent advancements, there is little exploration of\nsystematic methods to train models for recognizing sound events and sources in\nalternative scenarios, such as distinguishing fireworks from gunshots at\noutdoor events in similar situations. This study introduces causal reasoning\nand counterfactual analysis in the audio domain. We use counterfactual\ninstances and include them in our model across different aspects. Our model\nconsiders acoustic characteristics and sound source information from\nhuman-annotated reference texts. To validate the effectiveness of our model, we\nconducted pre-training utilizing multiple audio captioning datasets. We then\nevaluate with several common downstream tasks, demonstrating the merits of the\nproposed method as one of the first works leveraging counterfactual information\nin audio domain. Specifically, the top-1 accuracy in open-ended language-based\naudio retrieval task increased by more than 43%.\n",
        "title": "Learning Audio Concepts from Counterfactual Natural Language",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04936",
        "abstract_url": "http://arxiv.org/abs/2401.04936",
        "authors": [
            {
                "last_name": "De Sterck",
                "first_name": "H."
            },
            {
                "last_name": "Falgout",
                "first_name": "R. D."
            },
            {
                "last_name": "Krzysik",
                "first_name": "O. A."
            },
            {
                "last_name": "Schroder",
                "first_name": "J. B."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We consider the parallel-in-time solution of scalar nonlinear conservation\nlaws in one spatial dimension. The equations are discretized in space with a\nconservative finite-volume method using weighted essentially non-oscillatory\n(WENO) reconstructions, and in time with high-order explicit Runge-Kutta\nmethods. The solution of the global, discretized space-time problem is sought\nvia a nonlinear iteration that uses a novel linearization strategy in cases of\nnon-differentiable equations. Under certain choices of discretization and\nalgorithmic parameters, the nonlinear iteration coincides with Newton's method,\nalthough, more generally, it is a preconditioned residual correction scheme. At\neach nonlinear iteration, the linearized problem takes the form of a certain\ndiscretization of a linear conservation law over the space-time domain in\nquestion. An approximate parallel-in-time solution of the linearized problem is\ncomputed with a single multigrid reduction-in-time (MGRIT) iteration. The MGRIT\niteration employs a novel coarse-grid operator that is a modified conservative\nsemi-Lagrangian discretization and generalizes those we have developed\npreviously for non-conservative scalar linear hyperbolic problems. Numerical\ntests are performed for the inviscid Burgers and Buckley--Leverett equations.\nFor many test problems, the solver converges in just a handful of iterations\nwith convergence rate independent of mesh resolution, including problems with\n(interacting) shocks and rarefactions.\n",
        "title": "Parallel-in-time solution of scalar nonlinear conservation laws",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04938",
        "abstract_url": "http://arxiv.org/abs/2401.04938",
        "authors": [
            {
                "last_name": "Fatima",
                "first_name": "Rumsha"
            },
            {
                "last_name": "Younis",
                "first_name": "Shahzad"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Faraz"
            },
            {
                "last_name": "Imran",
                "first_name": "Hamna"
            },
            {
                "last_name": "Sultan",
                "first_name": "Haseeb"
            },
            {
                "last_name": "Rasool",
                "first_name": "Shahzad"
            },
            {
                "last_name": "Rafiq",
                "first_name": "Mehak"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "LG"
        ],
        "abstract": "  The reliable diagnosis of cardiac conditions through electrocardiogram (ECG)\nanalysis critically depends on accurately detecting P waves and measuring the\nPR interval. However, achieving consistent and generalizable diagnoses across\ndiverse populations presents challenges due to the inherent global variations\nobserved in ECG signals. This paper is focused on applying the Q learning\nreinforcement algorithm to the various ECG datasets available in the\nPhysioNet/Computing in Cardiology Challenge (CinC). Five ECG beats, including\nNormal Sinus Rhythm, Atrial Flutter, Atrial Fibrillation, 1st Degree\nAtrioventricular Block, and Left Atrial Enlargement, are included to study\nvariations of P waves and PR Interval on Lead II and Lead V1. Q-Agent\nclassified 71,672 beat samples in 8,867 patients with an average accuracy of\n90.4% and only 9.6% average hamming loss over misclassification. The average\nclassification time at the 100th episode containing around 40,000 samples is\n0.04 seconds. An average training reward of 344.05 is achieved at an alpha,\ngamma, and SoftMax temperature rate of 0.001, 0.9, and 0.1, respectively.\n",
        "title": "Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform\n  Variations Related to P Wave and PR Interval",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04941",
        "abstract_url": "http://arxiv.org/abs/2401.04941",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Gaojun"
            },
            {
                "last_name": "Ezerman",
                "first_name": "Martianus Frederic"
            },
            {
                "last_name": "G\u00fcneri",
                "first_name": "Cem"
            },
            {
                "last_name": "Ling",
                "first_name": "San"
            },
            {
                "last_name": "\u00d6zbudak",
                "first_name": "Ferruh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The $b$-symbol metric is a generalization of the Hamming metric. Linear\ncodes, in the $b$-symbol metric, have been used in the read channel whose\noutputs consist of $b$ consecutive symbols. The Griesmer bound outperforms the\nSingleton bound for $\\mathbb{F}_q$-linear codes in the Hamming metric, when $q$\nis fixed and the length is large enough. This scenario is also applicable in\nthe $b$-symbol metric. Shi, Zhu, and Helleseth recently made a conjecture on\ncyclic codes in the $b$-symbol metric. In this paper, we present the $b$-symbol\nGriesmer bound for linear codes by concatenating linear codes and simplex\ncodes. Based on cyclic codes and extended cyclic codes, we propose two families\nof distance-optimal linear codes with respect to the $b$-symbol Griesmer bound.\n",
        "title": "Griesmer Bound and Constructions of Linear Codes in $b$-Symbol Metric",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04942",
        "abstract_url": "http://arxiv.org/abs/2401.04942",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Beiwen"
            },
            {
                "last_name": "Gao",
                "first_name": "Huan-ang"
            },
            {
                "last_name": "Cui",
                "first_name": "Leiyao"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yupeng"
            },
            {
                "last_name": "Luo",
                "first_name": "Lan"
            },
            {
                "last_name": "Wang",
                "first_name": "Baofeng"
            },
            {
                "last_name": "Zhi",
                "first_name": "Rong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guyue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the past several years, road anomaly segmentation is actively explored in\nthe academia and drawing growing attention in the industry. The rationale\nbehind is straightforward: if the autonomous car can brake before hitting an\nanomalous object, safety is promoted. However, this rationale naturally calls\nfor a temporally informed setting while existing methods and benchmarks are\ndesigned in an unrealistic frame-wise manner. To bridge this gap, we contribute\nthe first video anomaly segmentation dataset for autonomous driving. Since\nplacing various anomalous objects on busy roads and annotating them in every\nframe are dangerous and expensive, we resort to synthetic data. To improve the\nrelevance of this synthetic dataset to real-world applications, we train a\ngenerative adversarial network conditioned on rendering G-buffers for\nphotorealism enhancement. Our dataset consists of 120,000 high-resolution\nframes at a 60 FPS framerate, as recorded in 7 different towns. As an initial\nbenchmarking, we provide baselines using latest supervised and unsupervised\nroad anomaly segmentation methods. Apart from conventional ones, we focus on\ntwo new metrics: temporal consistency and latencyaware streaming accuracy. We\nbelieve the latter is valuable as it measures whether an anomaly segmentation\nalgorithm can truly prevent a car from crashing in a temporally informed\nsetting.\n",
        "title": "Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic\n  Dataset and New Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04946",
        "abstract_url": "http://arxiv.org/abs/2401.04946",
        "authors": [
            {
                "last_name": "Mustapha",
                "first_name": "Kassem"
            },
            {
                "last_name": "McLean",
                "first_name": "William"
            },
            {
                "last_name": "Dick",
                "first_name": "Josef"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We investigate a second-order accurate time-stepping scheme for solving a\ntime-fractional diffusion equation with a Caputo derivative of order~$\\alpha\n\\in (0,1)$. The basic idea of our scheme is based on local integration followed\nby linear interpolation. It reduces to the standard Crank--Nicolson scheme in\nthe classical diffusion case, that is, as $\\alpha\\to 1$. Using a novel\napproach, we show that the proposed scheme is $\\alpha$-robust and second-order\naccurate in the $L^2(L^2)$-norm, assuming a suitable time-graded mesh. For\ncompleteness, we use the Galerkin finite element method for the spatial\ndiscretization and discuss the error analysis under reasonable regularity\nassumptions on the given data. Some numerical results are presented at the end.\n",
        "title": "An $\\alpha$-robust second-order accurate scheme for a subdiffusion\n  equation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04947",
        "abstract_url": "http://arxiv.org/abs/2401.04947",
        "authors": [
            {
                "last_name": "Hassan-Montero",
                "first_name": "Yusef"
            },
            {
                "last_name": "Herrero-Solana",
                "first_name": "Victor"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Tagging-based systems enable users to categorize web resources by means of\ntags (freely chosen keywords), in order to refinding these resources later.\nTagging is implicitly also a social indexing process, since users share their\ntags and resources, constructing a social tag index, so-called folksonomy. At\nthe same time of tagging-based system, has been popularised an interface model\nfor visual information retrieval known as Tag-Cloud. In this model, the most\nfrequently used tags are displayed in alphabetical order. This paper presents a\nnovel approach to Tag-Cloud's tags selection, and proposes the use of\nclustering algorithms for visual layout, with the aim of improve browsing\nexperience. The results suggest that presented approach reduces the semantic\ndensity of tag set, and improves the visual consistency of Tag-Cloud layout.\n",
        "title": "Improving Tag-Clouds as Visual Information Retrieval Interfaces",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04950",
        "abstract_url": "http://arxiv.org/abs/2401.04950",
        "authors": [
            {
                "last_name": "Hristopulos",
                "first_name": "Dionissios T."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "IT",
            ""
        ],
        "abstract": "  Causal inference seeks to identify cause-and-effect interactions in coupled\nsystems. A recently proposed method by Liang detects causal relations by\nquantifying the direction and magnitude of information flow between time\nseries. The theoretical formulation of information flow for stochastic\ndynamical systems provides a general expression and a data-driven statistic for\nthe rate of entropy transfer between different system units. To advance\nunderstanding of information flow rate in terms of intuitive concepts and\nphysically meaningful parameters, we investigate statistical properties of the\ndata-driven information flow rate between coupled stochastic processes. We\nderive relations between the expectation of the information flow rate statistic\nand properties of the auto- and cross-correlation functions. Thus, we elucidate\nthe dependence of the information flow rate on the analytical properties and\ncharacteristic times of the correlation functions. Our analysis provides\ninsight into the influence of the sampling step, the strength of\ncross-correlations, and the temporal delay of correlations on information flow\nrate. We support the theoretical results with numerical simulations of\ncorrelated Gaussian processes.\n",
        "title": "Information Flow Rate for Cross-Correlated Stochastic Processes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04952",
        "abstract_url": "http://arxiv.org/abs/2401.04952",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Zekun"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Some argue that the essence of humanity, such as creativity and sentiment,\ncan never be mimicked by machines. This paper casts doubt on this belief by\nstudying a vital question: Can AI compose poetry as well as humans? To answer\nthe question, we propose ProFTAP, a novel evaluation framework inspired by\nTuring test to assess AI's poetry writing capability. We apply it on current\nlarge language models (LLMs) and find that recent LLMs do indeed possess the\nability to write classical Chinese poems nearly indistinguishable from those of\nhumans. We also reveal that various open-source LLMs can outperform GPT-4 on\nthis task.\n",
        "title": "Can AI Write Classical Chinese Poetry like Humans? An Empirical Study\n  Inspired by Turing Test",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04954",
        "abstract_url": "http://arxiv.org/abs/2401.04954",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jian-Guo"
            },
            {
                "last_name": "Witelski",
                "first_name": "Thomas"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaoqian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaqi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In this paper, we investigate the tumor instability by employing both\nanalytical and numerical techniques to validate previous results and extend the\nanalytical findings presented in a prior study by Feng et al 2023. Building\nupon the insights derived from the analytical reconstruction of key results in\nthe aforementioned work in one dimension (1D) and two dimensions (2D), we\nextend our analysis to three dimensions (3D). Specifically, we focus on the\ndetermination of boundary instability using perturbation and asymptotic\nanalysis along with spherical harmonics. Additionally, we have validated our\nanalytical results in a two-dimensional framework by implementing the\nAlternating Directional Implicit (ADI) method, as detailed in Witelski and\nBowen (2003). Our primary focus has been on ensuring that the numerical\nsimulation of the propagation speed aligns accurately with the analytical\nfindings. Furthermore, we have matched the simulated boundary stability with\nthe analytical predictions derived from the evolution function, which will be\ndefined in subsequent sections of our paper. These alignment is essential for\naccurately determining the stability or instability of tumor boundaries.\n",
        "title": "A Three-dimensional tumor growth model and its boundary instability",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04956",
        "abstract_url": "http://arxiv.org/abs/2401.04956",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Huafeng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Jin",
                "first_name": "Xin"
            },
            {
                "last_name": "Song",
                "first_name": "Qun"
            },
            {
                "last_name": "El-Yacoubi",
                "first_name": "Mounim A."
            },
            {
                "last_name": "Gao",
                "first_name": "Xinbo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR"
        ],
        "abstract": "  Eye movement (EM) is a new highly secure biometric behavioral modality that\nhas received increasing attention in recent years. Although deep neural\nnetworks, such as convolutional neural network (CNN), have recently achieved\npromising performance, current solutions fail to capture local and global\ntemporal dependencies within eye movement data. To overcome this problem, we\npropose in this paper a mixed transformer termed EmMixformer to extract time\nand frequency domain information for eye movement recognition. To this end, we\npropose a mixed block consisting of three modules, transformer, attention Long\nshort-term memory (attention LSTM), and Fourier transformer. We are the first\nto attempt leveraging transformer to learn long temporal dependencies within\neye movement. Second, we incorporate the attention mechanism into LSTM to\npropose attention LSTM with the aim to learn short temporal dependencies.\nThird, we perform self attention in the frequency domain to learn global\nfeatures. As the three modules provide complementary feature representations in\nterms of local and global dependencies, the proposed EmMixformer is capable of\nimproving recognition accuracy. The experimental results on our eye movement\ndataset and two public eye movement datasets show that the proposed EmMixformer\noutperforms the state of the art by achieving the lowest verification error.\n",
        "title": "EmMixformer: Mix transformer for eye movement recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04958",
        "abstract_url": "http://arxiv.org/abs/2401.04958",
        "authors": [
            {
                "last_name": "Mubasshir",
                "first_name": "Kazi Samin"
            },
            {
                "last_name": "Karim",
                "first_name": "Imtiaz"
            },
            {
                "last_name": "Bertino",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Fake base stations (FBSes) pose a significant security threat by\nimpersonating legitimate base stations. Though efforts have been made to defeat\nthis threat, up to this day, the presence of FBSes and the multi-step attacks\n(MSAs) stemming from them can lead to unauthorized surveillance, interception\nof sensitive information, and disruption of network services for legitimate\nusers. Therefore, detecting these malicious entities is crucial to ensure the\nsecurity and reliability of cellular networks. Traditional detection methods\noften rely on additional hardware, predefined rules, signal scanning, changing\nprotocol specifications, or cryptographic mechanisms that have limitations and\nincur huge infrastructure costs in accurately identifying FBSes. In this paper,\nwe develop FBSDetector-an effective and efficient detection solution that can\nreliably detect FBSes and MSAs from layer-3 network traces using machine\nlearning (ML) at the user equipment (UE) side. To develop FBSDetector, we\ncreated FBSAD and MSAD, the first-ever high-quality and large-scale datasets\nfor training machine learning models capable of detecting FBSes and MSAs. These\ndatasets capture the network traces in different real-world cellular network\nscenarios (including mobility and different attacker capabilities)\nincorporating legitimate base stations and FBSes. The combined network trace\nhas a volume of 6.6 GB containing 751963 packets. Our novel ML models,\nspecially designed to detect FBSes and MSAs, can effectively detect FBSes with\nan accuracy of 92% and a false positive rate of 5.96% and recognize MSAs with\nan accuracy of 86% and a false positive rate of 7.82%. We deploy FBSDetector as\na real-world solution to protect end-users through an Android app and validate\nin a controlled lab environment. Compared to the existing solutions that fail\nto detect FBSes, FBSDetector can detect FBSes in the wild in real time.\n",
        "title": "FBSDetector: Fake Base Station and Multi Step Attack Detection in\n  Cellular Networks using Machine Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04960",
        "abstract_url": "http://arxiv.org/abs/2401.04960",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hanli"
            },
            {
                "last_name": "Srikanthan",
                "first_name": "Anusha"
            },
            {
                "last_name": "Folk",
                "first_name": "Spencer"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vijay"
            },
            {
                "last_name": "Matni",
                "first_name": "Nikolai"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG",
            ""
        ],
        "abstract": "  Motivated by the increasing use of quadrotors for payload delivery, we\nconsider a joint trajectory generation and feedback control design problem for\na quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag\nforces from carried payloads can lead to catastrophic outcomes. Prior work\nmodel aerodynamic effects as residual dynamics or external disturbances in the\ncontrol problem leading to a reactive policy that could be catastrophic.\nMoreover, redesigning controllers and tuning control gains on hardware\nplatforms is a laborious effort. In this paper, we argue that adapting the\ntrajectory generation component keeping the controller fixed can improve\ntrajectory tracking for quadrotor systems experiencing drag forces. To achieve\nthis, we formulate a drag-aware planning problem by applying a suitable\nrelaxation to an optimal quadrotor control problem, introducing a tracking cost\nfunction which measures the ability of a controller to follow a reference\ntrajectory. This tracking cost function acts as a regularizer in trajectory\ngeneration and is learned from data obtained from simulation. Our experiments\nin both simulation and on the Crazyflie hardware platform show that changing\nthe planner reduces tracking error by as much as 83%. Evaluation on hardware\ndemonstrates that our planned path, as opposed to a baseline, avoids controller\nsaturation and catastrophic outcomes during aggressive maneuvers.\n",
        "title": "Why Change Your Controller When You Can Change Your Planner: Drag-Aware\n  Trajectory Generation for Quadrotor Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04961",
        "abstract_url": "http://arxiv.org/abs/2401.04961",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Yuncheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zixun"
            },
            {
                "last_name": "Hu",
                "first_name": "Yiwen"
            },
            {
                "last_name": "Li",
                "first_name": "Guanbin"
            },
            {
                "last_name": "Wan",
                "first_name": "Xiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Song"
            },
            {
                "last_name": "Cui",
                "first_name": "Shuguang"
            },
            {
                "last_name": "Huang",
                "first_name": "Silin"
            },
            {
                "last_name": "Li",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate polyp detection is critical for early colorectal cancer diagnosis.\nAlthough remarkable progress has been achieved in recent years, the complex\ncolon environment and concealed polyps with unclear boundaries still pose\nsevere challenges in this area. Existing methods either involve computationally\nexpensive context aggregation or lack prior modeling of polyps, resulting in\npoor performance in challenging cases. In this paper, we propose the Enhanced\nCenterNet with Contrastive Learning (ECC-PolypDet), a two-stage training \\&\nend-to-end inference framework that leverages images and bounding box\nannotations to train a general model and fine-tune it based on the inference\nscore to obtain a final robust model. Specifically, we conduct Box-assisted\nContrastive Learning (BCL) during training to minimize the intra-class\ndifference and maximize the inter-class difference between foreground polyps\nand backgrounds, enabling our model to capture concealed polyps. Moreover, to\nenhance the recognition of small polyps, we design the Semantic Flow-guided\nFeature Pyramid Network (SFFPN) to aggregate multi-scale features and the\nHeatmap Propagation (HP) module to boost the model's attention on polyp\ntargets. In the fine-tuning stage, we introduce the IoU-guided Sample\nRe-weighting (ISR) mechanism to prioritize hard samples by adaptively adjusting\nthe loss weight for each sample during fine-tuning. Extensive experiments on\nsix large-scale colonoscopy datasets demonstrate the superiority of our model\ncompared with previous state-of-the-art detectors.\n",
        "title": "ECC-PolypDet: Enhanced CenterNet with Contrastive Learning for Automatic\n  Polyp Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04962",
        "abstract_url": "http://arxiv.org/abs/2401.04962",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Kailong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Xia",
                "first_name": "Qianchen"
            },
            {
                "last_name": "Liu",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Keyframe extraction aims to sum up a video's semantics with the minimum\nnumber of its frames. This paper puts forward a Large Model based Sequential\nKeyframe Extraction for video summarization, dubbed LMSKE, which contains three\nstages as below. First, we use the large model \"TransNetV21\" to cut the video\ninto consecutive shots, and employ the large model \"CLIP2\" to generate each\nframe's visual feature within each shot; Second, we develop an adaptive\nclustering algorithm to yield candidate keyframes for each shot, with each\ncandidate keyframe locating nearest to a cluster center; Third, we further\nreduce the above candidate keyframes via redundancy elimination within each\nshot, and finally concatenate them in accordance with the sequence of shots as\nthe final sequential keyframes. To evaluate LMSKE, we curate a benchmark\ndataset and conduct rich experiments, whose results exhibit that LMSKE performs\nmuch better than quite a few SOTA competitors with average F1 of 0.5311,\naverage fidelity of 0.8141, and average compression ratio of 0.9922.\n",
        "title": "Large Model based Sequential Keyframe Extraction for Video Summarization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04964",
        "abstract_url": "http://arxiv.org/abs/2401.04964",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zechen"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haolin"
            },
            {
                "last_name": "Yan",
                "first_name": "YuJie"
            },
            {
                "last_name": "Wu",
                "first_name": "Xihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD",
            ""
        ],
        "abstract": "  Relating speech to EEG holds considerable importance but challenging. In this\nstudy, deep convolutional network was employed to extract spatiotemporal\nfeatures from EEG data. Self-supervised speech representation and contextual\ntext embedding were used as speech features. Contrastive learning was used to\nrelated EEG features to speech features. The experimental results demonstrate\nthe benefits of using self-supervised speech representation and contextual text\nembedding. Through feature fusion and model ensemble, an accuracy of 60.29% was\nachieved, and the performance was ranked as No.2 in Task1 of the Auditory EEG\nChallenge (ICASSP 2024).\n",
        "title": "Self-supervised speech representation and contextual text embedding for\n  match-mismatch classification with EEG recording",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04965",
        "abstract_url": "http://arxiv.org/abs/2401.04965",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Xiran"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Yan",
                "first_name": "Yujie"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haolin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zechen"
            },
            {
                "last_name": "Wu",
                "first_name": "Xihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  To investigate the processing of speech in the brain, simple linear models\nare commonly used to establish a relationship between brain signals and speech\nfeatures. However, these linear models are ill-equipped to model a highly\ndynamic and complex non-linear system like the brain. Although non-linear\nmethods with neural networks have been developed recently, reconstructing\nunseen stimuli from unseen subjects' EEG is still a highly challenging task.\nThis work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams\nfrom EEG, in which the deep convolution neural network and extensive\nconcatenation operation were combined. With our ConvConcatNet model, the\nPearson correlation between the reconstructed and the target mel-spectrogram\ncan achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG\nChallenge. The codes and models to implement our work will be available on\nGithub: https://github.com/xuxiran/ConvConcatNet\n",
        "title": "ConvConcatNet: a deep convolutional neural network to reconstruct mel\n  spectrogram from the EEG",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04966",
        "abstract_url": "http://arxiv.org/abs/2401.04966",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Chenguang"
            },
            {
                "last_name": "Sun",
                "first_name": "Jie"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Don",
                "first_name": "WaiSun"
            },
            {
                "last_name": "Ju",
                "first_name": "Lili"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  A high-order multi-time-step (MTS) scheme for the bond-based peridynamic (PD)\nmodel, an extension of classical continuous mechanics widely used for analyzing\ndiscontinuous problems like cracks, is proposed. The MTS scheme discretizes the\nspatial domain with a meshfree method and advances in time with a high-order\nRunge-Kutta method. To effectively handle discontinuities (cracks) that appear\nin a local subdomain in the solution, the scheme employs the Taylor expansion\nand Lagrange interpolation polynomials with a finer time step size, that is,\ncoarse and fine time step sizes for smooth and discontinuous subdomains,\nrespectively, to achieve accurate and efficient simulations. By eliminating\nunnecessary fine-scale resolution imposed on the entire domain, the MTS scheme\noutperforms the standard PD scheme by significantly reducing computational\ncosts, particularly for problems with discontinuous solutions, as demonstrated\nby comprehensive theoretical analysis and numerical experiments.\n",
        "title": "A high-order multi-time-step scheme for bond-based peridynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04968",
        "abstract_url": "http://arxiv.org/abs/2401.04968",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Zhenmin"
            },
            {
                "last_name": "Shen",
                "first_name": "Shaojie"
            },
            {
                "last_name": "Ma",
                "first_name": "Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Cooperative decision-making of Connected Autonomous Vehicles (CAVs) presents\na longstanding challenge due to its inherent nonlinearity, non-convexity, and\ndiscrete characteristics, compounded by the diverse road topologies encountered\nin real-world traffic scenarios. The majority of current methodologies are only\napplicable to a single and specific scenario, predicated on scenario-specific\nassumptions. Consequently, their application in real-world environments is\nrestricted by the innumerable nature of traffic scenarios. In this study, we\npropose a unified optimization approach that exhibits the potential to address\ncooperative decision-making problems related to traffic scenarios with generic\nroad topologies. This development is grounded in the premise that the\ntopologies of various traffic scenarios can be universally represented as\nDirected Acyclic Graphs (DAGs). Particularly, the reference paths and time\nprofiles for all involved CAVs are determined in a fully cooperative manner,\ntaking into account factors such as velocities, accelerations, conflict\nresolutions, and overall traffic efficiency. The cooperative decision-making of\nCAVs is approximated as a mixed-integer linear programming (MILP) problem\nbuilding on the DAGs of road topologies. This favorably facilitates the use of\nstandard numerical solvers and the global optimality can be attained through\nthe optimization. Case studies corresponding to different multi-lane traffic\nscenarios featuring diverse topologies are scheduled as the test itineraries,\nand the efficacy of our proposed methodology is corroborated.\n",
        "title": "A Universal Cooperative Decision-Making Framework for Connected\n  Autonomous Vehicles with Generic Road Topologies",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04971",
        "abstract_url": "http://arxiv.org/abs/2401.04971",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shu"
            },
            {
                "last_name": "Xu",
                "first_name": "Zitao"
            },
            {
                "last_name": "Pan",
                "first_name": "Weike"
            },
            {
                "last_name": "Yang",
                "first_name": "Qiang"
            },
            {
                "last_name": "Ming",
                "first_name": "Zhong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Cross-domain sequential recommendation (CDSR) shifts the modeling of user\npreferences from flat to stereoscopic by integrating and learning interaction\ninformation from multiple domains at different granularities (ranging from\ninter-sequence to intra-sequence and from single-domain to cross-domain).In\nthis survey, we initially define the CDSR problem using a four-dimensional\ntensor and then analyze its multi-type input representations under\nmultidirectional dimensionality reductions. Following that, we provide a\nsystematic overview from both macro and micro views. From a macro view, we\nabstract the multi-level fusion structures of various models across domains and\ndiscuss their bridges for fusion. From a micro view, focusing on the existing\nmodels, we specifically discuss the basic technologies and then explain the\nauxiliary learning technologies. Finally, we exhibit the available public\ndatasets and the representative experimental results as well as provide some\ninsights into future directions for research in CDSR.\n",
        "title": "A Survey on Cross-Domain Sequential Recommendation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04972",
        "abstract_url": "http://arxiv.org/abs/2401.04972",
        "authors": [
            {
                "last_name": "Stewart",
                "first_name": "Ian"
            },
            {
                "last_name": "Mihalcea",
                "first_name": "Rada"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Machine translation often suffers from biased data and algorithms that can\nlead to unacceptable errors in system output. While bias in gender norms has\nbeen investigated, less is known about whether MT systems encode bias about\nsocial relationships, e.g. sentences such as \"the lawyer kissed her wife.\" We\ninvestigate the degree of bias against same-gender relationships in MT systems,\nusing generated template sentences drawn from several noun-gender languages\n(e.g. Spanish). We find that three popular MT services consistently fail to\naccurately translate sentences concerning relationships between nouns of the\nsame gender. The error rate varies considerably based on the context, e.g.\nsame-gender sentences referencing high female-representation occupations are\ntranslated with lower accuracy. We provide this work as a case study in the\nevaluation of intrinsic bias in NLP systems, with respect to social\nrelationships.\n",
        "title": "Whose wife is it anyway? Assessing bias against same-gender\n  relationships in machine translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04973",
        "abstract_url": "http://arxiv.org/abs/2401.04973",
        "authors": [
            {
                "last_name": "Ju",
                "first_name": "Lili"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Lu",
                "first_name": "Junke"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Existing nonlocal diffusion models are predominantly classified into two\ncategories: bond-based models, which involve a single-fold integral and usually\nsimulate isotropic diffusion, and state-based models, which contain a\ndouble-fold integral and can additionally prototype anisotropic diffusion.\nWhile bond-based models exhibit computational efficiency, they are somewhat\nlimited in their modeling capabilities. In this paper, we develop a novel\nbond-based nonlocal diffusion model with matrix-valued coefficients in\nnon-divergence form. Our approach incorporates the coefficients into a\ncovariance matrix and employs the multivariate Gaussian function with\ntruncation to define the kernel function, and subsequently model the nonlocal\ndiffusion process through the bond-based formulation. We successfully establish\nthe well-posedness of the proposed model along with deriving some of its\nproperties on maximum principle and mass conservation. Furthermore, an\nefficient linear collocation scheme is designed for numerical solution of our\nmodel. Comprehensive experiments in two and three dimensions are conducted to\nshowcase application of the proposed nonlocal model to both isotropic and\nanisotropic diffusion problems and to demonstrate numerical accuracy and\neffective asymptotic compatibility of the proposed collocation scheme.\n",
        "title": "A novel bond-based nonlocal diffusion model with matrix-valued\n  coefficients in non-divergence form and its collocation discretization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04975",
        "abstract_url": "http://arxiv.org/abs/2401.04975",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Qian"
            },
            {
                "last_name": "Cui",
                "first_name": "Ruoxuan"
            },
            {
                "last_name": "Li",
                "first_name": "Yuke"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haoqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Action recognition in videos poses a challenge due to its high computational\ncost, especially for Joint Space-Time video transformers (Joint VT). Despite\ntheir effectiveness, the excessive number of tokens in such architectures\nsignificantly limits their efficiency. In this paper, we propose HaltingVT, an\nefficient video transformer adaptively removing redundant video patch tokens,\nwhich is primarily composed of a Joint VT and a Glimpser module. Specifically,\nHaltingVT applies data-adaptive token reduction at each layer, resulting in a\nsignificant reduction in the overall computational cost. Besides, the Glimpser\nmodule quickly removes redundant tokens in shallow transformer layers, which\nmay even be misleading for video recognition tasks based on our observations.\nTo further encourage HaltingVT to focus on the key motion-related information\nin videos, we design an effective Motion Loss during training. HaltingVT\nacquires video analysis capabilities and token halting compression strategies\nsimultaneously in a unified training process, without requiring additional\ntraining procedures or sub-networks. On the Mini-Kinetics dataset, we achieved\n75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely\nlow 9.9 GFLOPs. The code is available at\nhttps://github.com/dun-research/HaltingVT.\n",
        "title": "HaltingVT: Adaptive Token Halting Transformer for Efficient Video\n  Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04976",
        "abstract_url": "http://arxiv.org/abs/2401.04976",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Haobo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Mu",
                "first_name": "Da"
            },
            {
                "last_name": "Dang",
                "first_name": "Yonghao"
            },
            {
                "last_name": "Yin",
                "first_name": "Jianqin"
            },
            {
                "last_name": "Tang",
                "first_name": "Jin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Recently, 2D convolution has been found unqualified in sound event detection\n(SED). It enforces translation equivariance on sound events along frequency\naxis, which is not a shift-invariant dimension. To address this issue, dynamic\nconvolution is used to model the frequency dependency of sound events. In this\npaper, we proposed the first full-dynamic method named \\emph{full-frequency\ndynamic convolution} (FFDConv). FFDConv generates frequency kernels for every\nfrequency band, which is designed directly in the structure for\nfrequency-dependent modeling. It physically furnished 2D convolution with the\ncapability of frequency-dependent modeling. FFDConv outperforms not only the\nbaseline by 6.6\\% in DESED real validation dataset in terms of PSDS1, but\noutperforms the other full-dynamic methods. In addition, by visualizing\nfeatures of sound events, we observed that FFDConv could effectively extract\ncoherent features in specific frequency bands, consistent with the vocal\ncontinuity of sound events. This proves that FFDConv has great\nfrequency-dependent perception ability.\n",
        "title": "Full-frequency dynamic convolution: a physical frequency-dependent\n  convolution for sound event detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04978",
        "abstract_url": "http://arxiv.org/abs/2401.04978",
        "authors": [
            {
                "last_name": "Wetzel",
                "first_name": "Sebastian Johann"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  I introduce a unified framework for interpreting neural network classifiers\ntailored toward automated scientific discovery. In contrast to neural\nnetwork-based regression, for classification, it is in general impossible to\nfind a one-to-one mapping from the neural network to a symbolic equation even\nif the neural network itself bases its classification on a quantity that can be\nwritten as a closed-form equation. In this paper, I embed a trained neural\nnetwork into an equivalence class of classifying functions that base their\ndecisions on the same quantity. I interpret neural networks by finding an\nintersection between this equivalence class and human-readable equations\ndefined by the search space of symbolic regression. The approach is not limited\nto classifiers or full neural networks and can be applied to arbitrary neurons\nin hidden layers or latent spaces or to simplify the process of interpreting\nneural network regressors.\n",
        "title": "Closed-Form Interpretation of Neural Network Classifiers with Symbolic\n  Regression Gradients",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04979",
        "abstract_url": "http://arxiv.org/abs/2401.04979",
        "authors": [
            {
                "last_name": "Oh",
                "first_name": "YongKyung"
            },
            {
                "last_name": "Lim",
                "first_name": "Dongyoung"
            },
            {
                "last_name": "Kim",
                "first_name": "Sungil"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  To handle the complexities of irregular and incomplete time series data, we\npropose an invertible solution of Neural Differential Equations (NDE)-based\nmethod. While NDE-based methods are a powerful method for analyzing\nirregularly-sampled time series, they typically do not guarantee reversible\ntransformations in their standard form. Our method suggests the variation of\nNeural Controlled Differential Equations (Neural CDEs) with Neural Flow, which\nensures invertibility while maintaining a lower computational burden.\nAdditionally, it enables the training of a dual latent space, enhancing the\nmodeling of dynamic temporal dynamics. Our research presents an advanced\nframework that excels in both classification and interpolation tasks. At the\ncore of our approach is an enhanced dual latent states architecture, carefully\ndesigned for high precision across various time series tasks. Empirical\nanalysis demonstrates that our method significantly outperforms existing\nmodels. This work significantly advances irregular time series analysis,\nintroducing innovative techniques and offering a versatile tool for diverse\npractical applications.\n",
        "title": "Invertible Solution of Neural Differential Equations for Analysis of\n  Irregularly-Sampled Time Series",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04980",
        "abstract_url": "http://arxiv.org/abs/2401.04980",
        "authors": [
            {
                "last_name": "Attard",
                "first_name": "Daniel"
            },
            {
                "last_name": "Bajada",
                "first_name": "Josef"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  In recent years, significant advancements have been made in the field of\nautonomous driving with the aim of increasing safety and efficiency. However,\nresearch that focuses on tractor-trailer vehicles is relatively sparse. Due to\nthe physical characteristics and articulated joints, such vehicles require\ntailored models. While turning, the back wheels of the trailer turn at a\ntighter radius and the truck often has to deviate from the centre of the lane\nto accommodate this. Due to the lack of publicly available models, this work\ndevelops truck and trailer models using the high-fidelity simulation software\nCARLA, together with several roundabout scenarios, to establish a baseline\ndataset for benchmarks. Using a twin-q soft actor-critic algorithm, we train a\nquasi-end-to-end autonomous driving model which is able to achieve a 73%\nsuccess rate on different roundabouts.\n",
        "title": "Autonomous Navigation of Tractor-Trailer Vehicles through Roundabout\n  Intersections",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04984",
        "abstract_url": "http://arxiv.org/abs/2401.04984",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Luanyuan"
            },
            {
                "last_name": "Du",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            },
            {
                "last_name": "Tang",
                "first_name": "Jinhui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Learning correspondences aims to find correct correspondences (inliers) from\nthe initial correspondence set with an uneven correspondence distribution and a\nlow inlier rate, which can be regarded as graph data. Recent advances usually\nuse graph neural networks (GNNs) to build a single type of graph or simply\nstack local graphs into the global one to complete the task. But they ignore\nthe complementary relationship between different types of graphs, which can\neffectively capture potential relationships among sparse correspondences. To\naddress this problem, we propose MGNet to effectively combine multiple\ncomplementary graphs. To obtain information integrating implicit and explicit\nlocal graphs, we construct local graphs from implicit and explicit aspects and\ncombine them effectively, which is used to build a global graph. Moreover, we\npropose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse\ncorrespondence information at once in the global graph, which can capture and\namplify discriminative features. Extensive experiments demonstrate that MGNet\noutperforms state-of-the-art methods in different visual tasks. The code is\nprovided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.\n",
        "title": "MGNet: Learning Correspondences via Multiple Graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04986",
        "abstract_url": "http://arxiv.org/abs/2401.04986",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Miyatake",
                "first_name": "Yuto"
            },
            {
                "last_name": "Cui",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Wei",
                "first_name": "Shikui"
            },
            {
                "last_name": "Furihata",
                "first_name": "Daisuke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, there has been growing interest in using physics-informed neural\nnetworks (PINNs) to solve differential equations. However, the preservation of\nstructure, such as energy and stability, in a suitable manner has yet to be\nestablished. This limitation could be a potential reason why the learning\nprocess for PINNs is not always efficient and the numerical results may suggest\nnonphysical behavior. Besides, there is little research on their applications\non downstream tasks. To address these issues, we propose structure-preserving\nPINNs to improve their performance and broaden their applications for\ndownstream tasks. Firstly, by leveraging prior knowledge about the physical\nsystem, a structure-preserving loss function is designed to assist the PINN in\nlearning the underlying structure. Secondly, a framework that utilizes\nstructure-preserving PINN for robust image recognition is proposed. Here,\npreserving the Lyapunov structure of the underlying system ensures the\nstability of the system. Experimental results demonstrate that the proposed\nmethod improves the numerical accuracy of PINNs for partial differential\nequations. Furthermore, the robustness of the model against adversarial\nperturbations in image data is enhanced.\n",
        "title": "Structure-Preserving Physics-Informed Neural Networks With Energy or\n  Lyapunov Structure",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04988",
        "abstract_url": "http://arxiv.org/abs/2401.04988",
        "authors": [
            {
                "last_name": "Jeziorek",
                "first_name": "Kamil"
            },
            {
                "last_name": "Wzorek",
                "first_name": "Piotr"
            },
            {
                "last_name": "Blachut",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Pinna",
                "first_name": "Andrea"
            },
            {
                "last_name": "Kryjak",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Event-based vision is an emerging research field involving processing data\ngenerated by Dynamic Vision Sensors (neuromorphic cameras). One of the latest\nproposals in this area are Graph Convolutional Networks (GCNs), which allow to\nprocess events in its original sparse form while maintaining high detection and\nclassification performance. In this paper, we present the hardware\nimplementation of a~graph generation process from an event camera data stream,\ntaking into account both the advantages and limitations of FPGAs. We propose\nvarious ways to simplify the graph representation and use scaling and\nquantisation of values. We consider both undirected and directed graphs that\nenable the use of PointNet convolution. The results obtained show that by\nappropriately modifying the graph representation, it is possible to create\na~hardware module for graph generation. Moreover, the proposed modifications\nhave no significant impact on object detection performance, only 0.08% mAP less\nfor the base model and the N-Caltech data set.Finally, we describe the proposed\nhardware architecture of the graph generation module.\n",
        "title": "Optimising Graph Representation for Hardware Implementation of Graph\n  Convolutional Networks for Event-based Vision",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04993",
        "abstract_url": "http://arxiv.org/abs/2401.04993",
        "authors": [
            {
                "last_name": "Hamidi",
                "first_name": "Shayan Mohajer"
            },
            {
                "last_name": "Yang",
                "first_name": "En-Hui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Federated learning (FL) is a promising technology via which some edge\ndevices/clients collaboratively train a machine learning model orchestrated by\na server. Learning an unfair model is known as a critical problem in federated\nlearning, where the trained model may unfairly advantage or disadvantage some\nof the devices. To tackle this problem, in this work, we propose AdaFed. The\ngoal of AdaFed is to find an updating direction for the server along which (i)\nall the clients' loss functions are decreasing; and (ii) more importantly, the\nloss functions for the clients with larger values decrease with a higher rate.\nAdaFed adaptively tunes this common direction based on the values of local\ngradients and loss functions. We validate the effectiveness of AdaFed on a\nsuite of federated datasets, and demonstrate that AdaFed outperforms\nstate-of-the-art fair FL methods.\n",
        "title": "AdaFed: Fair Federated Learning via Adaptive Common Descent Direction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04996",
        "abstract_url": "http://arxiv.org/abs/2401.04996",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanyuan"
            },
            {
                "last_name": "Su",
                "first_name": "Lili"
            },
            {
                "last_name": "Joe-Wong",
                "first_name": "Carlee"
            },
            {
                "last_name": "Yeh",
                "first_name": "Edmund"
            },
            {
                "last_name": "Ioannidis",
                "first_name": "Stratis"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  As edge computing capabilities increase, model learning deployments in\ndiverse edge environments have emerged. In experimental design networks,\nintroduced recently, network routing and rate allocation are designed to aid\nthe transfer of data from sensors to heterogeneous learners. We design\nefficient experimental design network algorithms that are (a) distributed and\n(b) use multicast transmissions. This setting poses significant challenges as\nclassic decentralization approaches often operate on (strictly) concave\nobjectives under differentiable constraints. In contrast, the problem we study\nhere has a non-convex, continuous DR-submodular objective, while multicast\ntransmissions naturally result in non-differentiable constraints. From a\ntechnical standpoint, we propose a distributed Frank-Wolfe and a distributed\nprojected gradient ascent algorithm that, coupled with a relaxation of\nnon-differentiable constraints, yield allocations within a $1-1/e$ factor from\nthe optimal. Numerical evaluations show that our proposed algorithms outperform\ncompetitors with respect to model learning quality.\n",
        "title": "Distributed Experimental Design Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04997",
        "abstract_url": "http://arxiv.org/abs/2401.04997",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Lanling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Li",
                "first_name": "Bingqian"
            },
            {
                "last_name": "Wang",
                "first_name": "Jinpeng"
            },
            {
                "last_name": "Cai",
                "first_name": "Mingchen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recently, large language models such as ChatGPT have showcased remarkable\nabilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by\nexperiments to systematically analyze the impact of different factors on two\npublic datasets. Finally, we summarize promising directions to shed lights on\nfuture research.\n",
        "title": "Prompting Large Language Models for Recommender Systems: A Comprehensive\n  Framework and Empirical Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04998",
        "abstract_url": "http://arxiv.org/abs/2401.04998",
        "authors": [
            {
                "last_name": "Jing",
                "first_name": ""
            },
            {
                "last_name": "Lin",
                "first_name": ""
            },
            {
                "last_name": "Silfvenius",
                "first_name": "Christofer"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In the era of sustainable transportation, the significance of electric\nvehicles (EVs) and their battery technology is becoming increasingly paramount.\nThis study addresses the critical aspect of EV battery reliability, an\nessential factor in the vehicles' sustainability, performance, and longevity.\nCurrent efforts to enhance EV battery reliability tend to focus on isolated\nareas, often missing the broader, interconnected challenges within the system.\nThis research investigates these challenges across micro, meso, and macro\nlevels, presenting a novel lifecycle framework that includes \"Zero\"-Life\nreliability and phases such as use, reuse, repurpose, and recycling. By\nadopting a holistic approach and delving into system cognition, the study aims\nto bridge the gap between isolated improvements and comprehensive system\noptimization, aligning with global sustainability goals and contributing to the\nadvancement of sustainable transportation and EV technology.\n",
        "title": "Some Critical Thinking on EV Battery Reliability: from Enhancement to\n  Optimization -- comprehensive perspectives, lifecycle innovation, system\n  cognation, and strategic insights",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05007",
        "abstract_url": "http://arxiv.org/abs/2401.05007",
        "authors": [
            {
                "last_name": "Mukendi",
                "first_name": "Christian Mulomba"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  he evaluation of the impact of actions undertaken is essential in management.\nThis paper assesses the impact of efforts considered to mitigate risk and\ncreate safe environments on a global scale. We measure this impact by looking\nat the probability of improvement over a specific short period of time. Using\nthe World Risk Index, we conduct a temporal analysis of global disaster risk\ndynamics from 2011 to 2021. This temporal exploration through the lens of the\nWorld Risk Index provides insights into the complex dynamics of disaster risk.\nWe found that, despite sustained efforts, the global landscape remains divided\ninto two main clusters: high susceptibility and moderate susceptibility,\nregardless of geographical location. This clustering was achieved using a\nsemi-supervised approach through the Label Spreading algorithm, with 98%\naccuracy. We also found that the prediction of clusters achieved through\nsupervised learning on the period considered in this study (one, three, and\nfive years) showed that the Logistic regression (almost 99% at each stage)\nperformed better than other classifiers. This suggests that the current\npolicies and mechanisms are not effective in helping countries move from a\nhazardous position to a safer one during the period considered. In fact,\nstatistical projections using a scenario analysis indicate that there is only a\n1% chance of such a shift occurring within a five-year timeframe. This sobering\nreality highlights the need for a paradigm shift. Traditional long-term\ndisaster management strategies are not effective for countries that are highly\nvulnerable. Our findings indicate the need for an innovative approach that is\ntailored to the specific vulnerabilities of these nations. As the threat of\nvulnerability persists, our research calls for the development of new\nstrategies that can effectively address the ongoing challenges of disaster risk\nmanagement\n",
        "title": "Temporal Analysis of World Disaster Risk:A Machine Learning Approach to\n  Cluster Dynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05010",
        "abstract_url": "http://arxiv.org/abs/2401.05010",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Chunpeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Haishuai"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xilu"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhi"
            },
            {
                "last_name": "Bu",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Few-shot Learning aims to learn and distinguish new categories with a very\nlimited number of available images, presenting a significant challenge in the\nrealm of deep learning. Recent researchers have sought to leverage the\nadditional textual or linguistic information of these rare categories with a\npre-trained language model to facilitate learning, thus partially alleviating\nthe problem of insufficient supervision signals. However, the full potential of\nthe textual information and pre-trained language model have been underestimated\nin the few-shot learning till now, resulting in limited performance\nenhancements. To address this, we propose a simple but effective framework for\nfew-shot learning tasks, specifically designed to exploit the textual\ninformation and language model. In more detail, we explicitly exploit the\nzero-shot capability of the pre-trained language model with the learnable\nprompt. And we just add the visual feature with the textual feature for\ninference directly without the intricate designed fusion modules in previous\nworks. Additionally, we apply the self-ensemble and distillation to further\nenhance these components. Our extensive experiments conducted across four\nwidely used few-shot datasets demonstrate that our simple framework achieves\nimpressive results. Particularly noteworthy is its outstanding performance in\nthe 1-shot learning task, surpassing state-of-the-art methods by an average of\n3.0\\% in classification accuracy. \\footnote{We will make the source codes of\nthe proposed framework publicly available upon acceptance. }.\n",
        "title": "Less is More : A Closer Look at Multi-Modal Few-Shot Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05011",
        "abstract_url": "http://arxiv.org/abs/2401.05011",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Na"
            },
            {
                "last_name": "Chen",
                "first_name": "Weiling"
            },
            {
                "last_name": "Ma",
                "first_name": "Keng Teck"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semi-supervised 3D object detection is a promising yet under-explored\ndirection to reduce data annotation costs, especially for cluttered indoor\nscenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this\ntask by utilizing a teacher model to generate pseudo-labels for unlabeled\nsamples. However, the availability of unlabeled samples in the 3D domain is\nrelatively limited compared to its 2D counterpart due to the greater effort\nrequired to collect 3D data. Moreover, the loose consistency regularization in\nSESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to\neither low-quality supervision or a limited amount of pseudo labels. To address\nthese issues, we present a novel Dual-Perspective Knowledge Enrichment approach\nnamed DPKE for semi-supervised 3D object detection. Our DPKE enriches the\nknowledge of limited training data, particularly unlabeled data, from two\nperspectives: data-perspective and feature-perspective. Specifically, from the\ndata-perspective, we propose a class-probabilistic data augmentation method\nthat augments the input data with additional instances based on the varying\ndistribution of class probabilities. Our DPKE achieves feature-perspective\nknowledge enrichment by designing a geometry-aware feature matching method that\nregularizes feature-level similarity between object proposals from the student\nand teacher models. Extensive experiments on the two benchmark datasets\ndemonstrate that our DPKE achieves superior performance over existing\nstate-of-the-art approaches under various label ratio conditions. The source\ncode will be made available to the public.\n",
        "title": "Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object\n  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05012",
        "abstract_url": "http://arxiv.org/abs/2401.05012",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Shubao"
            },
            {
                "last_name": "Jin",
                "first_name": "Ming"
            },
            {
                "last_name": "Hou",
                "first_name": "Zhaoxiang"
            },
            {
                "last_name": "Yang",
                "first_name": "Chengyi"
            },
            {
                "last_name": "Li",
                "first_name": "Zengxiang"
            },
            {
                "last_name": "Wen",
                "first_name": "Qingsong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Time series forecasting is crucial and challenging in the real world. The\nrecent surge in interest regarding time series foundation models, which cater\nto a diverse array of downstream tasks, is noteworthy. However, existing\nmethods often overlook the multi-scale nature of time series, an aspect crucial\nfor precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical\nmulti-scale masked time series modeling method designed for long-term\nforecasting. Specifically, it comprises four integral components: (1)\nhierarchical multi-scale transformer (HMT) to capture temporal information at\ndifferent scales; (2) decoupled encoder-decoder (DED) forces the encoder to\nfocus on feature extraction, while the decoder to focus on pretext tasks; (3)\nmulti-scale masked reconstruction (MMR) provides multi-stage supervision\nsignals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to\ncapture dependencies between different scales for forecasting. Collectively,\nthese components enhance multi-scale feature extraction capabilities in masked\ntime series modeling and contribute to improved prediction accuracy. We conduct\nextensive experiments on 7 mainstream datasets to prove that HiMTM has obvious\nadvantages over contemporary self-supervised and end-to-end learning methods.\nThe effectiveness of HiMTM is further showcased by its application in the\nindustry of natural gas demand forecasting.\n",
        "title": "HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for\n  Long-Term Forecasting",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05014",
        "abstract_url": "http://arxiv.org/abs/2401.05014",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Jinjing"
            },
            {
                "last_name": "Chen",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Source-free cross-modal knowledge transfer is a crucial yet challenging task,\nwhich aims to transfer knowledge from one source modality (e.g., RGB) to the\ntarget modality (e.g., depth or infrared) with no access to the task-relevant\n(TR) source data due to memory and privacy concerns. A recent attempt leverages\nthe paired task-irrelevant (TI) data and directly matches the features from\nthem to eliminate the modality gap. However, it ignores a pivotal clue that the\npaired TI data could be utilized to effectively estimate the source data\ndistribution and better facilitate knowledge transfer to the target modality.\nTo this end, we propose a novel yet concise framework to unlock the potential\nof paired TI data for enhancing source-free cross-modal knowledge transfer. Our\nwork is buttressed by two key technical components. Firstly, to better estimate\nthe source data distribution, we introduce a Task-irrelevant data-Guided\nModality Bridging (TGMB) module. It translates the target modality data (e.g.,\ninfrared) into the source-like RGB images based on paired TI data and the\nguidance of the available source model to alleviate two key gaps: 1)\ninter-modality gap between the paired TI data; 2) intra-modality gap between TI\nand TR target data. We then propose a Task-irrelevant data-Guided Knowledge\nTransfer (TGKT) module that transfers knowledge from the source model to the\ntarget model by leveraging the paired TI data. Notably, due to the\nunavailability of labels for the TR target data and its less reliable\nprediction from the source model, our TGKT model incorporates a self-supervised\npseudo-labeling approach to enable the target model to learn from its\npredictions. Extensive experiments show that our method achieves\nstate-of-the-art performance on three datasets (RGB-to-depth and\nRGB-to-infrared).\n",
        "title": "Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential\n  of Task-Irrelevant Data",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05015",
        "abstract_url": "http://arxiv.org/abs/2401.05015",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Farnia",
                "first_name": "Farzan"
            },
            {
                "last_name": "Leung",
                "first_name": "Ho-fung"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Reinforcement learning (RL) problems where the learner attempts to infer an\nunobserved reward from some feedback variables have been studied in several\nrecent papers. The setting of Interaction-Grounded Learning (IGL) is an example\nof such feedback-based reinforcement learning tasks where the learner optimizes\nthe return by inferring latent binary rewards from the interaction with the\nenvironment. In the IGL setting, a relevant assumption used in the RL\nliterature is that the feedback variable $Y$ is conditionally independent of\nthe context-action $(X,A)$ given the latent reward $R$. In this work, we\npropose Variational Information-based IGL (VI-IGL) as an information-theoretic\nmethod to enforce the conditional independence assumption in the IGL-based RL\nproblem. The VI-IGL framework learns a reward decoder using an\ninformation-based objective based on the conditional mutual information (MI)\nbetween the context-action $(X,A)$ and the feedback variable $Y$ observed from\nthe environment. To estimate and optimize the information-based terms for the\ncontinuous random variables in the RL problem, VI-IGL leverages the variational\nrepresentation of mutual information and results in a min-max optimization\nproblem. Furthermore, we extend the VI-IGL framework to general $f$-Information\nmeasures in the information theory literature, leading to the generalized\n$f$-VI-IGL framework to address the RL problem under the IGL condition.\nFinally, we provide the empirical results of applying the VI-IGL method to\nseveral reinforcement learning settings, which indicate an improved performance\nin comparison to the previous IGL-based RL algorithm.\n",
        "title": "An Information Theoretic Approach to Interaction-Grounded Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05018",
        "abstract_url": "http://arxiv.org/abs/2401.05018",
        "authors": [
            {
                "last_name": "Idrees",
                "first_name": "Sarmad"
            },
            {
                "last_name": "Choi",
                "first_name": "Jongeun"
            },
            {
                "last_name": "Sohn",
                "first_name": "Seokman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To achieve seamless collaboration between robots and humans in a shared\nenvironment, accurately predicting future human movements is essential. Human\nmotion prediction has traditionally been approached as a sequence prediction\nproblem, leveraging historical human motion data to estimate future poses.\nBeginning with vanilla recurrent networks, the research community has\ninvestigated a variety of methods for learning human motion dynamics,\nencompassing graph-based and generative approaches. Despite these efforts,\nachieving accurate long-term predictions continues to be a significant\nchallenge. In this regard, we present the Adversarial Motion Transformer\n(AdvMT), a novel model that integrates a transformer-based motion encoder and a\ntemporal continuity discriminator. This combination effectively captures\nspatial and temporal dependencies simultaneously within frames. With\nadversarial training, our method effectively reduces the unwanted artifacts in\npredictions, thereby ensuring the learning of more realistic and fluid human\nmotions. The evaluation results indicate that AdvMT greatly enhances the\naccuracy of long-term predictions while also delivering robust short-term\npredictions\n",
        "title": "AdvMT: Adversarial Motion Transformer for Long-term Human Motion\n  Prediction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05019",
        "abstract_url": "http://arxiv.org/abs/2401.05019",
        "authors": [
            {
                "last_name": "Xin",
                "first_name": "Jinghao"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinwoo"
            },
            {
                "last_name": "Chu",
                "first_name": "Shengjia"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Existing Global Path Planning (GPP) algorithms predominantly presume planning\nin a static environment. This assumption immensely limits their applications to\nUnmanned Surface Vehicles (USVs) that typically navigate in dynamic\nenvironments. To address this limitation, we present OkayPlan, a GPP algorithm\ncapable of generating safe and short paths in dynamic scenarios at a real-time\nexecuting speed (125 Hz on a desktop-class computer). Specifically, we approach\nthe challenge of dynamic obstacle avoidance by formulating the path planning\nproblem as an obstacle kinematics augmented optimization problem, which can be\nefficiently resolved through a PSO-based optimizer at a real-time speed.\nMeanwhile, a Dynamic Prioritized Initialization (DPI) mechanism that adaptively\ninitializes potential solutions for the optimization problem is established to\nfurther ameliorate the solution quality. Additionally, a relaxation strategy\nthat facilitates the autonomous tuning of OkayPlan's hyperparameters in dynamic\nenvironments is devised. Comparative experiments involving canonical and\ncontemporary GPP algorithms, along with ablation studies, have been conducted\nto substantiate the efficacy of our approach. Results indicate that OkayPlan\noutstrips existing methods in terms of path safety, length optimality, and\ncomputational efficiency, establishing it as a potent GPP technique for dynamic\nenvironments. The video and code associated with this paper are accessible at\nhttps://github.com/XinJingHao/OkayPlan.\n",
        "title": "OkayPlan: Obstacle Kinematics Augmented Dynamic Real-time Path Planning\n  via Particle Swarm Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05031",
        "abstract_url": "http://arxiv.org/abs/2401.05031",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenchao"
            },
            {
                "last_name": "Hong",
                "first_name": "Zicong"
            },
            {
                "last_name": "Guo",
                "first_name": "Song"
            },
            {
                "last_name": "Wang",
                "first_name": "Haozhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Zeng",
                "first_name": "Deze"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Transformer model empowered architectures have become a pillar of cloud\nservices that keeps reshaping our society. However, the dynamic query loads and\nheterogeneous user requirements severely challenge current transformer serving\nsystems, which rely on pre-training multiple variants of a foundation model,\ni.e., with different sizes, to accommodate varying service demands.\nUnfortunately, such a mechanism is unsuitable for large transformer models due\nto the additional training costs and excessive I/O delay. In this paper, we\nintroduce OTAS, the first elastic serving system specially tailored for\ntransformer models by exploring lightweight token management. We develop a\nnovel idea called token adaptation that adds prompting tokens to improve\naccuracy and removes redundant tokens to accelerate inference. To cope with\nfluctuating query loads and diverse user requests, we enhance OTAS with\napplication-aware selective batching and online token adaptation. OTAS first\nbatches incoming queries with similar service-level objectives to improve the\ningress throughput. Then, to strike a tradeoff between the overhead of token\nincrement and the potentials for accuracy improvement, OTAS adaptively adjusts\nthe token execution strategy by solving an optimization problem. We implement\nand evaluate a prototype of OTAS with multiple datasets, which show that OTAS\nimproves the system utility by at least 18.2%.\n",
        "title": "OTAS: An Elastic Transformer Serving System via Token Adaptation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05033",
        "abstract_url": "http://arxiv.org/abs/2401.05033",
        "authors": [
            {
                "last_name": "Ulmer",
                "first_name": "Dennis"
            },
            {
                "last_name": "Mansimov",
                "first_name": "Elman"
            },
            {
                "last_name": "Lin",
                "first_name": "Kaixiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Justin"
            },
            {
                "last_name": "Gao",
                "first_name": "Xibin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) are powerful dialogue agents, but specializing\nthem towards fulfilling a specific function can be challenging. Instructing\ntuning, i.e. tuning models on instruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an effective method to do so, yet\nrequires a number of data samples that a) might not be available or b) costly\nto generate. Furthermore, this cost increases when the goal is to make the LLM\nfollow a specific workflow within a dialogue instead of single instructions.\nInspired by the self-play technique in reinforcement learning and the use of\nLLMs to simulate human agents, we propose a more effective method for data\ncollection through LLMs engaging in a conversation in various roles. This\napproach generates a training data via \"self-talk\" of LLMs that can be refined\nand utilized for supervised fine-tuning. We introduce an automated way to\nmeasure the (partial) success of a dialogue. This metric is used to filter the\ngenerated conversational data that is fed back in LLM for training. Based on\nour automated and human evaluations of conversation quality, we demonstrate\nthat such self-talk data improves results. In addition, we examine the various\ncharacteristics that showcase the quality of generated dialogues and how they\ncan be connected to their potential utility as training data.\n",
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05039",
        "abstract_url": "http://arxiv.org/abs/2401.05039",
        "authors": [
            {
                "last_name": "Hsieh",
                "first_name": "Chou-Ying"
            },
            {
                "last_name": "Chang",
                "first_name": "Chia-Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Po-Hsiu"
            },
            {
                "last_name": "Kuo",
                "first_name": "Sy-Yen"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Maximal Biclique Enumeration (MBE) holds critical importance in graph theory\nwith applications extending across fields such as bioinformatics, social\nnetworks, and recommendation systems. However, its computational complexity\npresents barriers for efficiently scaling to large graphs. To address these\nchallenges, we introduce cuMBE, a GPU-optimized parallel algorithm for MBE.\nUtilizing a unique data structure, called compact array, cuMBE eradicates the\nneed for recursion, thereby significantly minimizing dynamic memory\nrequirements and computational overhead. The algorithm utilizes a hybrid\nparallelism approach, in which GPU thread blocks handle coarse-grained tasks\nassociated with part of the search process. Besides, we implement three\nfine-grained optimizations within each thread block to enhance performance.\nFurther, we integrate a work-stealing mechanism to mitigate workload imbalances\namong thread blocks. Our experiments reveal that cuMBE achieves an geometric\nmean speedup of 4.02x and 4.13x compared to the state-of-the-art serial\nalgorithm and parallel CPU-based algorithm on both common and real-world\ndatasets, respectively.\n",
        "title": "Accelerating Maximal Biclique Enumeration on GPUs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05041",
        "abstract_url": "http://arxiv.org/abs/2401.05041",
        "authors": [
            {
                "last_name": "Iommazzo",
                "first_name": "Gabriele"
            },
            {
                "last_name": "D'Ambrosio",
                "first_name": "Claudia"
            },
            {
                "last_name": "Frangioni",
                "first_name": "Antonio"
            },
            {
                "last_name": "Liberti",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We discuss the issue of finding a good mathematical programming solver\nconfiguration for a particular instance of a given problem, and we propose a\ntwo-phase approach to solve it. In the first phase we learn the relationships\nbetween the instance, the configuration and the performance of the configured\nsolver on the given instance. A specific difficulty of learning a good solver\nconfiguration is that parameter settings may not all be independent; this\nrequires enforcing (hard) constraints, something that many widely used\nsupervised learning methods cannot natively achieve. We tackle this issue in\nthe second phase of our approach, where we use the learnt information to\nconstruct and solve an optimization problem having an explicit representation\nof the dependency/consistency constraints on the configuration parameter\nsettings. We discuss computational results for two different instantiations of\nthis approach on a unit commitment problem arising in the short-term planning\nof hydro valleys. We use logistic regression as the supervised learning\nmethodology and consider CPLEX as the solver of interest.\n",
        "title": "Learning to Configure Mathematical Programming Solvers by Mathematical\n  Programming",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05042",
        "abstract_url": "http://arxiv.org/abs/2401.05042",
        "authors": [
            {
                "last_name": "Raftopoulos",
                "first_name": "Raoul"
            },
            {
                "last_name": "D'Oro",
                "first_name": "Salvatore"
            },
            {
                "last_name": "Melodia",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Schembra",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The Open Radio Access Network (Open RAN) paradigm, and its reference\narchitecture proposed by the O-RAN Alliance, is paving the way toward open,\ninteroperable, observable and truly intelligent cellular networks. Crucial to\nthis evolution is Machine Learning (ML), which will play a pivotal role by\nproviding the necessary tools to realize the vision of self-organizing O-RAN\nsystems. However, to be actionable, ML algorithms need to demonstrate high\nreliability, effectiveness in delivering high performance, and the ability to\nadapt to varying network conditions, traffic demands and performance\nrequirements. To address these challenges, in this paper we propose a novel\nDeep Reinforcement Learning (DRL) agent design for O-RAN applications that can\nlearn control policies under varying Service Level Agreement (SLAs) with\nheterogeneous minimum performance requirements. We focus on the case of RAN\nslicing and SLAs specifying maximum tolerable end-to-end latency levels. We use\nthe OpenRAN Gym open-source environment to train a DRL agent that can adapt to\nvarying SLAs and compare it against the state-of-the-art. We show that our\nagent maintains a low SLA violation rate that is 8.3x and 14.4x lower than\napproaches based on Deep Q- Learning (DQN) and Q-Learning while consuming\nrespectively 0.3x and 0.6x fewer resources without the need for re-training.\n",
        "title": "DRL-based Latency-Aware Network Slicing in O-RAN with Time-Varying SLAs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05043",
        "abstract_url": "http://arxiv.org/abs/2401.05043",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Kaizheng"
            },
            {
                "last_name": "Shariatmadar",
                "first_name": "Keivan"
            },
            {
                "last_name": "Manchingal",
                "first_name": "Shireen Kudukkil"
            },
            {
                "last_name": "Cuzzolin",
                "first_name": "Fabio"
            },
            {
                "last_name": "Moens",
                "first_name": "David"
            },
            {
                "last_name": "Hallez",
                "first_name": "Hans"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Uncertainty estimation is increasingly attractive for improving the\nreliability of neural networks. In this work, we present novel credal-set\ninterval neural networks (CreINNs) designed for classification tasks. CreINNs\npreserve the traditional interval neural network structure, capturing weight\nuncertainty through deterministic intervals, while forecasting credal sets\nusing the mathematical framework of probability intervals. Experimental\nvalidations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN)\nshowcase that CreINNs outperform epistemic uncertainty estimation when compared\nto variational Bayesian neural networks (BNNs) and deep ensembles (DEs).\nFurthermore, CreINNs exhibit a notable reduction in computational complexity\ncompared to variational BNNs and demonstrate smaller model sizes than DEs.\n",
        "title": "CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation\n  in Classification Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05045",
        "abstract_url": "http://arxiv.org/abs/2401.05045",
        "authors": [
            {
                "last_name": "Barletta",
                "first_name": "Luca"
            },
            {
                "last_name": "Dytso",
                "first_name": "Alex"
            },
            {
                "last_name": "Shamai",
                "first_name": "Shlomo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers a discrete-time Poisson noise channel with an input\namplitude constraint $\\mathsf{A}$ and a dark current parameter $\\lambda$. It is\nknown that the capacity-achieving distribution for this channel is discrete\nwith finitely many points. Recently, for $\\lambda=0$, a lower bound of order\n$\\sqrt{\\mathsf{A}}$ and an upper bound of order $\\mathsf{A} \\log^2(\\mathsf{A})$\nhave been demonstrated on the cardinality of the support of the optimal input\ndistribution.\n  In this work, we improve these results in several ways. First, we provide\nupper and lower bounds that hold for non-zero dark current. Second, we produce\na sharper upper bound with a far simpler technique. In particular, for\n$\\lambda=0$, we sharpen the upper bound from the order of $\\mathsf{A}\n\\log^2(\\mathsf{A})$ to the order of $\\mathsf{A}$. Finally, some other\nadditional information about the location of the support is provided.\n",
        "title": "Improved Bounds on the Number of Support Points of the\n  Capacity-Achieving Input for Amplitude Constrained Poisson Channels",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05049",
        "abstract_url": "http://arxiv.org/abs/2401.05049",
        "authors": [
            {
                "last_name": "Vargis",
                "first_name": "Tom Richard"
            },
            {
                "last_name": "Ghiasvand",
                "first_name": "Siavash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This work prioritizes building a modular pipeline that utilizes existing\nmodels to systematically restore images, rather than creating new restoration\nmodels from scratch. Restoration is carried out at an object-specific level,\nwith each object regenerated using its corresponding class label information.\nThe approach stands out by providing complete user control over the entire\nrestoration process. Users can select models for specialized restoration steps,\ncustomize the sequence of steps to meet their needs, and refine the resulting\nregenerated image with depth awareness. The research provides two distinct\npathways for implementing image regeneration, allowing for a comparison of\ntheir respective strengths and limitations. The most compelling aspect of this\nversatile system is its adaptability. This adaptability enables users to target\nparticular object categories, including medical images, by providing models\nthat are trained on those object classes.\n",
        "title": "Content-Aware Depth-Adaptive Image Restoration",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05054",
        "abstract_url": "http://arxiv.org/abs/2401.05054",
        "authors": [
            {
                "last_name": "Jinnai",
                "first_name": "Yuu"
            },
            {
                "last_name": "Honda",
                "first_name": "Ukyo"
            },
            {
                "last_name": "Morimura",
                "first_name": "Tetsuro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peinan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  One of the most important challenges in text generation systems is to produce\noutputs that are not only correct but also diverse. Recently, Minimum\nBayes-Risk (MBR) decoding has gained prominence for generating sentences of the\nhighest quality among the decoding algorithms. However, existing algorithms\nproposed for generating diverse outputs are predominantly based on beam search\nor random sampling, thus their output quality is capped by these underlying\nmethods. In this paper, we investigate an alternative approach -- we develop\ndiversity-promoting decoding algorithms by enforcing diversity objectives to\nMBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and\n$k$-medoids MBR (KMBR), methods to generate a set of sentences with high\nquality and diversity. We evaluate DMBR and KMBR on a variety of directed text\ngeneration tasks using encoder-decoder models and a large language model with\nprompting. The experimental results show that the proposed method achieves a\nbetter trade-off than the diverse beam search and sampling algorithms.\n",
        "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05055",
        "abstract_url": "http://arxiv.org/abs/2401.05055",
        "authors": [
            {
                "last_name": "Xiang",
                "first_name": "Yawen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Heng"
            },
            {
                "last_name": "Li",
                "first_name": "Chengyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Fangwei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongbo"
            },
            {
                "last_name": "Xie",
                "first_name": "Yongqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Motion deblurring is one of the fundamental problems of computer vision and\nhas received continuous attention. The variability in blur, both within and\nacross images, imposes limitations on non-blind deblurring techniques that rely\non estimating the blur kernel. As a response, blind motion deblurring has\nemerged, aiming to restore clear and detailed images without prior knowledge of\nthe blur type, fueled by the advancements in deep learning methodologies.\nDespite strides in this field, a comprehensive synthesis of recent progress in\ndeep learning-based blind motion deblurring is notably absent. This paper fills\nthat gap by providing an exhaustive overview of the role of deep learning in\nblind motion deblurring, encompassing datasets, evaluation metrics, and methods\ndeveloped over the last six years. Specifically, we first introduce the types\nof motion blur and the fundamental principles of deblurring. Next, we outline\nthe shortcomings of traditional non-blind deblurring algorithms, emphasizing\nthe advantages of employing deep learning techniques for deblurring tasks.\nFollowing this, we categorize and summarize existing blind motion deblurring\nmethods based on different backbone networks, including convolutional neural\nnetworks, generative adversarial networks, recurrent neural networks, and\nTransformer networks. Subsequently, we elaborate not only on the fundamental\nprinciples of these different categories but also provide a comprehensive\nsummary and comparison of their advantages and limitations. Qualitative and\nquantitative experimental results conducted on four widely used datasets\nfurther compare the performance of SOTA methods. Finally, an analysis of\npresent challenges and future pathways. All collected models, benchmark\ndatasets, source code links, and codes for evaluation have been made publicly\navailable at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey\n",
        "title": "Application of Deep Learning in Blind Motion Deblurring: Current Status\n  and Future Prospects",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05057",
        "abstract_url": "http://arxiv.org/abs/2401.05057",
        "authors": [
            {
                "last_name": "Oelerich",
                "first_name": "Thies"
            },
            {
                "last_name": "Beck",
                "first_name": "Florian"
            },
            {
                "last_name": "Hartl-Nesic",
                "first_name": "Christian"
            },
            {
                "last_name": "Kugi",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This work presents a novel online model-predictive trajectory planner for\nrobotic manipulators called BoundMPC. This planner allows the collision-free\nfollowing of Cartesian reference paths in the end-effector's position and\norientation, including via-points, within desired asymmetric bounds of the\northogonal path error. The path parameter synchronizes the position and\norientation reference paths. The decomposition of the path error into the\ntangential direction, describing the path progress, and the orthogonal\ndirection, which represents the deviation from the path, is well known for the\nposition from the path-following control in the literature. This paper extends\nthis idea to the orientation by utilizing the Lie theory of rotations.\nMoreover, the orthogonal error plane is further decomposed into basis\ndirections to define asymmetric Cartesian error bounds easily. Using piecewise\nlinear position and orientation reference paths with via-points is\ncomputationally very efficient and allows replanning the pose trajectories\nduring the robot's motion. This feature makes it possible to use this planner\nfor dynamically changing environments and varying goals. The flexibility and\nperformance of BoundMPC are experimentally demonstrated by two scenarios on a\n7-DoF Kuka LBR iiwa 14 R820 robot. The first scenario shows the transfer of a\nlarger object from a start to a goal pose through a confined space where the\nobject must be tilted. The second scenario deals with grasping an object from a\ntable where the grasping point changes during the robot's motion, and\ncollisions with other obstacles in the scene must be avoided.\n",
        "title": "BoundMPC: Cartesian Trajectory Planning with Error Bounds based on Model\n  Predictive Control in the Joint Space",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05060",
        "abstract_url": "http://arxiv.org/abs/2401.05060",
        "authors": [
            {
                "last_name": "Costa-juss\u00e0",
                "first_name": "Marta R."
            },
            {
                "last_name": "Meglioli",
                "first_name": "Mariano Coria"
            },
            {
                "last_name": "Andrews",
                "first_name": "Pierre"
            },
            {
                "last_name": "Dale",
                "first_name": "David"
            },
            {
                "last_name": "Hansanti",
                "first_name": "Prangthip"
            },
            {
                "last_name": "Kalbassi",
                "first_name": "Elahe"
            },
            {
                "last_name": "Mourachko",
                "first_name": "Alex"
            },
            {
                "last_name": "Ropers",
                "first_name": "Christophe"
            },
            {
                "last_name": "Wood",
                "first_name": "Carleigh"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "",
            ""
        ],
        "abstract": "  Research in toxicity detection in natural language processing for the speech\nmodality (audio-based) is quite limited, particularly for languages other than\nEnglish. To address these limitations and lay the groundwork for truly\nmultilingual audio-based toxicity detection, we introduce MuTox, the first\nhighly multilingual audio-based dataset with toxicity labels. The dataset\ncomprises 20,000 audio utterances for English and Spanish, and 4,000 for the\nother 19 languages. To demonstrate the quality of this dataset, we trained the\nMuTox audio-based toxicity classifier, which enables zero-shot toxicity\ndetection across a wide range of languages. This classifier outperforms\nexisting text-based trainable classifiers by more than 1% AUC, while expanding\nthe language coverage more than tenfold. When compared to a wordlist-based\nclassifier that covers a similar number of languages, MuTox improves precision\nand recall by approximately 2.5 times. This significant improvement underscores\nthe potential of MuTox in advancing the field of audio-based toxicity\ndetection.\n",
        "title": "MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot\n  Detector",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05064",
        "abstract_url": "http://arxiv.org/abs/2401.05064",
        "authors": [
            {
                "last_name": "Torres",
                "first_name": "Bernardo"
            },
            {
                "last_name": "Lattner",
                "first_name": "Stefan"
            },
            {
                "last_name": "Richard",
                "first_name": "Ga\u00ebl"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            ""
        ],
        "abstract": "  Significant strides have been made in creating voice identity representations\nusing speech data. However, the same level of progress has not been achieved\nfor singing voices. To bridge this gap, we suggest a framework for training\nsinger identity encoders to extract representations suitable for various\nsinging-related tasks, such as singing voice similarity and synthesis. We\nexplore different self-supervised learning techniques on a large collection of\nisolated vocal tracks and apply data augmentations during training to ensure\nthat the representations are invariant to pitch and content variations. We\nevaluate the quality of the resulting representations on singer similarity and\nidentification tasks across multiple datasets, with a particular emphasis on\nout-of-domain generalization. Our proposed framework produces high-quality\nembeddings that outperform both speaker verification and wav2vec 2.0\npre-trained baselines on singing voice while operating at 44.1 kHz. We release\nour code and trained models to facilitate further research on singing voice and\nrelated areas.\n",
        "title": "Singer Identity Representation Learning using Self-Supervised Techniques",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05069",
        "abstract_url": "http://arxiv.org/abs/2401.05069",
        "authors": [
            {
                "last_name": "Grzeszczyk",
                "first_name": "Michal K."
            },
            {
                "last_name": "Trzci\u0144ski",
                "first_name": "Tomasz"
            },
            {
                "last_name": "Sitek",
                "first_name": "Arkadiusz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we present a novel, machine-learning approach for constructing\nMulticlass Interpretable Scoring Systems (MISS) - a fully data-driven\nmethodology for generating single, sparse, and user-friendly scoring systems\nfor multiclass classification problems. Scoring systems are commonly utilized\nas decision support models in healthcare, criminal justice, and other domains\nwhere interpretability of predictions and ease of use are crucial. Prior\nmethods for data-driven scoring, such as SLIM (Supersparse Linear Integer\nModel), were limited to binary classification tasks and extensions to\nmulticlass domains were primarily accomplished via one-versus-all-type\ntechniques. The scores produced by our method can be easily transformed into\nclass probabilities via the softmax function. We demonstrate techniques for\ndimensionality reduction and heuristics that enhance the training efficiency\nand decrease the optimality gap, a measure that can certify the optimality of\nthe model. Our approach has been extensively evaluated on datasets from various\ndomains, and the results indicate that it is competitive with other machine\nlearning models in terms of classification performance metrics and provides\nwell-calibrated class probabilities.\n",
        "title": "MISS: Multiclass Interpretable Scoring Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05072",
        "abstract_url": "http://arxiv.org/abs/2401.05072",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yichong"
            },
            {
                "last_name": "Feng",
                "first_name": "Xiaocheng"
            },
            {
                "last_name": "Li",
                "first_name": "Baohang"
            },
            {
                "last_name": "Fu",
                "first_name": "Chengpeng"
            },
            {
                "last_name": "Huo",
                "first_name": "Wenshuai"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            },
            {
                "last_name": "Qin",
                "first_name": "Bing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Although large language models (LLMs) have shown surprising language\nunderstanding and generation capabilities, they have yet to gain a\nrevolutionary advancement in the field of machine translation. One potential\ncause of the limited performance is the misalignment between the\ntranslation-specific understanding and general understanding inside LLMs. To\nalign the translation-specific understanding to the general one, we propose a\nnovel translation process xIoD (Cross-Lingual Interpretation of Difficult\nwords), explicitly incorporating the general understanding on the content\nincurring inconsistent understanding to guide the translation. Specifically,\nxIoD performs the cross-lingual interpretation for the difficult-to-translate\nwords and enhances the translation with the generated interpretations.\nFurthermore, we reframe the external tools of QE to tackle the challenges of\nxIoD in the detection of difficult words and the generation of helpful\ninterpretations. We conduct experiments on the self-constructed benchmark\nChallengeMT, which includes cases in which multiple SOTA translation systems\nconsistently underperform. Experimental results show the effectiveness of our\nxIoD, which improves up to +3.85 COMET.\n",
        "title": "Aligning Translation-Specific Understanding to General Understanding in\n  Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05073",
        "abstract_url": "http://arxiv.org/abs/2401.05073",
        "authors": [
            {
                "last_name": "Leon",
                "first_name": "Florin"
            },
            {
                "last_name": "Gavrilescu",
                "first_name": "Marius"
            },
            {
                "last_name": "Floria",
                "first_name": "Sabina-Adriana"
            },
            {
                "last_name": "Minea",
                "first_name": "Alina-Adriana"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  This paper proposes a classification framework aimed at identifying\ncorrelations between job ad requirements and transversal skill sets, with a\nfocus on predicting the necessary skills for individual job descriptions using\na deep learning model. The approach involves data collection, preprocessing,\nand labeling using ESCO (European Skills, Competences, and Occupations)\ntaxonomy. Hierarchical classification and multi-label strategies are used for\nskill identification, while augmentation techniques address data imbalance,\nenhancing model robustness. A comparison between results obtained with\nEnglish-specific and multi-language sentence embedding models reveals close\naccuracy. The experimental case studies detail neural network configurations,\nhyperparameters, and cross-validation results, highlighting the efficacy of the\nhierarchical approach and the suitability of the multi-language model for the\ndiverse European job market. Thus, a new approach is proposed for the\nhierarchical classification of transversal skills from job ads.\n",
        "title": "Hierarchical Classification of Transversal Skills in Job Ads Based on\n  Sentence Embeddings",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05074",
        "abstract_url": "http://arxiv.org/abs/2401.05074",
        "authors": [
            {
                "last_name": "Wietzke",
                "first_name": "Thore"
            },
            {
                "last_name": "Gall",
                "first_name": "Jan"
            },
            {
                "last_name": "Graichen",
                "first_name": "Knut"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper presents a new approach to predict the occupancy for building\nenergy systems (BES). A Gaussian Process (GP) is used to model the occupancy\nand is represented as a state space model that is equivalent to the full GP if\nKalman filtering and smoothing is used. The combination of GPs and mechanistic\nmodels is called Latent Force Model (LFM). An LFM-based model predictive\ncontrol (MPC) concept for BES is presented that benefits from the extrapolation\ncapability of mechanistic models and the learning ability of GPs to predict the\noccupancy within the building. Simulations with EnergyPlus and a comparison\nwith real-world data from the Bosch Research Campus in Renningen show that a\nreduced energy demand and thermal discomfort can be obtained with the LFM-based\nMPC scheme by accounting for the predicted stochastic occupancy.\n",
        "title": "Occupancy Prediction for Building Energy Systems with Latent Force\n  Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05083",
        "abstract_url": "http://arxiv.org/abs/2401.05083",
        "authors": [
            {
                "last_name": "Onuoha",
                "first_name": "Okechi"
            },
            {
                "last_name": "Kurawa",
                "first_name": "Suleiman"
            },
            {
                "last_name": "Tang",
                "first_name": "Zezhi"
            },
            {
                "last_name": "Dong",
                "first_name": "Yi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA",
            ""
        ],
        "abstract": "  This paper considers the distributed leader-follower stress-matrix-based\naffine formation control problem of discrete-time linear multi-agent systems\nwith static and dynamic leaders. In leader-follower multi-agent formation\ncontrol, the aim is to drive a set of agents comprising leaders and followers\nto form any desired geometric pattern and simultaneously execute any required\nmanoeuvre by controlling only a few agents denoted as leaders. Existing works\nin literature are mostly limited to the cases where the agents' inter-agent\ncommunications are either in the continuous-time settings or the sampled-data\ncases where the leaders are constrained to constant (or zero) velocities or\naccelerations. Here, we relax these constraints and study the discrete-time\ncases where the leaders can have stationary or time-varying velocities. We\npropose control laws in the study of different situations and provide some\nsufficient conditions to guarantee the overall system stability. Simulation\nstudy is used to demonstrate the efficacy of our proposed control laws.\n",
        "title": "Discrete-Time Stress Matrix-Based Formation Control of General Linear\n  Multi-Agent Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05085",
        "abstract_url": "http://arxiv.org/abs/2401.05085",
        "authors": [
            {
                "last_name": "Aute",
                "first_name": "Shubhada"
            },
            {
                "last_name": "Panolan",
                "first_name": "Fahad"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Minimum sum vertex cover of an $n$-vertex graph $G$ is a bijection $\\phi :\nV(G) \\to [n]$ that minimizes the cost $\\sum_{\\{u,v\\} \\in E(G)} \\min \\{\\phi(u),\n\\phi(v) \\}$. Finding a minimum sum vertex cover of a graph (the MSVC problem)\nis NP-hard. MSVC is studied well in the realm of approximation algorithms. The\nbest-known approximation factor in polynomial time for the problem is $16/9$\n[Bansal, Batra, Farhadi, and Tetali, SODA 2021]. Recently, Stankovic\n[APPROX/RANDOM 2022] proved that achieving an approximation ratio better than\n$1.014$ for MSVC is NP-hard, assuming the Unique Games Conjecture. We study the\nMSVC problem from the perspective of parameterized algorithms. The parameters\nwe consider are the size of a minimum vertex cover and the size of a minimum\nclique modulator of the input graph. We obtain the following results.\n  1. MSVC can be solved in $2^{2^{O(k)}} n^{O(1)}$ time, where $k$ is the size\nof a minimum vertex cover.\n  2. MSVC can be solved in $f(k)\\cdot n^{O(1)}$ time for some computable\nfunction $f$, where $k$ is the size of a minimum clique modulator.\n",
        "title": "Parameterized Algorithms for Minimum Sum Vertex Cover",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05093",
        "abstract_url": "http://arxiv.org/abs/2401.05093",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Lei",
                "first_name": "Jie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With recent advancements in aerospace technology, the volume of unlabeled\nremote sensing image (RSI) data has increased dramatically. Effectively\nleveraging this data through self-supervised learning (SSL) is vital in the\nfield of remote sensing. However, current methodologies, particularly\ncontrastive learning (CL), a leading SSL method, encounter specific challenges\nin this domain. Firstly, CL often mistakenly identifies geographically adjacent\nsamples with similar semantic content as negative pairs, leading to confusion\nduring model training. Secondly, as an instance-level discriminative task, it\ntends to neglect the essential fine-grained features and complex details\ninherent in unstructured RSIs. To overcome these obstacles, we introduce\nSwiMDiff, a novel self-supervised pre-training framework designed for RSIs.\nSwiMDiff employs a scene-wide matching approach that effectively recalibrates\nlabels to recognize data from the same scene as false negatives. This\nadjustment makes CL more applicable to the nuances of remote sensing.\nAdditionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through\nthe implementation of pixel-level diffusion constraints, we enhance the\nencoder's ability to capture both the global semantic information and the\nfine-grained features of the images more comprehensively. Our proposed\nframework significantly enriches the information available for downstream tasks\nin remote sensing. Demonstrating exceptional performance in change detection\nand land-cover classification tasks, SwiMDiff proves its substantial utility\nand value in the field of remote sensing.\n",
        "title": "SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion\n  Constraint for Remote Sensing Image",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05097",
        "abstract_url": "http://arxiv.org/abs/2401.05097",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Junhoo"
            },
            {
                "last_name": "Kim",
                "first_name": "Yearim"
            },
            {
                "last_name": "Lee",
                "first_name": "Hyunho"
            },
            {
                "last_name": "Kwak",
                "first_name": "Nojun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Although meta-learning seems promising performance in the realm of rapid\nadaptability, it is constrained by fixed cardinality. When faced with tasks of\nvarying cardinalities that were unseen during training, the model lacks its\nability. In this paper, we address and resolve this challenge by harnessing\n`label equivalence' emerged from stochastic numeric label assignments during\nepisodic task sampling. Questioning what defines ``true\" meta-learning, we\nintroduce the ``any-way\" learning paradigm, an innovative model training\napproach that liberates model from fixed cardinality constraints. Surprisingly,\nthis model not only matches but often outperforms traditional fixed-way models\nin terms of performance, convergence speed, and stability. This disrupts\nestablished notions about domain generalization. Furthermore, we argue that the\ninherent label equivalence naturally lacks semantic information. To bridge this\nsemantic information gap arising from label equivalence, we further propose a\nmechanism for infusing semantic class information into the model. This would\nenhance the model's comprehension and functionality. Experiments conducted on\nrenowned architectures like MAML and ProtoNet affirm the effectiveness of our\nmethod.\n",
        "title": "Any-Way Meta Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05098",
        "abstract_url": "http://arxiv.org/abs/2401.05098",
        "authors": [
            {
                "last_name": "Agouzal",
                "first_name": "Eki"
            },
            {
                "last_name": "Argaud",
                "first_name": "Jean-Philippe"
            },
            {
                "last_name": "Bergmann",
                "first_name": "Michel"
            },
            {
                "last_name": "Fert\u00e9",
                "first_name": "Guilhem"
            },
            {
                "last_name": "Michel-Ponnelle",
                "first_name": "Sylvie"
            },
            {
                "last_name": "Taddei",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We propose a projection-based model order reduction procedure for the ageing\nof large prestressed concrete structures. Our work is motivated by applications\nin the nuclear industry, particularly in the simulation of containment\nbuildings. Such numerical simulations involve a multi-modeling approach: a\nthree-dimensional nonlinear thermo-hydro-visco-elastic rheological model is\nused for concrete; and prestressing cables are described by a one-dimensional\nlinear thermo-elastic behavior. A kinematic linkage is performed in order to\nconnect the concrete nodes and the steel nodes: coincident points in each\nmaterial are assumed to have the same displacement. We develop an adaptive\nalgorithm based on a Proper Orthogonal Decomposition (POD) in time and greedy\nin parameter to build a reduced order model (ROM). The nonlinearity of the\noperator entails that the computational cost of the ROM assembly scales with\nthe size of the high-fidelity model. We develop an hyper-reduction strategy\nbased on empirical quadrature to bypass this computational bottleneck: our\napproach relies on the construction of a reduced mesh to speed up online\nassembly costs of the ROM. We provide numerical results for a standard section\nof a double-walled containment building using a qualified and broadly-used\nindustrial grade finite element solver for structural mechanics\n(code$\\_$aster).\n",
        "title": "Projection-based model order reduction for prestressed concrete with an\n  application to the standard section of a nuclear containment building",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05100",
        "abstract_url": "http://arxiv.org/abs/2401.05100",
        "authors": [
            {
                "last_name": "Moriyasu",
                "first_name": "Ryuta"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Sho"
            },
            {
                "last_name": "Kashima",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Model Predictive Control (MPC) is a versatile approach capable of\naccommodating diverse control requirements, holding significant promise for a\nbroad spectrum of industrial applications. Noteworthy challenges associated\nwith MPC include the substantial computational burden and the inherent\ndifficulty in ensuring system stability. Recently, a rapid computation\ntechnique has been introduced as a potential solution. This method guides the\ninput toward convergence with the optimal control problem solution by employing\nthe primal-dual gradient (PDG) dynamics as a controller. Furthermore, stability\nassurances grounded in dissipativity theory have been established. However,\nthese assurances are applicable solely to continuous-time feedback systems. As\na consequence, when the controller undergoes discretization and is implemented\nas a sampled-data system, stability cannot be guaranteed. In this paper, we\npropose a discrete-time dynamical controller, incorporating specific\nmodifications to the PDG approach, and present stability conditions relevant to\nthe resulting sampled-data system. Additionally, we introduce an extension\ndesigned to enhance control performance. Numerical examples substantiate that\nour proposed method not only enhances control effectiveness but also\neffectively discerns stability degradation resulting from discretization, a\nnuance often overlooked by conventional methods.\n",
        "title": "Sampled-Data Primal-Dual Gradient Dynamics in Model Predictive Control",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05102",
        "abstract_url": "http://arxiv.org/abs/2401.05102",
        "authors": [
            {
                "last_name": "Huleihel",
                "first_name": "Bashar"
            },
            {
                "last_name": "Sabag",
                "first_name": "Oron"
            },
            {
                "last_name": "Aharoni",
                "first_name": "Ziv"
            },
            {
                "last_name": "Permuter",
                "first_name": "Haim H."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper investigates the capacity of finite-state channels (FSCs) with\nfeedback. We derive an upper bound on the feedback capacity of FSCs by\nextending the duality upper bound method from mutual information to the case of\ndirected information. The upper bound is expressed as a multi-letter expression\nthat depends on a test distribution on the sequence of channel outputs. For any\nFSC, we show that if the test distribution is structured on a $Q$-graph, the\nupper bound can be formulated as a Markov decision process (MDP) whose state\nbeing a belief on the channel state. In the case of FSCs and states that are\neither unifilar or have a finite memory, the MDP state simplifies to take\nvalues in a finite set. Consequently, the MDP consists of a finite number of\nstates, actions, and disturbances. This finite nature of the MDP is of\nsignificant importance, as it ensures that dynamic programming algorithms can\nsolve the associated Bellman equation to establish analytical upper bounds,\neven for channels with large alphabets. We demonstrate the simplicity of\ncomputing bounds by establishing the capacity of a broad family of Noisy Output\nis the State (NOST) channels as a simple closed-form analytical expression.\nFurthermore, we introduce novel, nearly optimal analytical upper bounds on the\ncapacity of the Noisy Ising channel.\n",
        "title": "The Duality Upper Bound for Finite-State Channels with Feedback",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05111",
        "abstract_url": "http://arxiv.org/abs/2401.05111",
        "authors": [
            {
                "last_name": "Fujita",
                "first_name": "Kenichi"
            },
            {
                "last_name": "Sato",
                "first_name": "Hiroshi"
            },
            {
                "last_name": "Ashihara",
                "first_name": "Takanori"
            },
            {
                "last_name": "Kanagawa",
                "first_name": "Hiroki"
            },
            {
                "last_name": "Delcroix",
                "first_name": "Marc"
            },
            {
                "last_name": "Moriya",
                "first_name": "Takafumi"
            },
            {
                "last_name": "Ijima",
                "first_name": "Yusuke"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "LG",
            ""
        ],
        "abstract": "  The zero-shot text-to-speech (TTS) method, based on speaker embeddings\nextracted from reference speech using self-supervised learning (SSL) speech\nrepresentations, can reproduce speaker characteristics very accurately.\nHowever, this approach suffers from degradation in speech synthesis quality\nwhen the reference speech contains noise. In this paper, we propose a\nnoise-robust zero-shot TTS method. We incorporated adapters into the SSL model,\nwhich we fine-tuned with the TTS model using noisy reference speech. In\naddition, to further improve performance, we adopted a speech enhancement (SE)\nfront-end. With these improvements, our proposed SSL-based zero-shot TTS\nachieved high-quality speech synthesis with noisy reference speech. Through the\nobjective and subjective evaluations, we confirmed that the proposed method is\nhighly robust to noise in reference speech, and effectively works in\ncombination with SE.\n",
        "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on\n  self-supervised speech-representation model with adapters",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05112",
        "abstract_url": "http://arxiv.org/abs/2401.05112",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shuxin"
            },
            {
                "last_name": "Rigger",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Extensible Markup Language (XML) is a widely used file format for data\nstorage and transmission. Many XML processors support XPath, a query language\nthat enables the extraction of elements from XML documents. These systems can\nbe affected by logic bugs, which are bugs that cause the processor to return\nincorrect results. In order to tackle such bugs, we propose a new approach,\nwhich we realized as a system called XPress. As a test oracle, XPress relies on\ndifferential testing, which compares the results of multiple systems on the\nsame test input, and identifies bugs through discrepancies in their outputs. As\ntest inputs, XPress generates both XML documents and XPath queries. Aiming to\ngenerate meaningful queries that compute non-empty results, XPress selects a\nso-called targeted node to guide the XPath expression generation process. Using\nthe targeted node, XPress generates XPath expressions that reference existing\ncontext related to the targeted node, such as its tag name and attributes,\nwhile also guaranteeing that a predicate evaluates to true before further\nexpanding the query. We tested our approach on six mature XML processors,\nBaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system.\nIn total, we have found 20 unique bugs in these systems, of which 25 have been\nverified by the developers, and 12 of which have been fixed. XPress is\nefficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fast\nas naive random generation. We expect that the effectiveness and simplicity of\nour approach will help to improve the robustness of many XML processors.\n",
        "title": "Finding XPath Bugs in XML Document Processors via Differential Testing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05115",
        "abstract_url": "http://arxiv.org/abs/2401.05115",
        "authors": [
            {
                "last_name": "Tsiakas",
                "first_name": "Kostas"
            },
            {
                "last_name": "Murray-Rust",
                "first_name": "Dave"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  This paper aims to develop a semi-formal design space for Human-AI\ninteractions, by building a set of interaction primitives which specify the\ncommunication between users and AI systems during their interaction. We show\nhow these primitives can be combined into a set of interaction patterns which\ncan provide an abstract specification for exchanging messages between humans\nand AI/ML models to carry out purposeful interactions. The motivation behind\nthis is twofold: firstly, to provide a compact generalisation of existing\npractices, that highlights the similarities and differences between systems in\nterms of their interaction behaviours; and secondly, to support the creation of\nnew systems, in particular by opening the space of possibilities for\ninteractions with models. We present a short literature review on frameworks,\nguidelines and taxonomies related to the design and implementation of HAI\ninteractions, including human-in-the-loop, explainable AI, as well as hybrid\nintelligence and collaborative learning approaches. From the literature review,\nwe define a vocabulary for describing information exchanges in terms of\nproviding and requesting particular model-specific data types. Based on this\nvocabulary, a message passing model for interactions between humans and models\nis presented, which we demonstrate can account for existing systems and\napproaches. Finally, we build this into design patterns as mid-level constructs\nthat capture common interactional structures. We discuss how this approach can\nbe used towards a design space for Human-AI interactions that creates new\npossibilities for designs as well as keeping track of implementation issues and\nconcerns.\n",
        "title": "Unpacking Human-AI interactions: From interaction primitives to a design\n  space",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05121",
        "abstract_url": "http://arxiv.org/abs/2401.05121",
        "authors": [
            {
                "last_name": "Fayza",
                "first_name": "Farbin"
            },
            {
                "last_name": "Rao",
                "first_name": "Satyavolu Papa"
            },
            {
                "last_name": "Bunandar",
                "first_name": "Darius"
            },
            {
                "last_name": "Gupta",
                "first_name": "Udit"
            },
            {
                "last_name": "Joshi",
                "first_name": "Ajay"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG"
        ],
        "abstract": "  Photonic integrated circuits are finding use in a variety of applications\nincluding optical transceivers, LIDAR, bio-sensing, photonic quantum computing,\nand Machine Learning (ML). In particular, with the exponentially increasing\nsizes of ML models, photonics-based accelerators are getting special attention\nas a sustainable solution because they can perform ML inferences with multiple\norders of magnitude higher energy efficiency than CMOS-based accelerators.\nHowever, recent studies have shown that hardware manufacturing and\ninfrastructure contribute significantly to the carbon footprint of computing\ndevices, even surpassing the emissions generated during their use. For example,\nthe manufacturing process accounts for 74% of the total carbon emissions from\nApple in 2019. This prompts us to ask -- if we consider both the embodied\n(manufacturing) and operational carbon cost of photonics, is it indeed a viable\navenue for a sustainable future? So, in this paper, we build a carbon footprint\nmodel for photonic chips and investigate the sustainability of photonics-based\naccelerators by conducting a case study on ADEPT, a photonics-based accelerator\nfor deep neural network inference. Our analysis shows that photonics can reduce\nboth operational and embodied carbon footprints with its high energy efficiency\nand at least 4$\\times$ less fabrication carbon cost per unit area than 28 nm\nCMOS.\n",
        "title": "Photonics for Sustainable Computing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05125",
        "abstract_url": "http://arxiv.org/abs/2401.05125",
        "authors": [
            {
                "last_name": "Garda",
                "first_name": "Samuele"
            },
            {
                "last_name": "Leser",
                "first_name": "Ulf"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Biomedical entity linking (BEL) is the task of grounding entity mentions to a\nknowledge base (KB). A popular approach to the task are name-based methods,\ni.e. those identifying the most appropriate name in the KB for a given mention,\neither via dense retrieval or autoregressive modeling. However, as these\nmethods directly return KB names, they cannot cope with homonyms, i.e.\ndifferent KB entities sharing the exact same name. This significantly affects\ntheir performance, especially for KBs where homonyms account for a large amount\nof entity mentions (e.g. UMLS and NCBI Gene). We therefore present BELHD\n(Biomedical Entity Linking with Homonym Disambiguation), a new name-based\nmethod that copes with this challenge. Specifically, BELHD builds upon the\nBioSyn (Sung et al.,2020) model introducing two crucial extensions. First, it\nperforms a preprocessing of the KB in which it expands homonyms with an\nautomatically chosen disambiguating string, thus enforcing unique linking\ndecisions. Second, we introduce candidate sharing, a novel strategy to select\ncandidates for contrastive learning that enhances the overall training signal.\nExperiments with 10 corpora and five entity types show that BELHD improves upon\nstate-of-the-art approaches, achieving the best results in 6 out 10 corpora\nwith an average improvement of 4.55pp recall@1. Furthermore, the KB\npreprocessing is orthogonal to the core prediction model and thus can also\nimprove other methods, which we exemplify for GenBioEL (Yuan et al, 2022), a\ngenerative name-based BEL approach. Code is available at: link added upon\npublication.\n",
        "title": "BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05126",
        "abstract_url": "http://arxiv.org/abs/2401.05126",
        "authors": [
            {
                "last_name": "Nagamori",
                "first_name": "Teru"
            },
            {
                "last_name": "Shiota",
                "first_name": "Sayaka"
            },
            {
                "last_name": "Kiya",
                "first_name": "Hitoshi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We propose a novel method for privacy-preserving deep neural networks (DNNs)\nwith the Vision Transformer (ViT). The method allows us not only to train\nmodels and test with visually protected images but to also avoid the\nperformance degradation caused from the use of encrypted images, whereas\nconventional methods cannot avoid the influence of image encryption. A domain\nadaptation method is used to efficiently fine-tune ViT with encrypted images.\nIn experiments, the method is demonstrated to outperform conventional methods\nin an image classification task on the CIFAR-10 and ImageNet datasets in terms\nof classification accuracy.\n",
        "title": "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving\n  Vision Transformer",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05131",
        "abstract_url": "http://arxiv.org/abs/2401.05131",
        "authors": [
            {
                "last_name": "Pichon-Pharabod",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SC",
            ""
        ],
        "abstract": "  We provide an algorithm for computing an effective basis of homology of\nelliptic surfaces over the complex projective line on which integration of\nperiods can be carried out. This allows the heuristic recovery of several\nalgebraic invariants of the surface, notably the N\\'eron-Severi lattice, the\ntranscendental lattice, the Mordell-Weil group and the Mordell-Weil lattice.\nThis algorithm comes with a SageMath implementation.\n",
        "title": "A semi-numerical algorithm for the homology lattice and periods of\n  complex elliptic surfaces over the projective line",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05133",
        "abstract_url": "http://arxiv.org/abs/2401.05133",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Siqi"
            },
            {
                "last_name": "Marris",
                "first_name": "Luke"
            },
            {
                "last_name": "Lanctot",
                "first_name": "Marc"
            },
            {
                "last_name": "Piliouras",
                "first_name": "Georgios"
            },
            {
                "last_name": "Leibo",
                "first_name": "Joel Z."
            },
            {
                "last_name": "Heess",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA"
        ],
        "abstract": "  We study computationally efficient methods for finding equilibria in n-player\ngeneral-sum games, specifically ones that afford complex visuomotor skills. We\nshow how existing methods would struggle in this setting, either\ncomputationally or in theory. We then introduce NeuPL-JPSRO, a neural\npopulation learning algorithm that benefits from transfer learning of skills\nand converges to a Coarse Correlated Equilibrium (CCE) of the game. We show\nempirical convergence in a suite of OpenSpiel games, validated rigorously by\nexact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our\napproach enables adaptive coordination in a MuJoCo control domain and skill\ntransfer in capture-the-flag. Our work shows that equilibrium convergent\npopulation learning can be implemented at scale and in generality, paving the\nway towards solving real-world games between heterogeneous players with mixed\nmotives.\n",
        "title": "Neural Population Learning beyond Symmetric Zero-sum Games",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05134",
        "abstract_url": "http://arxiv.org/abs/2401.05134",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Abhisek"
            },
            {
                "last_name": "Bera",
                "first_name": "Shreyangshu"
            },
            {
                "last_name": "Saha",
                "first_name": "Sriparna"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Pushpak"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Samrat"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Over the past few years, the use of the Internet for healthcare-related tasks\nhas grown by leaps and bounds, posing a challenge in effectively managing and\nprocessing information to ensure its efficient utilization. During moments of\nemotional turmoil and psychological challenges, we frequently turn to the\ninternet as our initial source of support, choosing this over discussing our\nfeelings with others due to the associated social stigma. In this paper, we\npropose a new task of multi-modal medical concern summary (MMCS) generation,\nwhich provides a short and precise summary of patients' major concerns brought\nup during the consultation. Nonverbal cues, such as patients' gestures and\nfacial expressions, aid in accurately identifying patients' concerns. Doctors\nalso consider patients' personal information, such as age and gender, in order\nto describe the medical condition appropriately. Motivated by the potential\nefficacy of patients' personal context and visual gestures, we propose a\ntransformer-based multi-task, multi-modal intent-recognition, and medical\nconcern summary generation (IR-MMCSG) system. Furthermore, we propose a\nmultitasking framework for intent recognition and medical concern summary\ngeneration for doctor-patient consultations. We construct the first multi-modal\nmedical concern summary generation (MM-MediConSummation) corpus, which includes\npatient-doctor consultations annotated with medical concern summaries, intents,\npatient personal information, doctor's recommendations, and keywords. Our\nexperiments and analysis demonstrate (a) the significant role of patients'\nexpressions/gestures and their personal information in intent identification\nand medical concern summary generation, and (b) the strong correlation between\nintent recognition and patients' medical concern summary generation\n  The dataset and source code are available at https://github.com/NLP-RL/MMCSG.\n",
        "title": "Yes, this is what I was looking for! Towards Multi-modal Medical\n  Consultation Concern Summary Generation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05136",
        "abstract_url": "http://arxiv.org/abs/2401.05136",
        "authors": [
            {
                "last_name": "Tufano",
                "first_name": "Rosalia"
            },
            {
                "last_name": "Dabi\u0107",
                "first_name": "Ozren"
            },
            {
                "last_name": "Mastropaolo",
                "first_name": "Antonio"
            },
            {
                "last_name": "Ciniselli",
                "first_name": "Matteo"
            },
            {
                "last_name": "Bavota",
                "first_name": "Gabriele"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The automation of code review has been tackled by several researchers with\nthe goal of reducing its cost. The adoption of deep learning in software\nengineering pushed the automation to new boundaries, with techniques imitating\ndevelopers in generative tasks, such as commenting on a code change as a\nreviewer would do or addressing a reviewer's comment by modifying code. The\nperformance of these techniques is usually assessed through quantitative\nmetrics, e.g., the percentage of instances in the test set for which correct\npredictions are generated, leaving many open questions on the techniques'\ncapabilities. For example, knowing that an approach is able to correctly\naddress a reviewer's comment in 10% of cases is of little value without knowing\nwhat was asked by the reviewer: What if in all successful cases the code change\nrequired to address the comment was just the removal of an empty line? In this\npaper we aim at characterizing the cases in which three code review automation\ntechniques tend to succeed or fail in the two above-described tasks. The study\nhas a strong qualitative focus, with ~105 man-hours of manual inspection\ninvested in manually analyzing correct and wrong predictions generated by the\nthree techniques, for a total of 2,291 inspected predictions. The output of\nthis analysis are two taxonomies reporting, for each of the two tasks, the\ntypes of code changes on which the experimented techniques tend to succeed or\nto fail, pointing to areas for future work. A result of our manual analysis was\nalso the identification of several issues in the datasets used to train and\ntest the experimented techniques. Finally, we assess the importance of\nresearching in techniques specialized for code review automation by comparing\ntheir performance with ChatGPT, a general purpose large language model, finding\nthat ChatGPT struggles in commenting code as a human reviewer would do.\n",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05137",
        "abstract_url": "http://arxiv.org/abs/2401.05137",
        "authors": [
            {
                "last_name": "Daho",
                "first_name": "Mostafa El Habib"
            },
            {
                "last_name": "Li",
                "first_name": "Yihao"
            },
            {
                "last_name": "Zeghlache",
                "first_name": "Rachid"
            },
            {
                "last_name": "Boit\u00e9",
                "first_name": "Hugo Le"
            },
            {
                "last_name": "Deman",
                "first_name": "Pierre"
            },
            {
                "last_name": "Borderie",
                "first_name": "Laurent"
            },
            {
                "last_name": "Ren",
                "first_name": "Hugang"
            },
            {
                "last_name": "Mannivanan",
                "first_name": "Niranchana"
            },
            {
                "last_name": "Lepicard",
                "first_name": "Capucine"
            },
            {
                "last_name": "Cochener",
                "first_name": "B\u00e9atrice"
            },
            {
                "last_name": "Couturier",
                "first_name": "Aude"
            },
            {
                "last_name": "Tadayoni",
                "first_name": "Ramin"
            },
            {
                "last_name": "Conze",
                "first_name": "Pierre-Henri"
            },
            {
                "last_name": "Lamard",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Quellec",
                "first_name": "Gwenol\u00e9"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading\ncause of blindness worldwide. Traditionally, DR is monitored using Color Fundus\nPhotography (CFP), a widespread 2-D imaging modality. However, DR\nclassifications based on CFP have poor predictive power, resulting in\nsuboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a\nrecent 3-D imaging modality offering enhanced structural and functional\ninformation (blood flow) with a wider field of view. This paper investigates\nautomatic DR severity assessment using 3-D OCTA. A straightforward solution to\nthis task is a 3-D neural network classifier. However, 3-D architectures have\nnumerous parameters and typically require many training samples. A lighter\nsolution consists in using 2-D neural network classifiers processing 2-D\nen-face (or frontal) projections and/or 2-D cross-sectional slices. Such an\napproach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-face\nflow maps are often used to detect avascular zones and neovascularization, and\n2) cross-sectional slices are commonly analyzed to detect macular edemas, for\ninstance. However, arbitrary data reduction or selection might result in\ninformation loss. Two complementary strategies are thus proposed to optimally\nsummarize OCTA volumes with 2-D images: 1) a parametric en-face projection\noptimized through deep learning and 2) a cross-sectional slice selection\nprocess controlled through gradient-based attribution. The full summarization\nand DR classification pipeline is trained from end to end. The automatic 2-D\nsummary can be displayed in a viewer or printed in a report to support the\ndecision. We show that the proposed 2-D summarization and classification\npipeline outperforms direct 3-D classification with the advantage of improved\ninterpretability.\n",
        "title": "DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography\n  Angiography for Automatic Diabetic Retinopathy Diagnosis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05143",
        "abstract_url": "http://arxiv.org/abs/2401.05143",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose and analyze a general framework called nonlinear preconditioned\nprimal-dual with projection for solving nonconvex-nonconcave and non-smooth\nsaddle-point problems. The framework consists of two steps. The first is a\nnonlinear preconditioned map followed by a relaxed projection onto the\nseparating hyperspace we construct. One key to the method is the selection of\npreconditioned operators, which tailors to the structure of the saddle-point\nproblem and is allowed to be nonlinear and asymmetric. The other is the\nconstruction of separating hyperspace, which guarantees fast convergence. This\nframework paves the way for constructing nonlinear preconditioned primal-dual\nalgorithms. We show that weak convergence, and so is sublinear convergence\nunder the assumption of the convexity of saddle-point problems and linear\nconvergence under a metric subregularity. We also show that many existing\nprimal-daul methods, such as the generalized primal-dual algorithm method, are\nspecial cases of relaxed preconditioned primal-dual with projection.\n",
        "title": "Nonlinear preconditioned primal-dual method for a class of structured\n  minimax problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05144",
        "abstract_url": "http://arxiv.org/abs/2401.05144",
        "authors": [
            {
                "last_name": "McKechnie",
                "first_name": "Jack"
            },
            {
                "last_name": "McDonald",
                "first_name": "Graham"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large archival collections, such as email or government documents, must be\nmanually reviewed to identify any sensitive information before the collection\ncan be released publicly. Sensitivity classification has received a lot of\nattention in the literature. However, more recently, there has been increasing\ninterest in developing sensitivity-aware search engines that can provide users\nwith relevant search results, while ensuring that no sensitive documents are\nreturned to the user. Sensitivity-aware search would mitigate the need for a\nmanual sensitivity review prior to collections being made available publicly.\nTo develop such systems, there is a need for test collections that contain\nrelevance assessments for a set of information needs as well as ground-truth\nlabels for a variety of sensitivity categories. The well-known Enron email\ncollection contains a classification ground-truth that can be used to represent\nsensitive information, e.g., the Purely Personal and Personal but in\nProfessional Context categories can be used to represent sensitive personal\ninformation. However, the existing Enron collection does not contain a set of\ninformation needs and relevance assessments. In this work, we present a\ncollection of fifty information needs (topics) with crowdsourced query\nformulations (3 per topic) and relevance assessments (11,471 in total) for the\nEnron collection (mean number of relevant documents per topic = 11, variance =\n34.7). The developed information needs, queries and relevance judgements are\navailable on GitHub and will be available along with the existing Enron\ncollection through the popular ir_datasets library. Our proposed collection\nresults in the first freely available test collection for developing\nsensitivity-aware search systems.\n",
        "title": "SARA: A Collection of Sensitivity-Aware Relevance Assessments",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05145",
        "abstract_url": "http://arxiv.org/abs/2401.05145",
        "authors": [
            {
                "last_name": "Beinat",
                "first_name": "Matilda"
            },
            {
                "last_name": "Beinat",
                "first_name": "Julian"
            },
            {
                "last_name": "Shoaib",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Magenti",
                "first_name": "Jorge Gomez"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Projected to impact 1.6 million people in the UK by 2040 and costing\n{\\pounds}25 billion annually, dementia presents a growing challenge to society.\nThis study, a pioneering effort to predict the translational potential of\ndementia research using machine learning, hopes to address the slow translation\nof fundamental discoveries into practical applications despite dementia's\nsignificant societal and economic impact. We used the Dimensions database to\nextract data from 43,091 UK dementia research publications between the years\n1990-2023, specifically metadata (authors, publication year etc.), concepts\nmentioned in the paper, and the paper abstract. To prepare the data for machine\nlearning we applied methods such as one hot encoding and/or word embeddings. We\ntrained a CatBoost Classifier to predict if a publication will be cited in a\nfuture patent or clinical trial. We trained several model variations. The model\ncombining metadata, concept, and abstract embeddings yielded the highest\nperformance: for patent predictions, an Area Under the Receiver Operating\nCharacteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trial\npredictions, an AUROC of 0.81 and 75.11% accuracy. The results demonstrate that\nintegrating machine learning within current research methodologies can uncover\noverlooked publications, expediting the identification of promising research\nand potentially transforming dementia research by predicting real-world impact\nand guiding translational strategies.\n",
        "title": "Machine Learning to Promote Translational Research: Predicting Patent\n  and Clinical Trial Inclusion in Dementia Research",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05146",
        "abstract_url": "http://arxiv.org/abs/2401.05146",
        "authors": [
            {
                "last_name": "Romandini",
                "first_name": "Nicol\u00f2"
            },
            {
                "last_name": "Mora",
                "first_name": "Alessio"
            },
            {
                "last_name": "Mazzocca",
                "first_name": "Carlo"
            },
            {
                "last_name": "Montanari",
                "first_name": "Rebecca"
            },
            {
                "last_name": "Bellavista",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Federated Learning (FL) enables collaborative training of a Machine Learning\n(ML) model across multiple parties, facilitating the preservation of users' and\ninstitutions' privacy by keeping data stored locally. Instead of centralizing\nraw data, FL exchanges locally refined model parameters to build a global model\nincrementally. While FL is more compliant with emerging regulations such as the\nEuropean General Data Protection Regulation (GDPR), ensuring the right to be\nforgotten in this context - allowing FL participants to remove their data\ncontributions from the learned model - remains unclear. In addition, it is\nrecognized that malicious clients may inject backdoors into the global model\nthrough updates, e.g. to generate mispredictions on specially crafted data\nexamples. Consequently, there is the need for mechanisms that can guarantee\nindividuals the possibility to remove their data and erase malicious\ncontributions even after aggregation, without compromising the already acquired\n\"good\" knowledge. This highlights the necessity for novel Federated Unlearning\n(FU) algorithms, which can efficiently remove specific clients' contributions\nwithout full model retraining. This survey provides background concepts,\nempirical evidence, and practical guidelines to design/implement efficient FU\nschemes. Our study includes a detailed analysis of the metrics for evaluating\nunlearning in FL and presents an in-depth literature review categorizing\nstate-of-the-art FU contributions under a novel taxonomy. Finally, we outline\nthe most relevant and still open technical challenges, by identifying the most\npromising research directions in the field.\n",
        "title": "Federated Unlearning: A Survey on Methods, Design Guidelines, and\n  Evaluation Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05148",
        "abstract_url": "http://arxiv.org/abs/2401.05148",
        "authors": [
            {
                "last_name": "Gritz",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Hoppe",
                "first_name": "Anett"
            },
            {
                "last_name": "Ewerth",
                "first_name": "Ralph"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Nowadays, learning increasingly involves the usage of search engines and web\nresources. The related interdisciplinary research field search as learning aims\nto understand how people learn on the web. Previous work has investigated\nseveral feature classes to predict, for instance, the expected knowledge gain\nduring web search. Therein, eye-tracking features have not been extensively\nstudied so far. In this paper, we extend a previously used reading model from a\nline-based one to one that can detect reading sequences across multiple lines.\nWe use publicly available study data from a web-based learning task to examine\nthe relationship between our feature set and the participants' test scores. Our\nfindings demonstrate that learners with higher knowledge gain spent\nsignificantly more time reading, and processing more words in total. We also\nfind evidence that faster reading at the expense of more backward regressions\nmay be an indicator of better web-based learning. We make our code publicly\navailable at https://github.com/TIBHannover/reading_web_search.\n",
        "title": "On the Influence of Reading Sequences on Knowledge Gain during Web\n  Search",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05152",
        "abstract_url": "http://arxiv.org/abs/2401.05152",
        "authors": [
            {
                "last_name": "Fernandez-Cortizas",
                "first_name": "Miguel"
            },
            {
                "last_name": "Bavle",
                "first_name": "Hriday"
            },
            {
                "last_name": "Perez-Saura",
                "first_name": "David"
            },
            {
                "last_name": "Sanchez-Lopez",
                "first_name": "Jose Luis"
            },
            {
                "last_name": "Campoy",
                "first_name": "Pascual"
            },
            {
                "last_name": "Voos",
                "first_name": "Holger"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to\nenable multiple robots to operate in complex environments. Most CSLAM\ntechniques rely on raw sensor measurement or low-level features such as\nkeyframe descriptors, which can lead to wrong loop closures due to the lack of\ndeep understanding of the environment. Moreover, the exchange of these\nmeasurements and low-level features among the robots requires the transmission\nof a significant amount of data, which limits the scalability of the system. To\novercome these limitations, we present Multi S-Graphs, a decentralized CSLAM\nsystem that utilizes high-level semantic-relational information embedded in the\nfour-layered hierarchical and optimizable situational graphs for cooperative\nmap generation and localization while minimizing the information exchanged\nbetween the robots. To support this, we present a novel room-based descriptor\nwhich, along with its connected walls, is used to perform inter-robot loop\nclosures, addressing the challenges of multi-robot kidnapped problem\ninitialization. Multiple experiments in simulated and real environments\nvalidate the improvement in accuracy and robustness of the proposed approach\nwhile reducing the amount of data exchanged between robots compared to other\nstate-of-the-art approaches.\n  Software available within a docker image:\nhttps://github.com/snt-arg/multi_s_graphs_docker\n",
        "title": "Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational\n  Collaborative SLAM",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05153",
        "abstract_url": "http://arxiv.org/abs/2401.05153",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Yinghui"
            },
            {
                "last_name": "Qu",
                "first_name": "Litao"
            },
            {
                "last_name": "Zhang",
                "first_name": "ShiZhou"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiuwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yanning"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Fusion of a panchromatic (PAN) image and corresponding multispectral (MS)\nimage is also known as pansharpening, which aims to combine abundant spatial\ndetails of PAN and spectral information of MS. Due to the absence of\nhigh-resolution MS images, available deep-learning-based methods usually follow\nthe paradigm of training at reduced resolution and testing at both reduced and\nfull resolution. When taking original MS and PAN images as inputs, they always\nobtain sub-optimal results due to the scale variation. In this paper, we\npropose to explore the self-supervised representation of pansharpening by\ndesigning a cross-predictive diffusion model, named CrossDiff. It has two-stage\ntraining. In the first stage, we introduce a cross-predictive pretext task to\npre-train the UNet structure based on conditional DDPM, while in the second\nstage, the encoders of the UNets are frozen to directly extract spatial and\nspectral features from PAN and MS, and only the fusion head is trained to adapt\nfor pansharpening task. Extensive experiments show the effectiveness and\nsuperiority of the proposed model compared with state-of-the-art supervised and\nunsupervised methods. Besides, the cross-sensor experiments also verify the\ngeneralization ability of proposed self-supervised representation learners for\nother satellite's datasets. We will release our code for reproducibility.\n",
        "title": "CrossDiff: Exploring Self-Supervised Representation of Pansharpening via\n  Cross-Predictive Diffusion Model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05154",
        "abstract_url": "http://arxiv.org/abs/2401.05154",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Weichuang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jieru"
            },
            {
                "last_name": "Shen",
                "first_name": "Guan"
            },
            {
                "last_name": "Chen",
                "first_name": "Quan"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Guo",
                "first_name": "Minyi"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "PL"
        ],
        "abstract": "  With the increasing demand for computing capability given limited resource\nand power budgets, it is crucial to deploy applications to customized\naccelerators like FPGAs. However, FPGA programming is non-trivial. Although\nexisting high-level synthesis (HLS) tools improve productivity to a certain\nextent, they are limited in scope and capability to support sufficient\nFPGA-oriented optimizations. This paper focuses on FPGA-based accelerators and\nproposes POM, an optimizing framework built on multi-level intermediate\nrepresentation (MLIR). POM has several features which demonstrate its scope and\ncapability of performance optimization. First, most HLS tools depend\nexclusively on a single-level IR to perform all the optimizations, introducing\nexcessive information into the IR and making debugging an arduous task. In\ncontrast, POM introduces three layers of IR to perform operations at suitable\nabstraction levels, streamlining the implementation and debugging process and\nexhibiting better flexibility, extensibility, and systematicness. Second, POM\nintegrates the polyhedral model into MLIR, enabling advanced dependence\nanalysis and various FPGA-oriented loop transformations. By representing nested\nloops with integer sets and maps, loop transformations can be conducted\nconveniently through manipulations on polyhedral semantics. Finally, to further\nrelieve design effort, POM has a user-friendly programming interface (DSL) that\nallows a concise description of computation and includes a rich collection of\nscheduling primitives. An automatic design space exploration (DSE) engine is\nprovided to search for high-performance optimization schemes efficiently and\ngenerate optimized accelerators automatically. Experimental results show that\nPOM achieves a $6.46\\times$ average speedup on typical benchmark suites and a\n$6.06\\times$ average speedup on real-world applications compared to the\nstate-of-the-art.\n",
        "title": "An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator\n  Generation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05157",
        "abstract_url": "http://arxiv.org/abs/2401.05157",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yitao"
            },
            {
                "last_name": "Li",
                "first_name": "Heng-Chao"
            },
            {
                "last_name": "Liu",
                "first_name": "Nanqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Rui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the conventional change detection (CD) pipeline, two manually registered\nand labeled remote sensing datasets serve as the input of the model for\ntraining and prediction. However, in realistic scenarios, data from different\nperiods or sensors could fail to be aligned as a result of various coordinate\nsystems. Geometric distortion caused by coordinate shifting remains a thorny\nissue for CD algorithms. In this paper, we propose a reusable self-supervised\nframework for bitemporal geometric distortion in CD tasks. The whole framework\nis composed of Pretext Representation Pre-training, Bitemporal Image Alignment,\nand Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the\nkey components of the framework can be reused for assistance in the bitemporal\nimage alignment, while simultaneously enhancing the performance of the CD\ndecoder. Experimental results in 2 large-scale realistic scenarios demonstrate\nthat our proposed method can alleviate the bitemporal geometric distortion in\nCD tasks.\n",
        "title": "Toward distortion-aware change detection in realistic scenarios",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05159",
        "abstract_url": "http://arxiv.org/abs/2401.05159",
        "authors": [
            {
                "last_name": "Farooq",
                "first_name": "Muhammad Ali"
            },
            {
                "last_name": "Yao",
                "first_name": "Wang"
            },
            {
                "last_name": "Schukat",
                "first_name": "Michael"
            },
            {
                "last_name": "Little",
                "first_name": "Mark A"
            },
            {
                "last_name": "Corcoran",
                "first_name": "Peter"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  This study explores the utilization of Dermatoscopic synthetic data generated\nthrough stable diffusion models as a strategy for enhancing the robustness of\nmachine learning model training. Synthetic data generation plays a pivotal role\nin mitigating challenges associated with limited labeled datasets, thereby\nfacilitating more effective model training. In this context, we aim to\nincorporate enhanced data transformation techniques by extending the recent\nsuccess of few-shot learning and a small amount of data representation in\ntext-to-image latent diffusion models. The optimally tuned model is further\nused for rendering high-quality skin lesion synthetic data with diverse and\nrealistic characteristics, providing a valuable supplement and diversity to the\nexisting training data. We investigate the impact of incorporating newly\ngenerated synthetic data into the training pipeline of state-of-art machine\nlearning models, assessing its effectiveness in enhancing model performance and\ngeneralization to unseen real-world data. Our experimental results demonstrate\nthe efficacy of the synthetic data generated through stable diffusion models\nhelps in improving the robustness and adaptability of end-to-end CNN and vision\ntransformer models on two different real-world skin lesion datasets.\n",
        "title": "Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion\n  Models for Enhanced Skin Disease Classification using ViT and CNN",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05163",
        "abstract_url": "http://arxiv.org/abs/2401.05163",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Yang",
                "first_name": "Dingkang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yue"
            },
            {
                "last_name": "Lei",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lihua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Medical visual question answering (VQA) is a challenging multimodal task,\nwhere Vision-Language Pre-training (VLP) models can effectively improve the\ngeneralization performance. However, most methods in the medical field treat\nVQA as an answer classification task which is difficult to transfer to\npractical application scenarios. Additionally, due to the privacy of medical\nimages and the expensive annotation process, large-scale medical image-text\npairs datasets for pretraining are severely lacking. In this paper, we propose\na large-scale MultI-task Self-Supervised learning based framework (MISS) for\nmedical VQA tasks. Unlike existing methods, we treat medical VQA as a\ngenerative task. We unify the text encoder and multimodal encoder and align\nimage-text features through multi-task learning. Furthermore, we propose a\nTransfer-and-Caption method that extends the feature space of single-modal\nimage datasets using large language models (LLMs), enabling those traditional\nmedical vision field task data to be applied to VLP. Experiments show that our\nmethod achieves excellent results with fewer multimodal datasets and\ndemonstrates the advantages of generative VQA models. The code and model\nweights will be released upon the paper's acceptance.\n",
        "title": "MISS: A Generative Pretraining and Finetuning Approach for Med-VQA",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05164",
        "abstract_url": "http://arxiv.org/abs/2401.05164",
        "authors": [
            {
                "last_name": "Tarable",
                "first_name": "Alberto"
            },
            {
                "last_name": "Dossi",
                "first_name": "Laura"
            },
            {
                "last_name": "Virone",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Nordio",
                "first_name": "Alessandro"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Motivated by the challenges of future 6G communications where terahertz (THz)\nfrequencies, intelligent reflective surfaces (IRSs) and ultra-wideband (UWB)\nsignals coexist, we analyse and propose a set of efficient techniques for\nconfiguring the IRS when the signal bandwidth is a significant fraction of the\ncentral frequency (up to 50%). To the best of our knowledge this is the first\ntime that IRS configuration techniques are analyzed for such huge bandwidths.\nIn our work we take into account for the channel model, the power spectral\ndensity of the signal reflected by the IRS and the network geometry. We\nevaluate the proposed solutions in terms of achievable rate and compare it\nagainst an upper bound we derived. Our results hint rules for designing\nIRS-aided communication systems and allow to draw conclusions on the trade-off\nbetween performance and complexity required for configuring the IRS.\n",
        "title": "IRS Configuration Techniques for Ultra Wideband Signals and THz\n  Communications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05165",
        "abstract_url": "http://arxiv.org/abs/2401.05165",
        "authors": [
            {
                "last_name": "Seidl",
                "first_name": "Helmut"
            },
            {
                "last_name": "Erhard",
                "first_name": "Julian"
            },
            {
                "last_name": "Tilscher",
                "first_name": "Sarah"
            },
            {
                "last_name": "Schwarz",
                "first_name": "Michael"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            ""
        ],
        "abstract": "  The weakly relational domain of Octagons offers a decent compromise between\nprecision and efficiency for numerical properties. Here, we are concerned with\nthe construction of non-numerical relational domains. We provide a general\nconstruction of weakly relational domains, which we exemplify with an extension\nof constant propagation by disjunctions. Since for the resulting domain of\n2-disjunctive formulas, satisfiability is NP-complete, we provide a general\nconstruction for a further, more abstract weakly relational domain where the\nabstract operations of restriction and least upper bound can be efficiently\nimplemented. In the second step, we consider a relational domain that tracks\nconjunctions of inequalities between variables, and between variables and\nconstants for arbitrary partial orders of values. Examples are sub(multi)sets,\nas well as prefix, substring or scattered substring orderings on strings. When\nthe partial order is a lattice, we provide precise polynomial algorithms for\nsatisfiability, restriction, and the best abstraction of disjunction.\nComplementary to the constructions for lattices, we find that, in general,\nsatisfiability of conjunctions is NP-complete. We therefore again provide\npolynomial abstract versions of restriction, conjunction, and join. By using\nour generic constructions, these domains are extended to weakly relational\ndomains that additionally track disjunctions. For all our domains, we indicate\nhow abstract transformers for assignments and guards can be constructed.\n",
        "title": "Non-Numerical Weakly Relational Domains",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05166",
        "abstract_url": "http://arxiv.org/abs/2401.05166",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Siyang"
            },
            {
                "last_name": "Spitale",
                "first_name": "Micol"
            },
            {
                "last_name": "Luo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Palmero",
                "first_name": "Cristina"
            },
            {
                "last_name": "Barquero",
                "first_name": "German"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hengde"
            },
            {
                "last_name": "Escalera",
                "first_name": "Sergio"
            },
            {
                "last_name": "Valstar",
                "first_name": "Michel"
            },
            {
                "last_name": "Baur",
                "first_name": "Tobias"
            },
            {
                "last_name": "Ringeval",
                "first_name": "Fabien"
            },
            {
                "last_name": "Andre",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Gunes",
                "first_name": "Hatice"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n",
        "title": "REACT 2024: the Second Multiple Appropriate Facial Reaction Generation\n  Challenge",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05167",
        "abstract_url": "http://arxiv.org/abs/2401.05167",
        "authors": [
            {
                "last_name": "Krubinski",
                "first_name": "Mateusz"
            },
            {
                "last_name": "Matcovici",
                "first_name": "Stefan"
            },
            {
                "last_name": "Grigore",
                "first_name": "Diana"
            },
            {
                "last_name": "Voinea",
                "first_name": "Daniel"
            },
            {
                "last_name": "Popa",
                "first_name": "Alin-Ionut"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Watermark text spotting in document images can offer access to an often\nunexplored source of information, providing crucial evidence about a record's\nscope, audience and sometimes even authenticity. Stemming from the problem of\ntext spotting, detecting and understanding watermarks in documents inherits the\nsame hardships - in the wild, writing can come in various fonts, sizes and\nforms, making generic recognition a very difficult problem. To address the lack\nof resources in this field and propel further research, we propose a novel\nbenchmark (K-Watermark) containing 65,447 data samples generated using Wrender,\na watermark text patterns rendering procedure. A validity study using humans\nraters yields an authenticity score of 0.51 against pre-generated watermarked\ndocuments. To prove the usefulness of the dataset and rendering technique, we\ndeveloped an end-to-end solution (Wextract) for detecting the bounding box\ninstances of watermark text, while predicting the depicted text. To deal with\nthis specific task, we introduce a variance minimization loss and a\nhierarchical self-attention mechanism. To the best of our knowledge, we are the\nfirst to propose an evaluation benchmark and a complete solution for retrieving\nwatermarks from documents surpassing baselines by 5 AP points in detection and\n4 points in character accuracy.\n",
        "title": "Watermark Text Pattern Spotting in Document Images",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05168",
        "abstract_url": "http://arxiv.org/abs/2401.05168",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Nanqing"
            },
            {
                "last_name": "Xu",
                "first_name": "Xun"
            },
            {
                "last_name": "Su",
                "first_name": "Yongyi"
            },
            {
                "last_name": "Liu",
                "first_name": "Chengxin"
            },
            {
                "last_name": "Gong",
                "first_name": "Peiliang"
            },
            {
                "last_name": "Li",
                "first_name": "Heng-Chao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Domain adaptation is crucial in aerial imagery, as the visual representation\nof these images can significantly vary based on factors such as geographic\nlocation, time, and weather conditions. Additionally, high-resolution aerial\nimages often require substantial storage space and may not be readily\naccessible to the public. To address these challenges, we propose a novel\nSource-Free Object Detection (SFOD) method. Specifically, our approach is built\nupon a self-training framework; however, self-training can lead to inaccurate\nlearning in the absence of labeled training data. To address this issue, we\nfurther integrate Contrastive Language-Image Pre-training (CLIP) to guide the\ngeneration of pseudo-labels, termed CLIP-guided Aggregation. By leveraging\nCLIP's zero-shot classification capability, we use it to aggregate scores with\nthe original predicted bounding boxes, enabling us to obtain refined scores for\nthe pseudo-labels. To validate the effectiveness of our method, we constructed\ntwo new datasets from different domains based on the DIOR dataset, named DIOR-C\nand DIOR-Cloudy. Experiments demonstrate that our method outperforms other\ncomparative algorithms.\n",
        "title": "CLIP-guided Source-free Object Detection in Aerial Images",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05170",
        "abstract_url": "http://arxiv.org/abs/2401.05170",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Junshuo"
            },
            {
                "last_name": "Huang",
                "first_name": "Yunlong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jianan"
            },
            {
                "last_name": "Xiong",
                "first_name": "Rujing"
            },
            {
                "last_name": "Qiu",
                "first_name": "Robert Caiming"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Device-free human activity recognition plays a pivotal role in wireless\nsensing. However, current systems often fail to accommodate signal transmission\nthrough walls or necessitate dedicated noise removal algorithms. To overcome\nthese limitations, we introduce TRTAR: a device-free passive human activity\nrecognition system integrated with a transmissive reconfigurable intelligent\nsurface (RIS). TRTAR eliminates the necessity for dedicated devices or noise\nremoval algorithms, while specifically addressing signal propagation through\nwalls. Unlike existing approaches, TRTAR solely employs a transmissive RIS at\nthe transmitter or receiver without modifying the inherent hardware structure.\nExperimental results demonstrate that TRTAR attains an average accuracy of\n98.13% when signals traverse concrete walls.\n",
        "title": "TRTAR: Transmissive RIS-assisted Through-the-wall Human Activity\n  Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05171",
        "abstract_url": "http://arxiv.org/abs/2401.05171",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Diversity schemes play a vital role in improving the performance of\nultra-reliable communication systems by transmitting over two or more\ncommunication channels to combat fading and co-channel interference.\nDetermining an appropriate transmission strategy that satisfies\nultra-reliability constraint necessitates derivation of statistics of channel\nin ultra-reliable region and, subsequently, integration of these statistics\ninto rate selection while incorporating a confidence interval to account for\npotential uncertainties that may arise during estimation. In this paper, we\npropose a novel framework for ultra-reliable real-time transmission considering\nboth spatial diversities and ultra-reliable channel statistics based on\nmultivariate extreme value theory. First, tail distribution of joint received\npower sequences obtained from different receivers is modeled while\nincorporating inter-relations of extreme events occurring rarely based on\nPoisson point process approach in MEVT. The optimum transmission strategies are\nthen developed by determining optimum transmission rate based on estimated\njoint tail distribution and incorporating confidence intervals into estimations\nto cope with the availability of limited data. Finally, system reliability is\nassessed by utilizing outage probability metric. Through analysis of data\nobtained from the engine compartment of Fiat Linea, our study showcases the\neffectiveness of proposed methodology in surpassing traditional\nextrapolation-based approaches. This innovative method not only achieves a\nhigher transmission rate, but also effectively addresses stringent requirements\nof ultra-reliability. The findings indicate that proposed rate selection\nframework offers a viable solution for achieving a desired target error\nprobability by employing a higher transmission rate and reducing the amount of\ntraining data compared to conventional rate selection methods.\n",
        "title": "Multivariate Extreme Value Theory Based Rate Selection for\n  Ultra-Reliable Communications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05176",
        "abstract_url": "http://arxiv.org/abs/2401.05176",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Zhaokun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziyin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Inspired by the increasing interest in leveraging large language models for\ntranslation, this paper evaluates the capabilities of large language models\n(LLMs) represented by ChatGPT in comparison to the mainstream neural machine\ntranslation (NMT) engines in translating Chinese diplomatic texts into English.\nSpecifically, we examine the translation quality of ChatGPT and NMT engines as\nmeasured by four automated metrics and human evaluation based on an\nerror-typology and six analytic rubrics. Our findings show that automated\nmetrics yield similar results for ChatGPT under different prompts and NMT\nsystems, while human annotators tend to assign noticeably higher scores to\nChatGPT when it is provided an example or contextual information about the\ntranslation task. Pairwise correlation between automated metrics and dimensions\nof human evaluation produces weak and non-significant results, suggesting the\ndivergence between the two methods of translation quality assessment. These\nfindings provide valuable insights into the potential of ChatGPT as a capable\nmachine translator, and the influence of prompt engineering on its performance.\n",
        "title": "Can ChatGPT Rival Neural Machine Translation? A Comparative Study",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05182",
        "abstract_url": "http://arxiv.org/abs/2401.05182",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Pingping"
            },
            {
                "last_name": "Wang",
                "first_name": "Jintao"
            },
            {
                "last_name": "Shao",
                "first_name": "Yulin"
            },
            {
                "last_name": "Ma",
                "first_name": "Shaodan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper presents a new integrated sensing and communication (ISAC)\nframework, leveraging the recent advancements of reconfigurable distributed\nantenna and reflecting surface (RDARS). RDARS is a programmable surface\nstructure comprising numerous elements, each of which can be flexibly\nconfigured to operate either in a reflection mode, resembling a passive\nreconfigurable intelligent surface (RIS), or in a connected mode, functioning\nas a remote transmit or receive antenna. Our RDARS-aided ISAC framework\neffectively mitigates the adverse impact of multiplicative fading when compared\nto the passive RIS-aided ISAC, and reduces cost and energy consumption when\ncompared to the active RIS-aided ISAC. Within our RDARS-aided ISAC framework,\nwe consider a radar output signal-to-noise ratio (SNR) maximization problem\nunder communication constraints to jointly optimize the active transmit\nbeamforming matrix of the base station (BS), the reflection and mode selection\nmatrices of RDARS, and the receive filter. To tackle the inherent non-convexity\nand the binary integer optimization introduced by the mode selection in this\noptimization challenge, we propose an efficient iterative algorithm with proved\nconvergence based on majorization minimization (MM) and penalty-based\nmethods.Numerical and simulation results demonstrate the superior performance\nof our new framework, and clearly verify substantial distribution, reflection\nas well as selection gains obtained by properly configuring the RDARS.\n",
        "title": "Integrated Sensing and Communication with Reconfigurable Distributed\n  Antenna and Reflecting Surface: Joint Beamforming and Mode Selection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05190",
        "abstract_url": "http://arxiv.org/abs/2401.05190",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Zijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhaopeng"
            },
            {
                "last_name": "Feng",
                "first_name": "Yang"
            },
            {
                "last_name": "Wang",
                "first_name": "Gaoang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Joey Tianyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Zuozhu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have shown impressive performance in various\nreasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its\nderivative methods, particularly in tasks involving multi-choice questions\n(MCQs). However, current works all process data uniformly without considering\nthe problem-solving difficulty, which means an excessive focus on simple\nquestions while insufficient to intricate ones. To address this challenge, we\ninspired by humans using heuristic strategies to categorize tasks and handle\nthem individually, propose to apply the Divide and Conquer to LLMs reasoning.\nFirst, we divide questions into different subsets based on the statistical\nconfidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer\ndemanding nuanced process ones with elaborately designed methods, including\nPrior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR),\nas well as their integration variants. Our experiments demonstrate that this\nproposed strategy significantly boosts the models' reasoning abilities across\nnine datasets involving arithmetic, commonsense, and logic tasks. For instance,\ncompared to baseline, we make a striking improvement on low confidence subsets\nof 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense. In\naddition, through extensive analysis on length of rationale and number of\noptions, we verify that longer reasoning paths in PKR could prevent models from\nreferring infer-harmful shortcuts, and also find that removing irrelevant\nchoices in FCR would substantially avoid models' confusion. The code is at\n\\url{https://github.com/AiMijie/Divide-and-Conquer}\n",
        "title": "Divide and Conquer for Large Language Models Reasoning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05191",
        "abstract_url": "http://arxiv.org/abs/2401.05191",
        "authors": [
            {
                "last_name": "Lai",
                "first_name": "Riwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Rui"
            },
            {
                "last_name": "Han",
                "first_name": "Qilong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chi"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Negative sampling is essential for implicit collaborative filtering to\nprovide proper negative training signals so as to achieve desirable\nperformance. We experimentally unveil a common limitation of all existing\nnegative sampling methods that they can only select negative samples of a fixed\nhardness level, leading to the false positive problem (FPP) and false negative\nproblem (FNP). We then propose a new paradigm called adaptive hardness negative\nsampling (AHNS) and discuss its three key criteria. By adaptively selecting\nnegative samples with appropriate hardnesses during the training process, AHNS\ncan well mitigate the impacts of FPP and FNP. Next, we present a concrete\ninstantiation of AHNS called AHNS_{p<0}, and theoretically demonstrate that\nAHNS_{p<0} can fit the three criteria of AHNS well and achieve a larger lower\nbound of normalized discounted cumulative gain. Besides, we note that existing\nnegative sampling methods can be regarded as more relaxed cases of AHNS.\nFinally, we conduct comprehensive experiments, and the results show that\nAHNS_{p<0} can consistently and substantially outperform several\nstate-of-the-art competitors on multiple datasets.\n",
        "title": "Adaptive Hardness Negative Sampling for Collaborative Filtering",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05193",
        "abstract_url": "http://arxiv.org/abs/2401.05193",
        "authors": [
            {
                "last_name": "Pacchiano",
                "first_name": "Aldo"
            },
            {
                "last_name": "Lee",
                "first_name": "Jonathan N."
            },
            {
                "last_name": "Brunskill",
                "first_name": "Emma"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We study the problem of experiment planning with function approximation in\ncontextual bandit problems. In settings where there is a significant overhead\nto deploying adaptive algorithms -- for example, when the execution of the data\ncollection policies is required to be distributed, or a human in the loop is\nneeded to implement these policies -- producing in advance a set of policies\nfor data collection is paramount. We study the setting where a large dataset of\ncontexts but not rewards is available and may be used by the learner to design\nan effective data collection strategy. Although when rewards are linear this\nproblem has been well studied, results are still missing for more complex\nreward models. In this work we propose two experiment planning strategies\ncompatible with function approximation. The first is an eluder planning and\nsampling procedure that can recover optimality guarantees depending on the\neluder dimension of the reward function class. For the second, we show that a\nuniform sampler achieves competitive optimality rates in the setting where the\nnumber of actions is small. We finalize our results introducing a statistical\ngap fleshing out the fundamental differences between planning and adaptive\nlearning and provide results for planning with model selection.\n",
        "title": "Experiment Planning with Function Approximation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05194",
        "abstract_url": "http://arxiv.org/abs/2401.05194",
        "authors": [
            {
                "last_name": "Caponio",
                "first_name": "Carmine"
            },
            {
                "last_name": "Stano",
                "first_name": "Pietro"
            },
            {
                "last_name": "Carli",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Olivieri",
                "first_name": "Ignazio"
            },
            {
                "last_name": "Ragone",
                "first_name": "Daniele"
            },
            {
                "last_name": "Sorniotti",
                "first_name": "Aldo"
            },
            {
                "last_name": "Montanaro",
                "first_name": "Umberto"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            ""
        ],
        "abstract": "  Mobile robotic systems are becoming increasingly popular. These systems are\nused in various indoor applications, raging from warehousing and manufacturing\nto test benches for assessment of advanced control strategies, such as\nartificial intelligence (AI)-based control solutions, just to name a few.\nScaled robotic cars are commonly equipped with a hierarchical control\nacthiecture that includes tasks dedicated to vehicle state estimation and\ncontrol. This paper covers both aspects by proposing (i) a federeted extended\nKalman filter (FEKF), and (ii) a novel deep reinforcement learning (DRL) path\ntracking controller trained via an expert demonstrator to expedite the learning\nphase and increase robustess to the simulation-to-reality gap. The paper also\npresents the formulation of a vehicle model along with an effective yet simple\nprocedure for identifying tis paramters. The experimentally validated model is\nused for (i) supporting the design of the FEKF and (ii) serving as a digital\ntwin for training the proposed DRL-based path tracking algorithm. Experimental\nresults confirm the ability of the FEKF to improve the estimate of the mobile\nrobot's position. Furthermore, the effectiveness of the DRL path tracking\nstrateguy is experimentally tested along manoeuvres not considered during\ntraining, showing also the ability of the AI-based solution to outpeform\nmodel-based control strategies and the demonstrator. The comparison with\nbenchmraking controllers is quantitavely evalueted through a set of key\nperformance indicators.\n",
        "title": "Modelling, Positioning, and Deep Reinforcement Learning Path Tracking\n  Control of Scaled Robotic Vehicles: Design and Experimental Validation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05196",
        "abstract_url": "http://arxiv.org/abs/2401.05196",
        "authors": [
            {
                "last_name": "Raus",
                "first_name": "Maren"
            },
            {
                "last_name": "Elshiaty",
                "first_name": "Yara"
            },
            {
                "last_name": "Petra",
                "first_name": "Stefania"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  We investigate the problem of minimizing Kullback-Leibler divergence between\na linear model $Ax$ and a positive vector $b$ in different convex domains\n(positive orthant, $n$-dimensional box, probability simplex). Our focus is on\nthe SMART method that employs efficient multiplicative updates. We explore the\nexponentiated gradient method, which can be viewed as a Bregman proximal\ngradient method and as a Riemannian gradient descent on the parameter manifold\nof a corresponding distribution of the exponential family. This dual\ninterpretation enables us to establish connections and achieve accelerated\nSMART iterates while smoothly incorporating constraints. The performance of the\nproposed acceleration schemes is demonstrated by large-scale numerical\nexamples.\n",
        "title": "Accelerated Bregmann divergence optimization with SMART: an information\n  geometry point of view",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05199",
        "abstract_url": "http://arxiv.org/abs/2401.05199",
        "authors": [
            {
                "last_name": "Taneja",
                "first_name": "Karan"
            },
            {
                "last_name": "Segal",
                "first_name": "Richard"
            },
            {
                "last_name": "Goodwin",
                "first_name": "Richard"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Automatic food recipe generation methods provide a creative tool for chefs to\nexplore and to create new, and interesting culinary delights. Given the recent\nsuccess of large language models (LLMs), they have the potential to create new\nrecipes that can meet individual preferences, dietary constraints, and adapt to\nwhat is in your refrigerator. Existing research on using LLMs to generate\nrecipes has shown that LLMs can be finetuned to generate realistic-sounding\nrecipes. However, on close examination, these generated recipes often fail to\nmeet basic requirements like including chicken as an ingredient in chicken\ndishes. In this paper, we propose RecipeMC, a text generation method using\nGPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to\ndefine reward functions to put soft constraints on text generation and thus\nimprove the credibility of the generated recipes. Our results show that human\nevaluators prefer recipes generated with RecipeMC more often than recipes\ngenerated with other baseline methods when compared with real recipes.\n",
        "title": "Monte Carlo Tree Search for Recipe Generation using GPT-2",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05200",
        "abstract_url": "http://arxiv.org/abs/2401.05200",
        "authors": [
            {
                "last_name": "Freire",
                "first_name": "Samuel Kernan"
            },
            {
                "last_name": "Wang",
                "first_name": "Chaofan"
            },
            {
                "last_name": "Foosherian",
                "first_name": "Mina"
            },
            {
                "last_name": "Wellsandt",
                "first_name": "Stefan"
            },
            {
                "last_name": "Ruiz-Arenas",
                "first_name": "Santiago"
            },
            {
                "last_name": "Niforatos",
                "first_name": "Evangelos"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "IR"
        ],
        "abstract": "  Managing knowledge efficiently is crucial for organizational success. In\nmanufacturing, operating factories has become increasing knowledge-intensive\nputting strain on the factory's capacity to train and support new operators. In\nthis paper, we introduce a Large Language Model (LLM)-based system designed to\nuse the extensive knowledge contained in factory documentation. The system aims\nto efficiently answer queries from operators and facilitate the sharing of new\nknowledge. To assess its effectiveness, we conducted an evaluation in a factory\nsetting. The results of this evaluation demonstrated the system's benefits;\nnamely, in enabling quicker information retrieval and more efficient resolution\nof issues. However, the study also highlighted a preference for learning from a\nhuman expert when such an option is available. Furthermore, we benchmarked\nseveral closed and open-sourced LLMs for this system. GPT-4 consistently\noutperformed its counterparts, with open-source models like StableBeluga2\ntrailing closely, presenting an attractive option given its data privacy and\ncustomization benefits. Overall, this work offers preliminary insights for\nfactories considering using LLM-tools for knowledge management.\n",
        "title": "Knowledge Sharing in Manufacturing using Large Language Models: User\n  Evaluation and Model Benchmarking",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05202",
        "abstract_url": "http://arxiv.org/abs/2401.05202",
        "authors": [
            {
                "last_name": "Russello",
                "first_name": "Helena"
            },
            {
                "last_name": "van der Tol",
                "first_name": "Rik"
            },
            {
                "last_name": "Holzhauer",
                "first_name": "Menno"
            },
            {
                "last_name": "van Henten",
                "first_name": "Eldert J."
            },
            {
                "last_name": "Kootstra",
                "first_name": "Gert"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This study presents an automated lameness detection system that uses\ndeep-learning image processing techniques to extract multiple locomotion traits\nassociated with lameness. Using the T-LEAP pose estimation model, the motion of\nnine keypoints was extracted from videos of walking cows. The videos were\nrecorded outdoors, with varying illumination conditions, and T-LEAP extracted\n99.6% of correct keypoints. The trajectories of the keypoints were then used to\ncompute six locomotion traits: back posture measurement, head bobbing, tracking\ndistance, stride length, stance duration, and swing duration. The three most\nimportant traits were back posture measurement, head bobbing, and tracking\ndistance. For the ground truth, we showed that a thoughtful merging of the\nscores of the observers could improve intra-observer reliability and agreement.\nWe showed that including multiple locomotion traits improves the classification\naccuracy from 76.6% with only one trait to 79.9% with the three most important\ntraits and to 80.1% with all six locomotion traits.\n",
        "title": "Video-based Automatic Lameness Detection of Dairy Cows using Pose\n  Estimation and Multiple Locomotion Traits",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05204",
        "abstract_url": "http://arxiv.org/abs/2401.05204",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yong"
            },
            {
                "last_name": "Luo",
                "first_name": "Senlin"
            },
            {
                "last_name": "Shang",
                "first_name": "Yu-Ming"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The verbalizer, which serves to map label words to class labels, is an\nessential component of prompt-tuning. In this paper, we present a novel\napproach to constructing verbalizers. While existing methods for verbalizer\nconstruction mainly rely on augmenting and refining sets of synonyms or related\nwords based on class names, this paradigm suffers from a narrow perspective and\nlack of abstraction, resulting in limited coverage and high bias in the\nlabel-word space. To address this issue, we propose a label-word construction\nprocess that incorporates scenario-specific concepts. Specifically, we extract\nrich concepts from task-specific scenarios as label-word candidates and then\ndevelop a novel cascade calibration module to refine the candidates into a set\nof label words for each class. We evaluate the effectiveness of our proposed\napproach through extensive experiments on {five} widely used datasets for\nzero-shot text classification. The results demonstrate that our method\noutperforms existing methods and achieves state-of-the-art results.\n",
        "title": "A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts\n  into a Verbalizer",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05206",
        "abstract_url": "http://arxiv.org/abs/2401.05206",
        "authors": [
            {
                "last_name": "Nordhagen",
                "first_name": "Even Marius"
            },
            {
                "last_name": "Sveinsson",
                "first_name": "Henrik Andersen"
            },
            {
                "last_name": "Malthe-S\u00f8renssen",
                "first_name": "Anders"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This Letter introduces an approach for precisely designing surface friction\nproperties using a conditional generative machine learning model, specifically\na diffusion denoising probabilistic model (DDPM). We created a dataset of\nsynthetic surfaces with frictional properties determined by molecular dynamics\nsimulations, which trained the DDPM to predict surface structures from desired\nfrictional outcomes. Unlike traditional trial-and-error and numerical\noptimization methods, our approach directly yields surface designs meeting\nspecified frictional criteria with high accuracy and efficiency. This\nadvancement in material surface engineering demonstrates the potential of\nmachine learning in reducing the iterative nature of surface design processes.\nOur findings not only provide a new pathway for precise surface property\ntailoring but also suggest broader applications in material science where\nsurface characteristics are critical.\n",
        "title": "Tailoring Frictional Properties of Surfaces Using Diffusion Models",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05211",
        "abstract_url": "http://arxiv.org/abs/2401.05211",
        "authors": [
            {
                "last_name": "Stiasny",
                "first_name": "Jochen"
            },
            {
                "last_name": "Chatzivasileiadis",
                "first_name": "Spyros"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  The ability to accurately approximate trajectories of dynamical systems\nenables their analysis, prediction, and control. Neural network (NN)-based\napproximations have attracted significant interest due to fast evaluation with\ngood accuracy over long integration time steps. In contrast to established\nnumerical approximation schemes such as Runge-Kutta methods, the estimation of\nthe error of the NN-based approximations proves to be difficult. In this work,\nwe propose to use the NN's predictions in a high-order implicit Runge-Kutta\n(IRK) method. The residuals in the implicit system of equations can be related\nto the NN's prediction error, hence, we can provide an error estimate at\nseveral points along a trajectory. We find that this error estimate highly\ncorrelates with the NN's prediction error and that increasing the order of the\nIRK method improves this estimate. We demonstrate this estimation methodology\nfor Physics-Informed Neural Network (PINNs) on the logistic equation as an\nillustrative example and then apply it to a four-state electric generator model\nthat is regularly used in power system modelling.\n",
        "title": "Error estimation for physics-informed neural networks with implicit\n  Runge-Kutta methods",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05215",
        "abstract_url": "http://arxiv.org/abs/2401.05215",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Wei"
            },
            {
                "last_name": "Gong",
                "first_name": "Dihong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Financial sentiment analysis refers to classifying financial text contents\ninto sentiment categories (e.g. positive, negative, and neutral). In this\npaper, we focus on the classification of financial news title, which is a\nchallenging task due to a lack of large amount of training samples. To overcome\nthis difficulty, we propose to adapt the pretrained large language models\n(LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge\namount of text corpora,have an advantage in text understanding and can be\neffectively adapted to domain-specific task while requiring very few amount of\ntraining samples. In particular, we adapt the open-source Llama2-7B model\n(2023) with the supervised fine-tuning (SFT) technique [4]. Experimental\nevaluation shows that even with the 7B model (which is relatively small for\nLLMs), our approach significantly outperforms the previous state-of-the-art\nalgorithms.\n",
        "title": "Pre-trained Large Language Models for Financial Sentiment Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05217",
        "abstract_url": "http://arxiv.org/abs/2401.05217",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Chenxi"
            },
            {
                "last_name": "Liu",
                "first_name": "Yujia"
            },
            {
                "last_name": "Li",
                "first_name": "Dingquan"
            },
            {
                "last_name": "jiang",
                "first_name": "Tingting"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality\nscores consistent with human perception without relying on pristine reference\nimages, serving as a crucial component in various visual tasks. Ensuring the\nrobustness of NR-IQA methods is vital for reliable comparisons of different\nimage processing techniques and consistent user experiences in recommendations.\nThe attack methods for NR-IQA provide a powerful instrument to test the\nrobustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on\nthe gradient of the NR-IQA model, leading to limitations when the gradient\ninformation is unavailable. In this paper, we present a pioneering query-based\nblack box attack against NR-IQA methods. We propose the concept of \\emph{score\nboundary} and leverage an adaptive iterative approach with multiple score\nboundaries. Meanwhile, the initial attack directions are also designed to\nleverage the characteristics of the Human Visual System (HVS). Experiments show\nour attack method outperforms all compared state-of-the-art methods and is far\nahead of previous black-box methods. The effective DBCNN model suffers a\nSpearman rank-order correlation coefficient (SROCC) decline of $0.6972$\nattacked by our method, revealing the vulnerability of NR-IQA to black-box\nattacks. The proposed attack method also provides a potent tool for further\nexploration into NR-IQA robustness.\n",
        "title": "Exploring Vulnerabilities of No-Reference Image Quality Assessment\n  Models: A Query-Based Black-Box Method",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05218",
        "abstract_url": "http://arxiv.org/abs/2401.05218",
        "authors": [
            {
                "last_name": "Mey",
                "first_name": "Alexander"
            },
            {
                "last_name": "Castro",
                "first_name": "Rui Manuel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We consider the task of identifying the causal parents of a target variable\namong a set of candidate variables from observational data. Our main assumption\nis that the candidate variables are observed in different environments which\nmay, for example, correspond to different settings of a machine or different\ntime intervals in a dynamical process. Under certain assumptions different\nenvironments can be regarded as interventions on the observed system. We assume\na linear relationship between target and covariates, which can be different in\neach environment with the only restriction that the causal structure is\ninvariant across environments. This is an extension of the ICP\n($\\textbf{I}$nvariant $\\textbf{C}$ausal $\\textbf{P}$rediction) principle by\nPeters et al. [2016], who assumed a fixed linear relationship across all\nenvironments. Within our proposed setting we provide sufficient conditions for\nidentifiability of the causal parents and introduce a practical method called\nLoLICaP ($\\textbf{Lo}$cally $\\textbf{L}$inear $\\textbf{I}$nvariant\n$\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based on a hypothesis test\nfor parent identification using a ratio of minimum and maximum statistics. We\nthen show in a simplified setting that the statistical power of LoLICaP\nconverges exponentially fast in the sample size, and finally we analyze the\nbehavior of LoLICaP experimentally in more general settings.\n",
        "title": "Invariant Causal Prediction with Locally Linear Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05219",
        "abstract_url": "http://arxiv.org/abs/2401.05219",
        "authors": [
            {
                "last_name": "Karayanni",
                "first_name": "Nader"
            },
            {
                "last_name": "Shahla",
                "first_name": "Robert J."
            },
            {
                "last_name": "Hsiao",
                "first_name": "Chieh-Lien"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            ""
        ],
        "abstract": "  The digital era has seen a marked increase in financial fraud. edge ML\nemerged as a promising solution for smartphone payment services fraud\ndetection, enabling the deployment of ML models directly on edge devices. This\napproach enables a more personalized real-time fraud detection. However, a\nsignificant gap in current research is the lack of a robust system for\nmonitoring data distribution shifts in these distributed edge ML applications.\nOur work bridges this gap by introducing a novel open-source framework designed\nfor continuous monitoring of data distribution shifts on a network of edge\ndevices. Our system includes an innovative calculation of the\nKolmogorov-Smirnov (KS) test over a distributed network of edge devices,\nenabling efficient and accurate monitoring of users behavior shifts. We\ncomprehensively evaluate the proposed framework employing both real-world and\nsynthetic financial transaction datasets and demonstrate the framework's\neffectiveness.\n",
        "title": "Distributed Monitoring for Data Distribution Shifts in Edge-ML Fraud\n  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05221",
        "abstract_url": "http://arxiv.org/abs/2401.05221",
        "authors": [
            {
                "last_name": "Lips",
                "first_name": "Johannes"
            },
            {
                "last_name": "DeYoung",
                "first_name": "Stefan"
            },
            {
                "last_name": "Sch\u00f6nsteiner",
                "first_name": "Max"
            },
            {
                "last_name": "Lens",
                "first_name": "Hendrik"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The creation of low-order dynamic models for complex industrial systems is\ncomplicated by disturbances and limited sensor accuracy. This work presents a\nsystem identification procedure that uses machine learning methods and process\nknowledge to robustly identify a low-order closed-loop model of a municipal\nsolid waste (MSW) grate incineration plant. These types of plants are known for\ntheir strong disturbances coming from fuel composition fluctuations. Using\nBayesian optimization, the algorithm ranks and selects inputs from the\navailable sensor data and chooses the model structure. This results in accurate\nmodels with low complexity while avoiding overfitting. The method is applied\nand validated using data of an industrial MSW incineration plant. The obtained\nmodels give excellent predictions and confidence intervals for the steam\ncapacity and intermediate quantities such as supply air flow and flue gas\ntemperature. The identified continuous-time models are fully given, and their\nstep-response dynamics are discussed. The models can be used to develop\nmodel-based unit control schemes for grate incineration plants. The presented\nmethod shows great potential for the identification of over-actuated systems or\ndisturbed systems with many sensors.\n",
        "title": "Closed-loop Identification of a MSW Grate Incinerator using Bayesian\n  Optimization for Selecting Model Inputs and Structure",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05224",
        "abstract_url": "http://arxiv.org/abs/2401.05224",
        "authors": [
            {
                "last_name": "Maniparambil",
                "first_name": "Mayug"
            },
            {
                "last_name": "Akshulakov",
                "first_name": "Raiymbek"
            },
            {
                "last_name": "Djilali",
                "first_name": "Yasser Abdelaziz Dahou"
            },
            {
                "last_name": "Narayan",
                "first_name": "Sanath"
            },
            {
                "last_name": "Seddik",
                "first_name": "Mohamed El Amine"
            },
            {
                "last_name": "Mangalam",
                "first_name": "Karttikeya"
            },
            {
                "last_name": "O'Connor",
                "first_name": "Noel E."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  Aligned text-image encoders such as CLIP have become the de facto model for\nvision-language tasks. Furthermore, modality-specific encoders achieve\nimpressive performances in their respective domains. This raises a central\nquestion: does an alignment exist between uni-modal vision and language\nencoders since they fundamentally represent the same physical world? Analyzing\nthe latent spaces structure of vision and language models on image-caption\nbenchmarks using the Centered Kernel Alignment (CKA), we find that the\nrepresentation spaces of unaligned and aligned encoders are semantically\nsimilar. In the absence of statistical similarity in aligned encoders like\nCLIP, we show that a possible matching of unaligned encoders exists without any\ntraining. We frame this as a seeded graph-matching problem exploiting the\nsemantic similarity between graphs and propose two methods - a Fast Quadratic\nAssignment Problem optimization, and a novel localized CKA metric-based\nmatching/retrieval. We demonstrate the effectiveness of this on several\ndownstream tasks including cross-lingual, cross-domain caption matching and\nimage classification.\n",
        "title": "Do Vision and Language Encoders Represent the World Similarly?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05225",
        "abstract_url": "http://arxiv.org/abs/2401.05225",
        "authors": [
            {
                "last_name": "Mart\u00edn-P\u00e9rez",
                "first_name": "Jorge"
            },
            {
                "last_name": "Lentisco",
                "first_name": "Carlos M."
            },
            {
                "last_name": "Bellido",
                "first_name": "Luis"
            },
            {
                "last_name": "Soto",
                "first_name": "Ignacio"
            },
            {
                "last_name": "Fern\u00e1ndez",
                "first_name": "David"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Tele-operated Driving (ToD) is a challenging use case for mobile network\noperators. Video captured by the built-in vehicle cameras must be streamed\nmeeting a latency requirement of 5 ms with a 99.999% reliability. Although 5G\noffers high bandwidth, ultra-low latencies and high reliability; ToD service\nrequirements are violated due to bad channel conditions. Ignoring the channel\nstate may lead to over-estimate the number of ToD vehicles that can meet the\nservice requirements, hence comprising the vehicle security. To fill this gap,\nin this letter we propose TOVAC, an algorithm that guarantees ToD service\nrequirements by taking adequate admission control and routing decisions. This\nis achieved by using a channel-based capacity graph that determines the maximum\nnumber of vehicles that can be tele-operated in any road section. We evaluate\nTOVAC considering cellular deployments from Turin and show that, unlike a state\nof the art solution, TOVAC guarantees the ToD service requirements.\n",
        "title": "TOVAC: Tele-operated Vehicle Admission Control and Routing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05226",
        "abstract_url": "http://arxiv.org/abs/2401.05226",
        "authors": [
            {
                "last_name": "Barletta",
                "first_name": "Giulio"
            },
            {
                "last_name": "Trezza",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Chiavazzo",
                "first_name": "Eliodoro"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We assume that a sufficiently large database is available, where a physical\nproperty of interest and a number of associated ruling primitive variables or\nobservables are stored. We introduce and test two machine learning approaches\nto discover possible groups or combinations of primitive variables: The first\napproach is based on regression models whereas the second on classification\nmodels. The variable group (here referred to as the new effective good\nvariable) can be considered as successfully found, when the physical property\nof interest is characterized by the following effective invariant behaviour: In\nthe first method, invariance of the group implies invariance of the property up\nto a given accuracy; in the other method, upon partition of the physical\nproperty values into two or more classes, invariance of the group implies\ninvariance of the class. For the sake of illustration, the two methods are\nsuccessfully applied to two popular empirical correlations describing the\nconvective heat transfer phenomenon and to the Newton's law of universal\ngravitation.\n",
        "title": "Learning effective good variables from physical data",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05232",
        "abstract_url": "http://arxiv.org/abs/2401.05232",
        "authors": [
            {
                "last_name": "Jakab",
                "first_name": "Daniel"
            },
            {
                "last_name": "Grua",
                "first_name": "Eoin Martino"
            },
            {
                "last_name": "Deegan",
                "first_name": "Brian Micheal"
            },
            {
                "last_name": "Scanlan",
                "first_name": "Anthony"
            },
            {
                "last_name": "Van De Ven",
                "first_name": "Pepijn"
            },
            {
                "last_name": "Eising",
                "first_name": "Ciar\u00e1n"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The Modulation Transfer Function (MTF) is an important image quality metric\ntypically used in the automotive domain. However, despite the fact that optical\nquality has an impact on the performance of computer vision in vehicle\nautomation, for many public datasets, this metric is unknown. Additionally,\nwide field-of-view (FOV) cameras have become increasingly popular, particularly\nfor low-speed vehicle automation applications. To investigate image quality in\ndatasets, this paper proposes an adaptation of the Natural Scenes Spatial\nFrequency Response (NS-SFR) algorithm to suit cameras with a wide\nfield-of-view.\n",
        "title": "Measuring Natural Scenes SFR of Automotive Fisheye Cameras",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05233",
        "abstract_url": "http://arxiv.org/abs/2401.05233",
        "authors": [
            {
                "last_name": "Duan",
                "first_name": "Yaqi"
            },
            {
                "last_name": "Wainwright",
                "first_name": "Martin J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT",
            "",
            "",
            ""
        ],
        "abstract": "  We introduce a novel framework for analyzing reinforcement learning (RL) in\ncontinuous state-action spaces, and use it to prove fast rates of convergence\nin both off-line and on-line settings. Our analysis highlights two key\nstability properties, relating to how changes in value functions and/or\npolicies affect the Bellman operator and occupation measures. We argue that\nthese properties are satisfied in many continuous state-action Markov decision\nprocesses, and demonstrate how they arise naturally when using linear function\napproximation methods. Our analysis offers fresh perspectives on the roles of\npessimism and optimism in off-line and on-line RL, and highlights the\nconnection between off-line RL and transfer learning.\n",
        "title": "Taming \"data-hungry\" reinforcement learning? Stability in continuous\n  state-action spaces",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05235",
        "abstract_url": "http://arxiv.org/abs/2401.05235",
        "authors": [
            {
                "last_name": "Camur",
                "first_name": "Mustafa Can"
            },
            {
                "last_name": "Vogiatzis",
                "first_name": "Chrysafis"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            ""
        ],
        "abstract": "  Centrality metrics have become a popular concept in network science and\noptimization. Over the years, centrality has been used to assign importance and\nidentify influential elements in various settings, including transportation,\ninfrastructure, biological, and social networks, among others. That said, most\nof the literature has focused on nodal versions of centrality. Recently, group\ncounterparts of centrality have started attracting scientific and practitioner\ninterest. The identification of sets of nodes that are influential within a\nnetwork is becoming increasingly more important. This is even more pronounced\nwhen these sets of nodes are required to induce a certain motif or structure.\nIn this study, we review group centrality metrics from an operations research\nand optimization perspective for the first time. This is particularly\ninteresting due to the rapid evolution and development of this area in the\noperations research community over the last decade. We first present a\nhistorical overview of how we have reached this point in the study of group\ncentrality. We then discuss the different structures and motifs that appear\nprominently in the literature, alongside the techniques and methodologies that\nare popular. We finally present possible avenues and directions for future\nwork, mainly in three areas: (i) probabilistic metrics to account for\nrandomness along with stochastic optimization techniques; (ii) structures and\nrelaxations that have not been yet studied; and (iii) new emerging applications\nthat can take advantage of group centrality. Our survey offers a concise review\nof group centrality and its intersection with network analysis and\noptimization.\n",
        "title": "A Survey on Optimization Studies of Group Centrality Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05236",
        "abstract_url": "http://arxiv.org/abs/2401.05236",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Ma",
                "first_name": "Wei-Chiu"
            },
            {
                "last_name": "Guan",
                "first_name": "Kaiyu"
            },
            {
                "last_name": "Torralba",
                "first_name": "Antonio"
            },
            {
                "last_name": "Wang",
                "first_name": "Shenlong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Our world is full of identical objects (\\emphe.g., cans of coke, cars of same\nmodel). These duplicates, when seen together, provide additional and strong\ncues for us to effectively reason about 3D. Inspired by this observation, we\nintroduce Structure from Duplicates (SfD), a novel inverse graphics framework\nthat reconstructs geometry, material, and illumination from a single image\ncontaining multiple identical objects. SfD begins by identifying multiple\ninstances of an object within an image, and then jointly estimates the 6DoF\npose for all instances.An inverse graphics pipeline is subsequently employed to\njointly reason about the shape, material of the object, and the environment\nlight, while adhering to the shared geometry and material constraint across\ninstances. Our primary contributions involve utilizing object duplicates as a\nrobust prior for single-image inverse graphics and proposing an in-plane\nrotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object\npose estimation. By leveraging multi-view cues from a single image, SfD\ngenerates more realistic and detailed 3D reconstructions, significantly\noutperforming existing single image reconstruction models and multi-view\nreconstruction approaches with a similar or greater number of observations.\n",
        "title": "Structure from Duplicates: Neural Inverse Graphics from a Pile of\n  Objects",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05239",
        "abstract_url": "http://arxiv.org/abs/2401.05239",
        "authors": [
            {
                "last_name": "Dumitrescu",
                "first_name": "Adrian-Tudor"
            },
            {
                "last_name": "Pouwelse",
                "first_name": "Johan"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "CR"
        ],
        "abstract": "  The Public Key Infrastructure existed in critical infrastructure systems\nsince the expansion of the World Wide Web, but to this day its limitations have\nnot been completely solved. With the rise of government-driven digital identity\nin Europe, it is more important than ever to understand how PKI can be an\nefficient frame for eID and to learn from mistakes encountered by other\ncountries in such critical systems. This survey aims to analyze the literature\non the problems and risks that PKI exhibits, establish a brief timeline of its\nevolution in the last decades and study how it was implemented in digital\nidentity projects.\n",
        "title": "Failures of public key infrastructure: 53 year survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05240",
        "abstract_url": "http://arxiv.org/abs/2401.05240",
        "authors": [
            {
                "last_name": "Luzio",
                "first_name": "Emanuele"
            },
            {
                "last_name": "Ponti",
                "first_name": "Moacir Antonelli"
            },
            {
                "last_name": "Arevalo",
                "first_name": "Christian Ramirez"
            },
            {
                "last_name": "Argerich",
                "first_name": "Luis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Machine learning models typically focus on specific targets like creating\nclassifiers, often based on known population feature distributions in a\nbusiness context. However, models calculating individual features adapt over\ntime to improve precision, introducing the concept of decoupling: shifting from\npoint evaluation to data distribution. We use calibration strategies as\nstrategy for decoupling machine learning (ML) classifiers from score-based\nactions within business logic frameworks. To evaluate these strategies, we\nperform a comparative analysis using a real-world business scenario and\nmultiple ML models. Our findings highlight the trade-offs and performance\nimplications of the approach, offering valuable insights for practitioners\nseeking to optimize their decoupling efforts. In particular, the Isotonic and\nBeta calibration methods stand out for scenarios in which there is shift\nbetween training and testing data.\n",
        "title": "Decoupling Decision-Making in Fraud Prevention through Classifier\n  Calibration for Business Logic Action",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05244",
        "abstract_url": "http://arxiv.org/abs/2401.05244",
        "authors": [
            {
                "last_name": "Thaler",
                "first_name": "Denny"
            },
            {
                "last_name": "Dhulipala",
                "first_name": "Somayajulu L. N."
            },
            {
                "last_name": "Bamer",
                "first_name": "Franz"
            },
            {
                "last_name": "Markert",
                "first_name": "Bernd"
            },
            {
                "last_name": "Shields",
                "first_name": "Michael D."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We present a new Subset Simulation approach using Hamiltonian neural\nnetwork-based Monte Carlo sampling for reliability analysis. The proposed\nstrategy combines the superior sampling of the Hamiltonian Monte Carlo method\nwith computationally efficient gradient evaluations using Hamiltonian neural\nnetworks. This combination is especially advantageous because the neural\nnetwork architecture conserves the Hamiltonian, which defines the acceptance\ncriteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves\nhigh acceptance rates at low computational cost. Our approach estimates small\nfailure probabilities using Subset Simulations. However, in low-probability\nsample regions, the gradient evaluation is particularly challenging. The\nremarkable accuracy of the proposed strategy is demonstrated on different\nreliability problems, and its efficiency is compared to the traditional\nHamiltonian Monte Carlo method. We note that this approach can reach its\nlimitations for gradient estimations in low-probability regions of complex and\nhigh-dimensional distributions. Thus, we propose techniques to improve gradient\nprediction in these particular situations and enable accurate estimations of\nthe probability of failure. The highlight of this study is the reliability\nanalysis of a system whose parameter distributions must be inferred with\nBayesian inference problems. In such a case, the Hamiltonian Monte Carlo method\nrequires a full model evaluation for each gradient evaluation and, therefore,\ncomes at a very high cost. However, using Hamiltonian neural networks in this\nframework replaces the expensive model evaluation, resulting in tremendous\nimprovements in computational efficiency.\n",
        "title": "Reliability Analysis of Complex Systems using Subset Simulations with\n  Hamiltonian Neural Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05247",
        "abstract_url": "http://arxiv.org/abs/2401.05247",
        "authors": [
            {
                "last_name": "Fern\u00e1ndez-C\u00f3rdoba",
                "first_name": "Cristina"
            },
            {
                "last_name": "Torres",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Vela",
                "first_name": "Carlos"
            },
            {
                "last_name": "Villanueva",
                "first_name": "Merc\u00e8"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The Zps-additive codes of length n are subgroups of Zps^n , and can be seen\nas a generalization of linear codes over Z2, Z4, or more general over Z2s . In\nthis paper, we show two methods for computing a parity-check matrix of a\nZps-additive code from a generator matrix of the code in standard form. We also\ncompare the performance of our results implemented in Magma with the current\navailable function in Magma for codes over finite rings in general. A time\ncomplexity analysis is also shown.\n",
        "title": "Computing efficiently a parity-check matrix for Zps-additive codes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05249",
        "abstract_url": "http://arxiv.org/abs/2401.05249",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Feng",
                "first_name": "Yansong"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The argument sufficiency assessment task aims to determine if the premises of\na given argument support its conclusion. To tackle this task, existing works\noften train a classifier on data annotated by humans. However, annotating data\nis laborious, and annotations are often inconsistent due to subjective\ncriteria. Motivated by the probability of sufficiency (PS) definition in the\ncausal literature, we propose CASA, a zero-shot causality-driven argument\nsufficiency assessment framework. PS measures how likely introducing the\npremise event would lead to the conclusion, when both the premise and\nconclusion events are absent. To estimate this probability, we propose to use\nlarge language models (LLMs) to generate contexts that are inconsistent with\nthe premise and conclusion, and revise them by injecting the premise event.\nExperiments on two logical fallacy detection datasets demonstrate that CASA\naccurately identifies insufficient arguments. We further deploy CASA in a\nwriting assistance application, and find that suggestions generated by CASA\nenhance the sufficiency of student-written arguments. Code and data are\navailable at https://github.com/xxxiaol/CASA.\n",
        "title": "CASA: Causality-driven Argument Sufficiency Assessment",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05251",
        "abstract_url": "http://arxiv.org/abs/2401.05251",
        "authors": [
            {
                "last_name": "Rudolf",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fl\u00f6gel",
                "first_name": "Daniel"
            },
            {
                "last_name": "Sch\u00fcrmann",
                "first_name": "Tobias"
            },
            {
                "last_name": "S\u00fc\u00df",
                "first_name": "Simon"
            },
            {
                "last_name": "Schwab",
                "first_name": "Stefan"
            },
            {
                "last_name": "Hohmann",
                "first_name": "S\u00f6ren"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Robust and performant controllers are essential for industrial applications.\nHowever, deriving controller parameters for complex and nonlinear systems is\nchallenging and time-consuming. To facilitate automatic controller\nparametrization, this work presents a novel approach using deep reinforcement\nlearning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the\ncontrol of parameter-variant systems, a class of systems with complex behavior\nwhich depends on the operating conditions. For this system class,\ngain-scheduling control structures are widely used in applications across\nindustries due to well-known design principles. Facilitating the expensive\ncontroller parametrization task regarding these control structures, we deploy\nan DRL agent. Based on control system observations, the agent autonomously\ndecides how to adapt the controller parameters. We make the adaptation process\nmore efficient by introducing BSGs to map the controller parameters which may\ndepend on numerous operating conditions. To preprocess time-series data and\nextract a fixed-length feature vector, we use a long short-term memory (LSTM)\nneural networks. Furthermore, this work contributes actor regularizations that\nare relevant to real-world environments which differ from training.\nAccordingly, we apply dropout layer normalization to the actor and critic\nnetworks of the truncated quantile critic (TQC) algorithm. To show our\napproach's working principle and effectiveness, we train and evaluate the DRL\nagent on the parametrization task of an industrial control structure with\nparameter lookup tables.\n",
        "title": "ReACT: Reinforcement Learning for Controller Parametrization using\n  B-Spline Geometries",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05252",
        "abstract_url": "http://arxiv.org/abs/2401.05252",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junsong"
            },
            {
                "last_name": "Wu",
                "first_name": "Yue"
            },
            {
                "last_name": "Luo",
                "first_name": "Simian"
            },
            {
                "last_name": "Xie",
                "first_name": "Enze"
            },
            {
                "last_name": "Paul",
                "first_name": "Sayak"
            },
            {
                "last_name": "Luo",
                "first_name": "Ping"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhenguo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This technical report introduces PIXART-{\\delta}, a text-to-image synthesis\nframework that integrates the Latent Consistency Model (LCM) and ControlNet\ninto the advanced PIXART-{\\alpha} model. PIXART-{\\alpha} is recognized for its\nability to generate high-quality images of 1024px resolution through a\nremarkably efficient training process. The integration of LCM in\nPIXART-{\\delta} significantly accelerates the inference speed, enabling the\nproduction of high-quality images in just 2-4 steps. Notably, PIXART-{\\delta}\nachieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,\nmarking a 7x improvement over the PIXART-{\\alpha}. Additionally,\nPIXART-{\\delta} is designed to be efficiently trainable on 32GB V100 GPUs\nwithin a single day. With its 8-bit inference capability (von Platen et al.,\n2023), PIXART-{\\delta} can synthesize 1024px images within 8GB GPU memory\nconstraints, greatly enhancing its usability and accessibility. Furthermore,\nincorporating a ControlNet-like module enables fine-grained control over\ntext-to-image diffusion models. We introduce a novel ControlNet-Transformer\narchitecture, specifically tailored for Transformers, achieving explicit\ncontrollability alongside high-quality image generation. As a state-of-the-art,\nopen-source image generation model, PIXART-{\\delta} offers a promising\nalternative to the Stable Diffusion family of models, contributing\nsignificantly to text-to-image synthesis.\n",
        "title": "PIXART-{\\delta}: Fast and Controllable Image Generation with Latent\n  Consistency Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05254",
        "abstract_url": "http://arxiv.org/abs/2401.05254",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Young-Min"
            },
            {
                "last_name": "Pang",
                "first_name": "Dandan"
            },
            {
                "last_name": "Thapa",
                "first_name": "Stuti"
            },
            {
                "last_name": "Sherman",
                "first_name": "Garrick"
            },
            {
                "last_name": "Ungar",
                "first_name": "Lyle"
            },
            {
                "last_name": "Tay",
                "first_name": "Louis"
            },
            {
                "last_name": "Guntuku",
                "first_name": "Sharath Chandra"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CL"
        ],
        "abstract": "  Although affective expressions of individuals have been extensively studied\nusing social media, research has primarily focused on the Western context.\nThere are substantial differences among cultures that contribute to their\naffective expressions. This paper examines the differences between Twitter (X)\nin the United States and Sina Weibo posts in China on two primary dimensions of\naffect - valence and arousal. We study the difference in the functional\nrelationship between arousal and valence (so-called V-shaped) among individuals\nin the US and China and explore the associated content differences.\nFurthermore, we correlate word usage and topics in both platforms to interpret\ntheir differences. We observe that for Twitter users, the variation in\nemotional intensity is less distinct between negative and positive emotions\ncompared to Weibo users, and there is a sharper escalation in arousal\ncorresponding with heightened emotions. From language features, we discover\nthat affective expressions are associated with personal life and feelings on\nTwitter, while on Weibo such discussions are about socio-political topics in\nthe society. These results suggest a West-East difference in the V-shaped\nrelationship between valence and arousal of affective expressions on social\nmedia influenced by content differences. Our findings have implications for\napplications and theories related to cultural differences in affective\nexpressions.\n",
        "title": "Language-based Valence and Arousal Expressions between the United States\n  and China: a Cross-Cultural Examination",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05268",
        "abstract_url": "http://arxiv.org/abs/2401.05268",
        "authors": [
            {
                "last_name": "Qiao",
                "first_name": "Shuofei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ningyu"
            },
            {
                "last_name": "Fang",
                "first_name": "Runnan"
            },
            {
                "last_name": "Luo",
                "first_name": "Yujie"
            },
            {
                "last_name": "Zhou",
                "first_name": "Wangchunshu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yuchen Eleanor"
            },
            {
                "last_name": "Lv",
                "first_name": "Chengfei"
            },
            {
                "last_name": "Chen",
                "first_name": "Huajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC",
            "LG",
            "MA"
        ],
        "abstract": "  Language agents have achieved considerable performance on various complex\ntasks. Despite the incessant exploration in this field, existing language agent\nsystems still struggle with costly, non-reproducible data reliance and face the\nchallenge of compelling a single model for multiple functions. To this end, we\nintroduce AutoAct, an automatic agent learning framework that does not rely on\nlarge-scale annotated data and synthetic trajectories from closed-source models\n(e.g., GPT-4). Given limited data with a tool library, AutoAct first\nautomatically synthesizes planning trajectories without any assistance from\nhumans or strong closed-source models. Then, AutoAct leverages a\ndivision-of-labor strategy to automatically differentiate based on the target\ntask information and synthesized trajectories, producing a sub-agent group to\ncomplete the task. We conduct comprehensive experiments with different LLMs,\nwhich demonstrates that AutoAct yields better or parallel performance compared\nto various strong baselines. We even notice that AutoAct, when using the\nLlama-2-13b model, can achieve performance comparable to that of the\nGPT-3.5-Turbo agent. Code will be available at\nhttps://github.com/zjunlp/AutoAct.\n",
        "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05272",
        "abstract_url": "http://arxiv.org/abs/2401.05272",
        "authors": [
            {
                "last_name": "Pueyo",
                "first_name": "Pablo"
            },
            {
                "last_name": "Dendarieta",
                "first_name": "Juan"
            },
            {
                "last_name": "Montijano",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Murillo",
                "first_name": "Ana C."
            },
            {
                "last_name": "Schwager",
                "first_name": "Mac"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We present CineMPC, a complete cinematographic system that autonomously\ncontrols a drone to film multiple targets recording user-specified aesthetic\nobjectives. Existing solutions in autonomous cinematography control only the\ncamera extrinsics, namely its position, and orientation. In contrast, CineMPC\nis the first solution that includes the camera intrinsic parameters in the\ncontrol loop, which are essential tools for controlling cinematographic effects\nlike focus, depth-of-field, and zoom. The system estimates the relative poses\nbetween the targets and the camera from an RGB-D image and optimizes a\ntrajectory for the extrinsic and intrinsic camera parameters to film the\nartistic and technical requirements specified by the user. The drone and the\ncamera are controlled in a nonlinear Model Predicted Control (MPC) loop by\nre-optimizing the trajectory at each time step in response to current\nconditions in the scene. The perception system of CineMPC can track the\ntargets' position and orientation despite the camera effects. Experiments in a\nphotorealistic simulation and with a real platform demonstrate the capabilities\nof the system to achieve a full array of cinematographic effects that are not\npossible without the control of the intrinsics of the camera. Code for CineMPC\nis implemented following a modular architecture in ROS and released to the\ncommunity.\n",
        "title": "CineMPC: A Fully Autonomous Drone Cinematography System Incorporating\n  Zoom, Focus, Pose, and Scene Composition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05273",
        "abstract_url": "http://arxiv.org/abs/2401.05273",
        "authors": [
            {
                "last_name": "Pereira",
                "first_name": "Jayr"
            },
            {
                "last_name": "Assumpcao",
                "first_name": "Andre"
            },
            {
                "last_name": "Trecenti",
                "first_name": "Julio"
            },
            {
                "last_name": "Airosa",
                "first_name": "Luiz"
            },
            {
                "last_name": "Lente",
                "first_name": "Caio"
            },
            {
                "last_name": "Cl\u00e9to",
                "first_name": "Jhonatan"
            },
            {
                "last_name": "Dobins",
                "first_name": "Guilherme"
            },
            {
                "last_name": "Nogueira",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Luis"
            },
            {
                "last_name": "Lotufo",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^encia\nArtificial), a groundbreaking system designed to integrate Large Language\nModels (LLMs) into the operational framework of Brazilian Federal Court of\nAccounts (TCU). The system automates various stages of case analysis, including\nbasic information extraction, admissibility examination, Periculum in mora and\nFumus boni iuris analyses, and recommendations generation. Through a series of\nexperiments, we demonstrate INACIA's potential in extracting relevant\ninformation from case documents, evaluating its legal plausibility, and\ngenerating judicial recommendations. Utilizing a validation dataset alongside\nLLMs, our evaluation methodology presents an innovative approach to assessing\nsystem performance, correlating highly with human judgment. The results\nhighlight INACIA's proficiency in handling complex legal tasks, indicating its\nsuitability for augmenting efficiency and judicial fairness within legal\nsystems. The paper also discusses potential enhancements and future\napplications, positioning INACIA as a model for worldwide AI integration in\nlegal domains.\n",
        "title": "INACIA: Integrating Large Language Models in Brazilian Audit Courts:\n  Opportunities and Challenges",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05286",
        "abstract_url": "http://arxiv.org/abs/2401.05286",
        "authors": [
            {
                "last_name": "Cavicchioni",
                "first_name": "Giulia"
            },
            {
                "last_name": "Guerrini",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Meneghetti",
                "first_name": "Alessio"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Locally recoverable codes deal with the task of reconstructing a lost symbol\nby relying on a portion of the remaining coordinates smaller than an\ninformation set. We consider the case of codes over finite chain rings,\ngeneralizing known results and bounds for codes over fields. In particular, we\npropose a new family of locally recoverable codes by extending a construction\nproposed in 2014 by Tamo and Barg, and we discuss its optimality.\n",
        "title": "A class of locally recoverable codes over finite chain rings",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05290",
        "abstract_url": "http://arxiv.org/abs/2401.05290",
        "authors": [
            {
                "last_name": "Hauser",
                "first_name": "Kris"
            },
            {
                "last_name": "Watson",
                "first_name": "Eleanor"
            },
            {
                "last_name": "Bae",
                "first_name": "Joonbum"
            },
            {
                "last_name": "Bankston",
                "first_name": "Josh"
            },
            {
                "last_name": "Behnke",
                "first_name": "Sven"
            },
            {
                "last_name": "Borgia",
                "first_name": "Bill"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            },
            {
                "last_name": "Dafarra",
                "first_name": "Stefano"
            },
            {
                "last_name": "van Erp",
                "first_name": "Jan B. F."
            },
            {
                "last_name": "Ferris",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fishel",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Hoffman",
                "first_name": "Guy"
            },
            {
                "last_name": "Ivaldi",
                "first_name": "Serena"
            },
            {
                "last_name": "Kanehiro",
                "first_name": "Fumio"
            },
            {
                "last_name": "Kheddar",
                "first_name": "Abderrahmane"
            },
            {
                "last_name": "Lannuzel",
                "first_name": "Gaelle"
            },
            {
                "last_name": "Morie",
                "first_name": "Jacqueline Ford"
            },
            {
                "last_name": "Naughton",
                "first_name": "Patrick"
            },
            {
                "last_name": "NGuyen",
                "first_name": "Steve"
            },
            {
                "last_name": "Oh",
                "first_name": "Paul"
            },
            {
                "last_name": "Padir",
                "first_name": "Taskin"
            },
            {
                "last_name": "Pippine",
                "first_name": "Jim"
            },
            {
                "last_name": "Park",
                "first_name": "Jaeheung"
            },
            {
                "last_name": "Pucci",
                "first_name": "Daniele"
            },
            {
                "last_name": "Vaz",
                "first_name": "Jean"
            },
            {
                "last_name": "Whitney",
                "first_name": "Peter"
            },
            {
                "last_name": "Wu",
                "first_name": "Peggy"
            },
            {
                "last_name": "Locke",
                "first_name": "David"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  The ANA Avatar XPRIZE was a four-year competition to develop a robotic\n\"avatar\" system to allow a human operator to sense, communicate, and act in a\nremote environment as though physically present. The competition featured a\nunique requirement that judges would operate the avatars after less than one\nhour of training on the human-machine interfaces, and avatar systems were\njudged on both objective and subjective scoring metrics. This paper presents a\nunified summary and analysis of the competition from technical, judging, and\norganizational perspectives. We study the use of telerobotics technologies and\ninnovations pursued by the competing teams in their avatar systems, and\ncorrelate the use of these technologies with judges' task performance and\nsubjective survey ratings. It also summarizes perspectives from team leads,\njudges, and organizers about the competition's execution and impact to inform\nthe future development of telerobotics and telepresence.\n",
        "title": "Analysis and Perspectives on the ANA Avatar XPRIZE Competition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05293",
        "abstract_url": "http://arxiv.org/abs/2401.05293",
        "authors": [
            {
                "last_name": "Alldieck",
                "first_name": "Thiemo"
            },
            {
                "last_name": "Kolotouros",
                "first_name": "Nikos"
            },
            {
                "last_name": "Sminchisescu",
                "first_name": "Cristian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Score Distillation Sampling (SDS) is a recent but already widely popular\nmethod that relies on an image diffusion model to control optimization problems\nusing text prompts. In this paper, we conduct an in-depth analysis of the SDS\nloss function, identify an inherent problem with its formulation, and propose a\nsurprisingly easy but effective fix. Specifically, we decompose the loss into\ndifferent factors and isolate the component responsible for noisy gradients. In\nthe original formulation, high text guidance is used to account for the noise,\nleading to unwanted side effects. Instead, we train a shallow network mimicking\nthe timestep-dependent denoising deficiency of the image diffusion model in\norder to effectively factor it out. We demonstrate the versatility and the\neffectiveness of our novel loss formulation through several qualitative and\nquantitative experiments, including optimization-based image synthesis and\nediting, zero-shot image translation network training, and text-to-3D\nsynthesis.\n",
        "title": "Score Distillation Sampling with Learned Manifold Corrective",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05294",
        "abstract_url": "http://arxiv.org/abs/2401.05294",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Mathai",
                "first_name": "Tejas Sudharshan"
            },
            {
                "last_name": "Liu",
                "first_name": "Jianfei"
            },
            {
                "last_name": "Parnell",
                "first_name": "Christopher"
            },
            {
                "last_name": "Summers",
                "first_name": "Ronald M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Purpose: Body composition measurements from routine abdominal CT can yield\npersonalized risk assessments for asymptomatic and diseased patients. In\nparticular, attenuation and volume measures of muscle and fat are associated\nwith important clinical outcomes, such as cardiovascular events, fractures, and\ndeath. This study evaluates the reliability of an Internal tool for the\nsegmentation of muscle and fat (subcutaneous and visceral) as compared to the\nwell-established public TotalSegmentator tool.\n  Methods: We assessed the tools across 900 CT series from the publicly\navailable SAROS dataset, focusing on muscle, subcutaneous fat, and visceral\nfat. The Dice score was employed to assess accuracy in subcutaneous fat and\nmuscle segmentation. Due to the lack of ground truth segmentations for visceral\nfat, Cohen's Kappa was utilized to assess segmentation agreement between the\ntools.\n  Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) for\nsubcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentation\nrespectively. A Wilcoxon signed-rank test revealed that our results were\nstatistically different with p<0.01. For visceral fat, the Cohen's kappa score\nof 0.856 indicated near-perfect agreement between the two tools. Our internal\ntool also showed very strong correlations for muscle volume (R^2=0.99), muscle\nattenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderate\ncorrelation for subcutaneous fat attenuation (R^2=0.45).\n  Conclusion: Our findings indicated that our Internal tool outperformed\nTotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen's\nKappa score for visceral fat suggests a reliable level of agreement between the\ntwo tools. These results demonstrate the potential of our tool in advancing the\naccuracy of body composition analysis.\n",
        "title": "Enhanced Muscle and Fat Segmentation for CT-Based Body Composition\n  Analysis: A Comparative Study",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05295",
        "abstract_url": "http://arxiv.org/abs/2401.05295",
        "authors": [
            {
                "last_name": "Regad\u00edo",
                "first_name": "Alberto"
            },
            {
                "last_name": "Esteban",
                "first_name": "Luis"
            },
            {
                "last_name": "S\u00e1nchez-Prieto",
                "first_name": "Sebasti\u00e1n"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  To address the possible lack or total absence of pulses from particle\ndetectors during the development of its associate electronics, we propose a\nmodel that can generate them without losing the features of the real ones. This\nmodel is based on artificial neural networks, namely Generative Adversarial\nNetworks (GAN). We describe the proposed network architecture, its training\nmethodology and the approach to train the GAN with real pulses from a\nscintillator receiving radiation from sources of ${}^{137}$Cs and ${}^{22}$Na.\nThe Generator was installed in a Xilinx's System-On-Chip (SoC). We show how the\nnetwork is capable of generating pulses with the same shape as the real ones\nthat even match the data distributions in the original pulse-height histogram\ndata.\n",
        "title": "Synthesis of pulses from particle detectors with a Generative\n  Adversarial Network (GAN)",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05300",
        "abstract_url": "http://arxiv.org/abs/2401.05300",
        "authors": [
            {
                "last_name": "Thrush",
                "first_name": "Tristan"
            },
            {
                "last_name": "Moore",
                "first_name": "Jared"
            },
            {
                "last_name": "Monares",
                "first_name": "Miguel"
            },
            {
                "last_name": "Potts",
                "first_name": "Christopher"
            },
            {
                "last_name": "Kiela",
                "first_name": "Douwe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Statements involving metalinguistic self-reference (\"This paper has six\nsections.\") are prevalent in many domains. Can large language models (LLMs)\nhandle such language? In this paper, we present \"I am a Strange Dataset\", a new\ndataset for addressing this question. There are two subtasks: generation and\nverification. In generation, models continue statements like \"The penultimate\nword in this sentence is\" (where a correct continuation is \"is\"). In\nverification, models judge the truth of statements like \"The penultimate word\nin this sentence is sentence.\" (false). We also provide minimally different\nmetalinguistic non-self-reference examples to complement the main dataset by\nprobing for whether models can handle metalinguistic language at all. The\ndataset is hand-crafted by experts and validated by non-expert annotators. We\ntest a variety of open-source LLMs (7B to 70B parameters) as well as\nclosed-source LLMs through APIs. All models perform close to chance across both\nsubtasks and even on the non-self-referential metalinguistic control data,\nthough we find some steady improvement with model scale. GPT 4 is the only\nmodel to consistently do significantly better than chance, and it is still only\nin the 60% range, while our untrained human annotators score well in the 89-93%\nrange. The dataset and evaluation toolkit are available at\nhttps://github.com/TristanThrush/i-am-a-strange-dataset.\n",
        "title": "I am a Strange Dataset: Metalinguistic Tests for Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05302",
        "abstract_url": "http://arxiv.org/abs/2401.05302",
        "authors": [
            {
                "last_name": "Verma",
                "first_name": "Mudit"
            },
            {
                "last_name": "Bhambri",
                "first_name": "Siddhant"
            },
            {
                "last_name": "Kambhampati",
                "first_name": "Subbarao"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "HC"
        ],
        "abstract": "  Large Language Models have shown exceptional generative abilities in various\nnatural language and generation tasks. However, possible anthropomorphization\nand leniency towards failure cases have propelled discussions on emergent\nabilities of Large Language Models especially on Theory of Mind (ToM) abilities\nin Large Language Models. While several false-belief tests exists to verify the\nability to infer and maintain mental models of another entity, we study a\nspecial application of ToM abilities that has higher stakes and possibly\nirreversible consequences : Human Robot Interaction. In this work, we explore\nthe task of Perceived Behavior Recognition, where a robot employs a Large\nLanguage Model (LLM) to assess the robot's generated behavior in a manner\nsimilar to human observer. We focus on four behavior types, namely -\nexplicable, legible, predictable, and obfuscatory behavior which have been\nextensively used to synthesize interpretable robot behaviors. The LLMs goal is,\ntherefore to be a human proxy to the agent, and to answer how a certain agent\nbehavior would be perceived by the human in the loop, for example \"Given a\nrobot's behavior X, would the human observer find it explicable?\". We conduct a\nhuman subject study to verify that the users are able to correctly answer such\na question in the curated situations (robot setting and plan) across five\ndomains. A first analysis of the belief test yields extremely positive results\ninflating ones expectations of LLMs possessing ToM abilities. We then propose\nand perform a suite of perturbation tests which breaks this illusion, i.e.\nInconsistent Belief, Uninformative Context and Conviction Test. We conclude\nthat, the high score of LLMs on vanilla prompts showcases its potential use in\nHRI settings, however to possess ToM demands invariance to trivial or\nirrelevant perturbations in the context which LLMs lack.\n",
        "title": "Theory of Mind abilities of Large Language Models in Human-Robot\n  Interaction : An Illusion?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05304",
        "abstract_url": "http://arxiv.org/abs/2401.05304",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Jessica"
            },
            {
                "last_name": "Flanigan",
                "first_name": "Bailey"
            },
            {
                "last_name": "Haghtalab",
                "first_name": "Nika"
            },
            {
                "last_name": "Jagadeesan",
                "first_name": "Meena"
            },
            {
                "last_name": "Podimata",
                "first_name": "Chara"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  A common explanation for negative user impacts of content recommender systems\nis misalignment between the platform's objective and user welfare. In this\nwork, we show that misalignment in the platform's objective is not the only\npotential cause of unintended impacts on users: even when the platform's\nobjective is fully aligned with user welfare, the platform's learning algorithm\ncan induce negative downstream impacts on users. The source of these user\nimpacts is that different pieces of content may generate observable user\nreactions (feedback information) at different rates; these feedback rates may\ncorrelate with content properties, such as controversiality or demographic\nsimilarity of the creator, that affect the user experience. Since differences\nin feedback rates can impact how often the learning algorithm engages with\ndifferent content, the learning algorithm may inadvertently promote content\nwith certain such properties. Using the multi-armed bandit framework with\nprobabilistic feedback, we examine the relationship between feedback rates and\na learning algorithm's engagement with individual arms for different no-regret\nalgorithms. We prove that no-regret algorithms can exhibit a wide range of\ndependencies: if the feedback rate of an arm increases, some no-regret\nalgorithms engage with the arm more, some no-regret algorithms engage with the\narm less, and other no-regret algorithms engage with the arm approximately the\nsame number of times. From a platform design perspective, our results highlight\nthe importance of looking beyond regret when measuring an algorithm's\nperformance, and assessing the nature of a learning algorithm's engagement with\ndifferent types of content as well as their resulting downstream impacts.\n",
        "title": "Can Probabilistic Feedback Drive User Impacts in Online Platforms?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05308",
        "abstract_url": "http://arxiv.org/abs/2401.05308",
        "authors": [
            {
                "last_name": "Farajzadeh",
                "first_name": "Amin"
            },
            {
                "last_name": "Yadav",
                "first_name": "Animesh"
            },
            {
                "last_name": "Yanikomeroglu",
                "first_name": "Halim"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CV",
            "LG"
        ],
        "abstract": "  The deployment of federated learning (FL) within vertical heterogeneous\nnetworks, such as those enabled by high-altitude platform station (HAPS),\noffers the opportunity to engage a wide array of clients, each endowed with\ndistinct communication and computational capabilities. This diversity not only\nenhances the training accuracy of FL models but also hastens their convergence.\nYet, applying FL in these expansive networks presents notable challenges,\nparticularly the significant non-IIDness in client data distributions. Such\ndata heterogeneity often results in slower convergence rates and reduced\neffectiveness in model training performance. Our study introduces a client\nselection strategy tailored to address this issue, leveraging user network\ntraffic behaviour. This strategy involves the prediction and classification of\nclients based on their network usage patterns while prioritizing user privacy.\nBy strategically selecting clients whose data exhibit similar patterns for\nparticipation in FL training, our approach fosters a more uniform and\nrepresentative data distribution across the network. Our simulations\ndemonstrate that this targeted client selection methodology significantly\nreduces the training loss of FL models in HAPS networks, thereby effectively\ntackling a crucial challenge in implementing large-scale FL systems.\n",
        "title": "Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05314",
        "abstract_url": "http://arxiv.org/abs/2401.05314",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Kevin"
            },
            {
                "last_name": "Liu",
                "first_name": "Chonghua"
            },
            {
                "last_name": "Chan",
                "first_name": "David M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "CV",
            "SD"
        ],
        "abstract": "  The Internet's wealth of content, with up to 60% published in English,\nstarkly contrasts the global population, where only 18.8% are English speakers,\nand just 5.1% consider it their native language, leading to disparities in\nonline information access. Unfortunately, automated processes for dubbing of\nvideo - replacing the audio track of a video with a translated alternative -\nremains a complex and challenging task due to pipelines, necessitating precise\ntiming, facial movement synchronization, and prosody matching. While end-to-end\ndubbing offers a solution, data scarcity continues to impede the progress of\nboth end-to-end and pipeline-based methods. In this work, we introduce\nAnim-400K, a comprehensive dataset of over 425K aligned animated video segments\nin Japanese and English supporting various video-related tasks, including\nautomated dubbing, simultaneous translation, guided video summarization, and\ngenre/theme/style classification. Our dataset is made publicly available for\nresearch purposes at https://github.com/davidmchan/Anim400K.\n",
        "title": "ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of\n  Video",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05318",
        "abstract_url": "http://arxiv.org/abs/2401.05318",
        "authors": [
            {
                "last_name": "Piazza",
                "first_name": "Cristina"
            },
            {
                "last_name": "Della Santina",
                "first_name": "Cosimo"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            },
            {
                "last_name": "Grioli",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Bicchi",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robot feet are crucial for maintaining dynamic stability and propelling the\nbody during walking, especially on uneven terrains. Traditionally, robot feet\nwere mostly designed as flat and stiff pieces of metal, which meets its\nlimitations when the robot is required to step on irregular grounds, e.g.\nstones. While one could think that adding compliance under such feet would\nsolve the problem, this is not the case. To address this problem, we introduced\nthe SoftFoot, an adaptive foot design that can enhance walking performance over\nirregular grounds. The proposed design is completely passive and varies its\nshape and stiffness based on the exerted forces, through a system of pulley,\ntendons, and springs opportunely placed in the structure. This paper outlines\nthe motivation behind the SoftFoot and describes the theoretical model which\nled to its final design. The proposed system has been experimentally tested and\ncompared with two analogous conventional feet, a rigid one and a compliant one,\nwith similar footprints and soles. The experimental validation focuses on the\nanalysis of the standing performance, measured in terms of the equivalent\nsupport surface extension and the compensatory ankle angle, and the rejection\nof impulsive forces, which is important in events such as stepping on\nunforeseen obstacles. Results show that the SoftFoot has the largest equivalent\nsupport surface when standing on obstacles, and absorbs impulsive loads in a\nway almost as good as a compliant foot.\n",
        "title": "Analytical Model and Experimental Testing of the SoftFoot: an Adaptive\n  Robot Foot for Walking over Obstacles and Irregular Terrains",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05319",
        "abstract_url": "http://arxiv.org/abs/2401.05319",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xueyu"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Sun",
                "first_name": "Jiankai"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  Large language models (LLMs) have made significant progress in code\ngeneration tasks, but their performance in tackling programming problems with\ncomplex data structures and algorithms remains suboptimal. To address this\nissue, we propose an in-context learning approach that guides LLMs to debug by\nusing a \"print debugging\" method, which involves inserting print statements to\ntrace and analysing logs for fixing the bug. We collect a Leetcode problem\ndataset and evaluate our method using the Leetcode online judging system.\nExperiments with GPT-4 demonstrate the effectiveness of our approach,\noutperforming rubber duck debugging in easy and medium-level Leetcode problems\nby 1.5% and 17.9%.\n",
        "title": "Leveraging Print Debugging to Improve Code Generation in Large Language\n  Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05321",
        "abstract_url": "http://arxiv.org/abs/2401.05321",
        "authors": [
            {
                "last_name": "Beame",
                "first_name": "Paul"
            },
            {
                "last_name": "Kornerup",
                "first_name": "Niels"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "",
            "",
            ""
        ],
        "abstract": "  We consider the time and space required for quantum computers to solve a wide\nvariety of problems involving matrices, many of which have only been analyzed\nclassically in prior work. Our main results show that for a range of linear\nalgebra problems -- including matrix-vector product, matrix inversion, matrix\nmultiplication and powering -- existing classical time-space tradeoffs, several\nof which are tight for every space bound, also apply to quantum algorithms. For\nexample, for almost all matrices $A$, including the discrete Fourier transform\n(DFT) matrix, we prove that quantum circuits with at most $T$ input queries and\n$S$ qubits of memory require $T=\\Omega(n^2/S)$ to compute matrix-vector product\n$Ax$ for $x \\in \\{0,1\\}^n$. We similarly prove that matrix multiplication for\n$n\\times n$ binary matrices requires $T=\\Omega(n^3 / \\sqrt{S})$. Because many\nof our lower bounds match deterministic algorithms with the same time and space\ncomplexity, we show that quantum computers cannot provide any asymptotic\nadvantage for these problems with any space bound. We obtain matching lower\nbounds for the stronger notion of quantum cumulative memory complexity -- the\nsum of the space per layer of a circuit.\n  We also consider Boolean (i.e. AND-OR) matrix multiplication and\nmatrix-vector products, improving the previous quantum time-space tradeoff\nlower bounds for $n\\times n$ Boolean matrix multiplication to\n$T=\\Omega(n^{2.5}/S^{1/3})$ from $T=\\Omega(n^{2.5}/S^{1/2})$.\n  Our improved lower bound for Boolean matrix multiplication is based on a new\ncoloring argument that extracts more from the strong direct product theorem\nused in prior work. Our tight lower bounds for linear algebra problems require\nadding a new bucketing method to the recording-query technique of Zhandry that\nlets us apply classical arguments to upper bound the success probability of\nquantum circuits.\n",
        "title": "Quantum Time-Space Tradeoffs for Matrix Problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05322",
        "abstract_url": "http://arxiv.org/abs/2401.05322",
        "authors": [
            {
                "last_name": "Schmidt",
                "first_name": "Carolin"
            },
            {
                "last_name": "Tygesen",
                "first_name": "Mathias"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Filipe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Urban mobility is on the cusp of transformation with the emergence of shared,\nconnected, and cooperative automated vehicles. Yet, for them to be accepted by\ncustomers, trust in their punctuality is vital. Many pilot initiatives operate\nwithout a fixed schedule, thus enhancing the importance of reliable arrival\ntime (AT) predictions. This study presents an AT prediction system for\nautonomous shuttles, utilizing separate models for dwell and running time\npredictions, validated on real-world data from five cities. Alongside\nestablished methods such as XGBoost, we explore the benefits of integrating\nspatial data using graph neural networks (GNN). To accurately handle the case\nof a shuttle bypassing a stop, we propose a hierarchical model combining a\nrandom forest classifier and a GNN. The results for the final AT prediction are\npromising, showing low errors even when predicting several stops ahead. Yet, no\nsingle model emerges as universally superior, and we provide insights into the\ncharacteristics of pilot sites that influence the model selection process.\nFinally, we identify dwell time prediction as the key determinant in overall AT\nprediction accuracy when autonomous shuttles are deployed in low-traffic areas\nor under regulatory speed limits. This research provides insights into the\ncurrent state of autonomous public transport prediction models and paves the\nway for more data-informed decision-making as the field advances.\n",
        "title": "Arrival Time Prediction for Autonomous Shuttle Services in the Real\n  World: Evidence from Five Cities",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05329",
        "abstract_url": "http://arxiv.org/abs/2401.05329",
        "authors": [
            {
                "last_name": "Sen",
                "first_name": "Argha"
            },
            {
                "last_name": "Pal",
                "first_name": "Bhupendra"
            },
            {
                "last_name": "Achari",
                "first_name": "Seemant"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Sandip"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In the landscape of next-generation cellular networks, a projected surge of\nover 12 billion subscriptions foreshadows a considerable upswing in the\nnetwork's overall energy consumption. The proliferation of User Equipment (UE)\ndrives this energy demand, urging 5G deployments to seek more energy-efficient\nmethodologies. In this work, we propose SmartMME, as a pivotal solution aimed\nat optimizing Base Station (BS) energy usage. By harnessing and analyzing\ncritical network states-such as UE connections, data traffic at individual UEs,\nand other pertinent metrics-our methodology intelligently orchestrates the BS's\npower states, making informed decisions on when to activate or deactivate the\nBS. This meticulous approach significantly curtails the network's overall\nenergy consumption. In a bid to validate its efficiency, we seamlessly\nintegrated our module into Network Simulator-3 (ns-3), conducting extensive\ntesting to demonstrate its prowess in effectively managing and reducing net\nenergy consumption. As advocates of collaborative progress, we've opted to\nopen-source this module, inviting the engagement and feedback of the wider\nresearch community on GitHub.\n",
        "title": "\\textit{SmartMME}: Implementation of Base Station Switching Off Strategy\n  in ns-3",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05334",
        "abstract_url": "http://arxiv.org/abs/2401.05334",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhaoxi"
            },
            {
                "last_name": "Moon",
                "first_name": "Gyeongsik"
            },
            {
                "last_name": "Guo",
                "first_name": "Kaiwen"
            },
            {
                "last_name": "Cao",
                "first_name": "Chen"
            },
            {
                "last_name": "Pidhorskyi",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Simon",
                "first_name": "Tomas"
            },
            {
                "last_name": "Joshi",
                "first_name": "Rohan"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Yichen"
            },
            {
                "last_name": "Pires",
                "first_name": "Bernardo"
            },
            {
                "last_name": "Wen",
                "first_name": "He"
            },
            {
                "last_name": "Evans",
                "first_name": "Lucas"
            },
            {
                "last_name": "Peng",
                "first_name": "Bo"
            },
            {
                "last_name": "Buffalini",
                "first_name": "Julia"
            },
            {
                "last_name": "Trimble",
                "first_name": "Autumn"
            },
            {
                "last_name": "McPhail",
                "first_name": "Kevyn"
            },
            {
                "last_name": "Schoeller",
                "first_name": "Melissa"
            },
            {
                "last_name": "Yu",
                "first_name": "Shoou-I"
            },
            {
                "last_name": "Romero",
                "first_name": "Javier"
            },
            {
                "last_name": "Zollh\u00f6fer",
                "first_name": "Michael"
            },
            {
                "last_name": "Sheikh",
                "first_name": "Yaser"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Saito",
                "first_name": "Shunsuke"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Existing photorealistic relightable hand models require extensive\nidentity-specific observations in different views, poses, and illuminations,\nand face challenges in generalizing to natural illuminations and novel\nidentities. To bridge this gap, we present URHand, the first universal\nrelightable hand model that generalizes across viewpoints, poses,\nilluminations, and identities. Our model allows few-shot personalization using\nimages captured with a mobile phone, and is ready to be photorealistically\nrendered under novel illuminations. To simplify the personalization process\nwhile retaining photorealism, we build a powerful universal relightable prior\nbased on neural relighting from multi-view images of hands captured in a light\nstage with hundreds of identities. The key challenge is scaling the\ncross-identity training while maintaining personalized fidelity and sharp\ndetails without compromising generalization under natural illuminations. To\nthis end, we propose a spatially varying linear lighting model as the neural\nrenderer that takes physics-inspired shading as input feature. By removing\nnon-linear activations and bias, our specifically designed lighting model\nexplicitly keeps the linearity of light transport. This enables single-stage\ntraining from light-stage data while generalizing to real-time rendering under\narbitrary continuous illuminations across diverse identities. In addition, we\nintroduce the joint learning of a physically based model and our neural\nrelighting model, which further improves fidelity and generalization. Extensive\nexperiments show that our approach achieves superior performance over existing\nmethods in terms of both quality and generalizability. We also demonstrate\nquick personalization of URHand from a short phone scan of an unseen identity.\n",
        "title": "URHand: Universal Relightable Hands",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05335",
        "abstract_url": "http://arxiv.org/abs/2401.05335",
        "authors": [
            {
                "last_name": "Shahbazi",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Claessens",
                "first_name": "Liesbeth"
            },
            {
                "last_name": "Niemeyer",
                "first_name": "Michael"
            },
            {
                "last_name": "Collins",
                "first_name": "Edo"
            },
            {
                "last_name": "Tonioni",
                "first_name": "Alessio"
            },
            {
                "last_name": "Van Gool",
                "first_name": "Luc"
            },
            {
                "last_name": "Tombari",
                "first_name": "Federico"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "LG"
        ],
        "abstract": "  We introduce InseRF, a novel method for generative object insertion in the\nNeRF reconstructions of 3D scenes. Based on a user-provided textual description\nand a 2D bounding box in a reference viewpoint, InseRF generates new objects in\n3D scenes. Recently, methods for 3D scene editing have been profoundly\ntransformed, owing to the use of strong priors of text-to-image diffusion\nmodels in 3D generative modeling. Existing methods are mostly effective in\nediting 3D scenes via style and appearance changes or removing existing\nobjects. Generating new objects, however, remains a challenge for such methods,\nwhich we address in this study. Specifically, we propose grounding the 3D\nobject insertion to a 2D object insertion in a reference view of the scene. The\n2D edit is then lifted to 3D using a single-view object reconstruction method.\nThe reconstructed object is then inserted into the scene, guided by the priors\nof monocular depth estimation methods. We evaluate our method on various 3D\nscenes and provide an in-depth analysis of the proposed components. Our\nexperiments with generative insertion of objects in several 3D scenes indicate\nthe effectiveness of our method compared to the existing methods. InseRF is\ncapable of controllable and 3D-consistent object insertion without requiring\nexplicit 3D information as input. Please visit our project page at\nhttps://mohamad-shahbazi.github.io/inserf.\n",
        "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05336",
        "abstract_url": "http://arxiv.org/abs/2401.05336",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Ronglai"
            },
            {
                "last_name": "Wei",
                "first_name": "Fangyun"
            },
            {
                "last_name": "Mak",
                "first_name": "Brian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The objective of sign language recognition is to bridge the communication gap\nbetween the deaf and the hearing. Numerous previous works train their models\nusing the well-established connectionist temporal classification (CTC) loss.\nDuring the inference stage, the CTC-based models typically take the entire sign\nvideo as input to make predictions. This type of inference scheme is referred\nto as offline recognition. In contrast, while mature speech recognition systems\ncan efficiently recognize spoken words on the fly, sign language recognition\nstill falls short due to the lack of practical online solutions. In this work,\nwe take the first step towards filling this gap. Our approach comprises three\nphases: 1) developing a sign language dictionary encompassing all glosses\npresent in a target sign language dataset; 2) training an isolated sign\nlanguage recognition model on augmented signs using both conventional\nclassification loss and our novel saliency loss; 3) employing a sliding window\napproach on the input sign sequence and feeding each sign clip to the\nwell-optimized model for online recognition. Furthermore, our online\nrecognition model can be extended to boost the performance of any offline\nmodel, and to support online translation by appending a gloss-to-text network\nonto the recognition model. By integrating our online framework with the\npreviously best-performing offline model, TwoStream-SLR, we achieve new\nstate-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T,\nand CSL-Daily. Code and models will be available at\nhttps://github.com/FangyunWei/SLRT\n",
        "title": "Towards Online Sign Language Recognition and Translation",
        "date": "2024-01-10",
        "group": "cs"
    }
]